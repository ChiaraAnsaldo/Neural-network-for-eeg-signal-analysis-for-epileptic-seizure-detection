{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Model, Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import  Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, GRU, Reshape, LSTM, TimeDistributed, SimpleRNN\n",
    "from random import shuffle\n",
    "import math\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import statistics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.DatasetManage import read_and_store_data\n",
    "from ipynb.fs.full.FeatureExtraction import feature_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathSpectogramFolder='Spectogram'\n",
    "\n",
    "interictalSpectograms=[]\n",
    "preictalSpectograms=[]  #This array contains syntetic data, it's created to have a balance dataset and it's used for training\n",
    "preictalRealSpectograms=[]  #This array containt the real preictal data, it's used for testing\n",
    "filesPath=[]\n",
    "\n",
    "patients = [\"01\",\"02\",\"03\",\"05\",\"09\",\"10\",\"23\"]\n",
    "nSeizure=0\n",
    "OutputPathModels = 'models/'\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "dir_name = \"model_checkpoint\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfInd = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score', 'MCC', 'Kappa', 'Time']\n",
    "channels = ['FP1-F7', 'F7-T7','T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'seizure']\n",
    "\n",
    "dataset = 'CHB_MIT'\n",
    "\n",
    "feature_extracted = 'Features.csv'\n",
    "\n",
    "sample_rate = 256\n",
    "time_window = 2\n",
    "step = time_window * sample_rate\n",
    "\n",
    "test_ratio = 0.3\n",
    "\n",
    "pca_tolerance = 0.9\n",
    "\n",
    "undersampling_rate = 0.2\n",
    "\n",
    "oversampling_neighbors = 11\n",
    "\n",
    "k_fold = 5\n",
    "\n",
    "batch = 10\n",
    "epochs = 100\n",
    "dropout_percentage = 0.2\n",
    "loss_function = 'mean_squared_error'\n",
    "metric = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_SGD (initial_learning_rate, decay_steps, decay_rate):\n",
    "    # We define the optimizer with an initial learning rate\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # We define the larning rate schedule with an exponential decay, specifying the number of decay steps and the decay rate\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "    \n",
    "    return optimizer, lr_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSpectogramData(indexPat):\n",
    "    global interictalSpectograms\n",
    "    global preictalSpectograms\n",
    "    global preictalRealSpectograms\n",
    "    global nSeizure\n",
    "    f = open(PathSpectogramFolder+'/paz'+patients[indexPat]+'/legendAllData.txt', 'r')\n",
    "    line=f.readline()\n",
    "    while(not \"SEIZURE\" in line):\n",
    "        line=f.readline()\n",
    "    nSeizure=int(line.split(\":\")[1].strip())\n",
    "    line=f.readline()\n",
    "    line=f.readline()#legge il numero di spectogrammi. non lo salvo dato che non mi serve\n",
    "    nSpectograms=int(line.strip())\n",
    "    nFileForSeizure=math.ceil(math.ceil(nSpectograms/50)/nSeizure)\n",
    "    line=f.readline()#leggo il percorso del primo file\n",
    "    \n",
    "    #Lettura path files Interictal\n",
    "    cont=-1\n",
    "    indFilePathRead=0\n",
    "    while(\"npy\" in line and indFilePathRead<nSeizure*nFileForSeizure):\n",
    "        if(indFilePathRead%nFileForSeizure==0):\n",
    "            interictalSpectograms.append([])\n",
    "            cont=cont+1\n",
    "            interictalSpectograms[cont].append(line.split(' ')[2].rstrip())#.rstrip() remove \\n\n",
    "            indFilePathRead=indFilePathRead+1\n",
    "        else:\n",
    "            if(len(line.split(' '))>=3):\n",
    "                interictalSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "            indFilePathRead=indFilePathRead+1\n",
    "            \n",
    "        line=f.readline()\n",
    "    line=f.readline()#leggo PREICTAL\n",
    "    line=f.readline()#leggo n° spectogram\n",
    "    line=f.readline()#leggo n°seizure(SEIZURE X)\n",
    "\n",
    "    #Lettura path files Preictal\n",
    "    cont=-1\n",
    "    indFilePathRead=0   \n",
    "    #while(line and indFilePathRead<nSeizure*nFileForSeizure):    \n",
    "    while(line.strip()!=\"\"):\n",
    "        if(\"SEIZURE\" in line):\n",
    "            line=f.readline()#ho letto n°seizure(SEIZURE X) perciò scorro in avanti\n",
    "            if(len(line.split(' '))>=3):\n",
    "                preictalSpectograms.append([])\n",
    "                cont=cont+1\n",
    "                preictalSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "                indFilePathRead=indFilePathRead+1\n",
    "        else:\n",
    "            if(len(line.split(' '))>=3):\n",
    "                preictalSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "            indFilePathRead=indFilePathRead+1\n",
    "            \n",
    "        line=f.readline()\n",
    "        \n",
    "    line=f.readline()#leggo REAL_PREICTAL\n",
    "    line=f.readline()#leggo n° spectogram\n",
    "    line=f.readline()#leggo n°seizure(SEIZURE X)\n",
    "\n",
    "    #Lettura path files Real Preictal\n",
    "    cont=-1\n",
    "    while(line):\n",
    "        if(\"SEIZURE\" in line):\n",
    "            line=f.readline()#ho letto n°seizure(SEIZURE X) perciò scorro in avanti\n",
    "            preictalRealSpectograms.append([])\n",
    "            cont=cont+1\n",
    "            preictalRealSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "        else:\n",
    "            preictalRealSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "            \n",
    "        line=f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesPathWithoutSeizure(indexSeizure):\n",
    "    for i in range(0, nSeizure):\n",
    "        if(i!=indexSeizure):\n",
    "            filesPath.extend(interictalSpectograms[i])\n",
    "            filesPath.extend(preictalSpectograms[i])\n",
    "    shuffle(filesPath)\n",
    "    return filesPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create array for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays_for_training(paths, start=0, end=100):\n",
    "    while True:\n",
    "        from_=int(len(paths)/100*start)\n",
    "        to_=int(len(paths)/100*end)\n",
    "        for i in range(from_, int(to_)):\n",
    "            f=paths[i]\n",
    "            x = np.load(PathSpectogramFolder+f)\n",
    "            x=np.array([x])\n",
    "            x=x.swapaxes(0,1)\n",
    "            x=x[0]\n",
    "            if('P' in f):\n",
    "                y = np.repeat([[0,1]], x.shape[0], axis=0)\n",
    "            else:\n",
    "                y = np.repeat([[1,0]], x.shape[0], axis=0)\n",
    "            yield(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create array for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays_for_predict(indexPat, paths, start=0, end=100):\n",
    "    while True:\n",
    "        f = open(PathSpectogramFolder+'/paz'+patients[indexPat]+'/legendAllData.txt', 'r')\n",
    "        line=f.readline()\n",
    "        while(not \"SEIZURE\" in line):\n",
    "            line=f.readline()\n",
    "        nSeizure=int(line.split(\":\")[1].strip())\n",
    "        from_=int(len(paths)/100*start)\n",
    "        to_=int(len(paths)/100*end)\n",
    "        for i in range(from_, int(to_)):\n",
    "            f=paths[i]\n",
    "            x = np.load(PathSpectogramFolder+f)\n",
    "            x=np.array([x])\n",
    "            x=x.swapaxes(0,1)\n",
    "            x=x[0]\n",
    "            yield(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    interictalSpectograms=[]\n",
    "    preictalSpectograms=[]  # This array contains syntetic data, it's created to have a balance dataset and it's used for training\n",
    "    preictalRealSpectograms=[]  #This array containt the real preictal data, it's used for testing\n",
    "    filesPath=[]\n",
    "    nSeizure=0\n",
    "\n",
    "    print(\"START\")\n",
    "    if not os.path.exists(OutputPathModels):\n",
    "        os.makedirs(OutputPathModels)\n",
    "    print(\"Parameters loaded\")\n",
    "\n",
    "    for indexPat in range(0, len(patients)):\n",
    "        loadSpectogramData(indexPat) \n",
    "        filesPath = getFilesPathWithoutSeizure(indexPat)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestData (features, test_ratio, k_fold, perfInd):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = test_ratio, shuffle = True)\n",
    "    results = pd.DataFrame(columns = perfInd)\n",
    "    kf = KFold(n_splits = k_fold, shuffle = True)\n",
    "    return x_train, x_test, y_train, y_test, results, kf\n",
    "\n",
    "# Here we define a method to split the dataset into training and testing sets, also returning the train/test\n",
    "# indexes for splitting the dataset into K folds for the K-fold cross validation\n",
    "\n",
    "def trainTestData_k_fold(features, test_ratio, k_fold):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size = test_ratio, shuffle = True, random_state=42)\n",
    "    kf = KFold(n_splits = k_fold, shuffle = True)\n",
    "    x_train = np.reshape(x_tr.values, (x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "    y_train = y_tr.values.astype(int)\n",
    "    x_test = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_test = y_ts.values.astype(int)\n",
    "    return x_train, x_test, y_train, y_test, kf\n",
    "\n",
    "# Here we define a method to split the dataset into training, validationa and testing sets\n",
    "\n",
    "def trainTestData_validation (features, test_ratio, val_ratio):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_1, x_ts, y_1, y_ts = train_test_split(x, y, test_size = test_ratio, random_state=42)\n",
    "    x_tr, x_v, y_tr, y_v = train_test_split(x_1, y_1, test_size = val_ratio, random_state=42)\n",
    "    x_train = np.reshape(x_tr.values, (x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "    y_train = y_tr.values.astype(int)\n",
    "    x_val = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_val = y_ts.values.astype(int)\n",
    "    x_test = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_test = y_ts.values.astype(int)\n",
    "    return x_train, x_test, y_train, y_test, x_val, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(train_loss, val_loss, train_acc, val_acc):\n",
    "    \n",
    "    epochs = range(len(train_loss))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss, label='Training loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_k_fold (train_loss, train_acc, val_loss, val_acc):\n",
    "    \n",
    "    avg_train_loss = np.mean(train_loss, axis=0)\n",
    "    avg_train_acc = np.mean(train_acc, axis=0)\n",
    "    avg_val_loss = np.mean(val_loss, axis=0)\n",
    "    avg_val_acc = np.mean(train_acc, axis=0)\n",
    "\n",
    "    # Plot delle curve di apprendimento mediate sulle K fold\n",
    "\n",
    "    epochs = range(1, len(train_loss[0]) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_loss, label='Training loss')\n",
    "    plt.plot(epochs, avg_val_loss, label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg_loss')\n",
    "    plt.title('Average train and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, avg_val_acc, label='Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg_accuracy')\n",
    "    plt.title('Average train and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation (history):\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(train_loss))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss, label='Training loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model, using sensitivity and FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model(model):\n",
    "\n",
    "    for indexPat in range(0, len(patients)):\n",
    "        \n",
    "        loadSpectogramData(indexPat)\n",
    "\n",
    "        sensResults = []\n",
    "        FPRResults = []\n",
    "\n",
    "        result='Patient '+patients[indexPat]+'\\n'     \n",
    "        result='Out Seizure, True Positive, False Positive, False negative, Second of Inter in Test, Sensitivity, FPR \\n'\n",
    "        for i in range(0, nSeizure):\n",
    "\n",
    "            filesPath=interictalSpectograms[i]\n",
    "            interPrediction=model.predict(generate_arrays_for_predict(indexPat, filesPath), max_queue_size=4, steps=len(filesPath), verbose=0)\n",
    "            filesPath=preictalRealSpectograms[i]\n",
    "            preictPrediction=model.predict(generate_arrays_for_predict(indexPat, filesPath), max_queue_size=4, steps=len(filesPath), verbose = 0)\n",
    "            \n",
    "            secondsInterictalInTest=len(interictalSpectograms[i])*50*30#50 spectograms for file, 30 seconds for each spectogram\n",
    "            acc=0#accumulator\n",
    "            fp=0\n",
    "            tp=0\n",
    "            fn=0\n",
    "            lastTenResult=list()\n",
    "            \n",
    "            for el in interPrediction:\n",
    "                if(el[1]>0.5):\n",
    "                    acc=acc+1\n",
    "                    lastTenResult.append(1)\n",
    "                else:\n",
    "                    lastTenResult.append(0)\n",
    "                if(len(lastTenResult)>10):\n",
    "                    acc=acc-lastTenResult.pop(0)\n",
    "                if(acc>=4):\n",
    "                  fp=fp+1\n",
    "                  lastTenResult=list()\n",
    "                  acc=0\n",
    "            \n",
    "            lastTenResult=list()\n",
    "            for el in preictPrediction:\n",
    "                if(el[1]>0.5):\n",
    "                    acc=acc+1\n",
    "                    lastTenResult.append(1)\n",
    "                else:\n",
    "                    lastTenResult.append(0)\n",
    "                if(len(lastTenResult)>10):\n",
    "                    acc=acc-lastTenResult.pop(0)\n",
    "                if(acc>=4):\n",
    "                  tp=tp+1 \n",
    "                else:\n",
    "                    if(len(lastTenResult)==10):\n",
    "                       fn=fn+1 \n",
    "                       \n",
    "            if (tp+fn)!=0:\n",
    "                sensitivity=tp/(tp+fn)\n",
    "            else:\n",
    "                sensitivity=0\n",
    "            FPR=fp/(secondsInterictalInTest/(60*60))\n",
    "            \n",
    "            result=result+str(i+1)+','+str(tp)+','+str(fp)+','+str(fn)+','+str(secondsInterictalInTest)+','\n",
    "            result=result+str(sensitivity)+','+str(FPR)+'\\n'\n",
    "            #print('True Positive, False Positive, False negative, Second of Inter in Test, Sensitivity, FPR')\n",
    "            #print(str(tp)+','+str(fp)+','+str(fn)+','+str(secondsInterictalInTest)+','+str(sensitivity)+','+str(FPR))\n",
    "        \n",
    "            sensResults.append(sensitivity)\n",
    "            FPRResults.append(FPR)\n",
    "  \n",
    "                    \n",
    "        sdSENS=round(statistics.stdev(sensResults)*100,2)\n",
    "        avSENS=round(statistics.mean(sensResults)*100,2)\n",
    "        \n",
    "        sdFPR=round(statistics.stdev(FPRResults)*100,2)\n",
    "        avFPR=round(statistics.mean(FPRResults)*100,2)\n",
    "\n",
    "        print('Patient '+patients[indexPat]+\"  & \"+str(avSENS)+\" +- \"+str(sdSENS)+\"   & \"+str(avFPR)+\" +- \"+str(sdFPR))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvImportFile = 'CHB'\n",
    "print('Reading data from', csvImportFile)\n",
    "df = pd.read_csv(csvImportFile, delimiter = ',', header = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(feature_extracted):\n",
    "    ft = pd.read_csv(feature_extracted, delimiter=',', header=0)\n",
    "else:\n",
    "    ft = feature_extraction(df, sample_rate, step, pca_tolerance, undersampling_rate, oversampling_neighbors)\n",
    "\n",
    "ft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, results, kf = trainTestData (ft, test_ratio, k_fold, perfInd)\n",
    "\n",
    "x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "y_train = y_train.values.astype(int)\n",
    "x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "y_test = y_test.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 5\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_k_fold (ft, test_ratio, k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, x_val, y_val = trainTestData_validation (ft, test_ratio, val_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # C1\n",
    "    model.add(Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(22, 59, 114)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # C2\n",
    "    model.add(Conv2D(32, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # C3\n",
    "    model.add(Conv2D(64, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def CNN_dropout():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # C1\n",
    "    model.add(Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(22, 59, 114)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # C2\n",
    "    model.add(Conv2D(32, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # C3\n",
    "    model.add(Conv2D(64, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def CNN_GRU():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # C1\n",
    "    model.add(Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(22, 59, 114)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # C2\n",
    "    model.add(Conv2D(32, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # C3\n",
    "    model.add(Conv2D(64, (2, 2), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='sigmoid'))\n",
    "\n",
    "    # Reshape layer\n",
    "    model.add(Reshape((16, 16)))\n",
    "\n",
    "    # GRU\n",
    "    model.add(GRU(128))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def CNN_LSTM():\n",
    "    \n",
    "    model = Sequential()\n",
    "    # C1\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(22, 59, 114)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # C2\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # C3\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    # LSTM\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(64))\n",
    "    \n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Model1.h5')\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=save_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "]\n",
    "\n",
    "model = CNN()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Recall(name='sen')])\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "history = model.fit(generate_arrays_for_training(filesPath, end=80),\n",
    "                        validation_data=generate_arrays_for_training(filesPath, start=20),\n",
    "                        steps_per_epoch=150,\n",
    "                        validation_steps=50,\n",
    "                        epochs=100, max_queue_size=2, shuffle=True, callbacks=[callbacks_list])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Creates a HDF5 file \n",
    "model.save(\"ModelOutSeizure3.h5\")\n",
    "print(\"Model saved\")\n",
    "        \n",
    "plot_performance(train_loss, val_loss, train_acc, val_acc)\n",
    "\n",
    "print('Training end')\n",
    "\n",
    "print('Evaluating Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()\n",
    "model = load_model('model_checkpoint/CNN.h5')\n",
    "print('Evaluating Model')\n",
    "evaluation_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Model2.h5')\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=save_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "]\n",
    "\n",
    "model = createModel2()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Recall(name='sen')])\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "history = model.fit(generate_arrays_for_training(filesPath, end=75), #end=75),#It take the first 75%\n",
    "                        validation_data=generate_arrays_for_training(filesPath, start=75),#start=75), #It take the last 25%\n",
    "                        steps_per_epoch=150, \n",
    "                        validation_steps=50,\n",
    "                        epochs=100, max_queue_size=2, shuffle=True, callbacks=[callbacks_list])# 100 epochs è meglio #aggiungere criterio di stop in base accuratezza\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "        \n",
    "plot_performance(train_loss, val_loss, train_acc, val_acc)\n",
    "\n",
    "print('Training end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()\n",
    "model = load_model('model_checkpoint/CNN_Dropout.h5')\n",
    "print('Evaluating Model')\n",
    "evaluation_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Model3.h5')\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=save_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "]\n",
    "\n",
    "model = CNN_GRU()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Recall(name='sen')])\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "history = model.fit(generate_arrays_for_training(filesPath, end=75),\n",
    "                        validation_data=generate_arrays_for_training(filesPath, start=75),\n",
    "                        steps_per_epoch=100,\n",
    "                        validation_steps=50,\n",
    "                        epochs=75, max_queue_size=2, shuffle=True, callbacks=[callbacks_list])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plot_performance(train_loss, val_loss, train_acc, val_acc)\n",
    "\n",
    "print('Training end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()\n",
    "model = load_model('model_checkpoint/CGRUNN.h5')\n",
    "print('Evaluating Model')\n",
    "evaluation_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Model4_1.h5')\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=save_path,\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=1,\n",
    "    save_best_only=True)\n",
    "]\n",
    "model = CNN_LSTM()\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Recall(name='sen')])\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "history = model.fit(generate_arrays_for_training(filesPath, end=75),\n",
    "                        validation_data=generate_arrays_for_training(filesPath, start=75),\n",
    "                        steps_per_epoch=100,\n",
    "                        validation_steps=50,\n",
    "                        epochs=200, max_queue_size=2, shuffle=True, callbacks=[callbacks_list])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plot_performance(train_loss, val_loss, train_acc, val_acc)\n",
    "\n",
    "print('Training end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()\n",
    "model = load_model('model_checkpoint/CNN_LSTM.h5')\n",
    "print('Evaluating Model')\n",
    "evaluation_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(train_loss[i])\n",
    "plt.title('Training Loss - All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(train_acc[i])\n",
    "plt.title('Training Accuracy - All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code illustrates the implementation of an enhanced GRU model with a single layer and Stochastic Gradient Descent optimization. \n",
    "# The model is trained to solve a binary classification problem\n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_SGD.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule),callbacks_list])\n",
    "\n",
    "plot_validation(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_validation(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_RMSprop.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with RMSprop optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001) \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_validation(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code illustrates the implementation of an improved GRU model with dropout regularization and Adam optimization.\n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer with dropout_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,input_shape=(None, x_train.shape[-1])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_validation(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code illustrates the implementation of an improved GRU model with dropout regularization and Root Mean Square Propagation optimization.\n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer with dropout_RMSprop.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,input_shape=(None, x_train.shape[-1])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with RMSprop optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_validation(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code implements a deep Gated Recurrent Unit (GRU) architecture with two layers, using the SGD optimization.\n",
    "\n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_2 layers_SGD.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1]),return_sequences=True))\n",
    "model.add(layers.GRU(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=700, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "\n",
    "plot_validation(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code implements a K-Fold Cross Validation technique combined with a deep Gated Recurrent Unit (GRU) architecture. \n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_2 layers k fold.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "k_fold = 5 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_k_fold (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing GRU with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1]),return_sequences=True))\n",
    "    model.add(layers.GRU(32))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 5, epochs = 700, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('GRU finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_k_fold(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'LSTM + GRU_SGD.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True,input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.GRU(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=6, epochs=1000, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule),callbacks_list])\n",
    "\n",
    "plot_validation(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'LSTM + GRU_10-fold.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "k_fold = 10 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_k_fold (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing GRU with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True,input_shape=(None, x_train.shape[-1])))\n",
    "    model.add(layers.GRU(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 5, epochs = 700, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('Vanilla_RNN finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_k_fold(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
