{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Model, Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.DatasetManage import read_and_store_data\n",
    "from ipynb.fs.full.FeatureExtraction import feature_extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = ['FP1-F7', 'F7-T7','T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'seizure']\n",
    "\n",
    "dataset = 'CHB_MIT'\n",
    "csvImportFile = 'CHB.csv'\n",
    "csvExportFile = 'CHB.csv'\n",
    "sample_rate = 256\n",
    "time_window = 2\n",
    "step = time_window * sample_rate\n",
    "\n",
    "test_ratio = 0.3 # ratio to split dataset into training and testing sets\n",
    "val_ratio = 0.2 # ratio to split dataset into validation and training sets\n",
    "\n",
    "pca_tolerance = 0.9 # desired percentage of variation in the data preserved\n",
    "\n",
    "undersampling_rate = 0.2 # undersampling rate for Cluster Centroids\n",
    "\n",
    "oversampling_neighbors = 11 # size of neighbourhood for K-nearest neighbours method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a method to split the dataset into training and testing sets, also returning the train/test\n",
    "# indexes for splitting the dataset into K folds for the K-fold cross validation\n",
    "\n",
    "def trainTestData_1 (features, test_ratio, k_fold):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size = test_ratio, shuffle = True, random_state=42)\n",
    "    kf = KFold(n_splits = k_fold, shuffle = True)\n",
    "    x_train = np.reshape(x_tr.values, (x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "    y_train = y_tr.values.astype(int)\n",
    "    x_test = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_test = y_ts.values.astype(int)\n",
    "    return x_train, x_test, y_train, y_test, kf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a method to split the dataset into training, validationa and testing sets\n",
    "\n",
    "def trainTestData_2 (features, test_ratio, val_ratio):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_1, x_ts, y_1, y_ts = train_test_split(x, y, test_size = test_ratio, random_state=42)\n",
    "    x_tr, x_v, y_tr, y_v = train_test_split(x_1, y_1, test_size = val_ratio, random_state=42)\n",
    "    x_train = np.reshape(x_tr.values, (x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "    y_train = y_tr.values.astype(int)\n",
    "    x_val = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_val = y_ts.values.astype(int)\n",
    "    x_test = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_test = y_ts.values.astype(int)\n",
    "    return x_train, x_test, y_train, y_test, x_val, y_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and store data from the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from CHB.csv\n"
     ]
    }
   ],
   "source": [
    "print('Reading data from', csvImportFile)\n",
    "df = pd.read_csv(csvImportFile, delimiter = ',', header = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7df9b78afc401d8961b8cd9fcf9810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"    bands = [0.5, 4, 8, 12, 30, 100]\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n",
      "c:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  \"'''\\n\",\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sampEn13'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3799\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3798\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3799\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info_axis\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3800\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m   3801\u001b[0m     \u001b[39m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sampEn13'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\Main-Mari_3 copy.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We compute the features and save them in the Feature.csv file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ft \u001b[39m=\u001b[39m feature_extraction(df, sample_rate, step, pca_tolerance, undersampling_rate, oversampling_neighbors)\n",
      "File \u001b[1;32mc:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\FeatureExtraction.ipynb:82\u001b[0m, in \u001b[0;36mfeature_extraction\u001b[1;34m(df, sample_rate, step, pca_tolerance, undersampling_rate, oversampling_neighbors)\u001b[0m\n\u001b[0;32m      1\u001b[0m {\n\u001b[0;32m      2\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mcells\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m      3\u001b[0m   {\n\u001b[0;32m      4\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m22\u001b[39m,\n\u001b[0;32m      6\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m      7\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m      8\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport pywt\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport math\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport numpy as np\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimport pandas as pd\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom pyentrp import entropy\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom tqdm.notebook import tqdm\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom scipy.signal import welch\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom scipy.integrate import simps\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom scipy.stats import skew, kurtosis, variation\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom sklearn.utils import shuffle\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m    ]\n\u001b[0;32m     20\u001b[0m   },\n\u001b[0;32m     21\u001b[0m   {\n\u001b[0;32m     22\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m23\u001b[39m,\n\u001b[0;32m     24\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m     25\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     26\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m     27\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfrom ipynb.fs.full.Preprocessing import removeNonNumericValues, dimentionalityReduction, featureNormalization, undersamplingClusterCentroids, oversamplingSMOTE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m    ]\n\u001b[0;32m     29\u001b[0m   },\n\u001b[0;32m     30\u001b[0m   {\n\u001b[0;32m     31\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mattachments\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m     32\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m     34\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m     35\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTIme Domain Features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m    ]\n\u001b[0;32m     37\u001b[0m   },\n\u001b[0;32m     38\u001b[0m   {\n\u001b[0;32m     39\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m24\u001b[39m,\n\u001b[0;32m     41\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m     42\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     43\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m     44\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdef computeTimeDomainFeatures (x):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    mean = np.mean(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     46\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    var = np.var(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     47\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    sk = skew(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    kurt = kurtosis(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    std = np.std(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    median = np.median(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    zcr = ((x[:-1] * x[1:]) < 0).sum() / len(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    if x.mean() != 0:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     53\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        cv = variation(x)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    else:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        cv = math.nan\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     56\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    if x.size > 0:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     57\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        rms = np.sqrt(x.dot(x)/x.size)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     58\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    else:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     59\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        rms = math.nan\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     60\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    p2p = x.max() - x.min()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    sampEn = entropy.sample_entropy(x, 1)[0]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     62\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    return mean, var, sk, kurt, std, median, zcr, cv, rms, p2p, sampEn\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m    ]\n\u001b[0;32m     64\u001b[0m   },\n\u001b[0;32m     65\u001b[0m   {\n\u001b[0;32m     66\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mattachments\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m     67\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     68\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m     69\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m     70\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSpectral Features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m    ]\n\u001b[0;32m     72\u001b[0m   },\n\u001b[0;32m     73\u001b[0m   {\n\u001b[0;32m     74\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     75\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m25\u001b[39m,\n\u001b[0;32m     76\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m     77\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m     78\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m     79\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     80\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCompute the average bandpower of an EEG signal\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttps://raphaelvallat.com/bandpower.html\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m---> 82\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     83\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     84\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdef psd (x, fs, win):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     85\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    bands = [0.5, 4, 8, 12, 30, 100]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    freqs, psd = welch(x, fs, nperseg = win)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    avg_power=[]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    while len(bands)>1:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        idx = np.logical_and(freqs >= bands[0], freqs <= bands[1])\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     90\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        power_simps = simps(psd[idx], dx=bands[1]-bands[0])\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     91\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        avg_power.append(power_simps)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        bands = np.copy(bands[1:])\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     93\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    for p in avg_power:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        yield p\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m    ]\n\u001b[0;32m     96\u001b[0m   },\n\u001b[0;32m     97\u001b[0m   {\n\u001b[0;32m     98\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mattachments\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m     99\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    100\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m    101\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m    102\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCorrelation Features\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m    ]\n\u001b[0;32m    104\u001b[0m   },\n\u001b[0;32m    105\u001b[0m   {\n\u001b[0;32m    106\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m26\u001b[39m,\n\u001b[0;32m    108\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m    109\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m    110\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m    111\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdef compute_correlation (left, right):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    return abs(np.correlate(left, right, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)).max()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m    ]\n\u001b[0;32m    114\u001b[0m   },\n\u001b[0;32m    115\u001b[0m   {\n\u001b[0;32m    116\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mattachments\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m    117\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmarkdown\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    118\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m    119\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m    120\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFeature Extraction\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m    ]\n\u001b[0;32m    122\u001b[0m   },\n\u001b[0;32m    123\u001b[0m   {\n\u001b[0;32m    124\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcell_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    125\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mexecution_count\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m27\u001b[39m,\n\u001b[0;32m    126\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {},\n\u001b[0;32m    127\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: [],\n\u001b[0;32m    128\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m    129\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdef feature_extraction (df, sample_rate, step, pca_tolerance, undersampling_rate, oversampling_neighbors):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFeature Extraction\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    132\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    ft = pd.DataFrame()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    133\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    c = 0\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    134\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    for i in tqdm(range (0, df.shape[0], step)):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    135\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        temp = df.iloc[i:i+step]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    136\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        for j in range(0, df.shape[1]-1):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    137\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            s = np.array(temp.iloc[:, j])\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    138\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    139\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            # Time Domain Features\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    140\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvar\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mskew\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)],ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mkurt\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstd\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mzcr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrms\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mp2p\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)],ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39msampEn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)] = computeTimeDomainFeatures(s)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    141\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    142\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            # Frequency Domain Features\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    143\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m            ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdeltaPower\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mthetaPower\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39malphaPower\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbetaPower\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)], ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mgammaPower\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+str(j)] = psd(s, sample_rate, s.shape[0])\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    144\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    145\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        ft.loc[c, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mseizure\u001b[39m\u001b[39m'\u001b[39m\u001b[39m] = temp[\u001b[39m\u001b[39m'\u001b[39m\u001b[39mseizure\u001b[39m\u001b[39m'\u001b[39m\u001b[39m].value_counts().idxmax()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    146\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m        c = c + 1\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    147\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    148\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    removeNonNumericValues(ft)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    149\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    150\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    ft = featureNormalization(ft)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    151\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNormalized features\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    152\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    153\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    removeNonNumericValues(ft)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    154\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    size = ft.shape\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    156\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mReducing features dimension\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    157\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    ft = dimentionalityReduction(ft, pca_tolerance)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    158\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    removeNonNumericValues(ft)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    159\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDimensions reduced from\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, size, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mto\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, ft.shape)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    160\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    size = ft.seizure.value_counts()\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    161\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    162\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUndersampling the majority class using Cluster Centroid Method\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    163\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    ft = undersamplingClusterCentroids(ft.loc[:, ft.columns != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mseizure\u001b[39m\u001b[39m'\u001b[39m\u001b[39m], ft[\u001b[39m\u001b[39m'\u001b[39m\u001b[39mseizure\u001b[39m\u001b[39m'\u001b[39m\u001b[39m], undersampling_rate)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    164\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    removeNonNumericValues(ft)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    165\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMajority class downsampled from (\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, size[0], \u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m, ft.shape[1], \u001b[39m\u001b[39m'\u001b[39m\u001b[39m) to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m, ft.shape, sep = \u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    166\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    167\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    size = ft.shape\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    168\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOversampling the minority class using SMOTE\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    169\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    ft = oversamplingSMOTE(ft.loc[:, ft.columns != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mseizure\u001b[39m\u001b[39m'\u001b[39m\u001b[39m], ft[\u001b[39m\u001b[39m'\u001b[39m\u001b[39mseizure\u001b[39m\u001b[39m'\u001b[39m\u001b[39m], oversampling_neighbors)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    170\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    ft = shuffle(ft)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    171\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    ft.reset_index(drop = True, inplace = True)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    172\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    removeNonNumericValues(ft)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMinority class upsampled from (\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, size[0], \u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m, ft.shape[1], \u001b[39m\u001b[39m'\u001b[39m\u001b[39m) to \u001b[39m\u001b[39m'\u001b[39m\u001b[39m, ft.shape, sep=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    174\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    175\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    print(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWriting features to a csv file\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    176\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    ft.to_csv(\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFeatures.csv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, index = False)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    177\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    178\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m    return ft\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    179\u001b[0m    ]\n\u001b[0;32m    180\u001b[0m   }\n\u001b[0;32m    181\u001b[0m  ],\n\u001b[0;32m    182\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    183\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mkernelspec\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    184\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mdisplay_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mPython 3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    185\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mlanguage\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    186\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m   },\n\u001b[0;32m    188\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mlanguage_info\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    189\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mcodemirror_mode\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    190\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mipython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    191\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m3\u001b[39m\n\u001b[0;32m    192\u001b[0m    },\n\u001b[0;32m    193\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mfile_extension\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m.py\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    194\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mmimetype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext/x-python\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    195\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mnbconvert_exporter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    197\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mpygments_lexer\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mipython3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    198\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m3.10.11\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m   },\n\u001b[0;32m    200\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39morig_nbformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m\n\u001b[0;32m    201\u001b[0m  },\n\u001b[0;32m    202\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mnbformat\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m4\u001b[39m,\n\u001b[0;32m    203\u001b[0m  \u001b[39m\"\u001b[39m\u001b[39mnbformat_minor\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m\n\u001b[0;32m    204\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    715\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 716\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1642\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1639\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj[key] \u001b[39m=\u001b[39m empty_value\n\u001b[0;32m   1641\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1642\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj[key] \u001b[39m=\u001b[39m infer_fill_value(value)\n\u001b[0;32m   1644\u001b[0m new_indexer \u001b[39m=\u001b[39m convert_from_missing_indexer_tuple(\n\u001b[0;32m   1645\u001b[0m     indexer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39maxes\n\u001b[0;32m   1646\u001b[0m )\n\u001b[0;32m   1647\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_with_indexer(new_indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3845\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3842\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(existing_piece, DataFrame):\n\u001b[0;32m   3843\u001b[0m             value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(value, (\u001b[39mlen\u001b[39m(existing_piece\u001b[39m.\u001b[39mcolumns), \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mT\n\u001b[1;32m-> 3845\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item_mgr(key, value)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3802\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3799\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39mget_loc(key)\n\u001b[0;32m   3800\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m   3801\u001b[0m     \u001b[39m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49minsert(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info_axis), key, value)\n\u001b[0;32m   3803\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iset_item_mgr(loc, value)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1264\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[1;34m(self, loc, item, value)\u001b[0m\n\u001b[0;32m   1255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_known_consolidated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mnot\u001b[39;00m block\u001b[39m.\u001b[39mis_extension \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks) \u001b[39m>\u001b[39m \u001b[39m100\u001b[39m:\n\u001b[0;32m   1258\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataFrame is highly fragmented.  This is usually the result \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1260\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mof calling `frame.insert` many times, which has poor performance.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider joining all columns at once using pd.concat(axis=1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1262\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minstead. To get a de-fragmented frame, use `newframe = frame.copy()`\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1263\u001b[0m         PerformanceWarning,\n\u001b[1;32m-> 1264\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1265\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\pandas\\util\\_exceptions.py:32\u001b[0m, in \u001b[0;36mfind_stack_level\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_stack_level\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m     28\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    Find the first place in the stack that is not inside pandas\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    (tests notwithstanding).\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     stack \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mstack()\n\u001b[0;32m     34\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     pkg_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(pd\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\inspect.py:1554\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack\u001b[39m(context\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1553\u001b[0m     \u001b[39m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1554\u001b[0m     \u001b[39mreturn\u001b[39;00m getouterframes(sys\u001b[39m.\u001b[39;49m_getframe(\u001b[39m1\u001b[39;49m), context)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\inspect.py:1531\u001b[0m, in \u001b[0;36mgetouterframes\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1529\u001b[0m framelist \u001b[39m=\u001b[39m []\n\u001b[0;32m   1530\u001b[0m \u001b[39mwhile\u001b[39;00m frame:\n\u001b[1;32m-> 1531\u001b[0m     frameinfo \u001b[39m=\u001b[39m (frame,) \u001b[39m+\u001b[39m getframeinfo(frame, context)\n\u001b[0;32m   1532\u001b[0m     framelist\u001b[39m.\u001b[39mappend(FrameInfo(\u001b[39m*\u001b[39mframeinfo))\n\u001b[0;32m   1533\u001b[0m     frame \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mf_back\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\inspect.py:1501\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m isframe(frame):\n\u001b[0;32m   1499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is not a frame or traceback object\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(frame))\n\u001b[1;32m-> 1501\u001b[0m filename \u001b[39m=\u001b[39m getsourcefile(frame) \u001b[39mor\u001b[39;00m getfile(frame)\n\u001b[0;32m   1502\u001b[0m \u001b[39mif\u001b[39;00m context \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1503\u001b[0m     start \u001b[39m=\u001b[39m lineno \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m context\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\inspect.py:706\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39many\u001b[39m(filename\u001b[39m.\u001b[39mendswith(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m\n\u001b[0;32m    704\u001b[0m              importlib\u001b[39m.\u001b[39mmachinery\u001b[39m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 706\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mexists(filename):\n\u001b[0;32m    707\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n\u001b[0;32m    708\u001b[0m \u001b[39m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     20\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We compute the features and save them in the Feature.csv file\n",
    "\n",
    "ft = feature_extraction(df, sample_rate, step, pca_tolerance, undersampling_rate, oversampling_neighbors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = pd.read_csv(\"Features.csv\", delimiter = ',', header = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset splitting without validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 5\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset splitting with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, x_val, y_val = trainTestData_2 (ft, test_ratio, val_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_SGD (initial_learning_rate, decay_steps, decay_rate):\n",
    "    # We define the optimizer with an initial learning rate\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # We define the larning rate schedule with an exponential decay, specifying the number of decay steps and the decay rate\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "    \n",
    "    return optimizer, lr_schedule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2 (history):\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(train_loss))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss, label='Training loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNN: model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "30/40 [=====================>........] - ETA: 0s - loss: 0.7954 - accuracy: 0.5000\n",
      "Epoch 1: val_loss improved from inf to 0.70011, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 2s 16ms/step - loss: 0.7866 - accuracy: 0.5176 - val_loss: 0.7001 - val_accuracy: 0.5536 - lr: 0.0010\n",
      "Epoch 2/350\n",
      "28/40 [====================>.........] - ETA: 0s - loss: 0.6546 - accuracy: 0.6339\n",
      "Epoch 2: val_loss improved from 0.70011 to 0.60354, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6682 - accuracy: 0.6326 - val_loss: 0.6035 - val_accuracy: 0.6964 - lr: 0.0010\n",
      "Epoch 3/350\n",
      "27/40 [===================>..........] - ETA: 0s - loss: 0.5923 - accuracy: 0.7176\n",
      "Epoch 3: val_loss improved from 0.60354 to 0.53551, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5859 - accuracy: 0.7125 - val_loss: 0.5355 - val_accuracy: 0.7560 - lr: 0.0010\n",
      "Epoch 4/350\n",
      "25/40 [=================>............] - ETA: 0s - loss: 0.5389 - accuracy: 0.7450\n",
      "Epoch 4: val_loss improved from 0.53551 to 0.48621, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5274 - accuracy: 0.7636 - val_loss: 0.4862 - val_accuracy: 0.7917 - lr: 0.0010\n",
      "Epoch 5/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.4519 - accuracy: 0.8333\n",
      "Epoch 5: val_loss improved from 0.48621 to 0.44864, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4841 - accuracy: 0.7955 - val_loss: 0.4486 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 6/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.4379 - accuracy: 0.8385\n",
      "Epoch 6: val_loss improved from 0.44864 to 0.42063, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4508 - accuracy: 0.8115 - val_loss: 0.4206 - val_accuracy: 0.8274 - lr: 0.0010\n",
      "Epoch 7/350\n",
      "26/40 [==================>...........] - ETA: 0s - loss: 0.4201 - accuracy: 0.8365\n",
      "Epoch 7: val_loss improved from 0.42063 to 0.39774, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4253 - accuracy: 0.8211 - val_loss: 0.3977 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 8/350\n",
      "20/40 [==============>...............] - ETA: 0s - loss: 0.4232 - accuracy: 0.8313\n",
      "Epoch 8: val_loss improved from 0.39774 to 0.38160, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4046 - accuracy: 0.8307 - val_loss: 0.3816 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 9/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.4011 - accuracy: 0.8229\n",
      "Epoch 9: val_loss improved from 0.38160 to 0.36562, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3894 - accuracy: 0.8371 - val_loss: 0.3656 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 10/350\n",
      "26/40 [==================>...........] - ETA: 0s - loss: 0.3759 - accuracy: 0.8317\n",
      "Epoch 10: val_loss improved from 0.36562 to 0.35199, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3749 - accuracy: 0.8371 - val_loss: 0.3520 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 11/350\n",
      "27/40 [===================>..........] - ETA: 0s - loss: 0.3773 - accuracy: 0.8426\n",
      "Epoch 11: val_loss improved from 0.35199 to 0.34081, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3626 - accuracy: 0.8435 - val_loss: 0.3408 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 12/350\n",
      "25/40 [=================>............] - ETA: 0s - loss: 0.3315 - accuracy: 0.8650\n",
      "Epoch 12: val_loss improved from 0.34081 to 0.33116, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3524 - accuracy: 0.8562 - val_loss: 0.3312 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 13/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.3420 - accuracy: 0.8542\n",
      "Epoch 13: val_loss improved from 0.33116 to 0.32459, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3435 - accuracy: 0.8626 - val_loss: 0.3246 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 14/350\n",
      "23/40 [================>.............] - ETA: 0s - loss: 0.3334 - accuracy: 0.8804\n",
      "Epoch 14: val_loss improved from 0.32459 to 0.31713, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3369 - accuracy: 0.8658 - val_loss: 0.3171 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 15/350\n",
      "29/40 [====================>.........] - ETA: 0s - loss: 0.3326 - accuracy: 0.8750\n",
      "Epoch 15: val_loss improved from 0.31713 to 0.31054, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.3300 - accuracy: 0.8722 - val_loss: 0.3105 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 16/350\n",
      "25/40 [=================>............] - ETA: 0s - loss: 0.3078 - accuracy: 0.8800\n",
      "Epoch 16: val_loss improved from 0.31054 to 0.30474, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3238 - accuracy: 0.8722 - val_loss: 0.3047 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 17/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.3327 - accuracy: 0.8698\n",
      "Epoch 17: val_loss improved from 0.30474 to 0.29956, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8786 - val_loss: 0.2996 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 18/350\n",
      "22/40 [===============>..............] - ETA: 0s - loss: 0.3157 - accuracy: 0.8750\n",
      "Epoch 18: val_loss improved from 0.29956 to 0.29475, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8850 - val_loss: 0.2948 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 19/350\n",
      "23/40 [================>.............] - ETA: 0s - loss: 0.2986 - accuracy: 0.9022\n",
      "Epoch 19: val_loss improved from 0.29475 to 0.29059, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3086 - accuracy: 0.8882 - val_loss: 0.2906 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 20/350\n",
      "39/40 [============================>.] - ETA: 0s - loss: 0.3052 - accuracy: 0.8878\n",
      "Epoch 20: val_loss improved from 0.29059 to 0.28672, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3045 - accuracy: 0.8882 - val_loss: 0.2867 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 21/350\n",
      "23/40 [================>.............] - ETA: 0s - loss: 0.3171 - accuracy: 0.8750\n",
      "Epoch 21: val_loss improved from 0.28672 to 0.28329, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3008 - accuracy: 0.8914 - val_loss: 0.2833 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 22/350\n",
      "22/40 [===============>..............] - ETA: 0s - loss: 0.2775 - accuracy: 0.9148\n",
      "Epoch 22: val_loss improved from 0.28329 to 0.28019, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2972 - accuracy: 0.8914 - val_loss: 0.2802 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 23/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.3019 - accuracy: 0.9010\n",
      "Epoch 23: val_loss improved from 0.28019 to 0.27732, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2940 - accuracy: 0.8914 - val_loss: 0.2773 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 24/350\n",
      "25/40 [=================>............] - ETA: 0s - loss: 0.2989 - accuracy: 0.8900\n",
      "Epoch 24: val_loss improved from 0.27732 to 0.27463, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2911 - accuracy: 0.8946 - val_loss: 0.2746 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 25/350\n",
      "21/40 [==============>...............] - ETA: 0s - loss: 0.3267 - accuracy: 0.8631\n",
      "Epoch 25: val_loss improved from 0.27463 to 0.27210, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2883 - accuracy: 0.8946 - val_loss: 0.2721 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 26/350\n",
      "26/40 [==================>...........] - ETA: 0s - loss: 0.2833 - accuracy: 0.8942\n",
      "Epoch 26: val_loss improved from 0.27210 to 0.26973, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2857 - accuracy: 0.8946 - val_loss: 0.2697 - val_accuracy: 0.8810 - lr: 0.0010\n",
      "Epoch 27/350\n",
      "37/40 [==========================>...] - ETA: 0s - loss: 0.2751 - accuracy: 0.8986\n",
      "Epoch 27: val_loss improved from 0.26973 to 0.26786, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2833 - accuracy: 0.8946 - val_loss: 0.2679 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 28/350\n",
      "38/40 [===========================>..] - ETA: 0s - loss: 0.2808 - accuracy: 0.8980\n",
      "Epoch 28: val_loss improved from 0.26786 to 0.26578, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2809 - accuracy: 0.8978 - val_loss: 0.2658 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 29/350\n",
      "25/40 [=================>............] - ETA: 0s - loss: 0.2655 - accuracy: 0.9100\n",
      "Epoch 29: val_loss improved from 0.26578 to 0.26355, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2787 - accuracy: 0.8978 - val_loss: 0.2636 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 30/350\n",
      "23/40 [================>.............] - ETA: 0s - loss: 0.2769 - accuracy: 0.9022\n",
      "Epoch 30: val_loss improved from 0.26355 to 0.26171, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2765 - accuracy: 0.8978 - val_loss: 0.2617 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 31/350\n",
      "22/40 [===============>..............] - ETA: 0s - loss: 0.2900 - accuracy: 0.8864\n",
      "Epoch 31: val_loss improved from 0.26171 to 0.25998, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2744 - accuracy: 0.8978 - val_loss: 0.2600 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 32/350\n",
      "34/40 [========================>.....] - ETA: 0s - loss: 0.2858 - accuracy: 0.8897\n",
      "Epoch 32: val_loss improved from 0.25998 to 0.25829, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2725 - accuracy: 0.8978 - val_loss: 0.2583 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 33/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.2699 - accuracy: 0.9010\n",
      "Epoch 33: val_loss improved from 0.25829 to 0.25671, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2706 - accuracy: 0.8978 - val_loss: 0.2567 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 34/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.2386 - accuracy: 0.9010\n",
      "Epoch 34: val_loss improved from 0.25671 to 0.25520, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2688 - accuracy: 0.8978 - val_loss: 0.2552 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 35/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.2806 - accuracy: 0.9010\n",
      "Epoch 35: val_loss improved from 0.25520 to 0.25377, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2671 - accuracy: 0.9010 - val_loss: 0.2538 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 36/350\n",
      "25/40 [=================>............] - ETA: 0s - loss: 0.2523 - accuracy: 0.9050\n",
      "Epoch 36: val_loss improved from 0.25377 to 0.25237, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2654 - accuracy: 0.9010 - val_loss: 0.2524 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 37/350\n",
      "28/40 [====================>.........] - ETA: 0s - loss: 0.2400 - accuracy: 0.9196\n",
      "Epoch 37: val_loss improved from 0.25237 to 0.25104, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2638 - accuracy: 0.9010 - val_loss: 0.2510 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 38/350\n",
      "21/40 [==============>...............] - ETA: 0s - loss: 0.2128 - accuracy: 0.9286\n",
      "Epoch 38: val_loss improved from 0.25104 to 0.24976, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2622 - accuracy: 0.9042 - val_loss: 0.2498 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 39/350\n",
      "22/40 [===============>..............] - ETA: 0s - loss: 0.2755 - accuracy: 0.9091\n",
      "Epoch 39: val_loss improved from 0.24976 to 0.24854, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2606 - accuracy: 0.9073 - val_loss: 0.2485 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 40/350\n",
      "23/40 [================>.............] - ETA: 0s - loss: 0.2787 - accuracy: 0.9022\n",
      "Epoch 40: val_loss improved from 0.24854 to 0.24727, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2591 - accuracy: 0.9073 - val_loss: 0.2473 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 41/350\n",
      "25/40 [=================>............] - ETA: 0s - loss: 0.2334 - accuracy: 0.9250\n",
      "Epoch 41: val_loss improved from 0.24727 to 0.24601, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2576 - accuracy: 0.9073 - val_loss: 0.2460 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 42/350\n",
      "20/40 [==============>...............] - ETA: 0s - loss: 0.2656 - accuracy: 0.8875\n",
      "Epoch 42: val_loss improved from 0.24601 to 0.24491, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2561 - accuracy: 0.9073 - val_loss: 0.2449 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 43/350\n",
      "38/40 [===========================>..] - ETA: 0s - loss: 0.2542 - accuracy: 0.9112\n",
      "Epoch 43: val_loss improved from 0.24491 to 0.24379, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2547 - accuracy: 0.9105 - val_loss: 0.2438 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 44/350\n",
      "22/40 [===============>..............] - ETA: 0s - loss: 0.3125 - accuracy: 0.8750\n",
      "Epoch 44: val_loss improved from 0.24379 to 0.24277, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2534 - accuracy: 0.9105 - val_loss: 0.2428 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 45/350\n",
      "21/40 [==============>...............] - ETA: 0s - loss: 0.2275 - accuracy: 0.9286\n",
      "Epoch 45: val_loss improved from 0.24277 to 0.24169, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2521 - accuracy: 0.9105 - val_loss: 0.2417 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 46/350\n",
      "23/40 [================>.............] - ETA: 0s - loss: 0.3012 - accuracy: 0.8804\n",
      "Epoch 46: val_loss improved from 0.24169 to 0.24072, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2507 - accuracy: 0.9105 - val_loss: 0.2407 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 47/350\n",
      "24/40 [=================>............] - ETA: 0s - loss: 0.2346 - accuracy: 0.9271\n",
      "Epoch 47: val_loss improved from 0.24072 to 0.24012, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2494 - accuracy: 0.9137 - val_loss: 0.2401 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 48/350\n",
      "36/40 [==========================>...] - ETA: 0s - loss: 0.2591 - accuracy: 0.9132"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\Main-Mari_3 copy.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Training of the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m350\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(x_val, y_val), callbacks\u001b[39m=\u001b[39;49m[tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mLearningRateScheduler(lr_schedule), callbacks_list])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# optimizer \"adam\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mmodel.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m plot_2(history)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, epochs=350, batch_size=8, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "\"\"\"\n",
    "# optimizer \"adam\"\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNN with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.4740 - accuracy: 0.7963 \n",
      "Epoch 1: val_loss improved from inf to 0.30712, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 2s 10ms/step - loss: 0.4508 - accuracy: 0.8010 - val_loss: 0.3071 - val_accuracy: 0.8810\n",
      "Epoch 2/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.2982 - accuracy: 0.8971\n",
      "Epoch 2: val_loss improved from 0.30712 to 0.25451, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2900 - accuracy: 0.9005 - val_loss: 0.2545 - val_accuracy: 0.8929\n",
      "Epoch 3/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9115\n",
      "Epoch 3: val_loss improved from 0.25451 to 0.22853, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2385 - accuracy: 0.9107 - val_loss: 0.2285 - val_accuracy: 0.9048\n",
      "Epoch 4/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.2035 - accuracy: 0.9306\n",
      "Epoch 4: val_loss improved from 0.22853 to 0.21219, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2074 - accuracy: 0.9311 - val_loss: 0.2122 - val_accuracy: 0.9107\n",
      "Epoch 5/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1834 - accuracy: 0.9388\n",
      "Epoch 5: val_loss improved from 0.21219 to 0.19946, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1843 - accuracy: 0.9388 - val_loss: 0.1995 - val_accuracy: 0.9167\n",
      "Epoch 6/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.1643 - accuracy: 0.9479\n",
      "Epoch 6: val_loss improved from 0.19946 to 0.18917, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1672 - accuracy: 0.9439 - val_loss: 0.1892 - val_accuracy: 0.9286\n",
      "Epoch 7/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.1694 - accuracy: 0.9397\n",
      "Epoch 7: val_loss improved from 0.18917 to 0.18035, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1528 - accuracy: 0.9490 - val_loss: 0.1804 - val_accuracy: 0.9286\n",
      "Epoch 8/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.1477 - accuracy: 0.9500\n",
      "Epoch 8: val_loss improved from 0.18035 to 0.17247, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9541 - val_loss: 0.1725 - val_accuracy: 0.9345\n",
      "Epoch 9/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.1289 - accuracy: 0.9609\n",
      "Epoch 9: val_loss improved from 0.17247 to 0.16679, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1306 - accuracy: 0.9592 - val_loss: 0.1668 - val_accuracy: 0.9405\n",
      "Epoch 10/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9617\n",
      "Epoch 10: val_loss improved from 0.16679 to 0.16102, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1210 - accuracy: 0.9617 - val_loss: 0.1610 - val_accuracy: 0.9464\n",
      "Epoch 11/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1113 - accuracy: 0.9696\n",
      "Epoch 11: val_loss improved from 0.16102 to 0.15557, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1134 - accuracy: 0.9668 - val_loss: 0.1556 - val_accuracy: 0.9464\n",
      "Epoch 12/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1109 - accuracy: 0.9647\n",
      "Epoch 12: val_loss improved from 0.15557 to 0.15113, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9643 - val_loss: 0.1511 - val_accuracy: 0.9464\n",
      "Epoch 13/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1015 - accuracy: 0.9707\n",
      "Epoch 13: val_loss improved from 0.15113 to 0.14741, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9719 - val_loss: 0.1474 - val_accuracy: 0.9464\n",
      "Epoch 14/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.0904 - accuracy: 0.9727\n",
      "Epoch 14: val_loss improved from 0.14741 to 0.14512, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0921 - accuracy: 0.9719 - val_loss: 0.1451 - val_accuracy: 0.9524\n",
      "Epoch 15/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0856 - accuracy: 0.9728\n",
      "Epoch 15: val_loss improved from 0.14512 to 0.14206, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0868 - accuracy: 0.9745 - val_loss: 0.1421 - val_accuracy: 0.9524\n",
      "Epoch 16/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.0665 - accuracy: 0.9792\n",
      "Epoch 16: val_loss improved from 0.14206 to 0.14129, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9719 - val_loss: 0.1413 - val_accuracy: 0.9524\n",
      "Epoch 17/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.0725 - accuracy: 0.9730\n",
      "Epoch 17: val_loss improved from 0.14129 to 0.13665, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9745 - val_loss: 0.1366 - val_accuracy: 0.9583\n",
      "Epoch 18/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0744 - accuracy: 0.9728\n",
      "Epoch 18: val_loss improved from 0.13665 to 0.13472, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9745 - val_loss: 0.1347 - val_accuracy: 0.9583\n",
      "Epoch 19/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0705 - accuracy: 0.9755\n",
      "Epoch 19: val_loss improved from 0.13472 to 0.13360, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9770 - val_loss: 0.1336 - val_accuracy: 0.9583\n",
      "Epoch 20/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0652 - accuracy: 0.9787\n",
      "Epoch 20: val_loss improved from 0.13360 to 0.13047, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9796 - val_loss: 0.1305 - val_accuracy: 0.9583\n",
      "Epoch 21/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.0575 - accuracy: 0.9816\n",
      "Epoch 21: val_loss improved from 0.13047 to 0.12875, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.9770 - val_loss: 0.1287 - val_accuracy: 0.9524\n",
      "Epoch 22/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.0604 - accuracy: 0.9812\n",
      "Epoch 22: val_loss did not improve from 0.12875\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9847 - val_loss: 0.1294 - val_accuracy: 0.9524\n",
      "Epoch 23/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0439 - accuracy: 0.9946\n",
      "Epoch 23: val_loss improved from 0.12875 to 0.12567, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9872 - val_loss: 0.1257 - val_accuracy: 0.9524\n",
      "Epoch 24/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0560 - accuracy: 0.9915\n",
      "Epoch 24: val_loss improved from 0.12567 to 0.12456, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0536 - accuracy: 0.9923 - val_loss: 0.1246 - val_accuracy: 0.9524\n",
      "Epoch 25/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0423 - accuracy: 0.9830\n",
      "Epoch 25: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0487 - accuracy: 0.9821 - val_loss: 0.1264 - val_accuracy: 0.9464\n",
      "Epoch 26/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0371 - accuracy: 0.9918\n",
      "Epoch 26: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.9898 - val_loss: 0.1271 - val_accuracy: 0.9464\n",
      "Epoch 27/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9923\n",
      "Epoch 27: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.9923 - val_loss: 0.1282 - val_accuracy: 0.9524\n",
      "Epoch 28/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0362 - accuracy: 0.9940\n",
      "Epoch 28: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0404 - accuracy: 0.9923 - val_loss: 0.1272 - val_accuracy: 0.9464\n",
      "Epoch 29/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0412 - accuracy: 0.9911\n",
      "Epoch 29: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9923 - val_loss: 0.1261 - val_accuracy: 0.9524\n",
      "Epoch 30/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0380 - accuracy: 0.9918\n",
      "Epoch 30: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0364 - accuracy: 0.9923 - val_loss: 0.1250 - val_accuracy: 0.9524\n",
      "Epoch 31/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0380 - accuracy: 0.9904\n",
      "Epoch 31: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0354 - accuracy: 0.9923 - val_loss: 0.1266 - val_accuracy: 0.9524\n",
      "Epoch 32/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0342 - accuracy: 0.9911\n",
      "Epoch 32: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.9923 - val_loss: 0.1253 - val_accuracy: 0.9524\n",
      "Epoch 33/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9948\n",
      "Epoch 33: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0311 - accuracy: 0.9949 - val_loss: 0.1253 - val_accuracy: 0.9524\n",
      "Epoch 34/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0294 - accuracy: 0.9936\n",
      "Epoch 34: val_loss improved from 0.12456 to 0.12449, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9949 - val_loss: 0.1245 - val_accuracy: 0.9524\n",
      "Epoch 35/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0266 - accuracy: 0.9960\n",
      "Epoch 35: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.9949 - val_loss: 0.1257 - val_accuracy: 0.9524\n",
      "Epoch 36/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.0312 - accuracy: 0.9922\n",
      "Epoch 36: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9949 - val_loss: 0.1257 - val_accuracy: 0.9524\n",
      "Epoch 37/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0294 - accuracy: 0.9914\n",
      "Epoch 37: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9949 - val_loss: 0.1249 - val_accuracy: 0.9524\n",
      "Epoch 38/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0248 - accuracy: 0.9946\n",
      "Epoch 38: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.9949 - val_loss: 0.1260 - val_accuracy: 0.9524\n",
      "Epoch 39/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9948\n",
      "Epoch 39: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.9949 - val_loss: 0.1261 - val_accuracy: 0.9524\n",
      "Epoch 40/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 40: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9949 - val_loss: 0.1252 - val_accuracy: 0.9524\n",
      "Epoch 41/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.0211 - accuracy: 0.9942\n",
      "Epoch 41: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.9949 - val_loss: 0.1291 - val_accuracy: 0.9524\n",
      "Epoch 42/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0233 - accuracy: 0.9917\n",
      "Epoch 42: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 0.1274 - val_accuracy: 0.9524\n",
      "Epoch 43/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0192 - accuracy: 0.9946\n",
      "Epoch 43: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.9949 - val_loss: 0.1293 - val_accuracy: 0.9524\n",
      "Epoch 44/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.0173 - accuracy: 0.9942\n",
      "Epoch 44: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9949 - val_loss: 0.1303 - val_accuracy: 0.9524\n",
      "Epoch 45/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0167 - accuracy: 0.9970\n",
      "Epoch 45: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 0.9974 - val_loss: 0.1306 - val_accuracy: 0.9524\n",
      "Epoch 46/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.0154 - accuracy: 0.9972\n",
      "Epoch 46: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9974 - val_loss: 0.1316 - val_accuracy: 0.9524\n",
      "Epoch 47/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0148 - accuracy: 0.9957\n",
      "Epoch 47: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.1320 - val_accuracy: 0.9524\n",
      "Epoch 48/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0151 - accuracy: 0.9972\n",
      "Epoch 48: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9974 - val_loss: 0.1322 - val_accuracy: 0.9524\n",
      "Epoch 49/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0150 - accuracy: 0.9958\n",
      "Epoch 49: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.9974 - val_loss: 0.1341 - val_accuracy: 0.9524\n",
      "Epoch 50/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.0150 - accuracy: 0.9963\n",
      "Epoch 50: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9974 - val_loss: 0.1356 - val_accuracy: 0.9524\n",
      "Epoch 51/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0128 - accuracy: 0.9973\n",
      "Epoch 51: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.9974 - val_loss: 0.1359 - val_accuracy: 0.9524\n",
      "Epoch 52/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0145 - accuracy: 0.9960\n",
      "Epoch 52: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9974 - val_loss: 0.1371 - val_accuracy: 0.9524\n",
      "Epoch 53/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0112 - accuracy: 0.9973\n",
      "Epoch 53: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9974 - val_loss: 0.1385 - val_accuracy: 0.9524\n",
      "Epoch 54/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 54: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.1406 - val_accuracy: 0.9524\n",
      "Epoch 55/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 55: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9524\n",
      "Epoch 56/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9974\n",
      "Epoch 56: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.1422 - val_accuracy: 0.9524\n",
      "Epoch 57/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 57: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 0.9524\n",
      "Epoch 58/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 58: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.1449 - val_accuracy: 0.9524\n",
      "Epoch 59/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 59: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1463 - val_accuracy: 0.9524\n",
      "Epoch 60/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 60: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.9524\n",
      "Epoch 61/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 61: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1485 - val_accuracy: 0.9524\n",
      "Epoch 62/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 62: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.9524\n",
      "Epoch 63/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 63: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.9524\n",
      "Epoch 64/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 64: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9524\n",
      "Epoch 65/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 65: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9524\n",
      "Epoch 66/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 66: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9524\n",
      "Epoch 67/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 67: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9524\n",
      "Epoch 68/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 68: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.9524\n",
      "Epoch 69/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000  \n",
      "Epoch 69: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9524\n",
      "Epoch 70/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 70: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1596 - val_accuracy: 0.9524\n",
      "Epoch 71/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 71: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1606 - val_accuracy: 0.9524\n",
      "Epoch 72/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 72: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1611 - val_accuracy: 0.9524\n",
      "Epoch 73/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 73: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1638 - val_accuracy: 0.9524\n",
      "Epoch 74/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 74: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9524\n",
      "Epoch 75/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 75: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1660 - val_accuracy: 0.9524\n",
      "Epoch 76/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 76: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.1669 - val_accuracy: 0.9524\n",
      "Epoch 77/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 77: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1678 - val_accuracy: 0.9524\n",
      "Epoch 78/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 78: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1694 - val_accuracy: 0.9524\n",
      "Epoch 79/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 79: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1711 - val_accuracy: 0.9524\n",
      "Epoch 80/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 80: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9524\n",
      "Epoch 81/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 81: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1731 - val_accuracy: 0.9524\n",
      "Epoch 82/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 82: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1734 - val_accuracy: 0.9524\n",
      "Epoch 83/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000    \n",
      "Epoch 83: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1751 - val_accuracy: 0.9524\n",
      "Epoch 84/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000  \n",
      "Epoch 84: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1765 - val_accuracy: 0.9524\n",
      "Epoch 85/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 85: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1776 - val_accuracy: 0.9583\n",
      "Epoch 86/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000  \n",
      "Epoch 86: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9583\n",
      "Epoch 87/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 87: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1796 - val_accuracy: 0.9583\n",
      "Epoch 88/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 88: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1817 - val_accuracy: 0.9583\n",
      "Epoch 89/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 89: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1818 - val_accuracy: 0.9583\n",
      "Epoch 90/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000    \n",
      "Epoch 90: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1839 - val_accuracy: 0.9583\n",
      "Epoch 91/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 91: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9583\n",
      "Epoch 92/350\n",
      "25/49 [==============>...............] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 92: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1864 - val_accuracy: 0.9583\n",
      "Epoch 93/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 93: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1874 - val_accuracy: 0.9583\n",
      "Epoch 94/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000  \n",
      "Epoch 94: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1881 - val_accuracy: 0.9583\n",
      "Epoch 95/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 95: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1892 - val_accuracy: 0.9583\n",
      "Epoch 96/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 96: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1899 - val_accuracy: 0.9583\n",
      "Epoch 97/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 97: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9583\n",
      "Epoch 98/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 98: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1920 - val_accuracy: 0.9524\n",
      "Epoch 99/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000  \n",
      "Epoch 99: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9524\n",
      "Epoch 100/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n",
      "Epoch 100: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1944 - val_accuracy: 0.9524\n",
      "Epoch 101/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 101: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1951 - val_accuracy: 0.9583\n",
      "Epoch 102/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 102: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9583\n",
      "Epoch 103/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 103: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9524\n",
      "Epoch 104/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000  \n",
      "Epoch 104: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1988 - val_accuracy: 0.9524\n",
      "Epoch 105/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000    \n",
      "Epoch 105: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2000 - val_accuracy: 0.9524\n",
      "Epoch 106/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
      "Epoch 106: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.9524\n",
      "Epoch 107/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000  \n",
      "Epoch 107: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2021 - val_accuracy: 0.9524\n",
      "Epoch 108/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000    \n",
      "Epoch 108: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9524\n",
      "Epoch 109/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000  \n",
      "Epoch 109: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2043 - val_accuracy: 0.9524\n",
      "Epoch 110/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000  \n",
      "Epoch 110: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2052 - val_accuracy: 0.9524\n",
      "Epoch 111/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000    \n",
      "Epoch 111: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2067 - val_accuracy: 0.9524\n",
      "Epoch 112/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 9.3184e-04 - accuracy: 1.0000\n",
      "Epoch 112: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2074 - val_accuracy: 0.9524\n",
      "Epoch 113/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000    \n",
      "Epoch 113: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 9.9580e-04 - accuracy: 1.0000 - val_loss: 0.2094 - val_accuracy: 0.9524\n",
      "Epoch 114/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 6.3759e-04 - accuracy: 1.0000\n",
      "Epoch 114: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 9.7267e-04 - accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.9524\n",
      "Epoch 115/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 7.4733e-04 - accuracy: 1.0000\n",
      "Epoch 115: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 9.3038e-04 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9524\n",
      "Epoch 116/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 9.3631e-04 - accuracy: 1.0000\n",
      "Epoch 116: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 9.1800e-04 - accuracy: 1.0000 - val_loss: 0.2118 - val_accuracy: 0.9524\n",
      "Epoch 117/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 117: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.7235e-04 - accuracy: 1.0000 - val_loss: 0.2145 - val_accuracy: 0.9524\n",
      "Epoch 118/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 7.4268e-04 - accuracy: 1.0000\n",
      "Epoch 118: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 8.4229e-04 - accuracy: 1.0000 - val_loss: 0.2143 - val_accuracy: 0.9524\n",
      "Epoch 119/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 8.2932e-04 - accuracy: 1.0000\n",
      "Epoch 119: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 8.2357e-04 - accuracy: 1.0000 - val_loss: 0.2157 - val_accuracy: 0.9524\n",
      "Epoch 120/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 8.1159e-04 - accuracy: 1.0000\n",
      "Epoch 120: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 7.8196e-04 - accuracy: 1.0000 - val_loss: 0.2172 - val_accuracy: 0.9524\n",
      "Epoch 121/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 7.9332e-04 - accuracy: 1.0000\n",
      "Epoch 121: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 7.7134e-04 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 0.9524\n",
      "Epoch 122/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 8.3540e-04 - accuracy: 1.0000\n",
      "Epoch 122: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 7.5591e-04 - accuracy: 1.0000 - val_loss: 0.2192 - val_accuracy: 0.9524\n",
      "Epoch 123/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 7.2348e-04 - accuracy: 1.0000\n",
      "Epoch 123: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 7.1589e-04 - accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9524\n",
      "Epoch 124/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 7.4346e-04 - accuracy: 1.0000\n",
      "Epoch 124: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 6.9400e-04 - accuracy: 1.0000 - val_loss: 0.2222 - val_accuracy: 0.9524\n",
      "Epoch 125/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 6.7692e-04 - accuracy: 1.0000\n",
      "Epoch 125: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 6.7786e-04 - accuracy: 1.0000 - val_loss: 0.2231 - val_accuracy: 0.9524\n",
      "Epoch 126/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 6.2259e-04 - accuracy: 1.0000\n",
      "Epoch 126: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 6.5565e-04 - accuracy: 1.0000 - val_loss: 0.2240 - val_accuracy: 0.9524\n",
      "Epoch 127/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 5.4319e-04 - accuracy: 1.0000\n",
      "Epoch 127: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 6.3759e-04 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9524\n",
      "Epoch 128/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.2339e-04 - accuracy: 1.0000\n",
      "Epoch 128: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 6.0976e-04 - accuracy: 1.0000 - val_loss: 0.2270 - val_accuracy: 0.9524\n",
      "Epoch 129/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 6.2693e-04 - accuracy: 1.0000\n",
      "Epoch 129: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 6.0519e-04 - accuracy: 1.0000 - val_loss: 0.2278 - val_accuracy: 0.9524\n",
      "Epoch 130/350\n",
      "21/49 [===========>..................] - ETA: 0s - loss: 3.9668e-04 - accuracy: 1.0000\n",
      "Epoch 130: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.7756e-04 - accuracy: 1.0000 - val_loss: 0.2293 - val_accuracy: 0.9524\n",
      "Epoch 131/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 6.1480e-04 - accuracy: 1.0000\n",
      "Epoch 131: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.6670e-04 - accuracy: 1.0000 - val_loss: 0.2303 - val_accuracy: 0.9524\n",
      "Epoch 132/350\n",
      "20/49 [===========>..................] - ETA: 0s - loss: 5.4247e-04 - accuracy: 1.0000\n",
      "Epoch 132: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 5.4540e-04 - accuracy: 1.0000 - val_loss: 0.2316 - val_accuracy: 0.9524\n",
      "Epoch 133/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 5.6293e-04 - accuracy: 1.0000\n",
      "Epoch 133: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.2322e-04 - accuracy: 1.0000 - val_loss: 0.2329 - val_accuracy: 0.9524\n",
      "Epoch 134/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 4.4216e-04 - accuracy: 1.0000\n",
      "Epoch 134: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.1304e-04 - accuracy: 1.0000 - val_loss: 0.2337 - val_accuracy: 0.9524\n",
      "Epoch 135/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 5.5773e-04 - accuracy: 1.0000\n",
      "Epoch 135: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.1270e-04 - accuracy: 1.0000 - val_loss: 0.2348 - val_accuracy: 0.9524\n",
      "Epoch 136/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 5.1112e-04 - accuracy: 1.0000\n",
      "Epoch 136: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.9175e-04 - accuracy: 1.0000 - val_loss: 0.2363 - val_accuracy: 0.9524\n",
      "Epoch 137/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 5.1341e-04 - accuracy: 1.0000\n",
      "Epoch 137: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.6155e-04 - accuracy: 1.0000 - val_loss: 0.2378 - val_accuracy: 0.9524\n",
      "Epoch 138/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 4.7625e-04 - accuracy: 1.0000\n",
      "Epoch 138: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.5782e-04 - accuracy: 1.0000 - val_loss: 0.2389 - val_accuracy: 0.9524\n",
      "Epoch 139/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 4.4201e-04 - accuracy: 1.0000\n",
      "Epoch 139: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.4299e-04 - accuracy: 1.0000 - val_loss: 0.2402 - val_accuracy: 0.9524\n",
      "Epoch 140/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 4.5456e-04 - accuracy: 1.0000\n",
      "Epoch 140: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.2682e-04 - accuracy: 1.0000 - val_loss: 0.2405 - val_accuracy: 0.9524\n",
      "Epoch 141/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 4.2117e-04 - accuracy: 1.0000\n",
      "Epoch 141: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 4.1391e-04 - accuracy: 1.0000 - val_loss: 0.2422 - val_accuracy: 0.9524\n",
      "Epoch 142/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 4.4905e-04 - accuracy: 1.0000\n",
      "Epoch 142: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.0095e-04 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9524\n",
      "Epoch 143/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 3.8842e-04 - accuracy: 1.0000\n",
      "Epoch 143: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.8760e-04 - accuracy: 1.0000 - val_loss: 0.2436 - val_accuracy: 0.9524\n",
      "Epoch 144/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 3.9979e-04 - accuracy: 1.0000\n",
      "Epoch 144: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.8249e-04 - accuracy: 1.0000 - val_loss: 0.2452 - val_accuracy: 0.9524\n",
      "Epoch 145/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 4.3224e-04 - accuracy: 1.0000\n",
      "Epoch 145: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.7110e-04 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 0.9524\n",
      "Epoch 146/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 4.1290e-04 - accuracy: 1.0000\n",
      "Epoch 146: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.5888e-04 - accuracy: 1.0000 - val_loss: 0.2475 - val_accuracy: 0.9524\n",
      "Epoch 147/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 2.5226e-04 - accuracy: 1.0000\n",
      "Epoch 147: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.5455e-04 - accuracy: 1.0000 - val_loss: 0.2478 - val_accuracy: 0.9524\n",
      "Epoch 148/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 3.2411e-04 - accuracy: 1.0000\n",
      "Epoch 148: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.3848e-04 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9524\n",
      "Epoch 149/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 3.5044e-04 - accuracy: 1.0000\n",
      "Epoch 149: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3516e-04 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.9524\n",
      "Epoch 150/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 3.0478e-04 - accuracy: 1.0000\n",
      "Epoch 150: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.2005e-04 - accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 0.9524\n",
      "Epoch 151/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.4753e-04 - accuracy: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 3.1249e-04 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9524\n",
      "Epoch 152/350\n",
      "22/49 [============>.................] - ETA: 0s - loss: 2.9981e-04 - accuracy: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.0076e-04 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 0.9524\n",
      "Epoch 153/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 2.5619e-04 - accuracy: 1.0000\n",
      "Epoch 153: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.9039e-04 - accuracy: 1.0000 - val_loss: 0.2537 - val_accuracy: 0.9524\n",
      "Epoch 154/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 2.8570e-04 - accuracy: 1.0000\n",
      "Epoch 154: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.8220e-04 - accuracy: 1.0000 - val_loss: 0.2551 - val_accuracy: 0.9524\n",
      "Epoch 155/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 3.0463e-04 - accuracy: 1.0000\n",
      "Epoch 155: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.7341e-04 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9524\n",
      "Epoch 156/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 3.0494e-04 - accuracy: 1.0000\n",
      "Epoch 156: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.6760e-04 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 0.9524\n",
      "Epoch 157/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 2.2556e-04 - accuracy: 1.0000\n",
      "Epoch 157: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.6041e-04 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 0.9524\n",
      "Epoch 158/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.6384e-04 - accuracy: 1.0000\n",
      "Epoch 158: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.5513e-04 - accuracy: 1.0000 - val_loss: 0.2588 - val_accuracy: 0.9524\n",
      "Epoch 159/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4761e-04 - accuracy: 1.0000\n",
      "Epoch 159: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 2.4761e-04 - accuracy: 1.0000 - val_loss: 0.2602 - val_accuracy: 0.9524\n",
      "Epoch 160/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.6701e-04 - accuracy: 1.0000\n",
      "Epoch 160: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.3727e-04 - accuracy: 1.0000 - val_loss: 0.2602 - val_accuracy: 0.9524\n",
      "Epoch 161/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 2.1917e-04 - accuracy: 1.0000\n",
      "Epoch 161: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 2.3219e-04 - accuracy: 1.0000 - val_loss: 0.2607 - val_accuracy: 0.9524\n",
      "Epoch 162/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 1.9597e-04 - accuracy: 1.0000\n",
      "Epoch 162: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.2895e-04 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 0.9524\n",
      "Epoch 163/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 2.0744e-04 - accuracy: 1.0000\n",
      "Epoch 163: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.1679e-04 - accuracy: 1.0000 - val_loss: 0.2633 - val_accuracy: 0.9524\n",
      "Epoch 164/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 1.6711e-04 - accuracy: 1.0000\n",
      "Epoch 164: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.1422e-04 - accuracy: 1.0000 - val_loss: 0.2641 - val_accuracy: 0.9524\n",
      "Epoch 165/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 1.7749e-04 - accuracy: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0519e-04 - accuracy: 1.0000 - val_loss: 0.2645 - val_accuracy: 0.9524\n",
      "Epoch 166/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 2.0370e-04 - accuracy: 1.0000\n",
      "Epoch 166: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0004e-04 - accuracy: 1.0000 - val_loss: 0.2656 - val_accuracy: 0.9524\n",
      "Epoch 167/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 1.3114e-04 - accuracy: 1.0000\n",
      "Epoch 167: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0354e-04 - accuracy: 1.0000 - val_loss: 0.2665 - val_accuracy: 0.9524\n",
      "Epoch 168/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.8136e-04 - accuracy: 1.0000\n",
      "Epoch 168: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8928e-04 - accuracy: 1.0000 - val_loss: 0.2674 - val_accuracy: 0.9524\n",
      "Epoch 169/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.7480e-04 - accuracy: 1.0000\n",
      "Epoch 169: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8363e-04 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 0.9524\n",
      "Epoch 170/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.6984e-04 - accuracy: 1.0000\n",
      "Epoch 170: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7779e-04 - accuracy: 1.0000 - val_loss: 0.2690 - val_accuracy: 0.9524\n",
      "Epoch 171/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.4393e-04 - accuracy: 1.0000\n",
      "Epoch 171: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.7293e-04 - accuracy: 1.0000 - val_loss: 0.2696 - val_accuracy: 0.9524\n",
      "Epoch 172/350\n",
      "20/49 [===========>..................] - ETA: 0s - loss: 2.1455e-04 - accuracy: 1.0000\n",
      "Epoch 172: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.6958e-04 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 0.9524\n",
      "Epoch 173/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.7808e-04 - accuracy: 1.0000\n",
      "Epoch 173: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6382e-04 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9524\n",
      "Epoch 174/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.4029e-04 - accuracy: 1.0000\n",
      "Epoch 174: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5948e-04 - accuracy: 1.0000 - val_loss: 0.2726 - val_accuracy: 0.9524\n",
      "Epoch 175/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.1734e-04 - accuracy: 1.0000\n",
      "Epoch 175: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5641e-04 - accuracy: 1.0000 - val_loss: 0.2729 - val_accuracy: 0.9524\n",
      "Epoch 176/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 1.4766e-04 - accuracy: 1.0000\n",
      "Epoch 176: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.5004e-04 - accuracy: 1.0000 - val_loss: 0.2738 - val_accuracy: 0.9524\n",
      "Epoch 177/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 1.2050e-04 - accuracy: 1.0000\n",
      "Epoch 177: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4582e-04 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9524\n",
      "Epoch 178/350\n",
      "22/49 [============>.................] - ETA: 0s - loss: 1.2560e-04 - accuracy: 1.0000\n",
      "Epoch 178: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.4194e-04 - accuracy: 1.0000 - val_loss: 0.2753 - val_accuracy: 0.9524\n",
      "Epoch 179/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 1.5557e-04 - accuracy: 1.0000\n",
      "Epoch 179: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.3655e-04 - accuracy: 1.0000 - val_loss: 0.2771 - val_accuracy: 0.9524\n",
      "Epoch 180/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 1.0750e-04 - accuracy: 1.0000\n",
      "Epoch 180: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3584e-04 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9524\n",
      "Epoch 181/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 1.0422e-04 - accuracy: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3129e-04 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.9524\n",
      "Epoch 182/350\n",
      "23/49 [=============>................] - ETA: 0s - loss: 1.5569e-04 - accuracy: 1.0000\n",
      "Epoch 182: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.2803e-04 - accuracy: 1.0000 - val_loss: 0.2787 - val_accuracy: 0.9524\n",
      "Epoch 183/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 1.1968e-04 - accuracy: 1.0000\n",
      "Epoch 183: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.2554e-04 - accuracy: 1.0000 - val_loss: 0.2795 - val_accuracy: 0.9524\n",
      "Epoch 184/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 8.4760e-05 - accuracy: 1.0000\n",
      "Epoch 184: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.2071e-04 - accuracy: 1.0000 - val_loss: 0.2801 - val_accuracy: 0.9524\n",
      "Epoch 185/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 1.0200e-04 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.1686e-04 - accuracy: 1.0000 - val_loss: 0.2806 - val_accuracy: 0.9524\n",
      "Epoch 186/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 1.2289e-04 - accuracy: 1.0000\n",
      "Epoch 186: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.1303e-04 - accuracy: 1.0000 - val_loss: 0.2820 - val_accuracy: 0.9524\n",
      "Epoch 187/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 1.2227e-04 - accuracy: 1.0000\n",
      "Epoch 187: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.1004e-04 - accuracy: 1.0000 - val_loss: 0.2824 - val_accuracy: 0.9524\n",
      "Epoch 188/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 1.1854e-04 - accuracy: 1.0000\n",
      "Epoch 188: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.0908e-04 - accuracy: 1.0000 - val_loss: 0.2831 - val_accuracy: 0.9524\n",
      "Epoch 189/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.1906e-04 - accuracy: 1.0000\n",
      "Epoch 189: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0409e-04 - accuracy: 1.0000 - val_loss: 0.2841 - val_accuracy: 0.9524\n",
      "Epoch 190/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 1.0541e-04 - accuracy: 1.0000\n",
      "Epoch 190: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 1.0200e-04 - accuracy: 1.0000 - val_loss: 0.2843 - val_accuracy: 0.9524\n",
      "Epoch 191/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.0386e-04 - accuracy: 1.0000\n",
      "Epoch 191: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 9.9969e-05 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.9524\n",
      "Epoch 192/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.0587e-04 - accuracy: 1.0000\n",
      "Epoch 192: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 9.7921e-05 - accuracy: 1.0000 - val_loss: 0.2865 - val_accuracy: 0.9524\n",
      "Epoch 193/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 1.0194e-04 - accuracy: 1.0000\n",
      "Epoch 193: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 9.5459e-05 - accuracy: 1.0000 - val_loss: 0.2871 - val_accuracy: 0.9524\n",
      "Epoch 194/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 9.2509e-05 - accuracy: 1.0000\n",
      "Epoch 194: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 9.0881e-05 - accuracy: 1.0000 - val_loss: 0.2880 - val_accuracy: 0.9524\n",
      "Epoch 195/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 8.3901e-05 - accuracy: 1.0000\n",
      "Epoch 195: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.9977e-05 - accuracy: 1.0000 - val_loss: 0.2881 - val_accuracy: 0.9524\n",
      "Epoch 196/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 9.0546e-05 - accuracy: 1.0000\n",
      "Epoch 196: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.6203e-05 - accuracy: 1.0000 - val_loss: 0.2893 - val_accuracy: 0.9524\n",
      "Epoch 197/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 6.6564e-05 - accuracy: 1.0000\n",
      "Epoch 197: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 8.5379e-05 - accuracy: 1.0000 - val_loss: 0.2901 - val_accuracy: 0.9524\n",
      "Epoch 198/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 8.7326e-05 - accuracy: 1.0000\n",
      "Epoch 198: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.2391e-05 - accuracy: 1.0000 - val_loss: 0.2899 - val_accuracy: 0.9524\n",
      "Epoch 199/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 8.0629e-05 - accuracy: 1.0000\n",
      "Epoch 199: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.0629e-05 - accuracy: 1.0000 - val_loss: 0.2910 - val_accuracy: 0.9524\n",
      "Epoch 200/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 8.2217e-05 - accuracy: 1.0000\n",
      "Epoch 200: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 7.7410e-05 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9524\n",
      "Epoch 201/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 7.1906e-05 - accuracy: 1.0000\n",
      "Epoch 201: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.6005e-05 - accuracy: 1.0000 - val_loss: 0.2931 - val_accuracy: 0.9524\n",
      "Epoch 202/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 7.9222e-05 - accuracy: 1.0000\n",
      "Epoch 202: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.3750e-05 - accuracy: 1.0000 - val_loss: 0.2935 - val_accuracy: 0.9524\n",
      "Epoch 203/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 7.3031e-05 - accuracy: 1.0000\n",
      "Epoch 203: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.1589e-05 - accuracy: 1.0000 - val_loss: 0.2943 - val_accuracy: 0.9524\n",
      "Epoch 204/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 8.2848e-05 - accuracy: 1.0000\n",
      "Epoch 204: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 7.0328e-05 - accuracy: 1.0000 - val_loss: 0.2947 - val_accuracy: 0.9524\n",
      "Epoch 205/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 5.5959e-05 - accuracy: 1.0000\n",
      "Epoch 205: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.9047e-05 - accuracy: 1.0000 - val_loss: 0.2961 - val_accuracy: 0.9524\n",
      "Epoch 206/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 6.9355e-05 - accuracy: 1.0000\n",
      "Epoch 206: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.5881e-05 - accuracy: 1.0000 - val_loss: 0.2961 - val_accuracy: 0.9524\n",
      "Epoch 207/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.7760e-05 - accuracy: 1.0000\n",
      "Epoch 207: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 6.5989e-05 - accuracy: 1.0000 - val_loss: 0.2971 - val_accuracy: 0.9524\n",
      "Epoch 208/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 6.3869e-05 - accuracy: 1.0000\n",
      "Epoch 208: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.2865e-05 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 0.9524\n",
      "Epoch 209/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 6.7153e-05 - accuracy: 1.0000\n",
      "Epoch 209: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 6.1032e-05 - accuracy: 1.0000 - val_loss: 0.2986 - val_accuracy: 0.9524\n",
      "Epoch 210/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 6.1200e-05 - accuracy: 1.0000\n",
      "Epoch 210: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.9510e-05 - accuracy: 1.0000 - val_loss: 0.2993 - val_accuracy: 0.9524\n",
      "Epoch 211/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 4.9076e-05 - accuracy: 1.0000\n",
      "Epoch 211: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.8148e-05 - accuracy: 1.0000 - val_loss: 0.3001 - val_accuracy: 0.9524\n",
      "Epoch 212/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 5.6221e-05 - accuracy: 1.0000\n",
      "Epoch 212: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.6591e-05 - accuracy: 1.0000 - val_loss: 0.3004 - val_accuracy: 0.9524\n",
      "Epoch 213/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 5.6546e-05 - accuracy: 1.0000\n",
      "Epoch 213: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.5021e-05 - accuracy: 1.0000 - val_loss: 0.3016 - val_accuracy: 0.9524\n",
      "Epoch 214/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 4.8003e-05 - accuracy: 1.0000\n",
      "Epoch 214: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.3638e-05 - accuracy: 1.0000 - val_loss: 0.3019 - val_accuracy: 0.9524\n",
      "Epoch 215/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 4.6848e-05 - accuracy: 1.0000\n",
      "Epoch 215: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.2532e-05 - accuracy: 1.0000 - val_loss: 0.3028 - val_accuracy: 0.9524\n",
      "Epoch 216/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 5.4462e-05 - accuracy: 1.0000\n",
      "Epoch 216: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.0805e-05 - accuracy: 1.0000 - val_loss: 0.3035 - val_accuracy: 0.9524\n",
      "Epoch 217/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 5.2627e-05 - accuracy: 1.0000\n",
      "Epoch 217: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.9408e-05 - accuracy: 1.0000 - val_loss: 0.3041 - val_accuracy: 0.9524\n",
      "Epoch 218/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 5.4343e-05 - accuracy: 1.0000\n",
      "Epoch 218: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.8212e-05 - accuracy: 1.0000 - val_loss: 0.3052 - val_accuracy: 0.9524\n",
      "Epoch 219/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 4.8912e-05 - accuracy: 1.0000\n",
      "Epoch 219: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.7406e-05 - accuracy: 1.0000 - val_loss: 0.3057 - val_accuracy: 0.9524\n",
      "Epoch 220/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 4.6014e-05 - accuracy: 1.0000\n",
      "Epoch 220: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 4.5860e-05 - accuracy: 1.0000 - val_loss: 0.3065 - val_accuracy: 0.9524\n",
      "Epoch 221/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 5.0694e-05 - accuracy: 1.0000\n",
      "Epoch 221: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.4969e-05 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9524\n",
      "Epoch 222/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.4834e-05 - accuracy: 1.0000\n",
      "Epoch 222: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.3254e-05 - accuracy: 1.0000 - val_loss: 0.3074 - val_accuracy: 0.9524\n",
      "Epoch 223/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.5199e-05 - accuracy: 1.0000\n",
      "Epoch 223: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.2281e-05 - accuracy: 1.0000 - val_loss: 0.3085 - val_accuracy: 0.9524\n",
      "Epoch 224/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 4.3252e-05 - accuracy: 1.0000\n",
      "Epoch 224: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.1332e-05 - accuracy: 1.0000 - val_loss: 0.3093 - val_accuracy: 0.9524\n",
      "Epoch 225/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 4.3032e-05 - accuracy: 1.0000\n",
      "Epoch 225: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.0755e-05 - accuracy: 1.0000 - val_loss: 0.3099 - val_accuracy: 0.9524\n",
      "Epoch 226/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 4.2870e-05 - accuracy: 1.0000\n",
      "Epoch 226: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.9076e-05 - accuracy: 1.0000 - val_loss: 0.3109 - val_accuracy: 0.9524\n",
      "Epoch 227/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.7013e-05 - accuracy: 1.0000\n",
      "Epoch 227: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.9168e-05 - accuracy: 1.0000 - val_loss: 0.3122 - val_accuracy: 0.9524\n",
      "Epoch 228/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 2.9662e-05 - accuracy: 1.0000\n",
      "Epoch 228: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.6695e-05 - accuracy: 1.0000 - val_loss: 0.3114 - val_accuracy: 0.9524\n",
      "Epoch 229/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.5642e-05 - accuracy: 1.0000\n",
      "Epoch 229: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.5930e-05 - accuracy: 1.0000 - val_loss: 0.3126 - val_accuracy: 0.9524\n",
      "Epoch 230/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.8642e-05 - accuracy: 1.0000\n",
      "Epoch 230: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.4974e-05 - accuracy: 1.0000 - val_loss: 0.3134 - val_accuracy: 0.9524\n",
      "Epoch 231/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 3.6200e-05 - accuracy: 1.0000\n",
      "Epoch 231: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.4336e-05 - accuracy: 1.0000 - val_loss: 0.3140 - val_accuracy: 0.9524\n",
      "Epoch 232/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.5428e-05 - accuracy: 1.0000\n",
      "Epoch 232: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3805e-05 - accuracy: 1.0000 - val_loss: 0.3148 - val_accuracy: 0.9524\n",
      "Epoch 233/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.2266e-05 - accuracy: 1.0000\n",
      "Epoch 233: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.2499e-05 - accuracy: 1.0000 - val_loss: 0.3155 - val_accuracy: 0.9524\n",
      "Epoch 234/350\n",
      "23/49 [=============>................] - ETA: 0s - loss: 2.9751e-05 - accuracy: 1.0000\n",
      "Epoch 234: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.1412e-05 - accuracy: 1.0000 - val_loss: 0.3164 - val_accuracy: 0.9524\n",
      "Epoch 235/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 2.7613e-05 - accuracy: 1.0000\n",
      "Epoch 235: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.0914e-05 - accuracy: 1.0000 - val_loss: 0.3169 - val_accuracy: 0.9524\n",
      "Epoch 236/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.9012e-05 - accuracy: 1.0000\n",
      "Epoch 236: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.0048e-05 - accuracy: 1.0000 - val_loss: 0.3178 - val_accuracy: 0.9524\n",
      "Epoch 237/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 2.3099e-05 - accuracy: 1.0000\n",
      "Epoch 237: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.9646e-05 - accuracy: 1.0000 - val_loss: 0.3183 - val_accuracy: 0.9524\n",
      "Epoch 238/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 2.6509e-05 - accuracy: 1.0000\n",
      "Epoch 238: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.8982e-05 - accuracy: 1.0000 - val_loss: 0.3187 - val_accuracy: 0.9524\n",
      "Epoch 239/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 2.5498e-05 - accuracy: 1.0000\n",
      "Epoch 239: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.7608e-05 - accuracy: 1.0000 - val_loss: 0.3189 - val_accuracy: 0.9524\n",
      "Epoch 240/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 2.8152e-05 - accuracy: 1.0000\n",
      "Epoch 240: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.6929e-05 - accuracy: 1.0000 - val_loss: 0.3201 - val_accuracy: 0.9524\n",
      "Epoch 241/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.8577e-05 - accuracy: 1.0000\n",
      "Epoch 241: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.6321e-05 - accuracy: 1.0000 - val_loss: 0.3202 - val_accuracy: 0.9524\n",
      "Epoch 242/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.5298e-05 - accuracy: 1.0000\n",
      "Epoch 242: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.5198e-05 - accuracy: 1.0000 - val_loss: 0.3213 - val_accuracy: 0.9524\n",
      "Epoch 243/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 2.4519e-05 - accuracy: 1.0000\n",
      "Epoch 243: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.4557e-05 - accuracy: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.9524\n",
      "Epoch 244/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 2.6171e-05 - accuracy: 1.0000\n",
      "Epoch 244: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.3924e-05 - accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 0.9524\n",
      "Epoch 245/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 2.1330e-05 - accuracy: 1.0000\n",
      "Epoch 245: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.3306e-05 - accuracy: 1.0000 - val_loss: 0.3236 - val_accuracy: 0.9524\n",
      "Epoch 246/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.2681e-05 - accuracy: 1.0000\n",
      "Epoch 246: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.2680e-05 - accuracy: 1.0000 - val_loss: 0.3241 - val_accuracy: 0.9524\n",
      "Epoch 247/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 1.8956e-05 - accuracy: 1.0000\n",
      "Epoch 247: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.2235e-05 - accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 0.9524\n",
      "Epoch 248/350\n",
      "23/49 [=============>................] - ETA: 0s - loss: 1.6557e-05 - accuracy: 1.0000\n",
      "Epoch 248: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.1346e-05 - accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9524\n",
      "Epoch 249/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 1.7619e-05 - accuracy: 1.0000\n",
      "Epoch 249: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.1331e-05 - accuracy: 1.0000 - val_loss: 0.3261 - val_accuracy: 0.9524\n",
      "Epoch 250/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 2.1096e-05 - accuracy: 1.0000\n",
      "Epoch 250: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0607e-05 - accuracy: 1.0000 - val_loss: 0.3269 - val_accuracy: 0.9524\n",
      "Epoch 251/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.8113e-05 - accuracy: 1.0000\n",
      "Epoch 251: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.0073e-05 - accuracy: 1.0000 - val_loss: 0.3271 - val_accuracy: 0.9524\n",
      "Epoch 252/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.7138e-05 - accuracy: 1.0000\n",
      "Epoch 252: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.9354e-05 - accuracy: 1.0000 - val_loss: 0.3281 - val_accuracy: 0.9524\n",
      "Epoch 253/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.8788e-05 - accuracy: 1.0000\n",
      "Epoch 253: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8778e-05 - accuracy: 1.0000 - val_loss: 0.3288 - val_accuracy: 0.9524\n",
      "Epoch 254/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 1.3672e-05 - accuracy: 1.0000\n",
      "Epoch 254: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.8530e-05 - accuracy: 1.0000 - val_loss: 0.3290 - val_accuracy: 0.9524\n",
      "Epoch 255/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.9255e-05 - accuracy: 1.0000\n",
      "Epoch 255: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8244e-05 - accuracy: 1.0000 - val_loss: 0.3305 - val_accuracy: 0.9524\n",
      "Epoch 256/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.8560e-05 - accuracy: 1.0000\n",
      "Epoch 256: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.7442e-05 - accuracy: 1.0000 - val_loss: 0.3308 - val_accuracy: 0.9524\n",
      "Epoch 257/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.9109e-05 - accuracy: 1.0000\n",
      "Epoch 257: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7195e-05 - accuracy: 1.0000 - val_loss: 0.3316 - val_accuracy: 0.9524\n",
      "Epoch 258/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 1.5583e-05 - accuracy: 1.0000\n",
      "Epoch 258: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6552e-05 - accuracy: 1.0000 - val_loss: 0.3318 - val_accuracy: 0.9524\n",
      "Epoch 259/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 1.6539e-05 - accuracy: 1.0000\n",
      "Epoch 259: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6193e-05 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 0.9524\n",
      "Epoch 260/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.4213e-05 - accuracy: 1.0000\n",
      "Epoch 260: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5700e-05 - accuracy: 1.0000 - val_loss: 0.3333 - val_accuracy: 0.9524\n",
      "Epoch 261/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.6524e-05 - accuracy: 1.0000\n",
      "Epoch 261: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5273e-05 - accuracy: 1.0000 - val_loss: 0.3336 - val_accuracy: 0.9524\n",
      "Epoch 262/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.4988e-05 - accuracy: 1.0000\n",
      "Epoch 262: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4934e-05 - accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 0.9524\n",
      "Epoch 263/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.5764e-05 - accuracy: 1.0000\n",
      "Epoch 263: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.4547e-05 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.9524\n",
      "Epoch 264/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 1.3833e-05 - accuracy: 1.0000\n",
      "Epoch 264: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4132e-05 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9524\n",
      "Epoch 265/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.5373e-05 - accuracy: 1.0000\n",
      "Epoch 265: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3790e-05 - accuracy: 1.0000 - val_loss: 0.3367 - val_accuracy: 0.9524\n",
      "Epoch 266/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 1.4118e-05 - accuracy: 1.0000\n",
      "Epoch 266: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3378e-05 - accuracy: 1.0000 - val_loss: 0.3376 - val_accuracy: 0.9524\n",
      "Epoch 267/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 1.2972e-05 - accuracy: 1.0000\n",
      "Epoch 267: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.3104e-05 - accuracy: 1.0000 - val_loss: 0.3381 - val_accuracy: 0.9524\n",
      "Epoch 268/350\n",
      "25/49 [==============>...............] - ETA: 0s - loss: 1.5905e-05 - accuracy: 1.0000\n",
      "Epoch 268: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.2857e-05 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9524\n",
      "Epoch 269/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 1.2462e-05 - accuracy: 1.0000\n",
      "Epoch 269: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.2602e-05 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9524\n",
      "Epoch 270/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 1.2544e-05 - accuracy: 1.0000\n",
      "Epoch 270: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.2083e-05 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.9524\n",
      "Epoch 271/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.2732e-05 - accuracy: 1.0000\n",
      "Epoch 271: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1882e-05 - accuracy: 1.0000 - val_loss: 0.3410 - val_accuracy: 0.9524\n",
      "Epoch 272/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.2399e-05 - accuracy: 1.0000\n",
      "Epoch 272: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1578e-05 - accuracy: 1.0000 - val_loss: 0.3411 - val_accuracy: 0.9524\n",
      "Epoch 273/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.1236e-05 - accuracy: 1.0000\n",
      "Epoch 273: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1252e-05 - accuracy: 1.0000 - val_loss: 0.3417 - val_accuracy: 0.9524\n",
      "Epoch 274/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.1839e-05 - accuracy: 1.0000\n",
      "Epoch 274: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1060e-05 - accuracy: 1.0000 - val_loss: 0.3427 - val_accuracy: 0.9524\n",
      "Epoch 275/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 1.0475e-05 - accuracy: 1.0000\n",
      "Epoch 275: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0656e-05 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9524\n",
      "Epoch 276/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.1046e-05 - accuracy: 1.0000\n",
      "Epoch 276: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0528e-05 - accuracy: 1.0000 - val_loss: 0.3436 - val_accuracy: 0.9524\n",
      "Epoch 277/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 8.8946e-06 - accuracy: 1.0000\n",
      "Epoch 277: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.0239e-05 - accuracy: 1.0000 - val_loss: 0.3445 - val_accuracy: 0.9524\n",
      "Epoch 278/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.0739e-05 - accuracy: 1.0000\n",
      "Epoch 278: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.8802e-06 - accuracy: 1.0000 - val_loss: 0.3451 - val_accuracy: 0.9524\n",
      "Epoch 279/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.0216e-05 - accuracy: 1.0000\n",
      "Epoch 279: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.6744e-06 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 0.9524\n",
      "Epoch 280/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 9.5692e-06 - accuracy: 1.0000\n",
      "Epoch 280: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.4026e-06 - accuracy: 1.0000 - val_loss: 0.3463 - val_accuracy: 0.9524\n",
      "Epoch 281/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 8.2910e-06 - accuracy: 1.0000\n",
      "Epoch 281: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.2064e-06 - accuracy: 1.0000 - val_loss: 0.3476 - val_accuracy: 0.9524\n",
      "Epoch 282/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 8.7688e-06 - accuracy: 1.0000\n",
      "Epoch 282: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.8696e-06 - accuracy: 1.0000 - val_loss: 0.3477 - val_accuracy: 0.9524\n",
      "Epoch 283/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 9.2036e-06 - accuracy: 1.0000\n",
      "Epoch 283: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.7260e-06 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.9524\n",
      "Epoch 284/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 7.6224e-06 - accuracy: 1.0000\n",
      "Epoch 284: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 8.5300e-06 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.9524\n",
      "Epoch 285/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 8.8643e-06 - accuracy: 1.0000\n",
      "Epoch 285: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.2009e-06 - accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 0.9524\n",
      "Epoch 286/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 6.8122e-06 - accuracy: 1.0000\n",
      "Epoch 286: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.1327e-06 - accuracy: 1.0000 - val_loss: 0.3497 - val_accuracy: 0.9524\n",
      "Epoch 287/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 8.4111e-06 - accuracy: 1.0000\n",
      "Epoch 287: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.1430e-06 - accuracy: 1.0000 - val_loss: 0.3509 - val_accuracy: 0.9524\n",
      "Epoch 288/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 7.7178e-06 - accuracy: 1.0000\n",
      "Epoch 288: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.6597e-06 - accuracy: 1.0000 - val_loss: 0.3515 - val_accuracy: 0.9524\n",
      "Epoch 289/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 6.8747e-06 - accuracy: 1.0000\n",
      "Epoch 289: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.4056e-06 - accuracy: 1.0000 - val_loss: 0.3529 - val_accuracy: 0.9524\n",
      "Epoch 290/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 7.1802e-06 - accuracy: 1.0000\n",
      "Epoch 290: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.2665e-06 - accuracy: 1.0000 - val_loss: 0.3531 - val_accuracy: 0.9524\n",
      "Epoch 291/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 5.5728e-06 - accuracy: 1.0000\n",
      "Epoch 291: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 7.1374e-06 - accuracy: 1.0000 - val_loss: 0.3537 - val_accuracy: 0.9524\n",
      "Epoch 292/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.3575e-06 - accuracy: 1.0000\n",
      "Epoch 292: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.8277e-06 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.9524\n",
      "Epoch 293/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 7.2572e-06 - accuracy: 1.0000\n",
      "Epoch 293: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.7354e-06 - accuracy: 1.0000 - val_loss: 0.3548 - val_accuracy: 0.9524\n",
      "Epoch 294/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 5.6710e-06 - accuracy: 1.0000\n",
      "Epoch 294: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.6366e-06 - accuracy: 1.0000 - val_loss: 0.3554 - val_accuracy: 0.9524\n",
      "Epoch 295/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 6.2486e-06 - accuracy: 1.0000\n",
      "Epoch 295: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.5772e-06 - accuracy: 1.0000 - val_loss: 0.3561 - val_accuracy: 0.9524\n",
      "Epoch 296/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.0331e-06 - accuracy: 1.0000\n",
      "Epoch 296: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.2956e-06 - accuracy: 1.0000 - val_loss: 0.3566 - val_accuracy: 0.9524\n",
      "Epoch 297/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.3237e-06 - accuracy: 1.0000\n",
      "Epoch 297: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 6.0345e-06 - accuracy: 1.0000 - val_loss: 0.3575 - val_accuracy: 0.9524\n",
      "Epoch 298/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 6.1259e-06 - accuracy: 1.0000\n",
      "Epoch 298: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.8883e-06 - accuracy: 1.0000 - val_loss: 0.3580 - val_accuracy: 0.9524\n",
      "Epoch 299/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.1277e-06 - accuracy: 1.0000\n",
      "Epoch 299: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.7553e-06 - accuracy: 1.0000 - val_loss: 0.3587 - val_accuracy: 0.9524\n",
      "Epoch 300/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 5.8315e-06 - accuracy: 1.0000\n",
      "Epoch 300: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.6060e-06 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.9524\n",
      "Epoch 301/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 5.6977e-06 - accuracy: 1.0000\n",
      "Epoch 301: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.5048e-06 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.9524\n",
      "Epoch 302/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 5.6239e-06 - accuracy: 1.0000\n",
      "Epoch 302: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.2713e-06 - accuracy: 1.0000 - val_loss: 0.3604 - val_accuracy: 0.9524\n",
      "Epoch 303/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 5.3054e-06 - accuracy: 1.0000\n",
      "Epoch 303: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.1996e-06 - accuracy: 1.0000 - val_loss: 0.3611 - val_accuracy: 0.9524\n",
      "Epoch 304/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 5.1259e-06 - accuracy: 1.0000\n",
      "Epoch 304: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.0706e-06 - accuracy: 1.0000 - val_loss: 0.3618 - val_accuracy: 0.9524\n",
      "Epoch 305/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 5.1515e-06 - accuracy: 1.0000\n",
      "Epoch 305: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.9181e-06 - accuracy: 1.0000 - val_loss: 0.3621 - val_accuracy: 0.9524\n",
      "Epoch 306/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.8552e-06 - accuracy: 1.0000\n",
      "Epoch 306: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.7994e-06 - accuracy: 1.0000 - val_loss: 0.3627 - val_accuracy: 0.9524\n",
      "Epoch 307/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 4.5810e-06 - accuracy: 1.0000\n",
      "Epoch 307: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.6997e-06 - accuracy: 1.0000 - val_loss: 0.3635 - val_accuracy: 0.9464\n",
      "Epoch 308/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 4.7000e-06 - accuracy: 1.0000\n",
      "Epoch 308: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.5639e-06 - accuracy: 1.0000 - val_loss: 0.3639 - val_accuracy: 0.9464\n",
      "Epoch 309/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.8591e-06 - accuracy: 1.0000\n",
      "Epoch 309: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.4842e-06 - accuracy: 1.0000 - val_loss: 0.3647 - val_accuracy: 0.9464\n",
      "Epoch 310/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 4.0064e-06 - accuracy: 1.0000\n",
      "Epoch 310: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.3473e-06 - accuracy: 1.0000 - val_loss: 0.3653 - val_accuracy: 0.9464\n",
      "Epoch 311/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 4.0059e-06 - accuracy: 1.0000\n",
      "Epoch 311: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.2282e-06 - accuracy: 1.0000 - val_loss: 0.3659 - val_accuracy: 0.9464\n",
      "Epoch 312/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 4.5992e-06 - accuracy: 1.0000\n",
      "Epoch 312: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.0817e-06 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 0.9464\n",
      "Epoch 313/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.3696e-06 - accuracy: 1.0000\n",
      "Epoch 313: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.0772e-06 - accuracy: 1.0000 - val_loss: 0.3671 - val_accuracy: 0.9524\n",
      "Epoch 314/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.6570e-06 - accuracy: 1.0000\n",
      "Epoch 314: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.9245e-06 - accuracy: 1.0000 - val_loss: 0.3674 - val_accuracy: 0.9464\n",
      "Epoch 315/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.4546e-06 - accuracy: 1.0000\n",
      "Epoch 315: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.8216e-06 - accuracy: 1.0000 - val_loss: 0.3685 - val_accuracy: 0.9464\n",
      "Epoch 316/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 3.9638e-06 - accuracy: 1.0000\n",
      "Epoch 316: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.7542e-06 - accuracy: 1.0000 - val_loss: 0.3691 - val_accuracy: 0.9464\n",
      "Epoch 317/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.7180e-06 - accuracy: 1.0000\n",
      "Epoch 317: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.6577e-06 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 0.9464\n",
      "Epoch 318/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.3925e-06 - accuracy: 1.0000\n",
      "Epoch 318: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.5525e-06 - accuracy: 1.0000 - val_loss: 0.3709 - val_accuracy: 0.9464\n",
      "Epoch 319/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.5394e-06 - accuracy: 1.0000\n",
      "Epoch 319: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.4502e-06 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 0.9464\n",
      "Epoch 320/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 2.9789e-06 - accuracy: 1.0000\n",
      "Epoch 320: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3723e-06 - accuracy: 1.0000 - val_loss: 0.3714 - val_accuracy: 0.9464\n",
      "Epoch 321/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.4551e-06 - accuracy: 1.0000\n",
      "Epoch 321: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3233e-06 - accuracy: 1.0000 - val_loss: 0.3722 - val_accuracy: 0.9464\n",
      "Epoch 322/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.3023e-06 - accuracy: 1.0000\n",
      "Epoch 322: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.2045e-06 - accuracy: 1.0000 - val_loss: 0.3730 - val_accuracy: 0.9464\n",
      "Epoch 323/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 3.1193e-06 - accuracy: 1.0000\n",
      "Epoch 323: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.1193e-06 - accuracy: 1.0000 - val_loss: 0.3732 - val_accuracy: 0.9464\n",
      "Epoch 324/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 2.7965e-06 - accuracy: 1.0000\n",
      "Epoch 324: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.0370e-06 - accuracy: 1.0000 - val_loss: 0.3739 - val_accuracy: 0.9464\n",
      "Epoch 325/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.2229e-06 - accuracy: 1.0000\n",
      "Epoch 325: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.9547e-06 - accuracy: 1.0000 - val_loss: 0.3748 - val_accuracy: 0.9464\n",
      "Epoch 326/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.0980e-06 - accuracy: 1.0000\n",
      "Epoch 326: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.8990e-06 - accuracy: 1.0000 - val_loss: 0.3754 - val_accuracy: 0.9464\n",
      "Epoch 327/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.1322e-06 - accuracy: 1.0000\n",
      "Epoch 327: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.8062e-06 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.9464\n",
      "Epoch 328/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.8040e-06 - accuracy: 1.0000\n",
      "Epoch 328: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.7760e-06 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.9464\n",
      "Epoch 329/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 2.9983e-06 - accuracy: 1.0000\n",
      "Epoch 329: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.6784e-06 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 0.9464\n",
      "Epoch 330/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.8975e-06 - accuracy: 1.0000\n",
      "Epoch 330: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.6132e-06 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9464\n",
      "Epoch 331/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 2.7116e-06 - accuracy: 1.0000\n",
      "Epoch 331: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.5368e-06 - accuracy: 1.0000 - val_loss: 0.3781 - val_accuracy: 0.9464\n",
      "Epoch 332/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 2.5718e-06 - accuracy: 1.0000\n",
      "Epoch 332: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.4704e-06 - accuracy: 1.0000 - val_loss: 0.3789 - val_accuracy: 0.9524\n",
      "Epoch 333/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 2.1020e-06 - accuracy: 1.0000\n",
      "Epoch 333: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 2.4261e-06 - accuracy: 1.0000 - val_loss: 0.3799 - val_accuracy: 0.9524\n",
      "Epoch 334/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 2.2278e-06 - accuracy: 1.0000\n",
      "Epoch 334: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.3477e-06 - accuracy: 1.0000 - val_loss: 0.3803 - val_accuracy: 0.9524\n",
      "Epoch 335/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 2.3773e-06 - accuracy: 1.0000\n",
      "Epoch 335: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.2979e-06 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 0.9524\n",
      "Epoch 336/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 2.3264e-06 - accuracy: 1.0000\n",
      "Epoch 336: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.2369e-06 - accuracy: 1.0000 - val_loss: 0.3816 - val_accuracy: 0.9524\n",
      "Epoch 337/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.2962e-06 - accuracy: 1.0000\n",
      "Epoch 337: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.2035e-06 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 0.9524\n",
      "Epoch 338/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 2.4008e-06 - accuracy: 1.0000\n",
      "Epoch 338: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.1333e-06 - accuracy: 1.0000 - val_loss: 0.3828 - val_accuracy: 0.9524\n",
      "Epoch 339/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0788e-06 - accuracy: 1.0000\n",
      "Epoch 339: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0788e-06 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 0.9524\n",
      "Epoch 340/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 2.0718e-06 - accuracy: 1.0000\n",
      "Epoch 340: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.0356e-06 - accuracy: 1.0000 - val_loss: 0.3840 - val_accuracy: 0.9524\n",
      "Epoch 341/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.8599e-06 - accuracy: 1.0000\n",
      "Epoch 341: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.9799e-06 - accuracy: 1.0000 - val_loss: 0.3845 - val_accuracy: 0.9524\n",
      "Epoch 342/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.8692e-06 - accuracy: 1.0000\n",
      "Epoch 342: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.9158e-06 - accuracy: 1.0000 - val_loss: 0.3855 - val_accuracy: 0.9524\n",
      "Epoch 343/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 1.9430e-06 - accuracy: 1.0000\n",
      "Epoch 343: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8829e-06 - accuracy: 1.0000 - val_loss: 0.3856 - val_accuracy: 0.9524\n",
      "Epoch 344/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.8979e-06 - accuracy: 1.0000\n",
      "Epoch 344: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8490e-06 - accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9524\n",
      "Epoch 345/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.8822e-06 - accuracy: 1.0000\n",
      "Epoch 345: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7849e-06 - accuracy: 1.0000 - val_loss: 0.3874 - val_accuracy: 0.9524\n",
      "Epoch 346/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 1.6634e-06 - accuracy: 1.0000\n",
      "Epoch 346: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7283e-06 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 0.9524\n",
      "Epoch 347/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.6622e-06 - accuracy: 1.0000\n",
      "Epoch 347: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7039e-06 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 0.9524\n",
      "Epoch 348/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.7334e-06 - accuracy: 1.0000\n",
      "Epoch 348: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6557e-06 - accuracy: 1.0000 - val_loss: 0.3886 - val_accuracy: 0.9524\n",
      "Epoch 349/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.6179e-06 - accuracy: 1.0000\n",
      "Epoch 349: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.6118e-06 - accuracy: 1.0000 - val_loss: 0.3896 - val_accuracy: 0.9524\n",
      "Epoch 350/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.5798e-06 - accuracy: 1.0000\n",
      "Epoch 350: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5654e-06 - accuracy: 1.0000 - val_loss: 0.3899 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGzCAYAAAD9pBdvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjbElEQVR4nO3deVxU9f7H8dfMAAPIpqIsivu+m1tqqRWlWZat2qrWrdtiy7W6Zoup/coWKyvNllvZri1qZWWpablgmnuauATiBu6AINvM+f1xYBRFBQQPMO/n48ED5nvOzHzOMDpvvuf7/R6bYRgGIiIiIhaxW12AiIiIeDeFEREREbGUwoiIiIhYSmFERERELKUwIiIiIpZSGBERERFLKYyIiIiIpRRGRERExFIKIyIiImIphRGRYho6dCgNGjQo1X3HjBmDzWYr24IqmMTERGw2G1OnTj2nz7tw4UJsNhsLFy70tBX3d1VeNTdo0IChQ4eW6WMWx9SpU7HZbCQmJp7z5xY5GwojUunZbLZifR3/YSVytpYuXcqYMWM4fPiw1aWIVHo+VhcgcrY++eSTQrc//vhj5s6de1J7y5Ytz+p53nvvPdxud6nu+9RTT/H444+f1fNL8Z3N76q4li5dytixYxk6dChhYWGFtsXHx2O36289keJSGJFK79Zbby10e9myZcydO/ek9hNlZmYSGBhY7Ofx9fUtVX0APj4++Pjon9u5cja/q7LgdDotfX6RykbRXbxCnz59aNOmDStXrqRXr14EBgbyxBNPAPDtt99yxRVXEB0djdPppHHjxjz77LO4XK5Cj3HiOISC8QYTJkzg3XffpXHjxjidTrp06cKKFSsK3beoMSM2m43hw4cza9Ys2rRpg9PppHXr1syZM+ek+hcuXEjnzp3x9/encePGvPPOO8Ueh7Jo0SJuuOEG6tWrh9PpJCYmhv/85z8cPXr0pOMLCgpi165dDBw4kKCgIGrVqsWjjz560mtx+PBhhg4dSmhoKGFhYQwZMqRYpyv+/PNPbDYbH3300Unbfv75Z2w2G7NnzwZg+/bt3HfffTRv3pyAgABq1qzJDTfcUKzxEEWNGSluzevWrWPo0KE0atQIf39/IiMjueOOOzhw4IBnnzFjxvDYY48B0LBhQ8+pwILaihoz8s8//3DDDTdQo0YNAgMDOf/88/nhhx8K7VMw/uXLL7/kueeeo27duvj7+3PJJZewdevWMx73qbz11lu0bt0ap9NJdHQ0999//0nHvmXLFq677joiIyPx9/enbt26DB48mNTUVM8+c+fO5YILLiAsLIygoCCaN2/u+Xckcjb0p5p4jQMHDnD55ZczePBgbr31ViIiIgBz0F9QUBAjRowgKCiIX3/9ldGjR5OWlsbLL798xsf9/PPPSU9P59///jc2m42XXnqJa6+9ln/++eeMf6EvXryYGTNmcN999xEcHMwbb7zBddddR1JSEjVr1gRg9erV9OvXj6ioKMaOHYvL5WLcuHHUqlWrWMf91VdfkZmZyb333kvNmjVZvnw5b775Jjt37uSrr74qtK/L5aJv375069aNCRMmMG/ePF555RUaN27MvffeC4BhGFx99dUsXryYe+65h5YtWzJz5kyGDBlyxlo6d+5Mo0aN+PLLL0/af/r06VSvXp2+ffsCsGLFCpYuXcrgwYOpW7cuiYmJTJkyhT59+rBx48YS9WqVpOa5c+fyzz//MGzYMCIjI9mwYQPvvvsuGzZsYNmyZdhsNq699lo2b97MF198wWuvvUZ4eDjAKX8nKSkp9OjRg8zMTB588EFq1qzJRx99xFVXXcXXX3/NNddcU2j/F154AbvdzqOPPkpqaiovvfQSt9xyC3/88Uexj7nAmDFjGDt2LLGxsdx7773Ex8czZcoUVqxYwZIlS/D19SUnJ4e+ffuSnZ3NAw88QGRkJLt27WL27NkcPnyY0NBQNmzYwJVXXkm7du0YN24cTqeTrVu3smTJkhLXJHISQ6SKuf/++40T39q9e/c2AOPtt98+af/MzMyT2v79738bgYGBRlZWlqdtyJAhRv369T23ExISDMCoWbOmcfDgQU/7t99+awDG999/72l75plnTqoJMPz8/IytW7d62tauXWsAxptvvulpGzBggBEYGGjs2rXL07ZlyxbDx8fnpMcsSlHHN378eMNmsxnbt28vdHyAMW7cuEL7duzY0ejUqZPn9qxZswzAeOmllzxteXl5xoUXXmgAxocffnjaekaNGmX4+voWes2ys7ONsLAw44477jht3XFxcQZgfPzxx562BQsWGICxYMGCQsdy/O+qJDUX9bxffPGFARi///67p+3ll182ACMhIeGk/evXr28MGTLEc/vhhx82AGPRokWetvT0dKNhw4ZGgwYNDJfLVehYWrZsaWRnZ3v2ff311w3AWL9+/UnPdbwPP/ywUE179+41/Pz8jMsuu8zzHIZhGJMmTTIA44MPPjAMwzBWr15tAMZXX311ysd+7bXXDMDYt2/faWsQKQ2dphGv4XQ6GTZs2EntAQEBnp/T09PZv38/F154IZmZmWzatOmMjzto0CCqV6/uuX3hhRcCZrf8mcTGxtK4cWPP7Xbt2hESEuK5r8vlYt68eQwcOJDo6GjPfk2aNOHyyy8/4+ND4ePLyMhg//799OjRA8MwWL169Un733PPPYVuX3jhhYWO5ccff8THx8fTUwLgcDh44IEHilXPoEGDyM3NZcaMGZ62X375hcOHDzNo0KAi687NzeXAgQM0adKEsLAwVq1aVaznKk3Nxz9vVlYW+/fv5/zzzwco8fMe//xdu3blggsu8LQFBQVx9913k5iYyMaNGwvtP2zYMPz8/Dy3S/KeOt68efPIycnh4YcfLjSg9q677iIkJMRzmig0NBQwT5VlZmYW+VgFg3S//fbbch8cLN5HYUS8Rp06dQr9B19gw4YNXHPNNYSGhhISEkKtWrU8g1+PP19+KvXq1St0uyCYHDp0qMT3Lbh/wX337t3L0aNHadKkyUn7FdVWlKSkJIYOHUqNGjU840B69+4NnHx8/v7+J51qOL4eMMdyREVFERQUVGi/5s2bF6ue9u3b06JFC6ZPn+5pmz59OuHh4Vx88cWetqNHjzJ69GhiYmJwOp2Eh4dTq1YtDh8+XKzfy/FKUvPBgwd56KGHiIiIICAggFq1atGwYUOgeO+HUz1/Uc9VMMNr+/bthdrP5j114vPCycfp5+dHo0aNPNsbNmzIiBEj+N///kd4eDh9+/Zl8uTJhY530KBB9OzZk3/9619EREQwePBgvvzySwUTKRMaMyJe4/i/eAscPnyY3r17ExISwrhx42jcuDH+/v6sWrWKkSNHFus/WofDUWS7YRjlet/icLlcXHrppRw8eJCRI0fSokULqlWrxq5duxg6dOhJx3eqesraoEGDeO6559i/fz/BwcF899133HTTTYVmHD3wwAN8+OGHPPzww3Tv3p3Q0FBsNhuDBw8u1w/AG2+8kaVLl/LYY4/RoUMHgoKCcLvd9OvX75x98Jb3+6Ior7zyCkOHDuXbb7/ll19+4cEHH2T8+PEsW7aMunXrEhAQwO+//86CBQv44YcfmDNnDtOnT+fiiy/ml19+OWfvHamaFEbEqy1cuJADBw4wY8YMevXq5WlPSEiwsKpjateujb+/f5EzKYozu2L9+vVs3ryZjz76iNtvv93TPnfu3FLXVL9+febPn8+RI0cK9TTEx8cX+zEGDRrE2LFj+eabb4iIiCAtLY3BgwcX2ufrr79myJAhvPLKK562rKysUi0yVtyaDx06xPz58xk7diyjR4/2tG/ZsuWkxyzJirr169cv8vUpOA1Yv379Yj9WSRQ8bnx8PI0aNfK05+TkkJCQQGxsbKH927ZtS9u2bXnqqadYunQpPXv25O233+b//u//ALDb7VxyySVccsklvPrqqzz//PM8+eSTLFiw4KTHEikJnaYRr1bw19zxf3Hm5OTw1ltvWVVSIQ6Hg9jYWGbNmsXu3bs97Vu3buWnn34q1v2h8PEZhsHrr79e6pr69+9PXl4eU6ZM8bS5XC7efPPNYj9Gy5Ytadu2LdOnT2f69OlERUUVCoMFtZ/YE/Dmm2+eNM24LGsu6vUCmDhx4kmPWa1aNYBihaP+/fuzfPly4uLiPG0ZGRm8++67NGjQgFatWhX3UEokNjYWPz8/3njjjULH9P7775OamsoVV1wBQFpaGnl5eYXu27ZtW+x2O9nZ2YB5+upEHTp0APDsI1Ja6hkRr9ajRw+qV6/OkCFDePDBB7HZbHzyySfl2h1eUmPGjOGXX36hZ8+e3HvvvbhcLiZNmkSbNm1Ys2bNae/bokULGjduzKOPPsquXbsICQnhm2++KfHYg+MNGDCAnj178vjjj5OYmEirVq2YMWNGicdTDBo0iNGjR+Pv78+dd9550oqlV155JZ988gmhoaG0atWKuLg45s2b55nyXB41h4SE0KtXL1566SVyc3OpU6cOv/zyS5E9ZZ06dQLgySefZPDgwfj6+jJgwABPSDne448/zhdffMHll1/Ogw8+SI0aNfjoo49ISEjgm2++KbfVWmvVqsWoUaMYO3Ys/fr146qrriI+Pp633nqLLl26eMZG/frrrwwfPpwbbriBZs2akZeXxyeffILD4eC6664DYNy4cfz+++9cccUV1K9fn7179/LWW29Rt27dQgNzRUpDYUS8Ws2aNZk9ezaPPPIITz31FNWrV+fWW2/lkksu8ax3YbVOnTrx008/8eijj/L0008TExPDuHHj+Pvvv88428fX15fvv//ec/7f39+fa665huHDh9O+fftS1WO32/nuu+94+OGH+fTTT7HZbFx11VW88sordOzYsdiPM2jQIJ566ikyMzMLzaIp8Prrr+NwOPjss8/IysqiZ8+ezJs3r1S/l5LU/Pnnn/PAAw8wefJkDMPgsssu46effio0mwmgS5cuPPvss7z99tvMmTMHt9tNQkJCkWEkIiKCpUuXMnLkSN58802ysrJo164d33//vad3oryMGTOGWrVqMWnSJP7zn/9Qo0YN7r77bp5//nnPOjjt27enb9++fP/99+zatYvAwEDat2/PTz/95JlJdNVVV5GYmMgHH3zA/v37CQ8Pp3fv3owdO9YzG0ektGxGRfoTUESKbeDAgWzYsKHI8QwiIpWJxoyIVAInLt2+ZcsWfvzxR/r06WNNQSIiZUg9IyKVQFRUlOd6Kdu3b2fKlClkZ2ezevVqmjZtanV5IiJnRWNGRCqBfv368cUXX5CcnIzT6aR79+48//zzCiIiUiWoZ0REREQspTEjIiIiYimFEREREbFUpRgz4na72b17N8HBwSVagllERESsYxgG6enpREdHn3Zxv0oRRnbv3k1MTIzVZYiIiEgp7Nixg7p1655ye6UII8HBwYB5MCEhIRZXIyIiIsWRlpZGTEyM53P8VCpFGCk4NRMSEqIwIiIiUsmcaYiFBrCKiIiIpRRGRERExFIKIyIiImKpSjFmREREyo5hGOTl5eFyuawuRSo5h8OBj4/PWS+7oTAiIuJFcnJy2LNnD5mZmVaXIlVEYGAgUVFR+Pn5lfoxFEZERLyE2+0mISEBh8NBdHQ0fn5+WkhSSs0wDHJycti3bx8JCQk0bdr0tAubnY7CiIiIl8jJycHtdhMTE0NgYKDV5UgVEBAQgK+vL9u3bycnJwd/f/9SPY4GsIqIeJnS/vUqUpSyeD/pHSkiIiKWUhgRERERSymMiIiI12nQoAETJ04s9v4LFy7EZrNx+PDhcqsJYOrUqYSFhZXrc1RECiMiIlJh2Wy2036NGTOmVI+7YsUK7r777mLv36NHD/bs2UNoaGipnk9Oz6tn07y/OIEdBzO5qWs9mkee/oqCIiJy7u3Zs8fz8/Tp0xk9ejTx8fGetqCgIM/PhmHgcrnw8TnzR1utWrVKVIefnx+RkZEluo8Un1f3jMxet5upSxPZfiDD6lJERM45wzDIzMmz5MswjGLVGBkZ6fkKDQ3FZrN5bm/atIng4GB++uknOnXqhNPpZPHixWzbto2rr76aiIgIgoKC6NKlC/PmzSv0uCeeprHZbPzvf//jmmuuITAwkKZNm/Ldd995tp94mqbgdMrPP/9My5YtCQoKol+/foXCU15eHg8++CBhYWHUrFmTkSNHMmTIEAYOHFii39OUKVNo3Lgxfn5+NG/enE8++aTQ73DMmDHUq1cPp9NJdHQ0Dz74oGf7W2+9RdOmTfH39yciIoLrr7++RM99rnh1z4gjf7EfdzH/UYiIVCVHc120Gv2zJc+9cVxfAv3K5iPo8ccfZ8KECTRq1Ijq1auzY8cO+vfvz3PPPYfT6eTjjz9mwIABxMfHU69evVM+ztixY3nppZd4+eWXefPNN7nlllvYvn07NWrUKHL/zMxMJkyYwCeffILdbufWW2/l0Ucf5bPPPgPgxRdf5LPPPuPDDz+kZcuWvP7668yaNYuLLrqo2Mc2c+ZMHnroISZOnEhsbCyzZ89m2LBh1K1bl4suuohvvvmG1157jWnTptG6dWuSk5NZu3YtAH/++ScPPvggn3zyCT169ODgwYMsWrSoBK/suePVYcRuLwgjFhciIiKlNm7cOC699FLP7Ro1atC+fXvP7WeffZaZM2fy3XffMXz48FM+ztChQ7npppsAeP7553njjTdYvnw5/fr1K3L/3Nxc3n77bRo3bgzA8OHDGTdunGf7m2++yahRo7jmmmsAmDRpEj/++GOJjm3ChAkMHTqU++67D4ARI0awbNkyJkyYwEUXXURSUhKRkZHExsbi6+tLvXr16Nq1KwBJSUlUq1aNK6+8kuDgYOrXr0/Hjh1L9PznineHkfxVkF1KIyLihQJ8HWwc19ey5y4rnTt3LnT7yJEjjBkzhh9++IE9e/aQl5fH0aNHSUpKOu3jtGvXzvNztWrVCAkJYe/evafcPzAw0BNEAKKiojz7p6amkpKS4gkGYF5UrlOnTrjd7mIf299//33SQNuePXvy+uuvA3DDDTcwceJEGjVqRL9+/ejfvz8DBgzAx8eHSy+9lPr163u29evXz3MaqqLx6jEjDrtO04iI97LZbAT6+VjyVZbXxKlWrVqh248++igzZ87k+eefZ9GiRaxZs4a2bduSk5Nz2sfx9fU96fU5XXAoav/ijoUpKzExMcTHx/PWW28REBDAfffdR69evcjNzSU4OJhVq1bxxRdfEBUVxejRo2nfvn25T08uDa8OI/b8fwzqGRERqTqWLFnC0KFDueaaa2jbti2RkZEkJiae0xpCQ0OJiIhgxYoVnjaXy8WqVatK9DgtW7ZkyZIlhdqWLFlCq1atPLcDAgIYMGAAb7zxBgsXLiQuLo7169cD4OPjQ2xsLC+99BLr1q0jMTGRX3/99SyOrHx49Wkah8aMiIhUOU2bNmXGjBkMGDAAm83G008/XaJTI2XlgQceYPz48TRp0oQWLVrw5ptvcujQoRL1Cj322GPceOONdOzYkdjYWL7//ntmzJjhmR00depUXC4X3bp1IzAwkE8//ZSAgADq16/P7Nmz+eeff+jVqxfVq1fnxx9/xO1207x58/I65FLz6jBS0DPiVhoREakyXn31Ve644w569OhBeHg4I0eOJC0t7ZzXMXLkSJKTk7n99ttxOBzcfffd9O3bF4ej+ONlBg4cyOuvv86ECRN46KGHaNiwIR9++CF9+vQBICwsjBdeeIERI0bgcrlo27Yt33//PTVr1iQsLIwZM2YwZswYsrKyaNq0KV988QWtW7cupyMuPZtxrk9wlUJaWhqhoaGkpqYSEhJSZo/7r4/+ZN7fKYy/ti03dT31dC8RkaogKyuLhIQEGjZsWOpLvUvpud1uWrZsyY033sizzz5rdTll5nTvq+J+fnt1z4gjf8SMxoyIiEhZ2759O7/88gu9e/cmOzubSZMmkZCQwM0332x1aRWOVw9gLRgzUgk6h0REpJKx2+1MnTqVLl260LNnT9avX8+8efNo2bKl1aVVOF7dM2LTbBoRESknMTExJ82EkaJ5d89IQRhRFhEREbGMd4cRu2bTiIiIWM2rw4hn0TONGREREbGMl4cR87uWgxcREbGOV4cRnaYRERGxnleHEbu9YDaNxYWIiIh4Ma8OIw6NGRER8Qp9+vTh4Ycf9txu0KABEydOPO19bDYbs2bNOuvnLqvHOZ0xY8bQoUOHcn2O8uTdYUSLnomIVGgDBgygX79+RW5btGgRNpuNdevWlfhxV6xYwd1333225RVyqkCwZ88eLr/88jJ9rqrGq8NIwYUTteiZiEjFdOeddzJ37lx27tx50rYPP/yQzp07065duxI/bq1atQgMDCyLEs8oMjISp9N5Tp6rsvLqMKLTNCLi1QwDcjKs+Srm/7tXXnkltWrVYurUqYXajxw5wldffcWdd97JgQMHuOmmm6hTpw6BgYG0bduWL7744rSPe+Jpmi1bttCrVy/8/f1p1aoVc+fOPek+I0eOpFmzZgQGBtKoUSOefvppcnNzAZg6dSpjx45l7dq12Gw2bDabp+YTT9OsX7+eiy++mICAAGrWrMndd9/NkSNHPNuHDh3KwIEDmTBhAlFRUdSsWZP777/f81zF4Xa7GTduHHXr1sXpdNKhQwfmzJnj2Z6Tk8Pw4cOJiorC39+f+vXrM378eMA8WzBmzBjq1auH0+kkOjqaBx98sNjPXRpevRy8ZtOIiFfLzYTno6157id2g1+1M+7m4+PD7bffztSpU3nyySc9l/H46quvcLlc3HTTTRw5coROnToxcuRIQkJC+OGHH7jtttto3LgxXbt2PeNzuN1urr32WiIiIvjjjz9ITU0tNL6kQHBwMFOnTiU6Opr169dz1113ERwczH//+18GDRrEX3/9xZw5c5g3bx4AoaGhJz1GRkYGffv2pXv37qxYsYK9e/fyr3/9i+HDhxcKXAsWLCAqKooFCxawdetWBg0aRIcOHbjrrrvOeDwAr7/+Oq+88grvvPMOHTt25IMPPuCqq65iw4YNNG3alDfeeIPvvvuOL7/8knr16rFjxw527NgBwDfffMNrr73GtGnTaN26NcnJyaxdu7ZYz1taXh1GCmbTKIuIiFRcd9xxBy+//DK//fYbffr0AcxTNNdddx2hoaGEhoby6KOPevZ/4IEH+Pnnn/nyyy+LFUbmzZvHpk2b+Pnnn4mONsPZ888/f9I4j6eeesrzc4MGDXj00UeZNm0a//3vfwkICCAoKAgfHx8iIyNP+Vyff/45WVlZfPzxx1SrZoaxSZMmMWDAAF588UUiIiIAqF69OpMmTcLhcNCiRQuuuOIK5s+fX+wwMmHCBEaOHMngwYMBePHFF1mwYAETJ05k8uTJJCUl0bRpUy644AJsNhv169f33DcpKYnIyEhiY2Px9fWlXr16xXodz4Z3hxGNGRERb+YbaPZQWPXcxdSiRQt69OjBBx98QJ8+fdi6dSuLFi1i3LhxALhcLp5//nm+/PJLdu3aRU5ODtnZ2cUeE/L3338TExPjCSIA3bt3P2m/6dOn88Ybb7Bt2zaOHDlCXl4eISEhxT6Ogudq3769J4gA9OzZE7fbTXx8vCeMtG7dGofD4dknKiqK9evXF+s50tLS2L17Nz179izU3rNnT08Px9ChQ7n00ktp3rw5/fr148orr+Syyy4D4IYbbmDixIk0atSIfv360b9/fwYMGICPT/lFBo0ZQSuwioiXstnMUyVWfBXMICimO++8k2+++Yb09HQ+/PBDGjduTO/evQF4+eWXef311xk5ciQLFixgzZo19O3bl5ycnDJ7qeLi4rjlllvo378/s2fPZvXq1Tz55JNl+hzH8/X1LXTbZrPhdpfdoljnnXceCQkJPPvssxw9epQbb7yR66+/HjCvNhwfH89bb71FQEAA9913H7169SrRmJWS8uowcmzRM4UREZGK7MYbb8Rut/P555/z8ccfc8cdd3jGjyxZsoSrr76aW2+9lfbt29OoUSM2b95c7Mdu2bIlO3bsYM+ePZ62ZcuWFdpn6dKl1K9fnyeffJLOnTvTtGlTtm/fXmgfPz8/XC7XGZ9r7dq1ZGRkeNqWLFmC3W6nefPmxa75dEJCQoiOjmbJkiWF2pcsWUKrVq0K7Tdo0CDee+89pk+fzjfffMPBgwcBCAgIYMCAAbzxxhssXLiQuLi4YvfMlIZXn6Y51jNicSEiInJaQUFBDBo0iFGjRpGWlsbQoUM925o2bcrXX3/N0qVLqV69Oq+++iopKSmFPnhPJzY2lmbNmjFkyBBefvll0tLSePLJJwvt07RpU5KSkpg2bRpdunThhx9+YObMmYX2adCgAQkJCaxZs4a6desSHBx80pTeW265hWeeeYYhQ4YwZswY9u3bxwMPPMBtt93mOUVTFh577DGeeeYZGjduTIcOHfjwww9Zs2YNn332GQCvvvoqUVFRdOzYEbvdzldffUVkZCRhYWFMnToVl8tFt27dCAwM5NNPPyUgIKDQuJKypp4RNJtGRKQyuPPOOzl06BB9+/YtNL7jqaee4rzzzqNv37706dOHyMhIBg4cWOzHtdvtzJw5k6NHj9K1a1f+9a9/8dxzzxXa56qrruI///kPw4cPp0OHDixdupSnn3660D7XXXcd/fr146KLLqJWrVpFTi8ODAzk559/5uDBg3Tp0oXrr7+eSy65hEmTJpXsxTiDBx98kBEjRvDII4/Qtm1b5syZw3fffUfTpk0Bc2bQSy+9ROfOnenSpQuJiYn8+OOP2O12wsLCeO+99+jZsyft2rVj3rx5fP/999SsWbNMazyezagEy4+mpaURGhpKampqiQcLnc6Uhdt4cc4mru9Ulwk3tC+zxxURqYiysrJISEigYcOG+Pv7W12OVBGne18V9/Pbq3tGHPlHr54RERER63h1GLFrBVYRERHLKYygAawiIiJW8uowouXgRURErFeqMDJ58mQaNGiAv78/3bp1Y/ny5cW637Rp07DZbCUa5VyetM6IiHijSjBvQSqRsng/lTiMTJ8+nREjRvDMM8+watUq2rdvT9++fdm7d+9p75eYmMijjz7KhRdeWOpiy5qu2isi3qRgVc/MzEyLK5GqpOD9dOKqsSVR4kXPXn31Ve666y6GDRsGwNtvv80PP/zABx98wOOPP17kfVwuF7fccgtjx45l0aJFHD58uNQFl6WC2TT6K0FEvIHD4SAsLMzzx2NgYKBnFVORkjIMg8zMTPbu3UtYWFiha+mUVInCSE5ODitXrmTUqFGeNrvdTmxsLHFxcae837hx46hduzZ33nknixYtOuPzZGdnk52d7bmdlpZWkjKLreAfoU7TiIi3KLii7Jl6s0WKKyws7LRXKi6OEoWR/fv343K5TlqyNiIigk2bNhV5n8WLF/P++++zZs2aYj/P+PHjGTt2bElKK5Vjp2nK/alERCoEm81GVFQUtWvXLtcLn4l38PX1PasekQLlem2a9PR0brvtNt577z3Cw8OLfb9Ro0YxYsQIz+20tDRiYmLKvD7NphERb+VwOMrkQ0SkLJQojISHh+NwOEhJSSnUnpKSUmQXzbZt20hMTGTAgAGetoJLIPv4+BAfH0/jxo1Pup/T6Tzp4kLlwXNtGo0ZERERsUyJZtP4+fnRqVMn5s+f72lzu93Mnz+f7t27n7R/ixYtWL9+PWvWrPF8XXXVVVx00UWsWbOmXHo7SiI/i2jMiIiIiIVKfJpmxIgRDBkyhM6dO9O1a1cmTpxIRkaGZ3bN7bffTp06dRg/fjz+/v60adOm0P3DwsIATmq3gsOmnhERERGrlTiMDBo0iH379jF69GiSk5Pp0KEDc+bM8QxqTUpKwm6vHAu7atEzERER69mMSrDIRnEvQVxS8zam8K+P/6RDTBiz7u9ZZo8rIiJSaWQehL0bIaYbOEq/cFlRivv5Xa6zaSq6gg4cnaYRERGv4MqDo4cgLwuSlsHvL8H+zea2+5ZB7ZaWlOXdYUSLnomIiDdwu+Gvr+GXp+FI8snbw+qbPSQW8eow4tCYERERqcoMA7bNh/njYM/aY+12HwiNgfaDods9EBBmWYng7WFEs2lERKSqKQgg88bAoSTITjXb/YLggv9AjwfAp/zX8ioJrw4jNk8YsbgQERGRs5F7FFZ/ao4D2fUnHEo8ts3hhK53wQUjoFpNy0o8Ha8OI1oOXkREKq2CHpA1X0DCb5Cx79g2hx90vhM6D4PgSPAPta7OYvDyMGJ+d+k0jYiIVBbZR2D7Ulj0CuxYdqw9NAY6DYHI9lC/BziDrKuxhLw6jGg2jYiIVArZ6RD3Fmz8Fvb9DYZ5nTd8/KHTUGh+OdTrAT5+lpZZWgojmD1dIiIiFUpeNuz8E3athCUTIfPAsW3B0dDySnMcSEiUZSWWFa8OI5raKyIiFYrbbQ5A3b4Elr8HabuObavRGHr/Fxr2rhIB5HheHUY8p2nUNSIiIlYxDEheB+u/gr9mFA4g1WqZq6K2vhY63lrmy7VXFF4dRjSbRkRELON2w6bZsPAF2LvhWLszBBr2gkZ9oONt4OtvWYnnipeHEfO7Fj0TEZFzIisNVn1kjgPZsQLSdprtDic07wdtb4Aml3pFADmeV4cRm2bTiIjIuXBoO6x4D1Z+fGxFVDDX/+hyF/QYDgHVravPYl4dRhxagVVERMpDToY5AHXZW5CbBTnpx6bjhjczT7+ENzNPxXhZL0hRvDuMaDaNiIiUpUPbYd10M4hk7C28rdFF5kXpml4Gdrs19VVQXh1G7HZdKE9ERM6S2wUpGyD+J1g0AVw5ZntYfeg9Eup0At8AqF7f2jorMO8OI2YWURgREZGSS15v9oKs/xrS9xxrr9/TPA3T9voqOxW3rHl1GHFoAKuIiJRE6i5zPZB1X548HTeyLZw3BNrdCPmfL1I8Xh1Gjp2mAcMwPLNrREREPLJSYeN3Zi9I4mIg/w9Yhx806wftBkHTS8HHaWmZlZlXhxHHceHDMBRkRUQE8wNh92rYtwm2/GKOBcnLOra9fk+z96PV1V49HbcseXUYsR+XPlyGgR2lERERr5WeDBtmwurPIGV94W3hzcwekHY3Qlg9a+qrwrw7jBw3s8rlNvB1WFeLiIhY4Ohh2DjLHIR6/CkY30Co2xki25mroka1V/d5OfLqMFKwzghoRo2IiNdwu2DbAljzGWz6AVzZx7bV7WrOgml3o07BnENeHUYKnabRjBoRkapt7yZY+zmsnQ5Hko+1125lho/W12otEIsojORTFhERqYIyD8Jf38Caz2H3qmPtATXM0y8dbtYpmArAq8NIodM0SiMiIlXDkb3gzjNDyPxnj52GsfuYS7F3uBma9gUfP2vrFA+vDiPHZRFcGjMiIlJ57V4Df34AyevMabnHi2hrBpC2N0BQLUvKk9Pz6jBis9mw28xTNOoZERGphNJ2myui/vrccQNRbWCzmz0hfZ+DLv/SaZgKzqvDCJinatwuQ2NGREQqC1ceHNgKf38Hv71onpIB89RL2xugwQX5M2EM8wJ1UuF5fRgxl4A3dJpGRKQiMww4sA0OJcLPo2D/5mPb6nSGDjdBpzsKLyAllYbXh5GCJeF1mkZEpIJx5cHS12F7nBk+Dm8/ts03EKo3hJ4Pmiuj6jRMpaYwYteVe0VEKpSDCeaKqFvnwo4/jrU7nBAUYZ6G6fscBNawrkYpU14fRgpm1GgFVhERCxkGbJ1nTsdd/zW4c81232pw0RNQswk0vBD8qllbp5QLhZH8NKIwIiJyjhkGbF8KCb/Btl9h54pj2xr2Mr9aXws1G1tXo5wTXh9GCsaMuNwWFyIi4i3cbtg8B+aNgf3xx9p9A6HjbdDmWqh3vmXlybnn9WHErjEjIiLl7/AOWP6OuTjZoURI3WG2+wVDi/7m1XHbXAsh0VZWKRbx+jDimU2j0zQiImUrY7956mX3Glj8Krhyjm1zhkDnO+DCEeAfalmJUjF4fRjRAFYRkTKWvB42/QhL34Sc9GPt9S+AjrdAtVrmjBgtSCb5FEZ0mkZE5OzlZsE/C+GPKeb3AjUaQXA0dBoKba/XeiBSJK8PIw7NphERKb2DCbDgeYj/EXKOmG12X2h8MbS+xlyQTKuiyhkojGg2jYhIybjdcCgBVn0Mf7wNeVlme3A0tBwA3e+D6g0sLVEqF68PIzaNGRERKZ7Mg7DifVjxHhxJOdbesBdcPBrqdFIviJSK14cRz2kajRkRESnMMMyr4/75AexaBXvWQt5Rc5uPP0R3hJ4PQbN+GgsiZ8Xrw4i94DSNekZEREyGYc6E+e2lwrNhAKLaQ48HodXV4PC1pj6pcrw+jOhCeSIi+ZLXw8qPYM+aY0uz2xzmYNT2gyG8qbk4mXpBpIx5fRgp6BlRx4iIeKXUnbD3b1jzOWyYcdwGG/R9HrrepR4QKXcKI+oZERFvtGO5eSpm02ww8qcT2uzQaqA5BiS6A9RqbmWF4kW8Pow48nsbNWZERKq81F2w72+Iewu2zT/WXru1eWXcXo9BVDvr6hOvpTCi2TQiUpWl7oK1X8D6r80gUsDuay5I1v1+iGhlXX0iKIx4xowoi4hIlXJgG/zyNGz+6bjTMA6zByT6POj9X/NnkQpAYURTe0WkqsjLNpdn3/gtLHoFXNlme/2e0OEWaNEfAqpbW6NIEbw+jOg0jYhUekf2wY4/4MdHIX3PsfZGF8HlL2ogqlR4Xh9GNJtGRCodtws2zoIt82D7YjicdGybXxCE1Ydej0Dra7UmiFQKXh9GHLo2jYhUFocSYf1XsO4r2B9/3AYbBEVAm2vh4qfBL9CqCkVKxevDyLEBrAojIlJB7VkHSybChpnHBqP6h0GnIeZF6mK6gTPYygpFzorCiOc0jcWFiIgcLzcLtvwCK6cWXhOkUR9zYbLW10BAmDW1iZQxrw8jDs2mEZGKICcDDu+A6g3MXpDl70LmAXObzW6Gj54PmReqE6liFEY0m0ZErLZzJXw91ByI6h8KWalme3A0tL0OOt8JNRpaWqJIefL6MGLTAFYRsYJhQMJvEDfZPB1TICsVAsPNKbmtBoLD6/+bFi/g9e9yh6b2isi5ZBjmQNRFr0LK+vxGG7S9HnqPNNcLaXoZBNW2tEyRc8m7w8iaL+i3fzlrba1xGy2srkZEqrJDibDtV/hrBiQuMtt8A6HjrXD+vVCjkdkW3tSyEkWs4t1hZMX/uHz/n3xje0SzaUSk7GXsh80/w4r3YPfqY+0OJ1zwHzj/Hi3PLoK3hxHfAAD8ydGYEREpO7lH4bcXYckbYLjMNrsP1O0KjS82T8loQKqIh700d5o8eTINGjTA39+fbt26sXz58lPuO2PGDDp37kxYWBjVqlWjQ4cOfPLJJ6UuuEz5OAFwkqvZNCJydgwDtsfBt8NhQjNY/JoZRCLawkVPwiPxcMdP0PsxBRGRE5S4Z2T69OmMGDGCt99+m27dujFx4kT69u1LfHw8tWufPOCqRo0aPPnkk7Ro0QI/Pz9mz57NsGHDqF27Nn379i2Tgyg1H38A/G05WmdEREondSdsnmNeKTfh92PtofWg3/PQcoB1tYlUEiUOI6+++ip33XUXw4YNA+Dtt9/mhx9+4IMPPuDxxx8/af8+ffoUuv3QQw/x0UcfsXjx4goTRtQzIiIl4nbB9qWwcwX8PgFyM8x2hx+0vRHaD4b6PcFeqs5nEa9TojCSk5PDypUrGTVqlKfNbrcTGxtLXFzcGe9vGAa//vor8fHxvPjii6fcLzs7m+zsbM/ttLS0kpRZfAU9I+SgLCIiZ5SVak7LjZsM+zcfa48+D5pcAh1vg+r1ratPpJIqURjZv38/LpeLiIiIQu0RERFs2rTplPdLTU2lTp06ZGdn43A4eOutt7j00ktPuf/48eMZO3ZsSUorHd/8nhFbLlk6TSMip5KeDBtmwcLxkHXYbPMPMy9Q1/RSc4VU9YKIlNo5mU0THBzMmjVrOHLkCPPnz2fEiBE0atTopFM4BUaNGsWIESM8t9PS0oiJiSn7wo47TZOprhEROd7BBDiwDVZ/bI4HKVCzKZx3O3QaCv4hlpUnUpWUKIyEh4fjcDhISUkp1J6SkkJkZOQp72e322nSpAkAHTp04O+//2b8+PGnDCNOpxOn01mS0krHE0ZytAKriJjysmHpG/Drc0DB/ws2iGxrhpDOd4DdYWWFIlVOicKIn58fnTp1Yv78+QwcOBAAt9vN/PnzGT58eLEfx+12FxoTYpnjwoiyiIgXy82CxMVmL8jmnyEvy2yv1cJcEbXPKIhobW2NIlVYiU/TjBgxgiFDhtC5c2e6du3KxIkTycjI8Myuuf3226lTpw7jx48HzPEfnTt3pnHjxmRnZ/Pjjz/yySefMGXKlLI9ktLwLZjam6tFz0S8kdsNy9+FX/8PctKPtQdFwMVPmT0hIlLuShxGBg0axL59+xg9ejTJycl06NCBOXPmeAa1JiUlYT9uIFdGRgb33XcfO3fuJCAggBYtWvDpp58yaNCgsjuK0tJpGhHvtXU+zHsGkvMvVhccBS2ugPOGmKdkCi7pLSLlzmYYFb9LIC0tjdDQUFJTUwkJKcMBY6s+ge+GM9/Vkfnnvcnz17Qtu8cWkYolKw1Wfwpbfoa8HEhaarb7BcOlY6DTHZoRI1LGivv57d3XpjmuZyRPV8oTqXoMw7xA3b5NMP9ZSN99bJvNDl3/Db3/C4E1rKtRRLw9jJgzdvxtueTkKYyIVBl5ORD/Ayx5vfDVcqs3hK53m+PFYrppUKpIBeHdYST/qr1OcsjKVRgRqRISl8Cse+BwknnbNxBqt4KGF0Kv/4JfoLX1ichJvDuMHHfV3qw8l8XFiEipHUqEfxbCtl+PLVAWFGEuz97tHgiqZWV1InIGXh5GzJ4Rf3LIVs+ISOWTngzL3zMXKXPl5Dfa4LzboO/z4Ay2tDwRKR4vDyP5PSM29YyIVBqGAas+gj/ehX1/g5H/h0TdrlDnPHNtEI0FEalUvDuM+KpnRKTSyMuGbQtg9Sewafax9phu0P1+aHmV1gYRqaS8O4xozIhIxZexHxa/Zq4LlJ1qttnscNGT0OEWCImytj4ROWteHkbye0ZsuWTnKIyIVAiGAfs3Q8pfsPNPWPkR5GaY24KjzB6QDjdDdAdLyxSRsuPlYeTYlYGNggtjiYh1cjJh2s3wz4LC7VHtzZ6QJpdqlVSRKsjLw4j/sZ8VRkSsdfAf+HY4bF8CDj+IbGcORG3WD5pfrvEgIlWYd4cRhy+GzY7NcCuMiFjB7Ya/v4WVUyFxMbjzwC8Ibp0B9bpZXZ2InCPeHUZsNrN3JDcThzsbl9vAYddfXyLlypUHW36BtV9A0jLI2HtsW+NLoN8LUKuZdfWJyDnn3WEEPGHESS7ZeS4C/fSSiJSblVNh4QuQvudYmzPEXCW1/WCo2diy0kTEOvrk9Vy5N5esXDeBfhbXI1IVpe2GZVPMlVIBAmuaM2JaXGkOTs1f80dEvJPXhxGbrxlG/MkhK1fTe0XK1JG98NuL8OcHx1ZK7f04XDii0Gw2EfFuXh9GPD0jtlyy87QKq0iZ2LnSDCFb5x63XHsX6Ho3tLvR2tpEpMJRGPFRz4hImdn7N8wdbQ5QLVCnE1zyDDTqbV1dIlKhKYwcN2ZEPSMipeB2QfxPsPknWDvNnJ5rc0D7m8zTMRqUKiJnoDCSf95aPSMipZD0B8y6x1ywrEDzK+CyZxVCRKTYFEbyR/FrzIhICRgGrJsO3z9kLhgYUB3aDTZXSm3YS6ulikiJKIyoZ0SkeA7+A+u+hJwjkPA77Flrtje7HK57D5zB1tYnIpWWwkj+lXvNdUYURkSKtG0BfDUEslKPtfkEwIWPwAX/AYf+KxGR0tP/IPk9IxrAKnKCg//AhplwKBFWfQIYEN0RYrpB7VbQrC8ER1pdpYhUAQoj+WNG/G05ZKtnRMScHTNvDMRNBuO4fxMdb4P+E8DX/5R3FREpDYUR9YyIHJOTAd/cBfE/mLcbXQThzaDxxdC8n7W1iUiVpTCSP2YkgGwOqmdEvNnBf+DrO2D3anA44dp3oPU1VlclIl5AYcQZBECQ7Si7c9UzIl4mLxuS/zKn6a6cCq5s8yJ2g7+Aet2srk5EvITCSEB1AELJIDtPPSPiJbLSYNEEWPG+OVW3QKOL4MrXoEZD62oTEa+jMOIfBkCoLYMs9YyIN8hOh0+ugV1/mrcDakCDntD5TmjURwuWicg5pzASEAZAiHpGpKrLSoOdy2HuGEhZb/YKXv2WuWqqAoiIWEhhRD0jUtW53bD4VVj4ArhzzTb/MLj1G/OKuiIiFlMYye8ZCSWDrJw8a2sRKWu7VsGcx2HHH+bt4Ghzim6fJyColrW1iYjkUxjJ7xnxsbmx5R45/b4ilYVhmKunzrjb7A3xDYT+L0PHW62uTETkJAojvgG47L443Ln4ZKeeeX+RiiwvB5a8DsvegqMHzbbm/eGKVyAk2traREROQWHEZiPPLxRH1n788tKsrkak9DIPwmc3HJslY/eFLndC3+fB7rC2NhGR01AYAVx+oZC1H98chRGphA4nmb0hm3+B1CTz1GP/l6HV1Z7LHYiIVGQKI4DbGQpAgCvd4kpESujv2fDt/ZB12LwdHAW3zYLaLaysSkSkRBRGAHf+IFanTtNIZZF9BL57ADbMMG9Hnwc9HzRXUM2fISYiUlkojAC2/P+8FUakUkjdCZ8PNhcuszmgxwNw0RM6JSMilZbCCOAIDAPAPy8dwzCwaTVKqai2x8FXQ+BIClSrZV7QLqaL1VWJiJwVhRHAt1oNwFwSPjPHRTWnXhapYPZvhaVvwKqPAQNqt4abp0FYPasrExE5a/rUBXyq5V+515bBkew8hRGpOAwD4ibDvGfAnb9CcIdboN8L4B9ibW0iImVEn7qALSA/jJBBelYeEfo/XiqCrDRzpszf35m3G18CvR6F+j2srUtEpIwpjMCx69PYMkjPyrW2FhGA9V/DL09B+h5z8bJ+46HLv3R1XRGpkhRGwLyUOuaYkV3ZulieWCg3C+aNgT+mmLerN4Dr3oe6na2sSkSkXCmMAATWBCDclkZ8lsKIWGTjd/DDCMjYZ97u9V+48BHw9be2LhGRcqYwAhAcCUCILZPMI2lAlLX1iPfZOg++HmYOUg2NMa8n0+oqq6sSETknFEYAnCFk2QLwN47iTtsDNLe6IvEWWWmw+FVY8gYYLmhzHVzzDjh8ra5MROScURgBsNlI96uFf3YS9vTdVlcj3iAnA5a/B0smwtFDZlub62DgFAUREfE6CiP5Mpy1qZWdhE/GHqtLkapuyzyY/TCk7jBv12wKl46FFldYWpaIiFUURvJl+deGNPDLTLG6FKnK1n0FM+4CDAitZ15Tpu0N4NA/RRHxXvofMF9OoDmINSB7r8WVSJWUkwlxk2DhC4ABHW6F/i+BXzWrKxMRsZzCSD5XkDmDJkhhRMraxm9hzhOQttO83f4muOpNsNutrUtEpIJQGCkQEm1+y91ncSFSZbhdMH8sLHndvB1S1xwb0uY6raQqInIchZF8jvwwUsO13+JKpNLbFw87lkP8j+YXQM+HoM8o8A2wtjYRkQpIYSSfb/U6AFQ3DoMrTwMKpXTSU+D9yyDrsHnb4YSrJ0O7GywtS0SkItMnbj7/6lHkGXZ8bG7I2Os5bSNSbLlHzSm7WYchpA5EtYcLRkBMF6srExGp0BRG8gUFOEmhOnU4gPvQDuwKI1Jce9bCivfh7+/MBczsPnDzdIhsa3VlIiKVgobz5wv292GHURuA7P3bLK5GKgXDMK+w+24fWPWRGURC68G17ymIiIiUgHpG8jl97OwggvP5m7z9/1hdjlQGC56Dxa+ZP7caCJ2GQMPeYHdYWpaISGWjMJLPZrOR4ogEA9wHEqwuRyqyXatg7mhIXGTeHvA6dBpqaUkiIpWZwshxDvrVgWywH060uhSpiNwuWDkV5owCV7Y5NuSiJxRERETOksLIcdICYiAbfNO2W12KVCSGYQ5O/fU52B9vtjXvD/0nQGgda2sTEakCSjWAdfLkyTRo0AB/f3+6devG8uXLT7nve++9x4UXXkj16tWpXr06sbGxp93fSllBMQD4Z+0zryUikpMJXw+DL283g0hAdbjsORj0mYKIiEgZKXEYmT59OiNGjOCZZ55h1apVtG/fnr59+7J3b9HXdFm4cCE33XQTCxYsIC4ujpiYGC677DJ27dp11sWXNb/gcFKNQPPGoURLa5EKIG03TO0PG2aC3Rd6/RceWgs9huu6MiIiZchmGIZRkjt069aNLl26MGnSJADcbjcxMTE88MADPP7442e8v8vlonr16kyaNInbb7+9yH2ys7PJzs723E5LSyMmJobU1FRCQkJKUm6JPDt7I1cvv5l29gQY/Dm0uKLcnksqsEOJ5imZzXMgOw0CasCgT6FBT6srExGpVNLS0ggNDT3j53eJ/rzLyclh5cqVxMbGHnsAu53Y2Fji4uKK9RiZmZnk5uZSo0aNU+4zfvx4QkNDPV8xMTElKbPUqgf6kmREmDcOanqv1zl6yLyo3ZQLYP2XZhCp3Rru+lVBRESkHJUojOzfvx+Xy0VERESh9oiICJKTk4v1GCNHjiQ6OrpQoDnRqFGjSE1N9Xzt2LGjJGWWWligH9uMKPPGvvhz8pxSQaTtNkPI3NGQkw4x58Mdv8A9i6BGQ6urExGp0s7pbJoXXniBadOmsXDhQvz9/U+5n9PpxOl0nsPKTNUD/Vjmrmve2LfpnD+/WORQInw1FNJ2Qlh96PUotL9ZF0sUETlHSvS/bXh4OA6Hg5SUlELtKSkpREZGnva+EyZM4IUXXmDevHm0a9eu5JWeA9UDfdlsFISReHNKp81mbVFSfrbHQdwkc2yIO88cG3L7t+oJERE5x0p0msbPz49OnToxf/58T5vb7Wb+/Pl07979lPd76aWXePbZZ5kzZw6dO3cufbXlrHo1PxKMKPJwmOMF0irejB8pA65cmD8OPrwcNs02g0iji2DI9woiIiIWKHE/9IgRIxgyZAidO3ema9euTJw4kYyMDIYNGwbA7bffTp06dRg/fjwAL774IqNHj+bzzz+nQYMGnrElQUFBBAUFleGhnL3qgX7k4kOCO5Km9l2wdxOE1rW6LClLBxPgm3/Brj/N2+1vhh4PQEQra+sSEfFiJQ4jgwYNYt++fYwePZrk5GQ6dOjAnDlzPINak5KSsB+3BsOUKVPIycnh+uuvL/Q4zzzzDGPGjDm76stYWKAvAPFGXZqyC/ZuhKanHmgrlUhOJqz+BH79P7PXyz8UBrwBrQdaXZmIiNcr1Qi94cOHM3z48CK3LVy4sNDtxMTE0jyFJfx9HQT4OtjirguOPzSItSo4shcWjof130B2qtkWcz5c9x6E1bO2NhERAXRtmpNUD/QlPj1/XZOUDdYWI6WXcQDWfAZLJkLmAbMtrL55SqbTMM2UERGpQPQ/8gmqV/NjY1p988bejeZgR4evtUVJ8RkGxE2GhS+Y64UARLSFvv8HDXppGXcRkQpIYeQE1QP92GDUJscnGL+8dNj7N0RVzKnIcgLDgLlPw9I3zdsRbaHrv8xBqj5+1tYmIiKnpDByAnMQq439wS2IPrQC9qxVGKkMtsw1p+smrzNv93sBuv5bPSEiIpWA/qc+QXiQufLrTv+mZsOetRZWI2dkGJC0DKbdbAYRuy9c/jKcf6+CiIhIJaGekRPUDjHDyGZHE7qCwkhFtj0Ovr4D0nebt5tfAVdPgsBTX4RRREQqHv3peIKIYPOaOWvz8gexJq8HV56FFUmRdq00e0MKgkjD3nDtuwoiIiKVkHpGTlDQM7I+MxycIeYCWfv+hsi2FlcmgHl13Rl3Q+Ii83Z0R7htFgSEWVmViIicBfWMnCAixOwZST6SA3XOMxt3rrCwIvHYMAvevtAMIg4/aH0N3DRdQUREpJJTGDlBwWmaw5m55EV1Mht3/mlhRUJuFsy6H74aApn7zSm79y2DG6ZCcITV1YmIyFnSaZoThAT44PSxk53n5lDNjtQC2LHc6rK8V3Y6fHo97FgGNjtc+Aj0egx8nFZXJiIiZURh5AQ2m42IEH+SDmayM7CVGUYObIHMgxoceS4d/AeWvQ3bfjVff/9QuOEjaHyR1ZWJiEgZ02maIkTkD2LdnRMINRqbjTpVc27kZJpX1p18Pix/51gQuf1bBRERkSpKPSNFqJ0/iDUlLQsaXAAHt8GWX6DZZRZXVsUd2g6fXGO+3gCN+kCHW8xpuxobIiJSZalnpAgFg1hT0rOgeX+zMf4nc7VPKR97N8EHfc0gElIHBn1qTtltd6OCiIhIFaeekSIUnKbZl5YNjXqDbyCk7TSXG49qb3F1VYxhmL1OM++BowehVgszhIREWV2ZiIicI+oZKULBwmfJaVngGwCNLzY3bPrRwqqqoB3L4cPL4fMbzSASfR4M+0lBRETEyyiMFCEqNACAPalZZoPnVM0PFlVUBW3+2TwtkxQHPv7Q40EY8r1mLImIeCGdpilCnTAzjOw6fBS328DerJ+5xkXyejicBGH1LK6wEtu1EuImm2HEcEPLAXD5SxASbXVlIiJiEfWMFCEy1B+7DXLy3OzPyIZqNSHmfHNj/Bxri6vMjuyFT6+Dv76BnCPQ4EK47gMFERERL6cwUgRfh91zjZpdh46ajS10quas7FgB3/wLjh6CiDZw4ydw6wzw8bO6MhERsZjCyCkcf6oGODZuJHExHD1sTVGVUVaqeZXd92Mh4Tew+8DAt6DVVQoiIiICKIycUp3q+WGkoGekZmMIbw7uPNg6z8LKKpHEJTClJ6ybbo65aXM9DJmt6dEiIlKIwsgpROf3jOwu6BmBY6dqNulUzWm5XTD3GZh6BaTugOoNYNgcuP59qN/d6upERKSCURg5hZNO0wA0v8L8vnUe5OVYUFUlkJcDM+6CJRMBAzreCvcshnrdrK5MREQqKE3tPYWC0zQ7Dx0XRup0gmq1IWOveTXZ5v0sqq4C2rsJtvwMKz8yl3S3+8A170Db662uTEREKjj1jJxC3aJ6Rux281opAKs/saCqCirpD3j7Apg72gwi1WrB4C8UREREpFgURk6hoGckPSuP1KO5xzZ0vM38Hv8TpKdYUFkFk7IBvhoC7lyo2wUuew4eWKUrHIuISLEpjJxCoJ8P4UHm1NMdBzOPbajdAup2BcMFqz+2qLoK4o93zB6R9D3mTKPbZkGP4eAfYnVlIiJSiSiMnEb9mtUASDyQUXhDlzvN73+8C7lH8Tr74uHPD+GnkflLul8Ft38LziCrKxMRkUpIYeQ06tcIBGD7gczCG9pcB6Ex5kDWNZ9bUJlFDu+AabfA5K4w+2HAgPOGwI0f60q7IiJSagojp1HQM7L9xJ4Rhy90H27+vPAF7xg7smGmuYDZptnmAmaRbaHLv6D/y2CzWV2diIhUYpraexoNws2ekcQTe0YAOg2BVR/B3o3wzZ1w+3fmbJuqxu2GBf8Hi14xb9ftAle9CbVbWluXiIhUGVXw07Ps1Ms/TZNUVBjxDTBPT/hWg8RFsP7Lc1zdOZCVCtNuPhZEejxgrqSqICIiImVIYeQ0GuSfpklOy+JojuvkHcKbQu/HzJ/njYEj+85dceVt9xp4+0LY/BM4nHDNu3DZ/4FDnWkiIlK2FEZOIyzQlxB/88M36WARvSMA3e6FsHrm9NaJbeCvb85hheUgaRls/hk+GQiHt5vHdsdP0H6Q1ZWJiEgVpTByGjab7dTTewv4+sMNH0FkO8jLMi8Q53afwyrL0LIp8EFf+PxGOHoIos+Dfy8yl8EXEREpJwojZ9Cktrl2xpaU9FPvVOc8uPMXcIaaV6lNXHSOqitDf82An58wf/YPM2fL3PwlBIRZWZWIiHgBhZEzaBEZDMDfyacJI2AOaG1zrflzZVp7JCcTvn8Yvh5mLmDW4RYYmWheaTeoltXViYiIF1AYOYMWUebS5pv2pJ155w43m9//+gYSfi/HqsrAwQSYeQ9M6gIrPzTbej4MA17XuiEiInJOKYycQcv8npGE/Rlk5RYxo+Z4dbtAq4HmReO+uBnWfQmGUf5FltTu1fD+pbD2C0jbCUERcNtMuHSsuaCbiIjIOaQwcga1gp3UqOaH24AtKUdOv7PNBte8Aw0uhJx0mHEXLHn93BRaXHv/ho8HQsY+c1zILV/DAyuh8cVWVyYiIl5KYeQMbDbbceNGinGqxtff7GW48BHz9m8vWb9c/Ja58PlgeKkxvHU+ZB2GOp1h2E/Q9FJwBltbn4iIeDWFkWJoEVkwbuQMg1gLOHzh4qfND/zcDJj79Lk/XXM4Cb5/CCZ3g8+uNxcvy9xvbqvbBW75SiFEREQqBC2nWQwtoswP7U3F6RkpYLNB3+fMdTvWTYfk9XD0MNz4EcR0LbxvbhY4/M7u2jZuN8x5HHycEN0RZt0HeUfNbXZf6PZvaH0thDcBZ4gGqYqISIWhMFIMLQt6RpLTMQwDW3E/yOudD1dOhNkPmxfUA3NRtDt+Mn/OyYTl78BvL0NgTbj4KWh3Y+mCwuY55mMdr35P8+rCdTpBcETJH1NEROQcUBgphqYRQdhtcDAjh31Hsqkd7F/8O3ceBsGR5lTfZW9B0lJzyfV98TB3tDl+AyA1A2beDWs/h0vHmWNNcjOh/wSo2djcJyvVnAljs0NMN0hcDDv+MLet/qzw87YcYK4Ma3ec9fGLiIiUJ4WRYvD3ddAwvBrb9mWwaU96ycIIQPPLza+cI7DqY/jydjiSP6g1rD70/i+kJ8PvL8M/C+GdXsfu+2Yn8A+FxhdBwqJj4z6cIZB9wmkjmx2ufQ/SdkHXuxVERESkUlAYKaYWUSFmGElOo1ezUq5M2nskbFsIqUnm7e7DzV6QgtDQ+hr47kHYvthc+yO8mbm0fNZh2DDT3Cc4Gtx5kLEX7D7Q9gYzyPyzADrfCW2vP9tDFREROacURoqpZWQwP6zbU/wZNUUJrQv3LYWlk8AvEHo8WHh8SM3GMOR7M4DUbmUux56eAocSYPWnEBKdP2XYBlt+hlotzQGpAEf2QWCNszpGERERKyiMFFPB9N4zXqPmTJzBcNGoU2+326FR72O3gyPMr3rnF96v5YDCt3UdGRERqaS0zkgxtYw2w8iWlHQyc/IsrkZERKTqUBgppuhQf6JC/clzG6xJOmx1OSIiIlWGwkgx2Ww2ujQwx2QsTzxocTUiIiJVh8JICXRpaIaRFQojIiIiZUZhpAS65veMrNp+mFyX2+JqREREqgaFkRJoWjuI0ABfjua62LC7BNepERERkVNSGCkBu91GlwbVAViRoFM1IiIiZUFhpIQ0iFVERKRsKYyUUMEg1j8TD+J2GxZXIyIiUvkpjJRQm+hQ/H3tHMrM5Z/9R6wuR0REpNJTGCkhPx87HWLCAFiecMjaYkRERKoAhZFS6NawJgBLt+23uBIREZHKr1RhZPLkyTRo0AB/f3+6devG8uXLT7nvhg0buO6662jQoAE2m42JEyeWttYK44Km4QAs2bpf40ZERETOUonDyPTp0xkxYgTPPPMMq1aton379vTt25e9e/cWuX9mZiaNGjXihRdeIDIy8qwLrgg6xIQR5PThUGau1hsRERE5SyUOI6+++ip33XUXw4YNo1WrVrz99tsEBgbywQcfFLl/ly5dePnllxk8eDBOp/OsC64IfB12zm9kzqpZtHWfxdWIiIhUbiUKIzk5OaxcuZLY2NhjD2C3ExsbS1xcXJkVlZ2dTVpaWqGviuaCJuapmsVbNG5ERETkbJQojOzfvx+Xy0VERESh9oiICJKTk8usqPHjxxMaGur5iomJKbPHLivdG5thZM2Ow7g0bkRERKTUKuRsmlGjRpGamur52rFjh9UlnaRJ7SCCnD5k5rjYnJJudTkiIiKVVonCSHh4OA6Hg5SUlELtKSkpZTo41el0EhISUuironHYbbSrGwrA6qTD1hYjIiJSiZUojPj5+dGpUyfmz5/vaXO73cyfP5/u3buXeXEVXcd6YQCs2aHFz0RERErLp6R3GDFiBEOGDKFz58507dqViRMnkpGRwbBhwwC4/fbbqVOnDuPHjwfMQa8bN270/Lxr1y7WrFlDUFAQTZo0KcNDOfc6xphX8FXPiIiISOmVOIwMGjSIffv2MXr0aJKTk+nQoQNz5szxDGpNSkrCbj/W4bJ79246duzouT1hwgQmTJhA7969Wbhw4dkfgYU65PeMbNl7hMOZOYQF+llbkIiISCVkMwyjwk8FSUtLIzQ0lNTU1Ao3fqTva78Tn5LOy9e344bOFW/Wj4iIiFWK+/ldIWfTVCb920YB8OP6PRZXIiIiUjkpjJylK9qZs4gWb91PamauxdWIiIhUPgojZ6lJ7WCaRwST6zL4ZWPZLfwmIiLiLRRGysAV7cxTNT/oVI2IiEiJKYyUgYJxI4u37OdwZo7F1YiIiFQuCiNloEntIFpEBpPnNvhlY8qZ7yAiIiIeCiNl5Ir83pEf1ulUjYiISEkojJSR/vnjRpZs1akaERGRklAYKSONax13qmaDTtWIiIgUl8JIGboyv3dktmbViIiIFJvCSBkqmFWzZOt+9qVnW1yNiIhI5aAwUoYa1QqifUwYLrfBt2t2WV2OiIhIpaAwUsZu6FQXgK/+3EkluAahiIiI5RRGytiA9tH4+diJT0ln9Y7DVpcjIiJS4SmMlLHQAF+uah8NwKRft1pcjYiISMWnMFIO7r+oCXYb/LppL+t2Hra6HBERkQpNYaQcNAyvxtUd6gAwdUmitcWIiIhUcAoj5eTmbvUA+GVjClm5LourERERqbgURspJp3rViQzx50h2Hr9t3md1OSIiIhWWwkg5sdttXJG/Iuv3a3dbXI2IiEjFpTBSjq7uYM6qmfNXMgn7MyyuRkREpGJSGClH7eqGcVHzWuS5DV7+eZPV5YiIiFRICiPl7PHLW2K3wY/rk1mVdMjqckRERCochZFy1jwymOvOM5eIf+HHTVoiXkRE5AQKI+fAiMua4fSxszzxIHP+Sra6HBERkQpFYeQciAoN4O5ejQB4+tu/OJiRY3FFIiIiFYfCyDky/OImNIsIYv+RHK55a4l6SERERPIpjJwjTh8Hrw3qQPVAX7YfyOT+z1eRkpZldVkiIiKWUxg5h1pHh7J45MW0qxuKy20we90eq0sSERGxnMLIOVbN6cO1Hc2L6H2nlVlFREQURqxwRbto7DZYu+Mw2w9oZVYREfFuCiMWqBXs5IKmtQCYujTR2mJEREQspjBikbsubAjAF8uTOHAk2+JqRERErKMwYpELmoTTtk4oWblunp29EZdbK7OKiIh3UhixiM1m47G+zbHbYNaa3Yyasc7qkkRERCyhMGKhXs1qMenm83DYbXz5506WbN1vdUkiIiLnnMKIxfq3jeK28+sDMOa7DWTnuSyuSERE5NxSGKkA/hPbjBrV/Niy9wgjv16nK/uKiIhXURipAEIDfXljcEd87DZmrdnNK79strokERGRc0ZhpIK4oGk4z1/bFoBJC7YyZeE2zbARERGvoDBSgdzYOYYHL24CwItzNnHjO3Fk5WoMiYiIVG0KIxXMfy5txrMD2xDs9GHl9kNM+Dne6pJERETKlcJIBWOz2bjt/Pq8flMHAN5fksCcv3R1XxERqboURiqoi1tEcOv59TAMuP/z1cxcvdPqkkRERMqFwkgFNvaqNlzbsQ4ut8F/pq9l4jzNshERkapHYaQCc9htTLihPff0bgzAxHlbeOe3bRZXJSIiUrYURio4u93G45e34In+LQAY/9Mmnp29UbNsRESkylAYqSTu7tWYh2ObAvD+4gQuePFXpi1PsrgqERGRs6cwUok8HNuMd27rRFSoP/uP5PD4jPVMnLdZy8eLiEilpjBSyfRtHcnv/72Ihy4xe0kmztvCM99tYG96lsWViYiIlI7CSCXk67Dzn0ubMfaq1gB8HLedrs/NZ9z3G9VLIiIilY7CSCU2pEcD3rrlPFpEBgPwwZIEXvlls65pIyIilYrNqAR/SqelpREaGkpqaiohISFWl1MhfbQ0kWe+2wBA2zqhDOoSw7Xn1SHQz8fiykRExFsV9/NbPSNVxJAeDXj+mrYEOX1YvyuVp2b9xeWvL2Lp1v1WlyYiInJa6hmpYvamZzFz1S6mLk1kT6o5qLVbwxrc07sx4UFOWkQF4+tQBhURkfJX3M9vhZEqKi0rlwk/xzNt+Q5yXG5Pe7OIIN4f0oWYGoEWViciIt5AYUQA2H34KFMWbmP+3ymkHs0lI8dFgK+D6zvVZWDHOnSMCcNut1ldpoiIVEEKI3KS5NQs7v98FSu3H/K0RYb407VhDdrVDeXW8+vj7+uwsEIREalKFEakSIZhsGTrAb5auYP5f+/lSHaeZ1uj8GoM7FiHzg2q0yEmTDNxRETkrCiMyBll5bqI23aAv5PTmLokkb3p2Z5tDruNdnVDublrPS5rFUlooK+FlYqISGWkMCIlkno0l2/X7GJF4iH+TDzomYlToGF4NTrWC+OCJuHUCQugeWQwYYF+FlUrIiKVgcKInJWdhzKZvW4P01fsIGF/xknb7TZzcbXmkcE0izCDSZ2wADrWC9O4ExERARRGpAwdyshh7c7DLPvnIH8mHiQlPYsdB48Wua+fj50OdcNoXLsadcICiA4LIDLEn4hQfyJD/Knm1DgUERFvoTAi5WrnoUzW7khlc0o62/YdIS0rj0170gqNOylKoJ+DWsFOmkUEExXqT41qftSo5kf1QD9qVvOjejU/agb5USPQDx8tziYiUqkV9/O7VH+mTp48mZdffpnk5GTat2/Pm2++SdeuXU+5/1dffcXTTz9NYmIiTZs25cUXX6R///6leWqpIOpWD6Ru9UCuIMrTZhgGCfszWLn9EDsPHWXX4aPsST1KcmoWKWnZHMnOIzPHxfYDmWw/kHnax7fZoEagH6GBvoQGHPsK8c//HuBDgK+DrFw3tYKd1A524vS14+dw4PS14/Sx4+djx+njwOlj3la4ERGpmEocRqZPn86IESN4++236datGxMnTqRv377Ex8dTu3btk/ZfunQpN910E+PHj+fKK6/k888/Z+DAgaxatYo2bdqUyUFIxWCz2WhUK4hGtYKK3J6elcvBjBx2HTrK5pR0DmTkcCAjh0MZORw87utQZg5uA8/2smK3YYaTIsKKX35gKXz71MHGedx2P4fdE4R8HDYc9vwvm/ndx3Hcz3Y7djuFvh+/v90Odpst/8t8TUVEqroSn6bp1q0bXbp0YdKkSQC43W5iYmJ44IEHePzxx0/af9CgQWRkZDB79mxP2/nnn0+HDh14++23i3yO7OxssrOPdfenpaURExOj0zRewuU2OJiRw770bFKP5pKWlWt+z/8y2/LIynXh52NnT2oWqZm5ZOe5yM5zk5PnJjvPTXaei1xXhT8LeVo2G2ZIsdnMn+3Hgord83P+7fx9bJghxnZcm91my2/P38Zx+xfctoMN87Hw7GNut9vMbeb+x+5bqFZsJ9V+6uM6tvHE3Y6/38nbine/E7ee/jGP33aa+53ueE/advz9bKfZVrr7nfx8x70up33MEtyvvF/Pk+6n4G21Oy9oWOaXCimX0zQ5OTmsXLmSUaNGedrsdjuxsbHExcUVeZ+4uDhGjBhRqK1v377MmjXrlM8zfvx4xo4dW5LSpApx2G3UCnZSK9h51o/ldhvkuNxk57o9YaUgqGTnHWs/FmBOuH3S9mP3y3Hl3851e0JQntuNy23gMgxcLoM8t4HbML+78r/y3Abu/O9nYhiQZxhA5Q5VIlLxXdUh2rLrlpUojOzfvx+Xy0VERESh9oiICDZt2lTkfZKTk4vcPzk5+ZTPM2rUqEIBpqBnRKSk7HYb/nZH/nTjirVwm2EYuA08IcVlmMHF7TbbT/zZ5TYwDDz7GYaBy31sG5DfbkYXz8+GgYEZbE7Zlr8/BhgYuN2FHwPMOgr2d5+mQ/XETcYJQer47SfvW/j1OdW2E7PZ8c9x+scs3f1O3Hjaxzyu4cRXqdCxn7SteK/paV/PUt7v5Oez7vWsrCr+VJAziwzxt+y5K+Q8S6fTidN59n8Vi1RkNpsNR/6pFxERb1ai6QXh4eE4HA5SUlIKtaekpBAZGVnkfSIjI0u0v4iIiHiXEoURPz8/OnXqxPz58z1tbreb+fPn07179yLv071790L7A8ydO/eU+4uIiIh3KfFpmhEjRjBkyBA6d+5M165dmThxIhkZGQwbNgyA22+/nTp16jB+/HgAHnroIXr37s0rr7zCFVdcwbRp0/jzzz959913y/ZIREREpFIqcRgZNGgQ+/btY/To0SQnJ9OhQwfmzJnjGaSalJSE3X6sw6VHjx58/vnnPPXUUzzxxBM0bdqUWbNmaY0RERERAbQcvIiIiJST4n5+a31sERERsZTCiIiIiFhKYUREREQspTAiIiIillIYEREREUspjIiIiIilFEZERETEUgojIiIiYqkKedXeExWsy5aWlmZxJSIiIlJcBZ/bZ1pftVKEkfT0dABiYmIsrkRERERKKj09ndDQ0FNurxTLwbvdbnbv3k1wcDA2m63MHjctLY2YmBh27NjhtcvMe/tr4O3HD3oNQK+Btx8/6DUor+M3DIP09HSio6MLXbfuRJWiZ8Rut1O3bt1ye/yQkBCvfPMdz9tfA28/ftBrAHoNvP34Qa9BeRz/6XpECmgAq4iIiFhKYUREREQs5dVhxOl08swzz+B0Oq0uxTLe/hp4+/GDXgPQa+Dtxw96Daw+/koxgFVERESqLq/uGRERERHrKYyIiIiIpRRGRERExFIKIyIiImIphRERERGxlFeHkcmTJ9OgQQP8/f3p1q0by5cvt7qkcjFmzBhsNluhrxYtWni2Z2Vlcf/991OzZk2CgoK47rrrSElJsbDis/f7778zYMAAoqOjsdlszJo1q9B2wzAYPXo0UVFRBAQEEBsby5YtWwrtc/DgQW655RZCQkIICwvjzjvv5MiRI+fwKErvTMc/dOjQk94T/fr1K7RPZT5+gPHjx9OlSxeCg4OpXbs2AwcOJD4+vtA+xXnvJyUlccUVVxAYGEjt2rV57LHHyMvLO5eHUirFOf4+ffqc9D645557Cu1TWY8fYMqUKbRr186zqmj37t356aefPNur8u8fznz8Fer3b3ipadOmGX5+fsYHH3xgbNiwwbjrrruMsLAwIyUlxerSytwzzzxjtG7d2tizZ4/na9++fZ7t99xzjxETE2PMnz/f+PPPP43zzz/f6NGjh4UVn70ff/zRePLJJ40ZM2YYgDFz5sxC21944QUjNDTUmDVrlrF27VrjqquuMho2bGgcPXrUs0+/fv2M9u3bG8uWLTMWLVpkNGnSxLjpppvO8ZGUzpmOf8iQIUa/fv0KvScOHjxYaJ/KfPyGYRh9+/Y1PvzwQ+Ovv/4y1qxZY/Tv39+oV6+eceTIEc8+Z3rv5+XlGW3atDFiY2ON1atXGz/++KMRHh5ujBo1yopDKpHiHH/v3r2Nu+66q9D7IDU11bO9Mh+/YRjGd999Z/zwww/G5s2bjfj4eOOJJ54wfH19jb/++sswjKr9+zeMMx9/Rfr9e20Y6dq1q3H//fd7brtcLiM6OtoYP368hVWVj2eeecZo3759kdsOHz5s+Pr6Gl999ZWn7e+//zYAIy4u7hxVWL5O/DB2u91GZGSk8fLLL3vaDh8+bDidTuOLL74wDMMwNm7caADGihUrPPv89NNPhs1mM3bt2nXOai8LpwojV1999SnvU5WOv8DevXsNwPjtt98Mwyjee//HH3807Ha7kZyc7NlnypQpRkhIiJGdnX1uD+AsnXj8hmF+GD300EOnvE9VOv4C1atXN/73v/953e+/QMHxG0bF+v175WmanJwcVq5cSWxsrKfNbrcTGxtLXFychZWVny1bthAdHU2jRo245ZZbSEpKAmDlypXk5uYWei1atGhBvXr1quxrkZCQQHJycqFjDg0NpVu3bp5jjouLIywsjM6dO3v2iY2NxW6388cff5zzmsvDwoULqV27Ns2bN+fee+/lwIEDnm1V8fhTU1MBqFGjBlC8935cXBxt27YlIiLCs0/fvn1JS0tjw4YN57D6s3fi8Rf47LPPCA8Pp02bNowaNYrMzEzPtqp0/C6Xi2nTppGRkUH37t297vd/4vEXqCi//0px1d6ytn//flwuV6EXGCAiIoJNmzZZVFX56datG1OnTqV58+bs2bOHsWPHcuGFF/LXX3+RnJyMn58fYWFhhe4TERFBcnKyNQWXs4LjKur3X7AtOTmZ2rVrF9ru4+NDjRo1qsTr0q9fP6699loaNmzItm3beOKJJ7j88suJi4vD4XBUueN3u908/PDD9OzZkzZt2gAU672fnJxc5PukYFtlUdTxA9x8883Ur1+f6Oho1q1bx8iRI4mPj2fGjBlA1Tj+9evX0717d7KysggKCmLmzJm0atWKNWvWeMXv/1THDxXr9++VYcTbXH755Z6f27VrR7du3ahfvz5ffvklAQEBFlYmVhk8eLDn57Zt29KuXTsaN27MwoULueSSSyysrHzcf//9/PXXXyxevNjqUixxquO/++67PT+3bduWqKgoLrnkErZt20bjxo3PdZnlonnz5qxZs4bU1FS+/vprhgwZwm+//WZ1WefMqY6/VatWFer375WnacLDw3E4HCeNmk5JSSEyMtKiqs6dsLAwmjVrxtatW4mMjCQnJ4fDhw8X2qcqvxYFx3W6339kZCR79+4ttD0vL4+DBw9WydelUaNGhIeHs3XrVqBqHf/w4cOZPXs2CxYsoG7dup724rz3IyMji3yfFGyrDE51/EXp1q0bQKH3QWU/fj8/P5o0aUKnTp0YP3487du35/XXX/ea3/+pjr8oVv7+vTKM+Pn50alTJ+bPn+9pc7vdzJ8/v9C5tKrqyJEjbNu2jaioKDp16oSvr2+h1yI+Pp6kpKQq+1o0bNiQyMjIQseclpbGH3/84Tnm7t27c/jwYVauXOnZ59dff8Xtdnv+wVYlO3fu5MCBA0RFRQFV4/gNw2D48OHMnDmTX3/9lYYNGxbaXpz3fvfu3Vm/fn2hYDZ37lxCQkI8Xd0V1ZmOvyhr1qwBKPQ+qKzHfyput5vs7Owq//s/lYLjL4qlv/8yHQ5biUybNs1wOp3G1KlTjY0bNxp33323ERYWVmjUcFXxyCOPGAsXLjQSEhKMJUuWGLGxsUZ4eLixd+9ewzDM6W316tUzfv31V+PPP/80unfvbnTv3t3iqs9Oenq6sXr1amP16tUGYLz66qvG6tWrje3btxuGYU7tDQsLM7799ltj3bp1xtVXX13k1N6OHTsaf/zxh7F48WKjadOmlWZq6+mOPz093Xj00UeNuLg4IyEhwZg3b55x3nnnGU2bNjWysrI8j1GZj98wDOPee+81QkNDjYULFxaaupiZmenZ50zv/YKpjZdddpmxZs0aY86cOUatWrUqxdTOMx3/1q1bjXHjxhl//vmnkZCQYHz77bdGo0aNjF69enkeozIfv2EYxuOPP2789ttvRkJCgrFu3Trj8ccfN2w2m/HLL78YhlG1f/+Gcfrjr2i/f68NI4ZhGG+++aZRr149w8/Pz+jatauxbNkyq0sqF4MGDTKioqIMPz8/o06dOsagQYOMrVu3erYfPXrUuO+++4zq1asbgYGBxjXXXGPs2bPHworP3oIFCwzgpK8hQ4YYhmFO73366aeNiIgIw+l0GpdccokRHx9f6DEOHDhg3HTTTUZQUJAREhJiDBs2zEhPT7fgaErudMefmZlpXHbZZUatWrUMX19fo379+sZdd911UhCvzMdvGEaRxw8YH374oWef4rz3ExMTjcsvv9wICAgwwsPDjUceecTIzc09x0dTcmc6/qSkJKNXr15GjRo1DKfTaTRp0sR47LHHCq0zYRiV9/gNwzDuuOMOo379+oafn59Rq1Yt45JLLvEEEcOo2r9/wzj98Ve037/NMAyjbPtaRERERIrPK8eMiIiISMWhMCIiIiKWUhgRERERSymMiIiIiKUURkRERMRSCiMiIiJiKYURERERsZTCiIiIiFhKYUREREQspTAiIiIillIYEREREUv9Pyk+0879MMu9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_4 (SimpleRNN)    (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.9524\n",
      "Test Loss: 0.3898967206478119\n",
      "Test Accuracy: 0.9523809552192688\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer \n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, epochs=350, batch_size=8, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot method for K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1 (train_loss, train_acc, val_loss, val_acc):\n",
    "    \n",
    "    avg_train_loss = np.mean(train_loss, axis=0)\n",
    "    avg_train_acc = np.mean(train_acc, axis=0)\n",
    "    avg_val_loss = np.mean(val_loss, axis=0)\n",
    "    avg_val_acc = np.mean(train_acc, axis=0)\n",
    "\n",
    "    # Plot delle curve di apprendimento mediate sulle K fold\n",
    "\n",
    "    epochs = range(1, len(train_loss[0]) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_loss, label='Training loss')\n",
    "    plt.plot(epochs, avg_val_loss, label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg_loss')\n",
    "    plt.title('Average train and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, avg_val_acc, label='Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg_accuracy')\n",
    "    plt.title('Average train and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation for the Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing vanilla RNN with K-fold\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.68943, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.68943 to 0.61284, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.61284 to 0.55715, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.55715 to 0.51534, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.51534 to 0.48297, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.48297 to 0.45693, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.45693 to 0.43575, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.43575 to 0.41810, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.41810 to 0.40304, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.40304 to 0.38989, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.38989 to 0.37841, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.37841 to 0.36837, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.36837 to 0.35952, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.35952 to 0.35156, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.35156 to 0.34423, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.34423 to 0.33752, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.33752 to 0.33135, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.33135 to 0.32568, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.32568 to 0.32059, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.32059 to 0.31577, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.31577 to 0.31129, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.31129 to 0.30715, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.30715 to 0.30343, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.30343 to 0.29983, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.29983 to 0.29653, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.29653 to 0.29341, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.29341 to 0.29048, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.29048 to 0.28775, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.28775 to 0.28519, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.28519 to 0.28276, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.28276 to 0.28042, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.28042 to 0.27824, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.27824 to 0.27619, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.27619 to 0.27422, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.27422 to 0.27234, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.27234 to 0.27057, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.27057 to 0.26882, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.26882 to 0.26717, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.26717 to 0.26561, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.26561 to 0.26413, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.26413 to 0.26266, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.26266 to 0.26121, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.26121 to 0.25979, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.25979 to 0.25847, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.25847 to 0.25718, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.25718 to 0.25592, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.25592 to 0.25465, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.25465 to 0.25346, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.25346 to 0.25231, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.25231 to 0.25117, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.25117 to 0.25011, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.25011 to 0.24902, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.24902 to 0.24800, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.24800 to 0.24700, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.24700 to 0.24604, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.24604 to 0.24510, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.24510 to 0.24417, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.24417 to 0.24323, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.24323 to 0.24239, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.24239 to 0.24151, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.24151 to 0.24066, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.24066 to 0.23983, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.23983 to 0.23901, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.23901 to 0.23822, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.23822 to 0.23744, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.23744 to 0.23665, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.23665 to 0.23590, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.23590 to 0.23518, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.23518 to 0.23447, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.23447 to 0.23374, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.23374 to 0.23304, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.23304 to 0.23234, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.23234 to 0.23166, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.23166 to 0.23098, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.23098 to 0.23037, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.23037 to 0.22975, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.22975 to 0.22913, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.22913 to 0.22849, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.22849 to 0.22783, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.22783 to 0.22720, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.22720 to 0.22660, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.22660 to 0.22602, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.22602 to 0.22549, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.22549 to 0.22488, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.22488 to 0.22432, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.22432 to 0.22378, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.22378 to 0.22320, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.22320 to 0.22263, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.22263 to 0.22210, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.22210 to 0.22154, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.22154 to 0.22101, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.22101 to 0.22049, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.22049 to 0.21999, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.21999 to 0.21947, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.21947 to 0.21896, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.21896 to 0.21846, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.21846 to 0.21797, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.21797 to 0.21747, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.21747 to 0.21699, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.21699 to 0.21652, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.21652 to 0.21609, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.21609 to 0.21562, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.21562 to 0.21516, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.21516 to 0.21475, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.21475 to 0.21430, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.21430 to 0.21382, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.21382 to 0.21337, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.21337 to 0.21292, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.21292 to 0.21249, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.21249 to 0.21203, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.21203 to 0.21163, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.21163 to 0.21117, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.21117 to 0.21076, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.21076 to 0.21034, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.21034 to 0.20993, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.20993 to 0.20953, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.20953 to 0.20918, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.20918 to 0.20878, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.20878 to 0.20835, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.20835 to 0.20795, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.20795 to 0.20757, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.20757 to 0.20719, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.20719 to 0.20680, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.20680 to 0.20644, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.20644 to 0.20607, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.20607 to 0.20573, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.20573 to 0.20537, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.20537 to 0.20500, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.20500 to 0.20465, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.20465 to 0.20428, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.20428 to 0.20394, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.20394 to 0.20358, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.20358 to 0.20325, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.20325 to 0.20290, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.20290 to 0.20255, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.20255 to 0.20221, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.20221 to 0.20187, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.20187 to 0.20152, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.20152 to 0.20117, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.20117 to 0.20082, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.20082 to 0.20047, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.20047 to 0.20016, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.20016 to 0.19983, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.19983 to 0.19949, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.19949 to 0.19915, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.19915 to 0.19884, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.19884 to 0.19852, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.19852 to 0.19822, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.19822 to 0.19795, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.19795 to 0.19763, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.19763 to 0.19732, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.19732 to 0.19704, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.19704 to 0.19675, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.19675 to 0.19644, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.19644 to 0.19615, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.19615 to 0.19586, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.19586 to 0.19557, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.19557 to 0.19530, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.19530 to 0.19500, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.19500 to 0.19475, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.19475 to 0.19445, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.19445 to 0.19417, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.19417 to 0.19389, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.19389 to 0.19360, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.19360 to 0.19329, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.19329 to 0.19300, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.19300 to 0.19273, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.19273 to 0.19244, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.19244 to 0.19216, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.19216 to 0.19193, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.19193 to 0.19166, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.19166 to 0.19139, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.19139 to 0.19112, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.19112 to 0.19084, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.19084 to 0.19057, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.19057 to 0.19032, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.19032 to 0.19005, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.19005 to 0.18983, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.18983 to 0.18957, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.18957 to 0.18931, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.18931 to 0.18906, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.18906 to 0.18882, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.18882 to 0.18856, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.18856 to 0.18829, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.18829 to 0.18804, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.18804 to 0.18778, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.18778 to 0.18753, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.18753 to 0.18728, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.18728 to 0.18704, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.18704 to 0.18681, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.18681 to 0.18654, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.18654 to 0.18632, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.18632 to 0.18616, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.18616 to 0.18593, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.18593 to 0.18571, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.18571 to 0.18545, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.18545 to 0.18520, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.18520 to 0.18497, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.18497 to 0.18475, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.18475 to 0.18454, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.18454 to 0.18433, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.18433 to 0.18411, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.18411 to 0.18389, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.18389 to 0.18370, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.18370 to 0.18350, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.18350 to 0.18327, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.18327 to 0.18306, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.18306 to 0.18285, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.18285 to 0.18266, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.18266 to 0.18244, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.18244 to 0.18224, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.18224 to 0.18205, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.18205 to 0.18187, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.18187 to 0.18166, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.18166 to 0.18145, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.18145 to 0.18124, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.18124 to 0.18102, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.18102 to 0.18084, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.18084 to 0.18067, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.18067 to 0.18048, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.18048 to 0.18030, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.18030 to 0.18012, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.18012 to 0.18000, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.18000 to 0.17979, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.17979 to 0.17962, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.17962 to 0.17945, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.17945 to 0.17928, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.17928 to 0.17912, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.17912 to 0.17894, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.17894 to 0.17877, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.17877 to 0.17858, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.17858 to 0.17841, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.17841 to 0.17823, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.17823 to 0.17804, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.17804 to 0.17787, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.17787 to 0.17771, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.17771 to 0.17753, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.17753 to 0.17737, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.17737 to 0.17725, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.17725 to 0.17709, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.17709 to 0.17691, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.17691 to 0.17674, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.17674 to 0.17656, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.17656 to 0.17640, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.17640 to 0.17624, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.17624 to 0.17608, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.17608 to 0.17595, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.17595 to 0.17579, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.17579 to 0.17563, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.17563 to 0.17547, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.17547 to 0.17532, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.17532 to 0.17517, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.17517 to 0.17503, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.17503 to 0.17489, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.17489 to 0.17474, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.17474 to 0.17459, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.17459 to 0.17449, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.17449 to 0.17435, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.17435 to 0.17419, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.17419 to 0.17403, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.17403 to 0.17390, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.17390 to 0.17371, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.17371 to 0.17358, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.17358 to 0.17343, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.17343 to 0.17330, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.17330 to 0.17313, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.17313 to 0.17301, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.17301 to 0.17294, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.17294 to 0.17279, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.17279 to 0.17273, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.17273 to 0.17264, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.17264 to 0.17250, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.17250 to 0.17238, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.17238 to 0.17224, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.17224 to 0.17211, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.17211 to 0.17197, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.17197 to 0.17184, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.17184 to 0.17170, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.17170 to 0.17156, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.17156 to 0.17144, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.17144 to 0.17130, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.17130 to 0.17116, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.17116 to 0.17105, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.17105 to 0.17091, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.17091 to 0.17078, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.17078 to 0.17065, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.17065 to 0.17057, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.17057 to 0.17043, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.17043 to 0.17034, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.17034 to 0.17021, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.17021 to 0.17007, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.17007 to 0.16996, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.16996 to 0.16986, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.16986 to 0.16974, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.16974 to 0.16963, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.16963 to 0.16948, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.16948 to 0.16935, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.16935 to 0.16926, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.16926 to 0.16915, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.16915 to 0.16903, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "Loss: 0.1690, Accuracy: 90.18%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16903\n",
      "Loss: 0.2156, Accuracy: 90.18%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16903\n",
      "Loss: 0.2371, Accuracy: 91.07%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16903\n",
      "Loss: 0.1905, Accuracy: 94.64%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16903\n",
      "Loss: 0.1874, Accuracy: 92.86%\n",
      "Vanilla_RNN finished in 189.71 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_10 (SimpleRNN)   (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Average accuracy: 0.9179\n",
      "Average loss: 0.1999\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN_K-fold.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "k_fold = 5 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing vanilla RNN with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 300, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('Vanilla_RNN finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_1(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prova 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing vanilla RNN with K-fold\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 7ms/step - loss: 1.2282 - accuracy: 0.2701 - val_loss: 1.0591 - val_accuracy: 0.3393 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.9764 - accuracy: 0.3906 - val_loss: 0.8644 - val_accuracy: 0.4464 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.8002 - accuracy: 0.5558 - val_loss: 0.7287 - val_accuracy: 0.5804 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.6783 - accuracy: 0.6696 - val_loss: 0.6348 - val_accuracy: 0.6696 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.5928 - accuracy: 0.7299 - val_loss: 0.5690 - val_accuracy: 0.7232 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.5310 - accuracy: 0.7612 - val_loss: 0.5222 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4851 - accuracy: 0.7924 - val_loss: 0.4879 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.4501 - accuracy: 0.8080 - val_loss: 0.4621 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.8214 - val_loss: 0.4423 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4014 - accuracy: 0.8348 - val_loss: 0.4266 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8415 - val_loss: 0.4138 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8460 - val_loss: 0.4033 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3571 - accuracy: 0.8594 - val_loss: 0.3945 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8594 - val_loss: 0.3871 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3373 - accuracy: 0.8616 - val_loss: 0.3807 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8683 - val_loss: 0.3752 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.8705 - val_loss: 0.3704 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8728 - val_loss: 0.3661 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3095 - accuracy: 0.8728 - val_loss: 0.3623 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3040 - accuracy: 0.8728 - val_loss: 0.3589 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8772 - val_loss: 0.3559 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2944 - accuracy: 0.8817 - val_loss: 0.3531 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8817 - val_loss: 0.3505 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8884 - val_loss: 0.3482 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2823 - accuracy: 0.8884 - val_loss: 0.3460 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2787 - accuracy: 0.8884 - val_loss: 0.3440 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8884 - val_loss: 0.3422 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.8906 - val_loss: 0.3404 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2695 - accuracy: 0.8906 - val_loss: 0.3388 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.8906 - val_loss: 0.3373 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8951 - val_loss: 0.3358 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2616 - accuracy: 0.8973 - val_loss: 0.3344 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2592 - accuracy: 0.8973 - val_loss: 0.3331 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.9018 - val_loss: 0.3319 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.9040 - val_loss: 0.3307 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.9040 - val_loss: 0.3295 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.9062 - val_loss: 0.3284 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.9062 - val_loss: 0.3274 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.9062 - val_loss: 0.3263 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.9062 - val_loss: 0.3253 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2435 - accuracy: 0.9085 - val_loss: 0.3244 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.9085 - val_loss: 0.3234 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2402 - accuracy: 0.9085 - val_loss: 0.3225 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.9107 - val_loss: 0.3216 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2371 - accuracy: 0.9107 - val_loss: 0.3208 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9129 - val_loss: 0.3199 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2342 - accuracy: 0.9152 - val_loss: 0.3191 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.9174 - val_loss: 0.3183 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.9196 - val_loss: 0.3174 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9196 - val_loss: 0.3167 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.9196 - val_loss: 0.3159 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9196 - val_loss: 0.3151 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9219 - val_loss: 0.3144 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2251 - accuracy: 0.9219 - val_loss: 0.3136 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9241 - val_loss: 0.3129 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2228 - accuracy: 0.9241 - val_loss: 0.3122 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2216 - accuracy: 0.9241 - val_loss: 0.3114 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9263 - val_loss: 0.3107 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2194 - accuracy: 0.9263 - val_loss: 0.3100 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9263 - val_loss: 0.3093 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2173 - accuracy: 0.9263 - val_loss: 0.3086 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2162 - accuracy: 0.9263 - val_loss: 0.3079 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2152 - accuracy: 0.9286 - val_loss: 0.3072 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9286 - val_loss: 0.3065 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9308 - val_loss: 0.3059 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9308 - val_loss: 0.3052 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9308 - val_loss: 0.3045 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9308 - val_loss: 0.3039 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.9308 - val_loss: 0.3032 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9308 - val_loss: 0.3026 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9308 - val_loss: 0.3020 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2068 - accuracy: 0.9308 - val_loss: 0.3013 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9308 - val_loss: 0.3007 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9308 - val_loss: 0.3001 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9308 - val_loss: 0.2995 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9308 - val_loss: 0.2988 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9330 - val_loss: 0.2982 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2017 - accuracy: 0.9353 - val_loss: 0.2976 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9353 - val_loss: 0.2970 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.9353 - val_loss: 0.2964 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.9353 - val_loss: 0.2958 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9353 - val_loss: 0.2953 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1978 - accuracy: 0.9375 - val_loss: 0.2947 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1971 - accuracy: 0.9375 - val_loss: 0.2941 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1963 - accuracy: 0.9375 - val_loss: 0.2935 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9375 - val_loss: 0.2929 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1948 - accuracy: 0.9375 - val_loss: 0.2923 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1941 - accuracy: 0.9397 - val_loss: 0.2918 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1934 - accuracy: 0.9420 - val_loss: 0.2912 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1927 - accuracy: 0.9420 - val_loss: 0.2906 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1920 - accuracy: 0.9420 - val_loss: 0.2900 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9420 - val_loss: 0.2895 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9420 - val_loss: 0.2889 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1900 - accuracy: 0.9420 - val_loss: 0.2883 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1893 - accuracy: 0.9420 - val_loss: 0.2878 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9420 - val_loss: 0.2872 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9420 - val_loss: 0.2867 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9420 - val_loss: 0.2861 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9420 - val_loss: 0.2856 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.9464 - val_loss: 0.2851 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.9464 - val_loss: 0.2846 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1848 - accuracy: 0.9442 - val_loss: 0.2840 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.9464 - val_loss: 0.2835 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9464 - val_loss: 0.2830 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.9464 - val_loss: 0.2824 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1823 - accuracy: 0.9487 - val_loss: 0.2819 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.9464 - val_loss: 0.2814 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9487 - val_loss: 0.2809 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9487 - val_loss: 0.2803 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1799 - accuracy: 0.9487 - val_loss: 0.2798 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.9487 - val_loss: 0.2793 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.9487 - val_loss: 0.2788 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1782 - accuracy: 0.9509 - val_loss: 0.2783 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9509 - val_loss: 0.2778 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9509 - val_loss: 0.2773 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.9509 - val_loss: 0.2768 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.9509 - val_loss: 0.2763 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1754 - accuracy: 0.9509 - val_loss: 0.2759 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1748 - accuracy: 0.9509 - val_loss: 0.2754 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9509 - val_loss: 0.2749 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1737 - accuracy: 0.9509 - val_loss: 0.2744 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1732 - accuracy: 0.9509 - val_loss: 0.2739 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1727 - accuracy: 0.9509 - val_loss: 0.2734 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1722 - accuracy: 0.9531 - val_loss: 0.2730 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1717 - accuracy: 0.9531 - val_loss: 0.2725 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9531 - val_loss: 0.2720 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.9531 - val_loss: 0.2716 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9531 - val_loss: 0.2711 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9531 - val_loss: 0.2706 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1692 - accuracy: 0.9531 - val_loss: 0.2702 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9531 - val_loss: 0.2697 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9531 - val_loss: 0.2693 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.9531 - val_loss: 0.2688 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.9531 - val_loss: 0.2684 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9531 - val_loss: 0.2679 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9531 - val_loss: 0.2675 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9531 - val_loss: 0.2670 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1654 - accuracy: 0.9531 - val_loss: 0.2666 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1650 - accuracy: 0.9531 - val_loss: 0.2661 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9531 - val_loss: 0.2657 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9531 - val_loss: 0.2653 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9531 - val_loss: 0.2648 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1633 - accuracy: 0.9531 - val_loss: 0.2644 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9531 - val_loss: 0.2640 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1624 - accuracy: 0.9531 - val_loss: 0.2635 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1620 - accuracy: 0.9531 - val_loss: 0.2631 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9531 - val_loss: 0.2627 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9531 - val_loss: 0.2622 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9531 - val_loss: 0.2618 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 0.9531 - val_loss: 0.2614 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9531 - val_loss: 0.2610 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1595 - accuracy: 0.9531 - val_loss: 0.2606 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9531 - val_loss: 0.2602 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9531 - val_loss: 0.2598 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9531 - val_loss: 0.2593 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.9531 - val_loss: 0.2589 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9531 - val_loss: 0.2585 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9531 - val_loss: 0.2581 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1567 - accuracy: 0.9531 - val_loss: 0.2577 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9531 - val_loss: 0.2573 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1560 - accuracy: 0.9531 - val_loss: 0.2569 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1556 - accuracy: 0.9531 - val_loss: 0.2565 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1552 - accuracy: 0.9531 - val_loss: 0.2561 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1549 - accuracy: 0.9531 - val_loss: 0.2557 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1545 - accuracy: 0.9531 - val_loss: 0.2554 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1541 - accuracy: 0.9531 - val_loss: 0.2550 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1538 - accuracy: 0.9531 - val_loss: 0.2546 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1534 - accuracy: 0.9531 - val_loss: 0.2542 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.9531 - val_loss: 0.2538 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1527 - accuracy: 0.9531 - val_loss: 0.2534 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1523 - accuracy: 0.9531 - val_loss: 0.2531 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1520 - accuracy: 0.9531 - val_loss: 0.2527 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.9531 - val_loss: 0.2523 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9531 - val_loss: 0.2520 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.9531 - val_loss: 0.2516 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1506 - accuracy: 0.9531 - val_loss: 0.2513 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1503 - accuracy: 0.9531 - val_loss: 0.2509 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9531 - val_loss: 0.2505 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.9531 - val_loss: 0.2502 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1493 - accuracy: 0.9531 - val_loss: 0.2498 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1489 - accuracy: 0.9531 - val_loss: 0.2495 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1486 - accuracy: 0.9531 - val_loss: 0.2491 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.9531 - val_loss: 0.2488 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1480 - accuracy: 0.9531 - val_loss: 0.2484 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9531 - val_loss: 0.2481 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1474 - accuracy: 0.9531 - val_loss: 0.2477 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.9531 - val_loss: 0.2474 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.9531 - val_loss: 0.2470 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.9531 - val_loss: 0.2467 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9531 - val_loss: 0.2464 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9531 - val_loss: 0.2460 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9531 - val_loss: 0.2457 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1452 - accuracy: 0.9531 - val_loss: 0.2454 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1449 - accuracy: 0.9531 - val_loss: 0.2450 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9554 - val_loss: 0.2447 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1444 - accuracy: 0.9554 - val_loss: 0.2444 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.9576 - val_loss: 0.2440 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1437 - accuracy: 0.9576 - val_loss: 0.2437 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1435 - accuracy: 0.9598 - val_loss: 0.2434 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9598 - val_loss: 0.2430 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9598 - val_loss: 0.2427 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1426 - accuracy: 0.9598 - val_loss: 0.2424 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1423 - accuracy: 0.9598 - val_loss: 0.2421 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9598 - val_loss: 0.2418 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9598 - val_loss: 0.2414 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.9598 - val_loss: 0.2411 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9598 - val_loss: 0.2408 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1410 - accuracy: 0.9598 - val_loss: 0.2405 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9598 - val_loss: 0.2402 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1404 - accuracy: 0.9598 - val_loss: 0.2399 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.9598 - val_loss: 0.2396 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9598 - val_loss: 0.2392 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.9598 - val_loss: 0.2389 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9598 - val_loss: 0.2386 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.9598 - val_loss: 0.2383 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1389 - accuracy: 0.9598 - val_loss: 0.2380 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9598 - val_loss: 0.2377 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9598 - val_loss: 0.2374 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9598 - val_loss: 0.2371 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1378 - accuracy: 0.9598 - val_loss: 0.2368 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9598 - val_loss: 0.2365 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1373 - accuracy: 0.9598 - val_loss: 0.2362 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1371 - accuracy: 0.9598 - val_loss: 0.2359 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.9598 - val_loss: 0.2356 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1366 - accuracy: 0.9598 - val_loss: 0.2353 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1364 - accuracy: 0.9598 - val_loss: 0.2350 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.9598 - val_loss: 0.2347 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1359 - accuracy: 0.9598 - val_loss: 0.2344 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9598 - val_loss: 0.2341 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1354 - accuracy: 0.9598 - val_loss: 0.2338 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1351 - accuracy: 0.9598 - val_loss: 0.2335 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1349 - accuracy: 0.9598 - val_loss: 0.2333 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1347 - accuracy: 0.9598 - val_loss: 0.2330 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9598 - val_loss: 0.2327 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1342 - accuracy: 0.9598 - val_loss: 0.2324 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1340 - accuracy: 0.9598 - val_loss: 0.2321 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9598 - val_loss: 0.2318 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.9598 - val_loss: 0.2316 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9598 - val_loss: 0.2313 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1331 - accuracy: 0.9598 - val_loss: 0.2310 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.9598 - val_loss: 0.2307 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9598 - val_loss: 0.2304 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1324 - accuracy: 0.9598 - val_loss: 0.2302 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9598 - val_loss: 0.2299 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1319 - accuracy: 0.9598 - val_loss: 0.2296 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1317 - accuracy: 0.9598 - val_loss: 0.2293 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9598 - val_loss: 0.2291 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1313 - accuracy: 0.9598 - val_loss: 0.2288 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.9598 - val_loss: 0.2285 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9621 - val_loss: 0.2282 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9621 - val_loss: 0.2280 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1304 - accuracy: 0.9621 - val_loss: 0.2277 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9621 - val_loss: 0.2274 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1300 - accuracy: 0.9621 - val_loss: 0.2271 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9621 - val_loss: 0.2269 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9621 - val_loss: 0.2266 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9621 - val_loss: 0.2264 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.9621 - val_loss: 0.2261 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9621 - val_loss: 0.2258 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1287 - accuracy: 0.9621 - val_loss: 0.2255 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1285 - accuracy: 0.9621 - val_loss: 0.2253 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1283 - accuracy: 0.9621 - val_loss: 0.2250 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9621 - val_loss: 0.2248 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1279 - accuracy: 0.9621 - val_loss: 0.2245 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9621 - val_loss: 0.2243 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9621 - val_loss: 0.2240 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9621 - val_loss: 0.2237 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1271 - accuracy: 0.9621 - val_loss: 0.2235 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9621 - val_loss: 0.2232 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1267 - accuracy: 0.9621 - val_loss: 0.2230 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9621 - val_loss: 0.2227 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1263 - accuracy: 0.9621 - val_loss: 0.2225 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1261 - accuracy: 0.9621 - val_loss: 0.2222 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1259 - accuracy: 0.9621 - val_loss: 0.2220 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1257 - accuracy: 0.9621 - val_loss: 0.2217 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9621 - val_loss: 0.2214 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9621 - val_loss: 0.2212 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9621 - val_loss: 0.2209 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9621 - val_loss: 0.2207 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9621 - val_loss: 0.2205 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1245 - accuracy: 0.9621 - val_loss: 0.2202 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1243 - accuracy: 0.9621 - val_loss: 0.2200 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9621 - val_loss: 0.2197 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1239 - accuracy: 0.9621 - val_loss: 0.2195 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1238 - accuracy: 0.9621 - val_loss: 0.2192 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9621 - val_loss: 0.2190 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1234 - accuracy: 0.9621 - val_loss: 0.2188 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9621 - val_loss: 0.2185 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9621 - val_loss: 0.2183 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1228 - accuracy: 0.9621 - val_loss: 0.2181 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1226 - accuracy: 0.9621 - val_loss: 0.2178 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1225 - accuracy: 0.9621 - val_loss: 0.2176 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1223 - accuracy: 0.9621 - val_loss: 0.2174 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.9621 - val_loss: 0.2171 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.9621 - val_loss: 0.2169 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.9621 - val_loss: 0.2167 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.9621 - val_loss: 0.2164 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9621 - val_loss: 0.2162 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9621 - val_loss: 0.2159 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.9621 - val_loss: 0.2157 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Loss: 0.2157, Accuracy: 89.29%\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 8ms/step - loss: 0.6391 - accuracy: 0.6496 - val_loss: 0.6775 - val_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5664 - accuracy: 0.6897 - val_loss: 0.6098 - val_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7388 - val_loss: 0.5583 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4772 - accuracy: 0.7723 - val_loss: 0.5182 - val_accuracy: 0.6786 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4478 - accuracy: 0.7969 - val_loss: 0.4862 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8125 - val_loss: 0.4604 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4055 - accuracy: 0.8192 - val_loss: 0.4389 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3897 - accuracy: 0.8393 - val_loss: 0.4209 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8482 - val_loss: 0.4053 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3648 - accuracy: 0.8527 - val_loss: 0.3918 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3547 - accuracy: 0.8549 - val_loss: 0.3799 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3458 - accuracy: 0.8571 - val_loss: 0.3694 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3380 - accuracy: 0.8638 - val_loss: 0.3601 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3310 - accuracy: 0.8638 - val_loss: 0.3518 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3246 - accuracy: 0.8661 - val_loss: 0.3444 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8683 - val_loss: 0.3377 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8750 - val_loss: 0.3315 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8750 - val_loss: 0.3258 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3044 - accuracy: 0.8772 - val_loss: 0.3207 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.8817 - val_loss: 0.3159 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2964 - accuracy: 0.8839 - val_loss: 0.3115 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.8862 - val_loss: 0.3074 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2895 - accuracy: 0.8906 - val_loss: 0.3035 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8906 - val_loss: 0.2999 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2834 - accuracy: 0.8996 - val_loss: 0.2965 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.8996 - val_loss: 0.2934 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2778 - accuracy: 0.9018 - val_loss: 0.2904 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.9018 - val_loss: 0.2875 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2727 - accuracy: 0.9018 - val_loss: 0.2848 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.9018 - val_loss: 0.2822 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2681 - accuracy: 0.9040 - val_loss: 0.2798 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2659 - accuracy: 0.9040 - val_loss: 0.2774 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2638 - accuracy: 0.9040 - val_loss: 0.2753 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2617 - accuracy: 0.9040 - val_loss: 0.2732 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2598 - accuracy: 0.9040 - val_loss: 0.2711 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2579 - accuracy: 0.9062 - val_loss: 0.2692 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2560 - accuracy: 0.9062 - val_loss: 0.2673 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2542 - accuracy: 0.9062 - val_loss: 0.2654 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2524 - accuracy: 0.9085 - val_loss: 0.2637 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2507 - accuracy: 0.9107 - val_loss: 0.2620 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.2489 - accuracy: 0.9107 - val_loss: 0.2604 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2473 - accuracy: 0.9107 - val_loss: 0.2588 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2457 - accuracy: 0.9129 - val_loss: 0.2573 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.9129 - val_loss: 0.2558 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2425 - accuracy: 0.9152 - val_loss: 0.2544 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.9174 - val_loss: 0.2530 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.9196 - val_loss: 0.2516 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2381 - accuracy: 0.9219 - val_loss: 0.2503 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2366 - accuracy: 0.9241 - val_loss: 0.2491 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2352 - accuracy: 0.9241 - val_loss: 0.2478 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2339 - accuracy: 0.9241 - val_loss: 0.2466 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2325 - accuracy: 0.9241 - val_loss: 0.2455 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2312 - accuracy: 0.9263 - val_loss: 0.2443 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2299 - accuracy: 0.9263 - val_loss: 0.2432 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2286 - accuracy: 0.9263 - val_loss: 0.2421 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 0.9263 - val_loss: 0.2411 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2261 - accuracy: 0.9263 - val_loss: 0.2400 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.9263 - val_loss: 0.2390 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.9263 - val_loss: 0.2381 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2225 - accuracy: 0.9286 - val_loss: 0.2371 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2214 - accuracy: 0.9308 - val_loss: 0.2362 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2202 - accuracy: 0.9308 - val_loss: 0.2352 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2191 - accuracy: 0.9330 - val_loss: 0.2343 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.9330 - val_loss: 0.2335 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2169 - accuracy: 0.9330 - val_loss: 0.2326 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9330 - val_loss: 0.2317 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9330 - val_loss: 0.2309 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9330 - val_loss: 0.2301 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2128 - accuracy: 0.9330 - val_loss: 0.2293 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2118 - accuracy: 0.9330 - val_loss: 0.2285 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9353 - val_loss: 0.2278 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2099 - accuracy: 0.9375 - val_loss: 0.2270 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9375 - val_loss: 0.2263 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2079 - accuracy: 0.9375 - val_loss: 0.2256 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9397 - val_loss: 0.2248 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9420 - val_loss: 0.2241 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9420 - val_loss: 0.2234 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9420 - val_loss: 0.2228 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2034 - accuracy: 0.9442 - val_loss: 0.2221 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2025 - accuracy: 0.9464 - val_loss: 0.2214 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9464 - val_loss: 0.2208 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2008 - accuracy: 0.9464 - val_loss: 0.2201 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1999 - accuracy: 0.9464 - val_loss: 0.2195 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.9464 - val_loss: 0.2189 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1983 - accuracy: 0.9464 - val_loss: 0.2183 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9464 - val_loss: 0.2177 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1966 - accuracy: 0.9464 - val_loss: 0.2171 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1958 - accuracy: 0.9464 - val_loss: 0.2165 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1951 - accuracy: 0.9464 - val_loss: 0.2159 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1943 - accuracy: 0.9464 - val_loss: 0.2154 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9464 - val_loss: 0.2148 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1928 - accuracy: 0.9464 - val_loss: 0.2143 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9487 - val_loss: 0.2137 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9487 - val_loss: 0.2132 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1906 - accuracy: 0.9487 - val_loss: 0.2126 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1898 - accuracy: 0.9487 - val_loss: 0.2121 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1891 - accuracy: 0.9487 - val_loss: 0.2116 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1885 - accuracy: 0.9487 - val_loss: 0.2111 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1877 - accuracy: 0.9487 - val_loss: 0.2106 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1871 - accuracy: 0.9487 - val_loss: 0.2101 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1864 - accuracy: 0.9487 - val_loss: 0.2096 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.9487 - val_loss: 0.2091 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9487 - val_loss: 0.2086 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9487 - val_loss: 0.2082 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9487 - val_loss: 0.2077 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.9487 - val_loss: 0.2073 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.9487 - val_loss: 0.2068 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1818 - accuracy: 0.9487 - val_loss: 0.2064 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9487 - val_loss: 0.2059 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1806 - accuracy: 0.9487 - val_loss: 0.2055 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.9487 - val_loss: 0.2050 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9487 - val_loss: 0.2046 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.9487 - val_loss: 0.2042 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1781 - accuracy: 0.9487 - val_loss: 0.2037 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1776 - accuracy: 0.9487 - val_loss: 0.2033 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9487 - val_loss: 0.2029 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9487 - val_loss: 0.2025 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9487 - val_loss: 0.2021 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.9487 - val_loss: 0.2017 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9487 - val_loss: 0.2013 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1741 - accuracy: 0.9487 - val_loss: 0.2009 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.9487 - val_loss: 0.2005 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1730 - accuracy: 0.9487 - val_loss: 0.2001 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9487 - val_loss: 0.1997 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9487 - val_loss: 0.1993 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1713 - accuracy: 0.9487 - val_loss: 0.1990 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9487 - val_loss: 0.1986 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1703 - accuracy: 0.9487 - val_loss: 0.1982 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.9487 - val_loss: 0.1979 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1692 - accuracy: 0.9487 - val_loss: 0.1975 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9487 - val_loss: 0.1972 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9487 - val_loss: 0.1968 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.9487 - val_loss: 0.1965 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9487 - val_loss: 0.1961 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1667 - accuracy: 0.9509 - val_loss: 0.1958 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9509 - val_loss: 0.1954 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1657 - accuracy: 0.9509 - val_loss: 0.1951 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1652 - accuracy: 0.9509 - val_loss: 0.1948 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1647 - accuracy: 0.9509 - val_loss: 0.1944 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9509 - val_loss: 0.1941 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9509 - val_loss: 0.1938 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1633 - accuracy: 0.9509 - val_loss: 0.1935 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9509 - val_loss: 0.1932 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1623 - accuracy: 0.9509 - val_loss: 0.1928 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1618 - accuracy: 0.9509 - val_loss: 0.1925 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.9509 - val_loss: 0.1922 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9509 - val_loss: 0.1919 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9509 - val_loss: 0.1916 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9509 - val_loss: 0.1913 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9509 - val_loss: 0.1910 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9509 - val_loss: 0.1907 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1587 - accuracy: 0.9509 - val_loss: 0.1904 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9509 - val_loss: 0.1901 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.9509 - val_loss: 0.1898 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.9509 - val_loss: 0.1895 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1569 - accuracy: 0.9509 - val_loss: 0.1893 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9509 - val_loss: 0.1890 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1561 - accuracy: 0.9509 - val_loss: 0.1887 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1557 - accuracy: 0.9509 - val_loss: 0.1884 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9509 - val_loss: 0.1882 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9509 - val_loss: 0.1879 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.9509 - val_loss: 0.1876 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9509 - val_loss: 0.1874 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1536 - accuracy: 0.9509 - val_loss: 0.1871 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.9509 - val_loss: 0.1868 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1528 - accuracy: 0.9509 - val_loss: 0.1866 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9509 - val_loss: 0.1863 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1520 - accuracy: 0.9509 - val_loss: 0.1860 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.9509 - val_loss: 0.1858 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1512 - accuracy: 0.9509 - val_loss: 0.1855 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9509 - val_loss: 0.1853 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1504 - accuracy: 0.9509 - val_loss: 0.1850 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1501 - accuracy: 0.9509 - val_loss: 0.1848 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1497 - accuracy: 0.9509 - val_loss: 0.1845 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1493 - accuracy: 0.9509 - val_loss: 0.1843 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1489 - accuracy: 0.9509 - val_loss: 0.1841 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1485 - accuracy: 0.9509 - val_loss: 0.1838 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1482 - accuracy: 0.9509 - val_loss: 0.1836 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1478 - accuracy: 0.9509 - val_loss: 0.1833 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1474 - accuracy: 0.9509 - val_loss: 0.1831 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1471 - accuracy: 0.9509 - val_loss: 0.1829 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1467 - accuracy: 0.9509 - val_loss: 0.1826 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1463 - accuracy: 0.9509 - val_loss: 0.1824 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9509 - val_loss: 0.1822 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1456 - accuracy: 0.9509 - val_loss: 0.1820 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.9509 - val_loss: 0.1817 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1449 - accuracy: 0.9509 - val_loss: 0.1815 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9509 - val_loss: 0.1813 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1442 - accuracy: 0.9509 - val_loss: 0.1811 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1439 - accuracy: 0.9509 - val_loss: 0.1809 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1436 - accuracy: 0.9509 - val_loss: 0.1807 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1432 - accuracy: 0.9509 - val_loss: 0.1804 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9509 - val_loss: 0.1802 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1425 - accuracy: 0.9509 - val_loss: 0.1800 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.9509 - val_loss: 0.1798 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9509 - val_loss: 0.1796 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.9509 - val_loss: 0.1794 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1412 - accuracy: 0.9509 - val_loss: 0.1792 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1409 - accuracy: 0.9509 - val_loss: 0.1790 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1406 - accuracy: 0.9509 - val_loss: 0.1788 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.9509 - val_loss: 0.1786 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9509 - val_loss: 0.1784 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9509 - val_loss: 0.1782 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9509 - val_loss: 0.1780 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9509 - val_loss: 0.1778 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1387 - accuracy: 0.9509 - val_loss: 0.1776 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9509 - val_loss: 0.1775 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1381 - accuracy: 0.9531 - val_loss: 0.1773 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9531 - val_loss: 0.1771 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1375 - accuracy: 0.9554 - val_loss: 0.1769 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1372 - accuracy: 0.9554 - val_loss: 0.1767 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1368 - accuracy: 0.9554 - val_loss: 0.1765 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1365 - accuracy: 0.9554 - val_loss: 0.1763 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.9554 - val_loss: 0.1762 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1359 - accuracy: 0.9554 - val_loss: 0.1760 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9554 - val_loss: 0.1758 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1354 - accuracy: 0.9554 - val_loss: 0.1756 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9554 - val_loss: 0.1755 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9554 - val_loss: 0.1753 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1345 - accuracy: 0.9554 - val_loss: 0.1752 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1342 - accuracy: 0.9554 - val_loss: 0.1750 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1339 - accuracy: 0.9554 - val_loss: 0.1748 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1336 - accuracy: 0.9554 - val_loss: 0.1746 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9554 - val_loss: 0.1745 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9554 - val_loss: 0.1743 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9554 - val_loss: 0.1742 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9554 - val_loss: 0.1740 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1322 - accuracy: 0.9576 - val_loss: 0.1738 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9576 - val_loss: 0.1737 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1317 - accuracy: 0.9576 - val_loss: 0.1735 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9576 - val_loss: 0.1733 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9576 - val_loss: 0.1732 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9576 - val_loss: 0.1730 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9576 - val_loss: 0.1729 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1304 - accuracy: 0.9576 - val_loss: 0.1727 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.9576 - val_loss: 0.1726 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9598 - val_loss: 0.1724 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9598 - val_loss: 0.1723 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9598 - val_loss: 0.1721 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1291 - accuracy: 0.9621 - val_loss: 0.1720 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1288 - accuracy: 0.9621 - val_loss: 0.1718 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1285 - accuracy: 0.9621 - val_loss: 0.1717 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1283 - accuracy: 0.9621 - val_loss: 0.1715 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1280 - accuracy: 0.9621 - val_loss: 0.1714 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9621 - val_loss: 0.1712 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9621 - val_loss: 0.1711 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1273 - accuracy: 0.9621 - val_loss: 0.1710 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1271 - accuracy: 0.9621 - val_loss: 0.1708 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1268 - accuracy: 0.9621 - val_loss: 0.1707 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9621 - val_loss: 0.1705 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1263 - accuracy: 0.9621 - val_loss: 0.1704 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.9621 - val_loss: 0.1703 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9621 - val_loss: 0.1702 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1256 - accuracy: 0.9621 - val_loss: 0.1700 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9621 - val_loss: 0.1699 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9621 - val_loss: 0.1698 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9621 - val_loss: 0.1696 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1246 - accuracy: 0.9621 - val_loss: 0.1695 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1244 - accuracy: 0.9621 - val_loss: 0.1694 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1242 - accuracy: 0.9621 - val_loss: 0.1693 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9621 - val_loss: 0.1691 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1237 - accuracy: 0.9621 - val_loss: 0.1690 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1235 - accuracy: 0.9621 - val_loss: 0.1689 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9621 - val_loss: 0.1688 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9621 - val_loss: 0.1686 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1228 - accuracy: 0.9621 - val_loss: 0.1685 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.9621 - val_loss: 0.1684 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1223 - accuracy: 0.9621 - val_loss: 0.1683 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.9621 - val_loss: 0.1682 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1219 - accuracy: 0.9621 - val_loss: 0.1680 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1217 - accuracy: 0.9621 - val_loss: 0.1679 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9621 - val_loss: 0.1678 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9621 - val_loss: 0.1677 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 0.9621 - val_loss: 0.1676 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1208 - accuracy: 0.9621 - val_loss: 0.1674 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.9621 - val_loss: 0.1673 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.9621 - val_loss: 0.1672 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1201 - accuracy: 0.9621 - val_loss: 0.1671 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.9621 - val_loss: 0.1670 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1197 - accuracy: 0.9621 - val_loss: 0.1669 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.9621 - val_loss: 0.1668 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1193 - accuracy: 0.9621 - val_loss: 0.1667 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9621 - val_loss: 0.1666 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1188 - accuracy: 0.9621 - val_loss: 0.1665 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1186 - accuracy: 0.9621 - val_loss: 0.1664 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1184 - accuracy: 0.9621 - val_loss: 0.1662 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9621 - val_loss: 0.1661 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9621 - val_loss: 0.1660 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9621 - val_loss: 0.1659 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.9621 - val_loss: 0.1658 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9621 - val_loss: 0.1657 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1172 - accuracy: 0.9621 - val_loss: 0.1656 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.9621 - val_loss: 0.1655 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9643 - val_loss: 0.1654 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9643 - val_loss: 0.1653 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1164 - accuracy: 0.9643 - val_loss: 0.1652 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9643 - val_loss: 0.1651 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1160 - accuracy: 0.9643 - val_loss: 0.1650 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.9643 - val_loss: 0.1649 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.9643 - val_loss: 0.1648 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Loss: 0.1648, Accuracy: 94.64%\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 9ms/step - loss: 0.7485 - accuracy: 0.4821 - val_loss: 0.8229 - val_accuracy: 0.5357 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6268 - accuracy: 0.6719 - val_loss: 0.7260 - val_accuracy: 0.6607 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.5431 - accuracy: 0.7946 - val_loss: 0.6571 - val_accuracy: 0.7054 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4847 - accuracy: 0.8393 - val_loss: 0.6064 - val_accuracy: 0.7321 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4427 - accuracy: 0.8527 - val_loss: 0.5678 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4112 - accuracy: 0.8527 - val_loss: 0.5379 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8594 - val_loss: 0.5141 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8616 - val_loss: 0.4946 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3524 - accuracy: 0.8728 - val_loss: 0.4783 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3396 - accuracy: 0.8728 - val_loss: 0.4646 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3291 - accuracy: 0.8750 - val_loss: 0.4530 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8750 - val_loss: 0.4431 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3125 - accuracy: 0.8817 - val_loss: 0.4343 - val_accuracy: 0.7946 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.8839 - val_loss: 0.4266 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3001 - accuracy: 0.8862 - val_loss: 0.4198 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2950 - accuracy: 0.8839 - val_loss: 0.4137 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2905 - accuracy: 0.8884 - val_loss: 0.4082 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8951 - val_loss: 0.4031 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2827 - accuracy: 0.8951 - val_loss: 0.3985 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8996 - val_loss: 0.3944 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2762 - accuracy: 0.9018 - val_loss: 0.3906 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2733 - accuracy: 0.9018 - val_loss: 0.3870 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.9062 - val_loss: 0.3837 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2681 - accuracy: 0.9129 - val_loss: 0.3807 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2658 - accuracy: 0.9129 - val_loss: 0.3777 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.9129 - val_loss: 0.3750 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2615 - accuracy: 0.9129 - val_loss: 0.3725 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2595 - accuracy: 0.9129 - val_loss: 0.3701 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.9129 - val_loss: 0.3678 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2558 - accuracy: 0.9152 - val_loss: 0.3655 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2541 - accuracy: 0.9152 - val_loss: 0.3634 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.9152 - val_loss: 0.3614 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2508 - accuracy: 0.9152 - val_loss: 0.3595 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2492 - accuracy: 0.9174 - val_loss: 0.3576 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.9174 - val_loss: 0.3558 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2462 - accuracy: 0.9174 - val_loss: 0.3541 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2448 - accuracy: 0.9196 - val_loss: 0.3525 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.9196 - val_loss: 0.3509 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2421 - accuracy: 0.9241 - val_loss: 0.3493 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2408 - accuracy: 0.9241 - val_loss: 0.3479 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2396 - accuracy: 0.9241 - val_loss: 0.3465 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2383 - accuracy: 0.9241 - val_loss: 0.3452 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2372 - accuracy: 0.9241 - val_loss: 0.3438 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.9241 - val_loss: 0.3426 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2349 - accuracy: 0.9263 - val_loss: 0.3414 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9263 - val_loss: 0.3402 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2326 - accuracy: 0.9263 - val_loss: 0.3391 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2316 - accuracy: 0.9263 - val_loss: 0.3379 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9263 - val_loss: 0.3369 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2295 - accuracy: 0.9263 - val_loss: 0.3358 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2285 - accuracy: 0.9286 - val_loss: 0.3348 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9286 - val_loss: 0.3338 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2266 - accuracy: 0.9286 - val_loss: 0.3328 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2256 - accuracy: 0.9286 - val_loss: 0.3319 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2247 - accuracy: 0.9286 - val_loss: 0.3309 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.9286 - val_loss: 0.3299 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2228 - accuracy: 0.9286 - val_loss: 0.3290 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9286 - val_loss: 0.3282 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9286 - val_loss: 0.3273 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2202 - accuracy: 0.9286 - val_loss: 0.3265 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9286 - val_loss: 0.3256 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2185 - accuracy: 0.9286 - val_loss: 0.3249 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.2177 - accuracy: 0.9286 - val_loss: 0.3240 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2169 - accuracy: 0.9308 - val_loss: 0.3233 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2161 - accuracy: 0.9308 - val_loss: 0.3225 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.9308 - val_loss: 0.3217 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2145 - accuracy: 0.9308 - val_loss: 0.3210 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2137 - accuracy: 0.9308 - val_loss: 0.3202 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9308 - val_loss: 0.3195 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2122 - accuracy: 0.9308 - val_loss: 0.3187 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2114 - accuracy: 0.9308 - val_loss: 0.3180 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9308 - val_loss: 0.3173 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2099 - accuracy: 0.9308 - val_loss: 0.3166 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.9330 - val_loss: 0.3158 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9330 - val_loss: 0.3151 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2077 - accuracy: 0.9330 - val_loss: 0.3144 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2070 - accuracy: 0.9330 - val_loss: 0.3138 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9330 - val_loss: 0.3131 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9330 - val_loss: 0.3124 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2049 - accuracy: 0.9330 - val_loss: 0.3118 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9353 - val_loss: 0.3111 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2036 - accuracy: 0.9353 - val_loss: 0.3104 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9375 - val_loss: 0.3098 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2022 - accuracy: 0.9375 - val_loss: 0.3091 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2015 - accuracy: 0.9375 - val_loss: 0.3085 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9375 - val_loss: 0.3079 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2003 - accuracy: 0.9375 - val_loss: 0.3072 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9375 - val_loss: 0.3066 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1990 - accuracy: 0.9375 - val_loss: 0.3060 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9375 - val_loss: 0.3054 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1978 - accuracy: 0.9375 - val_loss: 0.3048 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1972 - accuracy: 0.9375 - val_loss: 0.3042 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1966 - accuracy: 0.9375 - val_loss: 0.3037 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9375 - val_loss: 0.3031 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1955 - accuracy: 0.9375 - val_loss: 0.3026 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1949 - accuracy: 0.9375 - val_loss: 0.3021 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1943 - accuracy: 0.9375 - val_loss: 0.3016 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1938 - accuracy: 0.9375 - val_loss: 0.3011 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1932 - accuracy: 0.9375 - val_loss: 0.3006 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1926 - accuracy: 0.9375 - val_loss: 0.3001 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1921 - accuracy: 0.9375 - val_loss: 0.2996 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1916 - accuracy: 0.9375 - val_loss: 0.2991 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1910 - accuracy: 0.9375 - val_loss: 0.2986 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1905 - accuracy: 0.9375 - val_loss: 0.2982 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1900 - accuracy: 0.9375 - val_loss: 0.2977 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1895 - accuracy: 0.9375 - val_loss: 0.2972 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9375 - val_loss: 0.2968 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1884 - accuracy: 0.9375 - val_loss: 0.2964 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9375 - val_loss: 0.2959 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1874 - accuracy: 0.9375 - val_loss: 0.2955 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1869 - accuracy: 0.9375 - val_loss: 0.2950 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9375 - val_loss: 0.2946 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9397 - val_loss: 0.2943 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1855 - accuracy: 0.9397 - val_loss: 0.2939 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1850 - accuracy: 0.9397 - val_loss: 0.2934 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1845 - accuracy: 0.9397 - val_loss: 0.2931 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1840 - accuracy: 0.9397 - val_loss: 0.2927 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9397 - val_loss: 0.2923 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1831 - accuracy: 0.9397 - val_loss: 0.2919 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1826 - accuracy: 0.9397 - val_loss: 0.2916 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9397 - val_loss: 0.2912 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.9397 - val_loss: 0.2908 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9397 - val_loss: 0.2905 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.9397 - val_loss: 0.2901 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1803 - accuracy: 0.9397 - val_loss: 0.2897 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1799 - accuracy: 0.9397 - val_loss: 0.2894 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1794 - accuracy: 0.9397 - val_loss: 0.2890 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.9397 - val_loss: 0.2887 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.9397 - val_loss: 0.2883 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1781 - accuracy: 0.9397 - val_loss: 0.2880 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1777 - accuracy: 0.9397 - val_loss: 0.2876 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.9397 - val_loss: 0.2873 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1768 - accuracy: 0.9397 - val_loss: 0.2870 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1764 - accuracy: 0.9397 - val_loss: 0.2867 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.9397 - val_loss: 0.2863 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9420 - val_loss: 0.2860 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9420 - val_loss: 0.2857 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9420 - val_loss: 0.2854 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9420 - val_loss: 0.2850 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1738 - accuracy: 0.9420 - val_loss: 0.2847 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1734 - accuracy: 0.9420 - val_loss: 0.2844 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.9442 - val_loss: 0.2841 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1726 - accuracy: 0.9442 - val_loss: 0.2838 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1722 - accuracy: 0.9442 - val_loss: 0.2835 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1718 - accuracy: 0.9442 - val_loss: 0.2831 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1714 - accuracy: 0.9442 - val_loss: 0.2828 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9442 - val_loss: 0.2825 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1706 - accuracy: 0.9442 - val_loss: 0.2822 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9442 - val_loss: 0.2818 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9442 - val_loss: 0.2815 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9442 - val_loss: 0.2812 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9442 - val_loss: 0.2809 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9442 - val_loss: 0.2806 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9442 - val_loss: 0.2803 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9442 - val_loss: 0.2800 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1676 - accuracy: 0.9442 - val_loss: 0.2797 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1672 - accuracy: 0.9442 - val_loss: 0.2794 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1669 - accuracy: 0.9442 - val_loss: 0.2792 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9442 - val_loss: 0.2789 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1662 - accuracy: 0.9442 - val_loss: 0.2786 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1658 - accuracy: 0.9442 - val_loss: 0.2783 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.9442 - val_loss: 0.2780 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1651 - accuracy: 0.9442 - val_loss: 0.2776 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1648 - accuracy: 0.9442 - val_loss: 0.2774 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9464 - val_loss: 0.2771 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9464 - val_loss: 0.2768 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1638 - accuracy: 0.9464 - val_loss: 0.2765 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1634 - accuracy: 0.9464 - val_loss: 0.2762 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1631 - accuracy: 0.9464 - val_loss: 0.2759 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9464 - val_loss: 0.2756 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9464 - val_loss: 0.2754 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.9464 - val_loss: 0.2751 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1618 - accuracy: 0.9464 - val_loss: 0.2748 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9464 - val_loss: 0.2745 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9464 - val_loss: 0.2742 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1608 - accuracy: 0.9487 - val_loss: 0.2740 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9509 - val_loss: 0.2737 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9509 - val_loss: 0.2734 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1599 - accuracy: 0.9509 - val_loss: 0.2732 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9509 - val_loss: 0.2729 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1593 - accuracy: 0.9509 - val_loss: 0.2726 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1590 - accuracy: 0.9509 - val_loss: 0.2723 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1587 - accuracy: 0.9509 - val_loss: 0.2721 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9531 - val_loss: 0.2718 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.9531 - val_loss: 0.2716 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1578 - accuracy: 0.9531 - val_loss: 0.2713 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9531 - val_loss: 0.2711 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1572 - accuracy: 0.9531 - val_loss: 0.2708 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1569 - accuracy: 0.9531 - val_loss: 0.2705 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1566 - accuracy: 0.9531 - val_loss: 0.2702 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1563 - accuracy: 0.9531 - val_loss: 0.2700 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9531 - val_loss: 0.2697 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1557 - accuracy: 0.9531 - val_loss: 0.2695 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.9531 - val_loss: 0.2692 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.9531 - val_loss: 0.2689 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1549 - accuracy: 0.9531 - val_loss: 0.2687 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1546 - accuracy: 0.9531 - val_loss: 0.2684 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1543 - accuracy: 0.9531 - val_loss: 0.2682 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1540 - accuracy: 0.9531 - val_loss: 0.2679 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9531 - val_loss: 0.2677 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1535 - accuracy: 0.9531 - val_loss: 0.2675 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1532 - accuracy: 0.9531 - val_loss: 0.2672 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1529 - accuracy: 0.9531 - val_loss: 0.2670 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1526 - accuracy: 0.9531 - val_loss: 0.2667 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1524 - accuracy: 0.9531 - val_loss: 0.2665 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.9531 - val_loss: 0.2663 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9531 - val_loss: 0.2660 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1516 - accuracy: 0.9531 - val_loss: 0.2658 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1513 - accuracy: 0.9531 - val_loss: 0.2655 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1510 - accuracy: 0.9531 - val_loss: 0.2653 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9531 - val_loss: 0.2650 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1505 - accuracy: 0.9531 - val_loss: 0.2648 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9531 - val_loss: 0.2646 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1500 - accuracy: 0.9531 - val_loss: 0.2643 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1497 - accuracy: 0.9531 - val_loss: 0.2641 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9531 - val_loss: 0.2639 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9531 - val_loss: 0.2636 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9531 - val_loss: 0.2634 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1487 - accuracy: 0.9531 - val_loss: 0.2632 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9531 - val_loss: 0.2630 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9531 - val_loss: 0.2627 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1479 - accuracy: 0.9531 - val_loss: 0.2625 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.9531 - val_loss: 0.2623 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.9531 - val_loss: 0.2621 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1472 - accuracy: 0.9531 - val_loss: 0.2618 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1470 - accuracy: 0.9531 - val_loss: 0.2616 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.9531 - val_loss: 0.2614 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1465 - accuracy: 0.9531 - val_loss: 0.2611 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9531 - val_loss: 0.2609 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9531 - val_loss: 0.2607 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9531 - val_loss: 0.2605 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9531 - val_loss: 0.2603 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.9554 - val_loss: 0.2601 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1450 - accuracy: 0.9554 - val_loss: 0.2599 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9554 - val_loss: 0.2597 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9554 - val_loss: 0.2595 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.9554 - val_loss: 0.2593 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1441 - accuracy: 0.9554 - val_loss: 0.2591 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1439 - accuracy: 0.9554 - val_loss: 0.2589 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9554 - val_loss: 0.2587 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9554 - val_loss: 0.2584 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9554 - val_loss: 0.2582 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9554 - val_loss: 0.2580 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9554 - val_loss: 0.2578 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1425 - accuracy: 0.9554 - val_loss: 0.2576 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1422 - accuracy: 0.9554 - val_loss: 0.2574 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1420 - accuracy: 0.9554 - val_loss: 0.2573 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9554 - val_loss: 0.2571 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1416 - accuracy: 0.9554 - val_loss: 0.2569 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1414 - accuracy: 0.9554 - val_loss: 0.2567 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1411 - accuracy: 0.9554 - val_loss: 0.2565 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.9554 - val_loss: 0.2563 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1407 - accuracy: 0.9554 - val_loss: 0.2561 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1405 - accuracy: 0.9554 - val_loss: 0.2559 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.9554 - val_loss: 0.2557 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1400 - accuracy: 0.9554 - val_loss: 0.2555 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1398 - accuracy: 0.9554 - val_loss: 0.2553 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1396 - accuracy: 0.9554 - val_loss: 0.2551 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9554 - val_loss: 0.2549 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9554 - val_loss: 0.2547 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1390 - accuracy: 0.9554 - val_loss: 0.2545 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1387 - accuracy: 0.9554 - val_loss: 0.2544 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 0.9554 - val_loss: 0.2542 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9554 - val_loss: 0.2540 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9554 - val_loss: 0.2539 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1379 - accuracy: 0.9554 - val_loss: 0.2537 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1377 - accuracy: 0.9554 - val_loss: 0.2535 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1375 - accuracy: 0.9554 - val_loss: 0.2533 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1373 - accuracy: 0.9554 - val_loss: 0.2531 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1371 - accuracy: 0.9554 - val_loss: 0.2529 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9554 - val_loss: 0.2528 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1367 - accuracy: 0.9554 - val_loss: 0.2526 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1365 - accuracy: 0.9554 - val_loss: 0.2524 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.9554 - val_loss: 0.2522 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9554 - val_loss: 0.2520 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.9554 - val_loss: 0.2519 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9554 - val_loss: 0.2517 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9554 - val_loss: 0.2515 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1352 - accuracy: 0.9554 - val_loss: 0.2513 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9554 - val_loss: 0.2512 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.9554 - val_loss: 0.2510 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1347 - accuracy: 0.9554 - val_loss: 0.2508 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1345 - accuracy: 0.9554 - val_loss: 0.2506 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9554 - val_loss: 0.2504 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.9554 - val_loss: 0.2503 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9554 - val_loss: 0.2501 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9554 - val_loss: 0.2499 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.9554 - val_loss: 0.2497 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9554 - val_loss: 0.2496 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9554 - val_loss: 0.2494 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1329 - accuracy: 0.9554 - val_loss: 0.2493 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9554 - val_loss: 0.2491 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1325 - accuracy: 0.9554 - val_loss: 0.2489 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 0.9554 - val_loss: 0.2487 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1321 - accuracy: 0.9554 - val_loss: 0.2486 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1319 - accuracy: 0.9576 - val_loss: 0.2484 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9576 - val_loss: 0.2483 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1316 - accuracy: 0.9576 - val_loss: 0.2481 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9576 - val_loss: 0.2479 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9576 - val_loss: 0.2478 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Loss: 0.2478, Accuracy: 89.29%\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 9ms/step - loss: 0.6109 - accuracy: 0.7254 - val_loss: 0.5535 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5415 - accuracy: 0.7790 - val_loss: 0.5046 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.8237 - val_loss: 0.4688 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.8482 - val_loss: 0.4419 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.8594 - val_loss: 0.4212 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4018 - accuracy: 0.8638 - val_loss: 0.4050 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3837 - accuracy: 0.8728 - val_loss: 0.3918 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3686 - accuracy: 0.8795 - val_loss: 0.3810 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3559 - accuracy: 0.8839 - val_loss: 0.3719 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8884 - val_loss: 0.3642 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3354 - accuracy: 0.8951 - val_loss: 0.3575 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.8996 - val_loss: 0.3518 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8996 - val_loss: 0.3468 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.9062 - val_loss: 0.3424 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3065 - accuracy: 0.9085 - val_loss: 0.3385 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.9085 - val_loss: 0.3350 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.9085 - val_loss: 0.3319 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2914 - accuracy: 0.9085 - val_loss: 0.3291 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.9085 - val_loss: 0.3265 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2831 - accuracy: 0.9107 - val_loss: 0.3242 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2794 - accuracy: 0.9107 - val_loss: 0.3221 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.9107 - val_loss: 0.3202 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2726 - accuracy: 0.9107 - val_loss: 0.3184 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2696 - accuracy: 0.9129 - val_loss: 0.3167 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.9129 - val_loss: 0.3152 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2640 - accuracy: 0.9152 - val_loss: 0.3138 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.9174 - val_loss: 0.3125 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2590 - accuracy: 0.9174 - val_loss: 0.3112 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2568 - accuracy: 0.9174 - val_loss: 0.3100 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2545 - accuracy: 0.9174 - val_loss: 0.3089 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.9196 - val_loss: 0.3079 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.9196 - val_loss: 0.3069 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2485 - accuracy: 0.9219 - val_loss: 0.3060 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2466 - accuracy: 0.9219 - val_loss: 0.3050 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.9219 - val_loss: 0.3042 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2431 - accuracy: 0.9219 - val_loss: 0.3033 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2414 - accuracy: 0.9241 - val_loss: 0.3025 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2397 - accuracy: 0.9263 - val_loss: 0.3018 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2382 - accuracy: 0.9286 - val_loss: 0.3010 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2366 - accuracy: 0.9286 - val_loss: 0.3003 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2351 - accuracy: 0.9286 - val_loss: 0.2996 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2337 - accuracy: 0.9286 - val_loss: 0.2989 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.9308 - val_loss: 0.2983 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2309 - accuracy: 0.9308 - val_loss: 0.2976 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.9308 - val_loss: 0.2970 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2284 - accuracy: 0.9308 - val_loss: 0.2964 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.9308 - val_loss: 0.2958 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2259 - accuracy: 0.9330 - val_loss: 0.2952 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.2248 - accuracy: 0.9330 - val_loss: 0.2946 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.9330 - val_loss: 0.2941 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2225 - accuracy: 0.9330 - val_loss: 0.2935 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2214 - accuracy: 0.9353 - val_loss: 0.2930 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2204 - accuracy: 0.9353 - val_loss: 0.2925 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2194 - accuracy: 0.9353 - val_loss: 0.2919 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9375 - val_loss: 0.2914 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2174 - accuracy: 0.9375 - val_loss: 0.2909 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9375 - val_loss: 0.2904 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2155 - accuracy: 0.9375 - val_loss: 0.2899 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2146 - accuracy: 0.9375 - val_loss: 0.2894 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2137 - accuracy: 0.9375 - val_loss: 0.2890 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2128 - accuracy: 0.9375 - val_loss: 0.2885 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2119 - accuracy: 0.9375 - val_loss: 0.2880 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9375 - val_loss: 0.2876 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2102 - accuracy: 0.9375 - val_loss: 0.2871 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2094 - accuracy: 0.9375 - val_loss: 0.2867 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2086 - accuracy: 0.9375 - val_loss: 0.2862 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2078 - accuracy: 0.9375 - val_loss: 0.2858 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9375 - val_loss: 0.2854 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2062 - accuracy: 0.9375 - val_loss: 0.2849 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2055 - accuracy: 0.9375 - val_loss: 0.2845 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9397 - val_loss: 0.2841 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9397 - val_loss: 0.2837 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2033 - accuracy: 0.9397 - val_loss: 0.2832 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2026 - accuracy: 0.9397 - val_loss: 0.2828 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9397 - val_loss: 0.2824 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2012 - accuracy: 0.9397 - val_loss: 0.2820 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2005 - accuracy: 0.9397 - val_loss: 0.2816 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9397 - val_loss: 0.2812 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.9397 - val_loss: 0.2808 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1985 - accuracy: 0.9420 - val_loss: 0.2804 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1978 - accuracy: 0.9420 - val_loss: 0.2800 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9420 - val_loss: 0.2797 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9420 - val_loss: 0.2793 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1959 - accuracy: 0.9420 - val_loss: 0.2789 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1953 - accuracy: 0.9420 - val_loss: 0.2785 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9420 - val_loss: 0.2781 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1941 - accuracy: 0.9420 - val_loss: 0.2778 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9420 - val_loss: 0.2774 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9420 - val_loss: 0.2770 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1923 - accuracy: 0.9420 - val_loss: 0.2767 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1917 - accuracy: 0.9420 - val_loss: 0.2763 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1911 - accuracy: 0.9420 - val_loss: 0.2759 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9420 - val_loss: 0.2756 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1900 - accuracy: 0.9420 - val_loss: 0.2752 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1895 - accuracy: 0.9420 - val_loss: 0.2749 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1889 - accuracy: 0.9420 - val_loss: 0.2745 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1884 - accuracy: 0.9420 - val_loss: 0.2742 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9420 - val_loss: 0.2738 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9420 - val_loss: 0.2735 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9420 - val_loss: 0.2731 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9420 - val_loss: 0.2728 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.9420 - val_loss: 0.2724 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1853 - accuracy: 0.9420 - val_loss: 0.2721 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9420 - val_loss: 0.2718 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1842 - accuracy: 0.9420 - val_loss: 0.2714 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9420 - val_loss: 0.2711 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.9420 - val_loss: 0.2708 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1828 - accuracy: 0.9420 - val_loss: 0.2705 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1823 - accuracy: 0.9420 - val_loss: 0.2701 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.9420 - val_loss: 0.2698 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1813 - accuracy: 0.9420 - val_loss: 0.2695 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.9420 - val_loss: 0.2692 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1804 - accuracy: 0.9420 - val_loss: 0.2688 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1799 - accuracy: 0.9420 - val_loss: 0.2685 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1795 - accuracy: 0.9420 - val_loss: 0.2682 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 0.9442 - val_loss: 0.2679 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1786 - accuracy: 0.9442 - val_loss: 0.2676 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.1781 - accuracy: 0.9442 - val_loss: 0.2673 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1777 - accuracy: 0.9442 - val_loss: 0.2670 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1773 - accuracy: 0.9442 - val_loss: 0.2667 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1768 - accuracy: 0.9442 - val_loss: 0.2664 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1764 - accuracy: 0.9442 - val_loss: 0.2661 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1760 - accuracy: 0.9442 - val_loss: 0.2658 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1755 - accuracy: 0.9442 - val_loss: 0.2655 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9442 - val_loss: 0.2652 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.9442 - val_loss: 0.2649 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9442 - val_loss: 0.2646 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9464 - val_loss: 0.2643 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.9464 - val_loss: 0.2640 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.9464 - val_loss: 0.2637 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1727 - accuracy: 0.9464 - val_loss: 0.2634 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1723 - accuracy: 0.9464 - val_loss: 0.2632 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9464 - val_loss: 0.2629 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9464 - val_loss: 0.2626 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.9464 - val_loss: 0.2623 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1707 - accuracy: 0.9464 - val_loss: 0.2620 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9464 - val_loss: 0.2617 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1699 - accuracy: 0.9464 - val_loss: 0.2615 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.9464 - val_loss: 0.2612 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1692 - accuracy: 0.9487 - val_loss: 0.2609 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9487 - val_loss: 0.2606 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9487 - val_loss: 0.2604 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9487 - val_loss: 0.2601 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1677 - accuracy: 0.9487 - val_loss: 0.2598 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1673 - accuracy: 0.9509 - val_loss: 0.2595 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1670 - accuracy: 0.9509 - val_loss: 0.2593 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9509 - val_loss: 0.2590 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1662 - accuracy: 0.9509 - val_loss: 0.2587 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9509 - val_loss: 0.2585 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1655 - accuracy: 0.9509 - val_loss: 0.2582 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1652 - accuracy: 0.9509 - val_loss: 0.2580 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.9509 - val_loss: 0.2577 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9509 - val_loss: 0.2574 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1641 - accuracy: 0.9509 - val_loss: 0.2572 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1638 - accuracy: 0.9509 - val_loss: 0.2569 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.9509 - val_loss: 0.2567 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.9509 - val_loss: 0.2564 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9509 - val_loss: 0.2562 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1624 - accuracy: 0.9509 - val_loss: 0.2560 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1621 - accuracy: 0.9509 - val_loss: 0.2557 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1618 - accuracy: 0.9509 - val_loss: 0.2555 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9509 - val_loss: 0.2552 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1611 - accuracy: 0.9509 - val_loss: 0.2550 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1608 - accuracy: 0.9509 - val_loss: 0.2547 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9509 - val_loss: 0.2545 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9531 - val_loss: 0.2542 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1598 - accuracy: 0.9531 - val_loss: 0.2540 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.9531 - val_loss: 0.2538 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 0.9531 - val_loss: 0.2535 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1589 - accuracy: 0.9531 - val_loss: 0.2533 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1586 - accuracy: 0.9531 - val_loss: 0.2531 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.9531 - val_loss: 0.2528 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1580 - accuracy: 0.9531 - val_loss: 0.2526 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1577 - accuracy: 0.9531 - val_loss: 0.2524 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9531 - val_loss: 0.2521 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.9531 - val_loss: 0.2519 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9531 - val_loss: 0.2517 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9531 - val_loss: 0.2515 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9531 - val_loss: 0.2512 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9531 - val_loss: 0.2510 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1555 - accuracy: 0.9531 - val_loss: 0.2508 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9531 - val_loss: 0.2506 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1550 - accuracy: 0.9531 - val_loss: 0.2504 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9531 - val_loss: 0.2501 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9531 - val_loss: 0.2499 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9531 - val_loss: 0.2497 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1538 - accuracy: 0.9531 - val_loss: 0.2495 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.9531 - val_loss: 0.2493 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9531 - val_loss: 0.2490 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1529 - accuracy: 0.9531 - val_loss: 0.2488 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1527 - accuracy: 0.9531 - val_loss: 0.2486 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1524 - accuracy: 0.9531 - val_loss: 0.2484 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.9531 - val_loss: 0.2482 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1518 - accuracy: 0.9531 - val_loss: 0.2480 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1515 - accuracy: 0.9531 - val_loss: 0.2478 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9531 - val_loss: 0.2476 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1510 - accuracy: 0.9531 - val_loss: 0.2474 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1507 - accuracy: 0.9531 - val_loss: 0.2472 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1504 - accuracy: 0.9531 - val_loss: 0.2470 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9531 - val_loss: 0.2468 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1499 - accuracy: 0.9531 - val_loss: 0.2466 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1496 - accuracy: 0.9531 - val_loss: 0.2464 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1494 - accuracy: 0.9531 - val_loss: 0.2462 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1491 - accuracy: 0.9554 - val_loss: 0.2460 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1488 - accuracy: 0.9554 - val_loss: 0.2458 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9554 - val_loss: 0.2456 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9554 - val_loss: 0.2454 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1480 - accuracy: 0.9554 - val_loss: 0.2452 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9554 - val_loss: 0.2450 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9554 - val_loss: 0.2448 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1473 - accuracy: 0.9554 - val_loss: 0.2446 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1470 - accuracy: 0.9554 - val_loss: 0.2444 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.9554 - val_loss: 0.2442 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1465 - accuracy: 0.9554 - val_loss: 0.2441 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1463 - accuracy: 0.9554 - val_loss: 0.2439 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9554 - val_loss: 0.2437 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9554 - val_loss: 0.2435 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1455 - accuracy: 0.9554 - val_loss: 0.2433 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9554 - val_loss: 0.2431 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9554 - val_loss: 0.2429 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9554 - val_loss: 0.2427 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1445 - accuracy: 0.9554 - val_loss: 0.2425 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1443 - accuracy: 0.9554 - val_loss: 0.2423 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9576 - val_loss: 0.2422 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1438 - accuracy: 0.9576 - val_loss: 0.2420 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1435 - accuracy: 0.9576 - val_loss: 0.2418 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1433 - accuracy: 0.9576 - val_loss: 0.2416 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1430 - accuracy: 0.9576 - val_loss: 0.2415 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1428 - accuracy: 0.9576 - val_loss: 0.2413 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1426 - accuracy: 0.9576 - val_loss: 0.2411 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1423 - accuracy: 0.9576 - val_loss: 0.2409 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9576 - val_loss: 0.2408 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1418 - accuracy: 0.9576 - val_loss: 0.2406 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1416 - accuracy: 0.9576 - val_loss: 0.2404 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1414 - accuracy: 0.9576 - val_loss: 0.2403 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1411 - accuracy: 0.9576 - val_loss: 0.2401 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.9576 - val_loss: 0.2399 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9576 - val_loss: 0.2397 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1404 - accuracy: 0.9576 - val_loss: 0.2396 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9576 - val_loss: 0.2394 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1400 - accuracy: 0.9576 - val_loss: 0.2392 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1397 - accuracy: 0.9576 - val_loss: 0.2391 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9576 - val_loss: 0.2389 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9576 - val_loss: 0.2388 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1391 - accuracy: 0.9576 - val_loss: 0.2386 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1388 - accuracy: 0.9576 - val_loss: 0.2384 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9576 - val_loss: 0.2383 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.9576 - val_loss: 0.2381 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1382 - accuracy: 0.9576 - val_loss: 0.2380 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1379 - accuracy: 0.9576 - val_loss: 0.2378 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9576 - val_loss: 0.2377 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9576 - val_loss: 0.2375 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1373 - accuracy: 0.9576 - val_loss: 0.2373 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1371 - accuracy: 0.9576 - val_loss: 0.2372 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9576 - val_loss: 0.2370 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1366 - accuracy: 0.9576 - val_loss: 0.2369 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1364 - accuracy: 0.9576 - val_loss: 0.2367 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.9576 - val_loss: 0.2366 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9576 - val_loss: 0.2364 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.9576 - val_loss: 0.2363 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9576 - val_loss: 0.2361 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1353 - accuracy: 0.9576 - val_loss: 0.2360 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1351 - accuracy: 0.9598 - val_loss: 0.2358 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1349 - accuracy: 0.9576 - val_loss: 0.2357 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1347 - accuracy: 0.9598 - val_loss: 0.2355 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.9598 - val_loss: 0.2354 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9598 - val_loss: 0.2352 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1341 - accuracy: 0.9598 - val_loss: 0.2351 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.9598 - val_loss: 0.2350 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9598 - val_loss: 0.2348 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9598 - val_loss: 0.2346 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1333 - accuracy: 0.9598 - val_loss: 0.2345 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1330 - accuracy: 0.9598 - val_loss: 0.2344 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1329 - accuracy: 0.9598 - val_loss: 0.2342 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9598 - val_loss: 0.2341 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1324 - accuracy: 0.9598 - val_loss: 0.2339 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1322 - accuracy: 0.9598 - val_loss: 0.2338 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9598 - val_loss: 0.2337 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1318 - accuracy: 0.9598 - val_loss: 0.2335 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1316 - accuracy: 0.9621 - val_loss: 0.2334 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9621 - val_loss: 0.2333 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9621 - val_loss: 0.2331 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1310 - accuracy: 0.9621 - val_loss: 0.2330 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1308 - accuracy: 0.9621 - val_loss: 0.2329 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9621 - val_loss: 0.2327 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9621 - val_loss: 0.2326 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9621 - val_loss: 0.2325 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1301 - accuracy: 0.9621 - val_loss: 0.2323 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1299 - accuracy: 0.9621 - val_loss: 0.2322 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9621 - val_loss: 0.2320 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.9621 - val_loss: 0.2319 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9621 - val_loss: 0.2318 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9621 - val_loss: 0.2316 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1289 - accuracy: 0.9621 - val_loss: 0.2315 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1287 - accuracy: 0.9621 - val_loss: 0.2314 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1285 - accuracy: 0.9621 - val_loss: 0.2313 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1283 - accuracy: 0.9621 - val_loss: 0.2311 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9621 - val_loss: 0.2310 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1279 - accuracy: 0.9621 - val_loss: 0.2309 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9621 - val_loss: 0.2308 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Loss: 0.2308, Accuracy: 91.96%\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 8ms/step - loss: 0.7219 - accuracy: 0.6384 - val_loss: 0.6069 - val_accuracy: 0.6696 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.6257 - accuracy: 0.7098 - val_loss: 0.5230 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5572 - accuracy: 0.7500 - val_loss: 0.4648 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5065 - accuracy: 0.7768 - val_loss: 0.4231 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4678 - accuracy: 0.7991 - val_loss: 0.3923 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4376 - accuracy: 0.8080 - val_loss: 0.3689 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4134 - accuracy: 0.8192 - val_loss: 0.3506 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3936 - accuracy: 0.8326 - val_loss: 0.3360 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.8482 - val_loss: 0.3240 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3634 - accuracy: 0.8504 - val_loss: 0.3141 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3518 - accuracy: 0.8661 - val_loss: 0.3058 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.8683 - val_loss: 0.2988 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.8728 - val_loss: 0.2927 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.2874 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3189 - accuracy: 0.8750 - val_loss: 0.2828 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8839 - val_loss: 0.2787 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3076 - accuracy: 0.8884 - val_loss: 0.2750 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8929 - val_loss: 0.2717 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2982 - accuracy: 0.8951 - val_loss: 0.2687 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.8951 - val_loss: 0.2660 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2901 - accuracy: 0.8996 - val_loss: 0.2636 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.9018 - val_loss: 0.2613 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2832 - accuracy: 0.9018 - val_loss: 0.2592 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.9018 - val_loss: 0.2572 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2771 - accuracy: 0.9040 - val_loss: 0.2554 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2743 - accuracy: 0.9040 - val_loss: 0.2537 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2716 - accuracy: 0.9085 - val_loss: 0.2521 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2692 - accuracy: 0.9085 - val_loss: 0.2506 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.9129 - val_loss: 0.2492 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2645 - accuracy: 0.9152 - val_loss: 0.2478 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2624 - accuracy: 0.9152 - val_loss: 0.2465 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2603 - accuracy: 0.9152 - val_loss: 0.2453 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2583 - accuracy: 0.9174 - val_loss: 0.2441 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2564 - accuracy: 0.9174 - val_loss: 0.2430 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.9219 - val_loss: 0.2419 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2529 - accuracy: 0.9241 - val_loss: 0.2408 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.9241 - val_loss: 0.2397 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9286 - val_loss: 0.2387 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2480 - accuracy: 0.9286 - val_loss: 0.2378 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2464 - accuracy: 0.9286 - val_loss: 0.2368 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.9308 - val_loss: 0.2359 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2436 - accuracy: 0.9308 - val_loss: 0.2350 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9308 - val_loss: 0.2341 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2408 - accuracy: 0.9308 - val_loss: 0.2332 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.9308 - val_loss: 0.2324 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.9330 - val_loss: 0.2316 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2370 - accuracy: 0.9330 - val_loss: 0.2307 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2357 - accuracy: 0.9330 - val_loss: 0.2299 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.9330 - val_loss: 0.2292 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2334 - accuracy: 0.9330 - val_loss: 0.2284 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.9330 - val_loss: 0.2276 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2311 - accuracy: 0.9330 - val_loss: 0.2269 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.9330 - val_loss: 0.2261 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.9353 - val_loss: 0.2254 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2279 - accuracy: 0.9353 - val_loss: 0.2247 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2268 - accuracy: 0.9353 - val_loss: 0.2240 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2258 - accuracy: 0.9353 - val_loss: 0.2233 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.9353 - val_loss: 0.2226 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.9353 - val_loss: 0.2219 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2229 - accuracy: 0.9353 - val_loss: 0.2213 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9353 - val_loss: 0.2206 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.9353 - val_loss: 0.2200 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.9353 - val_loss: 0.2193 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2191 - accuracy: 0.9353 - val_loss: 0.2187 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2183 - accuracy: 0.9353 - val_loss: 0.2180 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2174 - accuracy: 0.9353 - val_loss: 0.2174 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2165 - accuracy: 0.9375 - val_loss: 0.2168 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2156 - accuracy: 0.9375 - val_loss: 0.2162 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2148 - accuracy: 0.9375 - val_loss: 0.2156 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2139 - accuracy: 0.9375 - val_loss: 0.2150 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9375 - val_loss: 0.2144 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9397 - val_loss: 0.2138 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2115 - accuracy: 0.9397 - val_loss: 0.2132 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9397 - val_loss: 0.2126 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9397 - val_loss: 0.2121 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2091 - accuracy: 0.9397 - val_loss: 0.2115 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2084 - accuracy: 0.9397 - val_loss: 0.2110 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2077 - accuracy: 0.9397 - val_loss: 0.2104 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9397 - val_loss: 0.2099 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2062 - accuracy: 0.9397 - val_loss: 0.2093 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9397 - val_loss: 0.2088 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2047 - accuracy: 0.9397 - val_loss: 0.2082 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9397 - val_loss: 0.2077 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2033 - accuracy: 0.9397 - val_loss: 0.2072 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2026 - accuracy: 0.9397 - val_loss: 0.2067 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2019 - accuracy: 0.9397 - val_loss: 0.2062 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2012 - accuracy: 0.9397 - val_loss: 0.2056 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2005 - accuracy: 0.9397 - val_loss: 0.2051 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1999 - accuracy: 0.9397 - val_loss: 0.2046 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1993 - accuracy: 0.9420 - val_loss: 0.2041 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9420 - val_loss: 0.2036 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1979 - accuracy: 0.9420 - val_loss: 0.2031 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1973 - accuracy: 0.9420 - val_loss: 0.2026 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1966 - accuracy: 0.9420 - val_loss: 0.2022 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1960 - accuracy: 0.9420 - val_loss: 0.2017 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1954 - accuracy: 0.9420 - val_loss: 0.2012 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1948 - accuracy: 0.9420 - val_loss: 0.2007 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1942 - accuracy: 0.9420 - val_loss: 0.2003 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9420 - val_loss: 0.1998 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1930 - accuracy: 0.9420 - val_loss: 0.1993 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1924 - accuracy: 0.9420 - val_loss: 0.1989 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.9420 - val_loss: 0.1984 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1913 - accuracy: 0.9442 - val_loss: 0.1980 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1907 - accuracy: 0.9442 - val_loss: 0.1975 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.9442 - val_loss: 0.1971 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1895 - accuracy: 0.9464 - val_loss: 0.1967 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1890 - accuracy: 0.9464 - val_loss: 0.1962 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1884 - accuracy: 0.9464 - val_loss: 0.1958 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9464 - val_loss: 0.1954 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1873 - accuracy: 0.9464 - val_loss: 0.1950 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1868 - accuracy: 0.9464 - val_loss: 0.1946 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1863 - accuracy: 0.9464 - val_loss: 0.1941 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1857 - accuracy: 0.9464 - val_loss: 0.1937 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1853 - accuracy: 0.9464 - val_loss: 0.1933 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9464 - val_loss: 0.1929 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1842 - accuracy: 0.9464 - val_loss: 0.1925 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9464 - val_loss: 0.1921 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.9464 - val_loss: 0.1917 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1827 - accuracy: 0.9464 - val_loss: 0.1913 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9464 - val_loss: 0.1910 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.9464 - val_loss: 0.1906 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9464 - val_loss: 0.1902 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1807 - accuracy: 0.9464 - val_loss: 0.1898 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1802 - accuracy: 0.9464 - val_loss: 0.1894 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9464 - val_loss: 0.1891 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9464 - val_loss: 0.1887 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1788 - accuracy: 0.9464 - val_loss: 0.1883 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1783 - accuracy: 0.9464 - val_loss: 0.1880 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9464 - val_loss: 0.1876 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1774 - accuracy: 0.9464 - val_loss: 0.1873 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9464 - val_loss: 0.1869 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1765 - accuracy: 0.9464 - val_loss: 0.1866 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9464 - val_loss: 0.1862 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1756 - accuracy: 0.9487 - val_loss: 0.1859 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9487 - val_loss: 0.1855 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9487 - val_loss: 0.1852 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1743 - accuracy: 0.9487 - val_loss: 0.1849 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1738 - accuracy: 0.9487 - val_loss: 0.1845 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1734 - accuracy: 0.9487 - val_loss: 0.1842 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.9487 - val_loss: 0.1839 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1725 - accuracy: 0.9487 - val_loss: 0.1835 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1721 - accuracy: 0.9487 - val_loss: 0.1832 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1717 - accuracy: 0.9487 - val_loss: 0.1829 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1712 - accuracy: 0.9487 - val_loss: 0.1826 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9487 - val_loss: 0.1823 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.9487 - val_loss: 0.1819 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9487 - val_loss: 0.1816 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.9487 - val_loss: 0.1813 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1692 - accuracy: 0.9487 - val_loss: 0.1810 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1688 - accuracy: 0.9487 - val_loss: 0.1807 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9487 - val_loss: 0.1804 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9487 - val_loss: 0.1801 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.9487 - val_loss: 0.1798 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1672 - accuracy: 0.9487 - val_loss: 0.1795 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1668 - accuracy: 0.9487 - val_loss: 0.1792 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9487 - val_loss: 0.1789 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1660 - accuracy: 0.9487 - val_loss: 0.1786 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1657 - accuracy: 0.9487 - val_loss: 0.1783 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1653 - accuracy: 0.9487 - val_loss: 0.1780 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1649 - accuracy: 0.9487 - val_loss: 0.1778 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9487 - val_loss: 0.1775 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1642 - accuracy: 0.9487 - val_loss: 0.1772 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1638 - accuracy: 0.9487 - val_loss: 0.1769 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9487 - val_loss: 0.1766 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.9487 - val_loss: 0.1764 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.9487 - val_loss: 0.1761 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9464 - val_loss: 0.1758 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9464 - val_loss: 0.1755 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1617 - accuracy: 0.9487 - val_loss: 0.1753 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1613 - accuracy: 0.9487 - val_loss: 0.1750 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1609 - accuracy: 0.9464 - val_loss: 0.1747 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.9487 - val_loss: 0.1745 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1603 - accuracy: 0.9464 - val_loss: 0.1742 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1599 - accuracy: 0.9464 - val_loss: 0.1739 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1596 - accuracy: 0.9464 - val_loss: 0.1737 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1592 - accuracy: 0.9464 - val_loss: 0.1734 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1589 - accuracy: 0.9464 - val_loss: 0.1732 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1586 - accuracy: 0.9487 - val_loss: 0.1729 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9487 - val_loss: 0.1727 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1579 - accuracy: 0.9487 - val_loss: 0.1724 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.9487 - val_loss: 0.1722 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.9487 - val_loss: 0.1719 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1569 - accuracy: 0.9487 - val_loss: 0.1717 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1566 - accuracy: 0.9487 - val_loss: 0.1714 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.9487 - val_loss: 0.1712 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1560 - accuracy: 0.9487 - val_loss: 0.1709 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1556 - accuracy: 0.9487 - val_loss: 0.1707 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9487 - val_loss: 0.1704 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9487 - val_loss: 0.1702 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1547 - accuracy: 0.9487 - val_loss: 0.1700 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9487 - val_loss: 0.1697 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9487 - val_loss: 0.1695 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9487 - val_loss: 0.1693 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1534 - accuracy: 0.9487 - val_loss: 0.1690 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.9487 - val_loss: 0.1688 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1528 - accuracy: 0.9487 - val_loss: 0.1686 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.9487 - val_loss: 0.1684 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9487 - val_loss: 0.1681 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9487 - val_loss: 0.1679 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.9487 - val_loss: 0.1677 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1513 - accuracy: 0.9487 - val_loss: 0.1675 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.9487 - val_loss: 0.1672 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1507 - accuracy: 0.9487 - val_loss: 0.1670 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.9487 - val_loss: 0.1668 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.9487 - val_loss: 0.1666 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9487 - val_loss: 0.1664 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9487 - val_loss: 0.1661 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9487 - val_loss: 0.1659 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1489 - accuracy: 0.9487 - val_loss: 0.1657 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9487 - val_loss: 0.1655 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.9487 - val_loss: 0.1653 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1480 - accuracy: 0.9487 - val_loss: 0.1651 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.9487 - val_loss: 0.1649 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1474 - accuracy: 0.9487 - val_loss: 0.1647 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9487 - val_loss: 0.1644 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1468 - accuracy: 0.9487 - val_loss: 0.1642 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1465 - accuracy: 0.9487 - val_loss: 0.1640 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9487 - val_loss: 0.1638 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1459 - accuracy: 0.9487 - val_loss: 0.1636 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1456 - accuracy: 0.9464 - val_loss: 0.1634 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1453 - accuracy: 0.9464 - val_loss: 0.1632 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9487 - val_loss: 0.1630 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9464 - val_loss: 0.1628 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9464 - val_loss: 0.1626 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1441 - accuracy: 0.9487 - val_loss: 0.1624 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1437 - accuracy: 0.9464 - val_loss: 0.1622 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1435 - accuracy: 0.9464 - val_loss: 0.1620 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9487 - val_loss: 0.1618 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1428 - accuracy: 0.9464 - val_loss: 0.1616 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1425 - accuracy: 0.9464 - val_loss: 0.1614 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1423 - accuracy: 0.9464 - val_loss: 0.1612 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.9464 - val_loss: 0.1610 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9464 - val_loss: 0.1608 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1414 - accuracy: 0.9464 - val_loss: 0.1607 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 0.9464 - val_loss: 0.1605 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.9464 - val_loss: 0.1603 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1406 - accuracy: 0.9464 - val_loss: 0.1601 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1404 - accuracy: 0.9464 - val_loss: 0.1599 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9464 - val_loss: 0.1597 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9464 - val_loss: 0.1595 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9464 - val_loss: 0.1594 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9464 - val_loss: 0.1592 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1391 - accuracy: 0.9464 - val_loss: 0.1590 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9464 - val_loss: 0.1588 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1386 - accuracy: 0.9464 - val_loss: 0.1586 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9487 - val_loss: 0.1585 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9487 - val_loss: 0.1583 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1379 - accuracy: 0.9487 - val_loss: 0.1581 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.9487 - val_loss: 0.1579 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.9487 - val_loss: 0.1578 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1372 - accuracy: 0.9487 - val_loss: 0.1576 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9487 - val_loss: 0.1574 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1367 - accuracy: 0.9487 - val_loss: 0.1572 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1365 - accuracy: 0.9487 - val_loss: 0.1571 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1363 - accuracy: 0.9487 - val_loss: 0.1569 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9487 - val_loss: 0.1567 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1358 - accuracy: 0.9487 - val_loss: 0.1566 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9487 - val_loss: 0.1564 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 0.9487 - val_loss: 0.1562 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1351 - accuracy: 0.9487 - val_loss: 0.1561 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9487 - val_loss: 0.1559 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1347 - accuracy: 0.9487 - val_loss: 0.1557 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9487 - val_loss: 0.1556 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9487 - val_loss: 0.1554 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1340 - accuracy: 0.9487 - val_loss: 0.1553 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1338 - accuracy: 0.9487 - val_loss: 0.1551 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9487 - val_loss: 0.1549 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9487 - val_loss: 0.1548 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1332 - accuracy: 0.9487 - val_loss: 0.1546 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.9487 - val_loss: 0.1545 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.9487 - val_loss: 0.1543 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9487 - val_loss: 0.1541 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 0.9509 - val_loss: 0.1540 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1321 - accuracy: 0.9509 - val_loss: 0.1538 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1319 - accuracy: 0.9509 - val_loss: 0.1537 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9509 - val_loss: 0.1535 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9509 - val_loss: 0.1534 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9509 - val_loss: 0.1532 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1310 - accuracy: 0.9509 - val_loss: 0.1531 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9509 - val_loss: 0.1529 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9509 - val_loss: 0.1528 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1304 - accuracy: 0.9509 - val_loss: 0.1526 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9509 - val_loss: 0.1525 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1300 - accuracy: 0.9509 - val_loss: 0.1523 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9509 - val_loss: 0.1522 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9509 - val_loss: 0.1520 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9509 - val_loss: 0.1519 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9509 - val_loss: 0.1517 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1290 - accuracy: 0.9509 - val_loss: 0.1516 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9531 - val_loss: 0.1514 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.9531 - val_loss: 0.1513 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1284 - accuracy: 0.9531 - val_loss: 0.1512 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1282 - accuracy: 0.9531 - val_loss: 0.1510 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9531 - val_loss: 0.1509 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9531 - val_loss: 0.1507 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9531 - val_loss: 0.1506 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1274 - accuracy: 0.9531 - val_loss: 0.1504 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9531 - val_loss: 0.1503 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1271 - accuracy: 0.9531 - val_loss: 0.1502 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1269 - accuracy: 0.9531 - val_loss: 0.1500 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Loss: 0.1500, Accuracy: 94.64%\n",
      "Vanilla_RNN finished in 275.41 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_20 (SimpleRNN)   (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Average accuracy: 0.9196\n",
      "Average loss: 0.2018\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing vanilla RNN with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 300, verbose = 1, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('Vanilla_RNN finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_1(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing vanilla RNN with K-fold\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.46640, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.46640 to 0.43242, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.43242 to 0.40616, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.40616 to 0.38537, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.38537 to 0.36865, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.36865 to 0.35482, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.35482 to 0.34320, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.34320 to 0.33319, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.33319 to 0.32475, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.32475 to 0.31755, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.31755 to 0.31106, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.31106 to 0.30529, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.30529 to 0.30016, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.30016 to 0.29559, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.29559 to 0.29134, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.29134 to 0.28758, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.28758 to 0.28418, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.28418 to 0.28105, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.28105 to 0.27815, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.27815 to 0.27550, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.27550 to 0.27312, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.27312 to 0.27091, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.27091 to 0.26876, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.26876 to 0.26671, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.26671 to 0.26481, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.26481 to 0.26303, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.26303 to 0.26148, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.26148 to 0.25989, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.25989 to 0.25838, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.25838 to 0.25710, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.25710 to 0.25590, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.25590 to 0.25458, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.25458 to 0.25332, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.25332 to 0.25203, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.25203 to 0.25088, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.25088 to 0.24979, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.24979 to 0.24872, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.24872 to 0.24771, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.24771 to 0.24668, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.24668 to 0.24570, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.24570 to 0.24478, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.24478 to 0.24385, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.24385 to 0.24296, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.24296 to 0.24211, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.24211 to 0.24129, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.24129 to 0.24050, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.24050 to 0.23971, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.23971 to 0.23895, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.23895 to 0.23823, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.23823 to 0.23750, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.23750 to 0.23669, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.23669 to 0.23597, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.23597 to 0.23535, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.23535 to 0.23469, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.23469 to 0.23405, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.23405 to 0.23347, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.23347 to 0.23296, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.23296 to 0.23231, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.23231 to 0.23171, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.23171 to 0.23111, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.23111 to 0.23049, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.23049 to 0.22990, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.22990 to 0.22933, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.22933 to 0.22869, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.22869 to 0.22815, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.22815 to 0.22759, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.22759 to 0.22708, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.22708 to 0.22653, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.22653 to 0.22598, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.22598 to 0.22547, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.22547 to 0.22495, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.22495 to 0.22442, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.22442 to 0.22392, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.22392 to 0.22344, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.22344 to 0.22293, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.22293 to 0.22248, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.22248 to 0.22203, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.22203 to 0.22157, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.22157 to 0.22116, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.22116 to 0.22070, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.22070 to 0.22022, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.22022 to 0.21974, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.21974 to 0.21926, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.21926 to 0.21883, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.21883 to 0.21841, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.21841 to 0.21799, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.21799 to 0.21751, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.21751 to 0.21710, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.21710 to 0.21670, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.21670 to 0.21627, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.21627 to 0.21579, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.21579 to 0.21536, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.21536 to 0.21495, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.21495 to 0.21462, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.21462 to 0.21424, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.21424 to 0.21385, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.21385 to 0.21346, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.21346 to 0.21316, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.21316 to 0.21268, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.21268 to 0.21229, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.21229 to 0.21191, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.21191 to 0.21154, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.21154 to 0.21115, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.21115 to 0.21084, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.21084 to 0.21043, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.21043 to 0.21008, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.21008 to 0.20971, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.20971 to 0.20933, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.20933 to 0.20896, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.20896 to 0.20859, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.20859 to 0.20823, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.20823 to 0.20788, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.20788 to 0.20753, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.20753 to 0.20718, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.20718 to 0.20683, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.20683 to 0.20648, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.20648 to 0.20608, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.20608 to 0.20579, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.20579 to 0.20545, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.20545 to 0.20513, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.20513 to 0.20480, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.20480 to 0.20445, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.20445 to 0.20414, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.20414 to 0.20383, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.20383 to 0.20353, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.20353 to 0.20322, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.20322 to 0.20282, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.20282 to 0.20252, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.20252 to 0.20222, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.20222 to 0.20191, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.20191 to 0.20159, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.20159 to 0.20124, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.20124 to 0.20095, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.20095 to 0.20065, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.20065 to 0.20036, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.20036 to 0.20008, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.20008 to 0.19983, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.19983 to 0.19953, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.19953 to 0.19924, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.19924 to 0.19894, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.19894 to 0.19870, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.19870 to 0.19838, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.19838 to 0.19809, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.19809 to 0.19778, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.19778 to 0.19748, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.19748 to 0.19720, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.19720 to 0.19693, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.19693 to 0.19661, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.19661 to 0.19632, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.19632 to 0.19604, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.19604 to 0.19569, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.19569 to 0.19543, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.19543 to 0.19518, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.19518 to 0.19493, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.19493 to 0.19472, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.19472 to 0.19447, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.19447 to 0.19420, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.19420 to 0.19394, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.19394 to 0.19369, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.19369 to 0.19342, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.19342 to 0.19315, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.19315 to 0.19297, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.19297 to 0.19270, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.19270 to 0.19242, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.19242 to 0.19230, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.19230 to 0.19207, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.19207 to 0.19177, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.19177 to 0.19147, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.19147 to 0.19122, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.19122 to 0.19097, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.19097 to 0.19075, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.19075 to 0.19051, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.19051 to 0.19028, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.19028 to 0.19006, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.19006 to 0.18981, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.18981 to 0.18958, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.18958 to 0.18934, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.18934 to 0.18913, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.18913 to 0.18890, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.18890 to 0.18868, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.18868 to 0.18849, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.18849 to 0.18825, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.18825 to 0.18812, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.18812 to 0.18792, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.18792 to 0.18771, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.18771 to 0.18748, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.18748 to 0.18727, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.18727 to 0.18698, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.18698 to 0.18674, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.18674 to 0.18653, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.18653 to 0.18628, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.18628 to 0.18606, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.18606 to 0.18588, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.18588 to 0.18575, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.18575 to 0.18552, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.18552 to 0.18531, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.18531 to 0.18521, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.18521 to 0.18500, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.18500 to 0.18478, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.18478 to 0.18452, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.18452 to 0.18433, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.18433 to 0.18411, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.18411 to 0.18388, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.18388 to 0.18368, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.18368 to 0.18351, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.18351 to 0.18331, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.18331 to 0.18311, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.18311 to 0.18292, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.18292 to 0.18273, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.18273 to 0.18254, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.18254 to 0.18249, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.18249 to 0.18224, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.18224 to 0.18207, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.18207 to 0.18183, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.18183 to 0.18162, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.18162 to 0.18142, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.18142 to 0.18122, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.18122 to 0.18104, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.18104 to 0.18080, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.18080 to 0.18064, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.18064 to 0.18046, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.18046 to 0.18028, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.18028 to 0.18010, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.18010 to 0.17994, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.17994 to 0.17978, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.17978 to 0.17967, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.17967 to 0.17949, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.17949 to 0.17930, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.17930 to 0.17917, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.17917 to 0.17898, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.17898 to 0.17887, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.17887 to 0.17871, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.17871 to 0.17847, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.17847 to 0.17827, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.17827 to 0.17812, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.17812 to 0.17795, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.17795 to 0.17780, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.17780 to 0.17763, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.17763 to 0.17746, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.17746 to 0.17728, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.17728 to 0.17712, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.17712 to 0.17695, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.17695 to 0.17680, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.17680\n",
      "\n",
      "Epoch 245: val_loss improved from 0.17680 to 0.17665, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.17665 to 0.17649, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.17649 to 0.17632, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.17632 to 0.17616, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.17616 to 0.17601, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.17601 to 0.17585, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.17585 to 0.17570, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.17570 to 0.17555, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.17555 to 0.17540, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.17540 to 0.17525, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.17525 to 0.17509, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.17509 to 0.17494, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.17494 to 0.17476, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.17476 to 0.17461, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.17461 to 0.17448, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.17448 to 0.17435, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.17435 to 0.17415, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.17415 to 0.17404, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.17404 to 0.17391, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.17391 to 0.17378, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.17378 to 0.17365, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.17365 to 0.17351, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.17351 to 0.17334, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.17334 to 0.17330, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.17330 to 0.17317, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.17317 to 0.17304, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.17304 to 0.17290, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.17290 to 0.17276, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.17276 to 0.17265, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.17265 to 0.17247, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.17247 to 0.17244, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.17244 to 0.17229, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.17229 to 0.17214, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.17214 to 0.17201, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.17201 to 0.17185, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.17185 to 0.17173, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.17173 to 0.17159, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.17159 to 0.17147, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.17147 to 0.17132, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.17132 to 0.17120, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.17120 to 0.17106, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.17106 to 0.17092, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.17092 to 0.17080, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.17080 to 0.17065, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.17065 to 0.17052, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.17052 to 0.17037, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.17037 to 0.17024, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.17024 to 0.17013, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.17013 to 0.16999, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.16999 to 0.16987, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.16987 to 0.16974, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.16974 to 0.16963, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.16963 to 0.16951, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.16951 to 0.16939, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.16939 to 0.16932, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.16932 to 0.16919, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "Loss: 0.1692, Accuracy: 94.64%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16919\n",
      "Loss: 0.1963, Accuracy: 91.07%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 35: val_loss improved from 0.16919 to 0.16888, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.16888 to 0.16759, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.16759 to 0.16630, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.16630 to 0.16506, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.16506 to 0.16386, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.16386 to 0.16275, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.16275 to 0.16162, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.16162 to 0.16070, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.16070 to 0.15963, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.15963 to 0.15859, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.15859 to 0.15770, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.15770 to 0.15672, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.15672 to 0.15579, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.15579 to 0.15500, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.15500 to 0.15409, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.15409 to 0.15326, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.15326 to 0.15239, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.15239 to 0.15150, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.15150 to 0.15066, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.15066 to 0.14991, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.14991 to 0.14911, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.14911 to 0.14835, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.14835 to 0.14760, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.14760 to 0.14685, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.14685 to 0.14615, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.14615 to 0.14545, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.14545 to 0.14476, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.14476 to 0.14409, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.14409 to 0.14342, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.14342 to 0.14276, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.14276 to 0.14208, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.14208 to 0.14148, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.14148 to 0.14083, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.14083 to 0.14022, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.14022 to 0.13959, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.13959 to 0.13896, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.13896 to 0.13835, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.13835 to 0.13776, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.13776 to 0.13720, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.13720 to 0.13664, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.13664 to 0.13608, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.13608 to 0.13552, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.13552 to 0.13495, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.13495 to 0.13442, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.13442 to 0.13388, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.13388 to 0.13336, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.13336 to 0.13289, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.13289 to 0.13245, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.13245 to 0.13195, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.13195 to 0.13144, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.13144 to 0.13098, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.13098 to 0.13049, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.13049 to 0.13001, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.13001 to 0.12954, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.12954 to 0.12907, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.12907 to 0.12868, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.12868 to 0.12823, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.12823 to 0.12780, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.12780 to 0.12734, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.12734 to 0.12692, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.12692 to 0.12654, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.12654 to 0.12610, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.12610 to 0.12572, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.12572 to 0.12529, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.12529 to 0.12485, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.12485 to 0.12440, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.12440 to 0.12398, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.12398 to 0.12377, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.12377 to 0.12339, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.12339 to 0.12296, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.12296 to 0.12255, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.12255 to 0.12216, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.12216 to 0.12183, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.12183 to 0.12142, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.12142 to 0.12102, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.12102 to 0.12063, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.12063 to 0.12030, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.12030 to 0.11991, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.11991 to 0.11953, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.11953 to 0.11916, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.11916 to 0.11879, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.11879 to 0.11847, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.11847 to 0.11808, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.11808 to 0.11772, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.11772 to 0.11747, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.11747 to 0.11710, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.11710 to 0.11676, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.11676 to 0.11641, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.11641 to 0.11607, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.11607 to 0.11573, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.11573 to 0.11538, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.11538 to 0.11508, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.11508 to 0.11475, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.11475 to 0.11444, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.11444 to 0.11412, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.11412 to 0.11382, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.11382 to 0.11354, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.11354 to 0.11322, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.11322 to 0.11298, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.11298 to 0.11266, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.11266 to 0.11235, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.11235 to 0.11205, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.11205 to 0.11182, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.11182 to 0.11152, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.11152 to 0.11123, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.11123 to 0.11094, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.11094 to 0.11066, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.11066 to 0.11040, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.11040 to 0.11012, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.11012 to 0.10984, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.10984 to 0.10957, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.10957 to 0.10939, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.10939 to 0.10911, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.10911 to 0.10888, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.10888 to 0.10864, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.10864 to 0.10837, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.10837 to 0.10813, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.10813 to 0.10786, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.10786 to 0.10761, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.10761 to 0.10735, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.10735 to 0.10712, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.10712 to 0.10693, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.10693 to 0.10668, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.10668 to 0.10645, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.10645 to 0.10621, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.10621 to 0.10599, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.10599 to 0.10576, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.10576 to 0.10552, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.10552 to 0.10530, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.10530 to 0.10508, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.10508 to 0.10488, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.10488 to 0.10468, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.10468 to 0.10446, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.10446 to 0.10423, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.10423 to 0.10404, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.10404 to 0.10385, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.10385 to 0.10364, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.10364 to 0.10345, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.10345 to 0.10326, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.10326 to 0.10306, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.10306 to 0.10287, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.10287 to 0.10268, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.10268 to 0.10250, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.10250 to 0.10231, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.10231 to 0.10212, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.10212 to 0.10192, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.10192 to 0.10173, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.10173 to 0.10155, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.10155 to 0.10139, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.10139 to 0.10124, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.10124 to 0.10110, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.10110 to 0.10093, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.10093 to 0.10075, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.10075 to 0.10060, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.10060 to 0.10044, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.10044 to 0.10028, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.10028 to 0.10010, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.10010 to 0.09995, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.09995 to 0.09979, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.09979 to 0.09964, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.09964 to 0.09948, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.09948 to 0.09934, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.09934 to 0.09919, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.09919 to 0.09905, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.09905 to 0.09889, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.09889 to 0.09874, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.09874 to 0.09860, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.09860 to 0.09846, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.09846 to 0.09834, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.09834 to 0.09821, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.09821 to 0.09808, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.09808 to 0.09794, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.09794 to 0.09780, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.09780 to 0.09768, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.09768 to 0.09754, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.09754 to 0.09741, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.09741 to 0.09739, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.09739 to 0.09726, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.09726 to 0.09711, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.09711 to 0.09701, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.09701 to 0.09691, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.09691 to 0.09678, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.09678 to 0.09666, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.09666 to 0.09652, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.09652 to 0.09639, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.09639 to 0.09627, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.09627 to 0.09615, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.09615 to 0.09603, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.09603 to 0.09593, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.09593 to 0.09582, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.09582 to 0.09570, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.09570 to 0.09559, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.09559 to 0.09546, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.09546 to 0.09536, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.09536 to 0.09525, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.09525 to 0.09515, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.09515 to 0.09504, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.09504 to 0.09497, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.09497 to 0.09489, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.09489 to 0.09478, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.09478 to 0.09466, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.09466 to 0.09456, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.09456 to 0.09445, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.09445 to 0.09442, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.09442 to 0.09432, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.09432 to 0.09421, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.09421 to 0.09413, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.09413 to 0.09404, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.09404 to 0.09393, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.09393 to 0.09384, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.09384 to 0.09376, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.09376 to 0.09367, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.09367 to 0.09356, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.09356 to 0.09345, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.09345 to 0.09343, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.09343 to 0.09336, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.09336 to 0.09328, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.09328 to 0.09322, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.09322 to 0.09316, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.09316 to 0.09308, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.09308 to 0.09300, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.09300 to 0.09296, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.09296 to 0.09286, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.09286 to 0.09277, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.09277 to 0.09269, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.09269 to 0.09262, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.09262 to 0.09254, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.09254 to 0.09246, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.09246 to 0.09241, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.09241 to 0.09233, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.09233 to 0.09226, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.09226 to 0.09217, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.09217 to 0.09209, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.09209 to 0.09207, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09207\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09207\n",
      "\n",
      "Epoch 271: val_loss improved from 0.09207 to 0.09205, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.09205 to 0.09199, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.09199 to 0.09190, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09190\n",
      "\n",
      "Epoch 275: val_loss improved from 0.09190 to 0.09185, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.09185 to 0.09177, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.09177 to 0.09167, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.09167 to 0.09160, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.09160 to 0.09154, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.09154 to 0.09147, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.09147 to 0.09140, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.09140 to 0.09132, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.09132 to 0.09126, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.09126 to 0.09119, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.09119 to 0.09111, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.09111 to 0.09107, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.09107 to 0.09100, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.09100 to 0.09093, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.09093 to 0.09086, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.09086 to 0.09081, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.09081 to 0.09075, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09075\n",
      "\n",
      "Epoch 293: val_loss improved from 0.09075 to 0.09069, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.09069 to 0.09064, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.09064 to 0.09059, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.09059 to 0.09050, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.09050 to 0.09044, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.09044 to 0.09039, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.09039 to 0.09033, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.09033 to 0.09027, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "Loss: 0.0903, Accuracy: 98.21%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.1526, Accuracy: 98.21%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.1962, Accuracy: 92.86%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.2637, Accuracy: 91.07%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.2185, Accuracy: 91.07%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.3254, Accuracy: 89.29%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.1954, Accuracy: 92.86%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.1466, Accuracy: 94.64%\n",
      "Vanilla_RNN finished in 372.18 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_30 (SimpleRNN)   (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Average accuracy: 0.9339\n",
      "Average loss: 0.1954\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN_10-fold.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "k_fold = 10 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing vanilla RNN with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 300, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('Vanilla_RNN finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_1(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average loss curve\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(train_loss[i])\n",
    "plt.title('Training Loss - All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(train_acc[i])\n",
    "plt.title('Training Accuracy - All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "40/40 [==============================] - 3s 22ms/step - loss: 0.7058 - accuracy: 0.4856 - val_loss: 0.7151 - val_accuracy: 0.4643 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5304 - val_loss: 0.7023 - val_accuracy: 0.5476 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6808 - accuracy: 0.5847 - val_loss: 0.6900 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6688 - accuracy: 0.6006 - val_loss: 0.6782 - val_accuracy: 0.5952 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6572 - accuracy: 0.6581 - val_loss: 0.6671 - val_accuracy: 0.6369 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6463 - accuracy: 0.6901 - val_loss: 0.6562 - val_accuracy: 0.6726 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6356 - accuracy: 0.7284 - val_loss: 0.6462 - val_accuracy: 0.7024 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6259 - accuracy: 0.7508 - val_loss: 0.6362 - val_accuracy: 0.7262 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6160 - accuracy: 0.7636 - val_loss: 0.6264 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6064 - accuracy: 0.7700 - val_loss: 0.6171 - val_accuracy: 0.7560 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5973 - accuracy: 0.7859 - val_loss: 0.6079 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5882 - accuracy: 0.8019 - val_loss: 0.5991 - val_accuracy: 0.7798 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5796 - accuracy: 0.8243 - val_loss: 0.5905 - val_accuracy: 0.7798 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5712 - accuracy: 0.8339 - val_loss: 0.5822 - val_accuracy: 0.7917 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5631 - accuracy: 0.8403 - val_loss: 0.5743 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5553 - accuracy: 0.8498 - val_loss: 0.5665 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5478 - accuracy: 0.8562 - val_loss: 0.5590 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.5405 - accuracy: 0.8722 - val_loss: 0.5517 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5333 - accuracy: 0.8818 - val_loss: 0.5448 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5266 - accuracy: 0.8850 - val_loss: 0.5380 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5199 - accuracy: 0.8946 - val_loss: 0.5315 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5135 - accuracy: 0.8946 - val_loss: 0.5251 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5072 - accuracy: 0.9010 - val_loss: 0.5188 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.9010 - val_loss: 0.5127 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4952 - accuracy: 0.9010 - val_loss: 0.5070 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4895 - accuracy: 0.9010 - val_loss: 0.5014 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4840 - accuracy: 0.9010 - val_loss: 0.4959 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4786 - accuracy: 0.9010 - val_loss: 0.4904 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.9010 - val_loss: 0.4850 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4682 - accuracy: 0.9010 - val_loss: 0.4801 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4634 - accuracy: 0.9010 - val_loss: 0.4751 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4585 - accuracy: 0.9010 - val_loss: 0.4702 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4537 - accuracy: 0.9042 - val_loss: 0.4654 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4491 - accuracy: 0.9042 - val_loss: 0.4607 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4446 - accuracy: 0.9073 - val_loss: 0.4561 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4402 - accuracy: 0.9105 - val_loss: 0.4517 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.9105 - val_loss: 0.4474 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.9105 - val_loss: 0.4432 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.9105 - val_loss: 0.4390 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.9105 - val_loss: 0.4350 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.9137 - val_loss: 0.4310 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4159 - accuracy: 0.9137 - val_loss: 0.4271 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4122 - accuracy: 0.9137 - val_loss: 0.4233 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4086 - accuracy: 0.9137 - val_loss: 0.4196 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4050 - accuracy: 0.9137 - val_loss: 0.4160 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4015 - accuracy: 0.9169 - val_loss: 0.4124 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3981 - accuracy: 0.9169 - val_loss: 0.4089 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3947 - accuracy: 0.9169 - val_loss: 0.4055 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3915 - accuracy: 0.9169 - val_loss: 0.4024 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3885 - accuracy: 0.9169 - val_loss: 0.3995 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3856 - accuracy: 0.9169 - val_loss: 0.3962 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 0.9169 - val_loss: 0.3931 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3794 - accuracy: 0.9169 - val_loss: 0.3900 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3764 - accuracy: 0.9169 - val_loss: 0.3869 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3735 - accuracy: 0.9201 - val_loss: 0.3840 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.9201 - val_loss: 0.3810 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3678 - accuracy: 0.9201 - val_loss: 0.3782 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.3651 - accuracy: 0.9201 - val_loss: 0.3754 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3624 - accuracy: 0.9201 - val_loss: 0.3726 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3598 - accuracy: 0.9233 - val_loss: 0.3699 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3572 - accuracy: 0.9233 - val_loss: 0.3674 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3548 - accuracy: 0.9233 - val_loss: 0.3648 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3523 - accuracy: 0.9233 - val_loss: 0.3622 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3499 - accuracy: 0.9233 - val_loss: 0.3598 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3475 - accuracy: 0.9233 - val_loss: 0.3573 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3452 - accuracy: 0.9233 - val_loss: 0.3552 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3431 - accuracy: 0.9233 - val_loss: 0.3529 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3409 - accuracy: 0.9233 - val_loss: 0.3506 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3387 - accuracy: 0.9233 - val_loss: 0.3483 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.9233 - val_loss: 0.3460 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.3344 - accuracy: 0.9233 - val_loss: 0.3439 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3323 - accuracy: 0.9233 - val_loss: 0.3417 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.3302 - accuracy: 0.9233 - val_loss: 0.3397 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3282 - accuracy: 0.9265 - val_loss: 0.3376 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.9265 - val_loss: 0.3356 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.9265 - val_loss: 0.3336 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3224 - accuracy: 0.9265 - val_loss: 0.3316 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3205 - accuracy: 0.9265 - val_loss: 0.3297 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.9265 - val_loss: 0.3278 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.9297 - val_loss: 0.3259 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.9297 - val_loss: 0.3240 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.9297 - val_loss: 0.3222 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3115 - accuracy: 0.9297 - val_loss: 0.3204 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3098 - accuracy: 0.9297 - val_loss: 0.3186 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3081 - accuracy: 0.9297 - val_loss: 0.3169 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3065 - accuracy: 0.9297 - val_loss: 0.3152 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3048 - accuracy: 0.9297 - val_loss: 0.3135 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.3032 - accuracy: 0.9297 - val_loss: 0.3119 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.3016 - accuracy: 0.9297 - val_loss: 0.3103 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.3001 - accuracy: 0.9297 - val_loss: 0.3088 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2986 - accuracy: 0.9297 - val_loss: 0.3073 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2971 - accuracy: 0.9297 - val_loss: 0.3057 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2956 - accuracy: 0.9297 - val_loss: 0.3041 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2941 - accuracy: 0.9297 - val_loss: 0.3026 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2926 - accuracy: 0.9297 - val_loss: 0.3012 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2912 - accuracy: 0.9297 - val_loss: 0.2997 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2898 - accuracy: 0.9297 - val_loss: 0.2983 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.9297 - val_loss: 0.2969 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2870 - accuracy: 0.9297 - val_loss: 0.2955 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2856 - accuracy: 0.9297 - val_loss: 0.2941 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2843 - accuracy: 0.9297 - val_loss: 0.2927 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2830 - accuracy: 0.9297 - val_loss: 0.2915 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2817 - accuracy: 0.9297 - val_loss: 0.2902 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2805 - accuracy: 0.9297 - val_loss: 0.2889 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2792 - accuracy: 0.9297 - val_loss: 0.2876 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2779 - accuracy: 0.9297 - val_loss: 0.2864 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.9297 - val_loss: 0.2851 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2754 - accuracy: 0.9297 - val_loss: 0.2839 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2742 - accuracy: 0.9297 - val_loss: 0.2827 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2730 - accuracy: 0.9297 - val_loss: 0.2815 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2719 - accuracy: 0.9297 - val_loss: 0.2803 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2707 - accuracy: 0.9297 - val_loss: 0.2792 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2696 - accuracy: 0.9297 - val_loss: 0.2781 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2685 - accuracy: 0.9297 - val_loss: 0.2769 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2674 - accuracy: 0.9297 - val_loss: 0.2758 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2663 - accuracy: 0.9297 - val_loss: 0.2747 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2652 - accuracy: 0.9297 - val_loss: 0.2739 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2643 - accuracy: 0.9297 - val_loss: 0.2728 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2633 - accuracy: 0.9297 - val_loss: 0.2718 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2622 - accuracy: 0.9297 - val_loss: 0.2707 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2612 - accuracy: 0.9297 - val_loss: 0.2697 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.9265 - val_loss: 0.2687 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2592 - accuracy: 0.9265 - val_loss: 0.2678 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2582 - accuracy: 0.9265 - val_loss: 0.2668 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2573 - accuracy: 0.9265 - val_loss: 0.2658 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2563 - accuracy: 0.9265 - val_loss: 0.2649 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2553 - accuracy: 0.9265 - val_loss: 0.2639 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2544 - accuracy: 0.9265 - val_loss: 0.2630 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2534 - accuracy: 0.9265 - val_loss: 0.2621 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2525 - accuracy: 0.9265 - val_loss: 0.2611 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2516 - accuracy: 0.9265 - val_loss: 0.2602 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2507 - accuracy: 0.9265 - val_loss: 0.2594 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2498 - accuracy: 0.9265 - val_loss: 0.2585 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2489 - accuracy: 0.9265 - val_loss: 0.2576 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2481 - accuracy: 0.9265 - val_loss: 0.2568 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2472 - accuracy: 0.9265 - val_loss: 0.2559 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2464 - accuracy: 0.9265 - val_loss: 0.2551 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2455 - accuracy: 0.9265 - val_loss: 0.2542 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2447 - accuracy: 0.9265 - val_loss: 0.2534 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2439 - accuracy: 0.9265 - val_loss: 0.2526 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2431 - accuracy: 0.9265 - val_loss: 0.2518 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2423 - accuracy: 0.9265 - val_loss: 0.2510 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2415 - accuracy: 0.9265 - val_loss: 0.2502 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2407 - accuracy: 0.9265 - val_loss: 0.2494 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2399 - accuracy: 0.9265 - val_loss: 0.2487 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2391 - accuracy: 0.9265 - val_loss: 0.2479 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2384 - accuracy: 0.9265 - val_loss: 0.2472 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2376 - accuracy: 0.9265 - val_loss: 0.2464 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2369 - accuracy: 0.9265 - val_loss: 0.2457 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2362 - accuracy: 0.9265 - val_loss: 0.2450 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2354 - accuracy: 0.9265 - val_loss: 0.2443 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2347 - accuracy: 0.9265 - val_loss: 0.2436 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2340 - accuracy: 0.9265 - val_loss: 0.2429 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2333 - accuracy: 0.9265 - val_loss: 0.2422 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2326 - accuracy: 0.9265 - val_loss: 0.2416 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2320 - accuracy: 0.9265 - val_loss: 0.2409 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2313 - accuracy: 0.9265 - val_loss: 0.2402 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2306 - accuracy: 0.9265 - val_loss: 0.2397 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2301 - accuracy: 0.9265 - val_loss: 0.2391 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2294 - accuracy: 0.9265 - val_loss: 0.2384 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2287 - accuracy: 0.9265 - val_loss: 0.2378 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2281 - accuracy: 0.9265 - val_loss: 0.2372 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2275 - accuracy: 0.9297 - val_loss: 0.2365 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2268 - accuracy: 0.9297 - val_loss: 0.2359 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2262 - accuracy: 0.9297 - val_loss: 0.2353 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2256 - accuracy: 0.9297 - val_loss: 0.2347 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2250 - accuracy: 0.9297 - val_loss: 0.2341 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.2244 - accuracy: 0.9297 - val_loss: 0.2335 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2238 - accuracy: 0.9297 - val_loss: 0.2329 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2232 - accuracy: 0.9297 - val_loss: 0.2324 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2226 - accuracy: 0.9297 - val_loss: 0.2318 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2220 - accuracy: 0.9297 - val_loss: 0.2312 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2214 - accuracy: 0.9297 - val_loss: 0.2307 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2209 - accuracy: 0.9297 - val_loss: 0.2301 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2203 - accuracy: 0.9297 - val_loss: 0.2296 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2197 - accuracy: 0.9297 - val_loss: 0.2290 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2192 - accuracy: 0.9297 - val_loss: 0.2285 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2186 - accuracy: 0.9297 - val_loss: 0.2279 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2181 - accuracy: 0.9297 - val_loss: 0.2274 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2176 - accuracy: 0.9297 - val_loss: 0.2269 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2170 - accuracy: 0.9297 - val_loss: 0.2264 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2165 - accuracy: 0.9297 - val_loss: 0.2259 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2160 - accuracy: 0.9297 - val_loss: 0.2253 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2155 - accuracy: 0.9297 - val_loss: 0.2248 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2149 - accuracy: 0.9297 - val_loss: 0.2243 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2144 - accuracy: 0.9297 - val_loss: 0.2239 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2139 - accuracy: 0.9297 - val_loss: 0.2234 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2134 - accuracy: 0.9297 - val_loss: 0.2229 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2129 - accuracy: 0.9297 - val_loss: 0.2224 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2125 - accuracy: 0.9297 - val_loss: 0.2219 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9297 - val_loss: 0.2215 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.2115 - accuracy: 0.9297 - val_loss: 0.2210 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2110 - accuracy: 0.9297 - val_loss: 0.2205 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2105 - accuracy: 0.9297 - val_loss: 0.2201 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2101 - accuracy: 0.9297 - val_loss: 0.2196 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2096 - accuracy: 0.9297 - val_loss: 0.2192 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2091 - accuracy: 0.9297 - val_loss: 0.2187 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2087 - accuracy: 0.9297 - val_loss: 0.2183 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2082 - accuracy: 0.9297 - val_loss: 0.2178 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2078 - accuracy: 0.9297 - val_loss: 0.2174 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2073 - accuracy: 0.9297 - val_loss: 0.2170 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9297 - val_loss: 0.2166 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2065 - accuracy: 0.9297 - val_loss: 0.2162 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2060 - accuracy: 0.9297 - val_loss: 0.2157 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2056 - accuracy: 0.9297 - val_loss: 0.2153 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9297 - val_loss: 0.2149 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2047 - accuracy: 0.9297 - val_loss: 0.2145 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9297 - val_loss: 0.2141 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2039 - accuracy: 0.9297 - val_loss: 0.2137 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2034 - accuracy: 0.9297 - val_loss: 0.2133 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9297 - val_loss: 0.2129 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2026 - accuracy: 0.9297 - val_loss: 0.2125 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2022 - accuracy: 0.9297 - val_loss: 0.2121 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9297 - val_loss: 0.2117 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2014 - accuracy: 0.9297 - val_loss: 0.2113 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9297 - val_loss: 0.2109 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2006 - accuracy: 0.9297 - val_loss: 0.2105 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2002 - accuracy: 0.9297 - val_loss: 0.2101 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9297 - val_loss: 0.2098 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1994 - accuracy: 0.9297 - val_loss: 0.2094 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1991 - accuracy: 0.9297 - val_loss: 0.2090 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1987 - accuracy: 0.9297 - val_loss: 0.2086 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1983 - accuracy: 0.9297 - val_loss: 0.2083 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1979 - accuracy: 0.9297 - val_loss: 0.2079 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1975 - accuracy: 0.9297 - val_loss: 0.2076 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1971 - accuracy: 0.9297 - val_loss: 0.2072 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1968 - accuracy: 0.9297 - val_loss: 0.2069 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1964 - accuracy: 0.9297 - val_loss: 0.2065 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1960 - accuracy: 0.9297 - val_loss: 0.2062 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.9297 - val_loss: 0.2058 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9297 - val_loss: 0.2055 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1949 - accuracy: 0.9297 - val_loss: 0.2051 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1946 - accuracy: 0.9297 - val_loss: 0.2048 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1943 - accuracy: 0.9297 - val_loss: 0.2045 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1939 - accuracy: 0.9297 - val_loss: 0.2041 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1935 - accuracy: 0.9297 - val_loss: 0.2038 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1932 - accuracy: 0.9297 - val_loss: 0.2034 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1928 - accuracy: 0.9297 - val_loss: 0.2031 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1925 - accuracy: 0.9297 - val_loss: 0.2028 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1922 - accuracy: 0.9297 - val_loss: 0.2025 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1918 - accuracy: 0.9297 - val_loss: 0.2022 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1915 - accuracy: 0.9297 - val_loss: 0.2018 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1912 - accuracy: 0.9297 - val_loss: 0.2015 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1908 - accuracy: 0.9297 - val_loss: 0.2012 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1905 - accuracy: 0.9329 - val_loss: 0.2009 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1902 - accuracy: 0.9329 - val_loss: 0.2006 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1898 - accuracy: 0.9329 - val_loss: 0.2003 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1895 - accuracy: 0.9329 - val_loss: 0.1999 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1892 - accuracy: 0.9329 - val_loss: 0.1996 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1889 - accuracy: 0.9329 - val_loss: 0.1993 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9329 - val_loss: 0.1990 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1883 - accuracy: 0.9329 - val_loss: 0.1988 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1880 - accuracy: 0.9329 - val_loss: 0.1985 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1876 - accuracy: 0.9329 - val_loss: 0.1982 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1873 - accuracy: 0.9329 - val_loss: 0.1979 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1870 - accuracy: 0.9329 - val_loss: 0.1976 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.9329 - val_loss: 0.1974 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1864 - accuracy: 0.9329 - val_loss: 0.1971 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9329 - val_loss: 0.1968 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1858 - accuracy: 0.9329 - val_loss: 0.1965 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1855 - accuracy: 0.9329 - val_loss: 0.1962 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1852 - accuracy: 0.9329 - val_loss: 0.1959 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1849 - accuracy: 0.9329 - val_loss: 0.1957 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1846 - accuracy: 0.9329 - val_loss: 0.1954 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.9329 - val_loss: 0.1951 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9329 - val_loss: 0.1948 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1838 - accuracy: 0.9329 - val_loss: 0.1946 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1835 - accuracy: 0.9329 - val_loss: 0.1943 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1832 - accuracy: 0.9329 - val_loss: 0.1940 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1829 - accuracy: 0.9329 - val_loss: 0.1938 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9329 - val_loss: 0.1935 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9361 - val_loss: 0.1933 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1821 - accuracy: 0.9361 - val_loss: 0.1930 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.9361 - val_loss: 0.1927 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9361 - val_loss: 0.1925 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1813 - accuracy: 0.9361 - val_loss: 0.1922 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1810 - accuracy: 0.9361 - val_loss: 0.1920 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1807 - accuracy: 0.9361 - val_loss: 0.1917 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9361 - val_loss: 0.1914 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9361 - val_loss: 0.1912 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1799 - accuracy: 0.9361 - val_loss: 0.1909 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1797 - accuracy: 0.9361 - val_loss: 0.1907 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1794 - accuracy: 0.9361 - val_loss: 0.1904 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9361 - val_loss: 0.1902 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9361 - val_loss: 0.1900 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1786 - accuracy: 0.9361 - val_loss: 0.1897 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1784 - accuracy: 0.9361 - val_loss: 0.1894 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1781 - accuracy: 0.9361 - val_loss: 0.1892 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.9361 - val_loss: 0.1890 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9361 - val_loss: 0.1887 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9361 - val_loss: 0.1885 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.9361 - val_loss: 0.1883 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9361 - val_loss: 0.1880 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9361 - val_loss: 0.1878 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1764 - accuracy: 0.9361 - val_loss: 0.1876 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.9361 - val_loss: 0.1873 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1759 - accuracy: 0.9361 - val_loss: 0.1871 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1757 - accuracy: 0.9361 - val_loss: 0.1869 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1754 - accuracy: 0.9361 - val_loss: 0.1867 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1752 - accuracy: 0.9361 - val_loss: 0.1865 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9361 - val_loss: 0.1863 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1747 - accuracy: 0.9361 - val_loss: 0.1860 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1744 - accuracy: 0.9361 - val_loss: 0.1858 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.9361 - val_loss: 0.1856 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9361 - val_loss: 0.1854 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 306/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1737 - accuracy: 0.9361 - val_loss: 0.1852 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1735 - accuracy: 0.9361 - val_loss: 0.1850 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1733 - accuracy: 0.9361 - val_loss: 0.1848 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1731 - accuracy: 0.9361 - val_loss: 0.1846 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1728 - accuracy: 0.9361 - val_loss: 0.1844 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1726 - accuracy: 0.9361 - val_loss: 0.1841 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.9361 - val_loss: 0.1839 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1722 - accuracy: 0.9361 - val_loss: 0.1837 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9361 - val_loss: 0.1835 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1717 - accuracy: 0.9361 - val_loss: 0.1833 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1715 - accuracy: 0.9361 - val_loss: 0.1831 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9361 - val_loss: 0.1829 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 318/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1711 - accuracy: 0.9361 - val_loss: 0.1827 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1709 - accuracy: 0.9361 - val_loss: 0.1825 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1706 - accuracy: 0.9361 - val_loss: 0.1823 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1704 - accuracy: 0.9361 - val_loss: 0.1820 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.9361 - val_loss: 0.1818 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1700 - accuracy: 0.9361 - val_loss: 0.1816 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9361 - val_loss: 0.1814 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 325/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1696 - accuracy: 0.9361 - val_loss: 0.1812 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9361 - val_loss: 0.1810 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9361 - val_loss: 0.1808 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.9361 - val_loss: 0.1806 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.9361 - val_loss: 0.1805 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1685 - accuracy: 0.9361 - val_loss: 0.1803 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1683 - accuracy: 0.9393 - val_loss: 0.1801 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9393 - val_loss: 0.1799 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1679 - accuracy: 0.9425 - val_loss: 0.1797 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1677 - accuracy: 0.9425 - val_loss: 0.1795 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1675 - accuracy: 0.9425 - val_loss: 0.1793 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.9425 - val_loss: 0.1791 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.9425 - val_loss: 0.1789 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9425 - val_loss: 0.1787 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1666 - accuracy: 0.9425 - val_loss: 0.1786 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1664 - accuracy: 0.9425 - val_loss: 0.1784 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.9425 - val_loss: 0.1782 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 342/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1660 - accuracy: 0.9425 - val_loss: 0.1780 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1658 - accuracy: 0.9425 - val_loss: 0.1778 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 344/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1656 - accuracy: 0.9425 - val_loss: 0.1777 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9425 - val_loss: 0.1775 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1652 - accuracy: 0.9425 - val_loss: 0.1773 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1651 - accuracy: 0.9425 - val_loss: 0.1771 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1649 - accuracy: 0.9425 - val_loss: 0.1769 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.9425 - val_loss: 0.1768 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9425 - val_loss: 0.1766 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 351/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9425 - val_loss: 0.1764 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1641 - accuracy: 0.9425 - val_loss: 0.1762 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1639 - accuracy: 0.9425 - val_loss: 0.1760 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1637 - accuracy: 0.9425 - val_loss: 0.1759 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 355/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9425 - val_loss: 0.1757 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1633 - accuracy: 0.9425 - val_loss: 0.1755 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.9425 - val_loss: 0.1753 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.9425 - val_loss: 0.1752 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1628 - accuracy: 0.9425 - val_loss: 0.1750 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1626 - accuracy: 0.9425 - val_loss: 0.1748 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 361/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1624 - accuracy: 0.9425 - val_loss: 0.1747 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 362/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.9425 - val_loss: 0.1745 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.9425 - val_loss: 0.1743 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1619 - accuracy: 0.9425 - val_loss: 0.1742 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1617 - accuracy: 0.9425 - val_loss: 0.1740 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1615 - accuracy: 0.9425 - val_loss: 0.1738 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1613 - accuracy: 0.9425 - val_loss: 0.1737 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 368/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1612 - accuracy: 0.9425 - val_loss: 0.1735 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9425 - val_loss: 0.1734 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 370/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1608 - accuracy: 0.9425 - val_loss: 0.1732 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.9425 - val_loss: 0.1731 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9425 - val_loss: 0.1729 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1603 - accuracy: 0.9425 - val_loss: 0.1727 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1601 - accuracy: 0.9425 - val_loss: 0.1726 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1599 - accuracy: 0.9425 - val_loss: 0.1724 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1598 - accuracy: 0.9425 - val_loss: 0.1722 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9425 - val_loss: 0.1721 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1594 - accuracy: 0.9425 - val_loss: 0.1719 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.9425 - val_loss: 0.1718 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9425 - val_loss: 0.1716 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 381/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1589 - accuracy: 0.9425 - val_loss: 0.1715 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9425 - val_loss: 0.1713 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1586 - accuracy: 0.9425 - val_loss: 0.1711 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1584 - accuracy: 0.9425 - val_loss: 0.1710 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1582 - accuracy: 0.9425 - val_loss: 0.1708 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1581 - accuracy: 0.9425 - val_loss: 0.1707 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9425 - val_loss: 0.1705 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9425 - val_loss: 0.1704 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1576 - accuracy: 0.9425 - val_loss: 0.1702 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.9425 - val_loss: 0.1700 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1572 - accuracy: 0.9425 - val_loss: 0.1699 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1571 - accuracy: 0.9425 - val_loss: 0.1697 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1569 - accuracy: 0.9425 - val_loss: 0.1696 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1568 - accuracy: 0.9425 - val_loss: 0.1694 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1566 - accuracy: 0.9425 - val_loss: 0.1693 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1564 - accuracy: 0.9457 - val_loss: 0.1692 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.9457 - val_loss: 0.1690 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9457 - val_loss: 0.1689 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9457 - val_loss: 0.1687 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9457 - val_loss: 0.1686 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1557 - accuracy: 0.9457 - val_loss: 0.1684 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9457 - val_loss: 0.1683 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1554 - accuracy: 0.9457 - val_loss: 0.1682 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1552 - accuracy: 0.9457 - val_loss: 0.1680 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1550 - accuracy: 0.9457 - val_loss: 0.1679 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1549 - accuracy: 0.9489 - val_loss: 0.1677 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9489 - val_loss: 0.1676 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1546 - accuracy: 0.9489 - val_loss: 0.1674 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9489 - val_loss: 0.1673 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1543 - accuracy: 0.9489 - val_loss: 0.1671 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9489 - val_loss: 0.1670 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1540 - accuracy: 0.9489 - val_loss: 0.1669 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1538 - accuracy: 0.9489 - val_loss: 0.1667 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9489 - val_loss: 0.1666 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1536 - accuracy: 0.9489 - val_loss: 0.1664 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 416/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1534 - accuracy: 0.9489 - val_loss: 0.1663 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 417/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1532 - accuracy: 0.9489 - val_loss: 0.1662 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 418/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1531 - accuracy: 0.9489 - val_loss: 0.1660 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1529 - accuracy: 0.9521 - val_loss: 0.1659 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 420/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1528 - accuracy: 0.9521 - val_loss: 0.1657 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1527 - accuracy: 0.9521 - val_loss: 0.1656 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1525 - accuracy: 0.9521 - val_loss: 0.1655 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1524 - accuracy: 0.9521 - val_loss: 0.1653 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9521 - val_loss: 0.1652 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 425/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9521 - val_loss: 0.1651 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9521 - val_loss: 0.1649 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1518 - accuracy: 0.9521 - val_loss: 0.1648 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9521 - val_loss: 0.1647 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1515 - accuracy: 0.9521 - val_loss: 0.1645 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9521 - val_loss: 0.1644 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.9521 - val_loss: 0.1643 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1511 - accuracy: 0.9521 - val_loss: 0.1642 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1510 - accuracy: 0.9521 - val_loss: 0.1640 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1508 - accuracy: 0.9521 - val_loss: 0.1639 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1507 - accuracy: 0.9521 - val_loss: 0.1638 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1505 - accuracy: 0.9521 - val_loss: 0.1636 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9553 - val_loss: 0.1635 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1503 - accuracy: 0.9553 - val_loss: 0.1634 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1501 - accuracy: 0.9553 - val_loss: 0.1633 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1500 - accuracy: 0.9553 - val_loss: 0.1631 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1499 - accuracy: 0.9553 - val_loss: 0.1630 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1497 - accuracy: 0.9553 - val_loss: 0.1629 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1496 - accuracy: 0.9553 - val_loss: 0.1627 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1495 - accuracy: 0.9553 - val_loss: 0.1626 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1493 - accuracy: 0.9553 - val_loss: 0.1625 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 446/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1492 - accuracy: 0.9553 - val_loss: 0.1624 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 447/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1491 - accuracy: 0.9553 - val_loss: 0.1622 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 448/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9553 - val_loss: 0.1621 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 449/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9553 - val_loss: 0.1620 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 450/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9553 - val_loss: 0.1619 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9553 - val_loss: 0.1618 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 452/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1484 - accuracy: 0.9553 - val_loss: 0.1616 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 453/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9553 - val_loss: 0.1615 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 454/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1481 - accuracy: 0.9553 - val_loss: 0.1614 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9553 - val_loss: 0.1612 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 456/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1479 - accuracy: 0.9553 - val_loss: 0.1611 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 457/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.9553 - val_loss: 0.1610 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9553 - val_loss: 0.1609 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1475 - accuracy: 0.9553 - val_loss: 0.1607 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 460/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1473 - accuracy: 0.9553 - val_loss: 0.1606 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 461/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1472 - accuracy: 0.9553 - val_loss: 0.1605 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1471 - accuracy: 0.9553 - val_loss: 0.1604 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1470 - accuracy: 0.9553 - val_loss: 0.1603 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1468 - accuracy: 0.9553 - val_loss: 0.1602 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 465/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1467 - accuracy: 0.9553 - val_loss: 0.1600 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "40/40 [==============================] - 1s 14ms/step - loss: 0.1466 - accuracy: 0.9553 - val_loss: 0.1599 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 467/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1465 - accuracy: 0.9553 - val_loss: 0.1598 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1463 - accuracy: 0.9553 - val_loss: 0.1597 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 469/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1462 - accuracy: 0.9553 - val_loss: 0.1596 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9553 - val_loss: 0.1595 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1459 - accuracy: 0.9553 - val_loss: 0.1593 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 472/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9553 - val_loss: 0.1592 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1457 - accuracy: 0.9553 - val_loss: 0.1591 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1455 - accuracy: 0.9553 - val_loss: 0.1590 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1454 - accuracy: 0.9553 - val_loss: 0.1589 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 476/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1453 - accuracy: 0.9553 - val_loss: 0.1588 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 477/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1452 - accuracy: 0.9553 - val_loss: 0.1587 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9553 - val_loss: 0.1585 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1449 - accuracy: 0.9553 - val_loss: 0.1584 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 480/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1448 - accuracy: 0.9553 - val_loss: 0.1583 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 481/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9553 - val_loss: 0.1582 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 482/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1446 - accuracy: 0.9553 - val_loss: 0.1581 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 483/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1445 - accuracy: 0.9553 - val_loss: 0.1581 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9553 - val_loss: 0.1579 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9553 - val_loss: 0.1578 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 486/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1441 - accuracy: 0.9553 - val_loss: 0.1577 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1440 - accuracy: 0.9553 - val_loss: 0.1576 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1439 - accuracy: 0.9553 - val_loss: 0.1575 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 489/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1438 - accuracy: 0.9553 - val_loss: 0.1574 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 490/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9553 - val_loss: 0.1573 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9553 - val_loss: 0.1572 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1434 - accuracy: 0.9553 - val_loss: 0.1571 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 493/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1433 - accuracy: 0.9553 - val_loss: 0.1570 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 494/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1432 - accuracy: 0.9553 - val_loss: 0.1569 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1431 - accuracy: 0.9553 - val_loss: 0.1568 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 496/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1429 - accuracy: 0.9553 - val_loss: 0.1566 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9553 - val_loss: 0.1565 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9553 - val_loss: 0.1564 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9553 - val_loss: 0.1563 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1425 - accuracy: 0.9553 - val_loss: 0.1562 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 501/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1423 - accuracy: 0.9553 - val_loss: 0.1561 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 502/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1422 - accuracy: 0.9553 - val_loss: 0.1560 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 503/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1421 - accuracy: 0.9553 - val_loss: 0.1559 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 504/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1420 - accuracy: 0.9553 - val_loss: 0.1558 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1419 - accuracy: 0.9553 - val_loss: 0.1557 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 506/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1418 - accuracy: 0.9553 - val_loss: 0.1556 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9553 - val_loss: 0.1555 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 508/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1416 - accuracy: 0.9553 - val_loss: 0.1554 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 509/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1414 - accuracy: 0.9553 - val_loss: 0.1553 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 510/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1413 - accuracy: 0.9553 - val_loss: 0.1552 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1412 - accuracy: 0.9553 - val_loss: 0.1551 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 512/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1411 - accuracy: 0.9553 - val_loss: 0.1550 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 513/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1410 - accuracy: 0.9553 - val_loss: 0.1549 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 514/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.9553 - val_loss: 0.1548 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1408 - accuracy: 0.9553 - val_loss: 0.1547 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1407 - accuracy: 0.9553 - val_loss: 0.1545 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 517/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1405 - accuracy: 0.9553 - val_loss: 0.1545 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 518/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.9553 - val_loss: 0.1543 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9553 - val_loss: 0.1542 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9553 - val_loss: 0.1541 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1401 - accuracy: 0.9553 - val_loss: 0.1540 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1400 - accuracy: 0.9553 - val_loss: 0.1539 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9553 - val_loss: 0.1538 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9553 - val_loss: 0.1537 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 525/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9553 - val_loss: 0.1536 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1396 - accuracy: 0.9553 - val_loss: 0.1535 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1395 - accuracy: 0.9553 - val_loss: 0.1534 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 528/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1393 - accuracy: 0.9553 - val_loss: 0.1533 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 529/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1392 - accuracy: 0.9553 - val_loss: 0.1532 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 530/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1391 - accuracy: 0.9553 - val_loss: 0.1531 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9553 - val_loss: 0.1530 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 532/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1389 - accuracy: 0.9553 - val_loss: 0.1529 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9553 - val_loss: 0.1528 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 534/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1387 - accuracy: 0.9553 - val_loss: 0.1527 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 535/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1386 - accuracy: 0.9553 - val_loss: 0.1526 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 536/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1385 - accuracy: 0.9553 - val_loss: 0.1525 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9553 - val_loss: 0.1524 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9553 - val_loss: 0.1523 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1382 - accuracy: 0.9553 - val_loss: 0.1522 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1381 - accuracy: 0.9553 - val_loss: 0.1521 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1380 - accuracy: 0.9553 - val_loss: 0.1520 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1379 - accuracy: 0.9553 - val_loss: 0.1519 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 543/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1378 - accuracy: 0.9553 - val_loss: 0.1518 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9553 - val_loss: 0.1517 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9553 - val_loss: 0.1515 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1374 - accuracy: 0.9553 - val_loss: 0.1514 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1373 - accuracy: 0.9553 - val_loss: 0.1513 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1372 - accuracy: 0.9553 - val_loss: 0.1513 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9553 - val_loss: 0.1511 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 550/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1370 - accuracy: 0.9553 - val_loss: 0.1511 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 551/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9553 - val_loss: 0.1510 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 552/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1368 - accuracy: 0.9553 - val_loss: 0.1509 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1367 - accuracy: 0.9553 - val_loss: 0.1508 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 554/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1366 - accuracy: 0.9553 - val_loss: 0.1507 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 555/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.9553 - val_loss: 0.1506 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1364 - accuracy: 0.9553 - val_loss: 0.1505 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1363 - accuracy: 0.9553 - val_loss: 0.1504 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1362 - accuracy: 0.9553 - val_loss: 0.1503 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 559/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1361 - accuracy: 0.9553 - val_loss: 0.1503 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 560/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1360 - accuracy: 0.9553 - val_loss: 0.1501 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1359 - accuracy: 0.9553 - val_loss: 0.1500 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.9553 - val_loss: 0.1500 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 563/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1357 - accuracy: 0.9553 - val_loss: 0.1499 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 564/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1356 - accuracy: 0.9553 - val_loss: 0.1498 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9553 - val_loss: 0.1497 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1354 - accuracy: 0.9553 - val_loss: 0.1496 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 567/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1353 - accuracy: 0.9553 - val_loss: 0.1495 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 568/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1352 - accuracy: 0.9553 - val_loss: 0.1494 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1352 - accuracy: 0.9553 - val_loss: 0.1494 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1351 - accuracy: 0.9553 - val_loss: 0.1493 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1350 - accuracy: 0.9553 - val_loss: 0.1492 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1349 - accuracy: 0.9553 - val_loss: 0.1491 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9553 - val_loss: 0.1490 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 574/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1347 - accuracy: 0.9553 - val_loss: 0.1489 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1346 - accuracy: 0.9553 - val_loss: 0.1488 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1345 - accuracy: 0.9553 - val_loss: 0.1488 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1344 - accuracy: 0.9553 - val_loss: 0.1487 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1343 - accuracy: 0.9553 - val_loss: 0.1486 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 579/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1342 - accuracy: 0.9553 - val_loss: 0.1485 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9553 - val_loss: 0.1484 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "40/40 [==============================] - 1s 13ms/step - loss: 0.1340 - accuracy: 0.9553 - val_loss: 0.1483 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 582/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1339 - accuracy: 0.9553 - val_loss: 0.1482 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1338 - accuracy: 0.9553 - val_loss: 0.1481 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 584/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1337 - accuracy: 0.9553 - val_loss: 0.1481 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1336 - accuracy: 0.9553 - val_loss: 0.1480 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9553 - val_loss: 0.1479 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 587/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9553 - val_loss: 0.1478 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9553 - val_loss: 0.1477 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1332 - accuracy: 0.9553 - val_loss: 0.1476 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.1331 - accuracy: 0.9553 - val_loss: 0.1476 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 591/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1330 - accuracy: 0.9553 - val_loss: 0.1475 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1329 - accuracy: 0.9553 - val_loss: 0.1474 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1329 - accuracy: 0.9553 - val_loss: 0.1473 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1328 - accuracy: 0.9553 - val_loss: 0.1472 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 595/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1327 - accuracy: 0.9553 - val_loss: 0.1472 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1326 - accuracy: 0.9553 - val_loss: 0.1471 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9553 - val_loss: 0.1470 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 598/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1324 - accuracy: 0.9553 - val_loss: 0.1469 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 599/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1323 - accuracy: 0.9553 - val_loss: 0.1469 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 600/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1322 - accuracy: 0.9553 - val_loss: 0.1468 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 601/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1321 - accuracy: 0.9553 - val_loss: 0.1467 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9553 - val_loss: 0.1466 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1319 - accuracy: 0.9553 - val_loss: 0.1465 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1318 - accuracy: 0.9553 - val_loss: 0.1464 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1317 - accuracy: 0.9553 - val_loss: 0.1464 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1317 - accuracy: 0.9553 - val_loss: 0.1463 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1316 - accuracy: 0.9553 - val_loss: 0.1462 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 608/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1315 - accuracy: 0.9553 - val_loss: 0.1461 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 609/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1314 - accuracy: 0.9553 - val_loss: 0.1461 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1313 - accuracy: 0.9553 - val_loss: 0.1460 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1312 - accuracy: 0.9553 - val_loss: 0.1459 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1311 - accuracy: 0.9553 - val_loss: 0.1458 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1310 - accuracy: 0.9553 - val_loss: 0.1458 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1309 - accuracy: 0.9585 - val_loss: 0.1457 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 615/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1309 - accuracy: 0.9585 - val_loss: 0.1456 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9585 - val_loss: 0.1455 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 617/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1307 - accuracy: 0.9585 - val_loss: 0.1455 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 618/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1306 - accuracy: 0.9585 - val_loss: 0.1454 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 619/1000\n",
      "40/40 [==============================] - 1s 14ms/step - loss: 0.1305 - accuracy: 0.9585 - val_loss: 0.1453 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1304 - accuracy: 0.9585 - val_loss: 0.1452 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 621/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9585 - val_loss: 0.1452 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1302 - accuracy: 0.9585 - val_loss: 0.1451 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1302 - accuracy: 0.9585 - val_loss: 0.1450 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1301 - accuracy: 0.9585 - val_loss: 0.1449 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1300 - accuracy: 0.9585 - val_loss: 0.1449 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 626/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1299 - accuracy: 0.9585 - val_loss: 0.1448 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 627/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9585 - val_loss: 0.1447 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 628/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1297 - accuracy: 0.9585 - val_loss: 0.1446 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 629/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9585 - val_loss: 0.1446 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9585 - val_loss: 0.1445 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 631/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9585 - val_loss: 0.1444 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1294 - accuracy: 0.9585 - val_loss: 0.1443 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 633/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1293 - accuracy: 0.9585 - val_loss: 0.1443 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 634/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1292 - accuracy: 0.9585 - val_loss: 0.1442 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1291 - accuracy: 0.9585 - val_loss: 0.1442 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9585 - val_loss: 0.1441 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1290 - accuracy: 0.9585 - val_loss: 0.1440 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 638/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9585 - val_loss: 0.1439 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 639/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1288 - accuracy: 0.9585 - val_loss: 0.1439 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 640/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9585 - val_loss: 0.1438 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 641/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1286 - accuracy: 0.9585 - val_loss: 0.1437 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 642/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.9585 - val_loss: 0.1436 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 643/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9585 - val_loss: 0.1436 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1284 - accuracy: 0.9585 - val_loss: 0.1435 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 645/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1283 - accuracy: 0.9585 - val_loss: 0.1434 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.9585 - val_loss: 0.1434 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1281 - accuracy: 0.9585 - val_loss: 0.1433 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 648/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1280 - accuracy: 0.9585 - val_loss: 0.1432 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9585 - val_loss: 0.1431 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9585 - val_loss: 0.1431 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1278 - accuracy: 0.9585 - val_loss: 0.1430 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 652/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9585 - val_loss: 0.1429 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1276 - accuracy: 0.9585 - val_loss: 0.1429 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1275 - accuracy: 0.9585 - val_loss: 0.1428 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 655/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1275 - accuracy: 0.9585 - val_loss: 0.1428 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1274 - accuracy: 0.9585 - val_loss: 0.1427 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 657/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1273 - accuracy: 0.9585 - val_loss: 0.1426 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 658/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1272 - accuracy: 0.9585 - val_loss: 0.1426 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 659/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1271 - accuracy: 0.9585 - val_loss: 0.1425 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 660/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1270 - accuracy: 0.9585 - val_loss: 0.1424 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 661/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1270 - accuracy: 0.9585 - val_loss: 0.1424 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1269 - accuracy: 0.9585 - val_loss: 0.1424 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1268 - accuracy: 0.9585 - val_loss: 0.1423 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9585 - val_loss: 0.1422 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 665/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1267 - accuracy: 0.9585 - val_loss: 0.1421 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 666/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9585 - val_loss: 0.1421 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9585 - val_loss: 0.1420 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 668/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1264 - accuracy: 0.9585 - val_loss: 0.1419 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1263 - accuracy: 0.9585 - val_loss: 0.1419 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1262 - accuracy: 0.9585 - val_loss: 0.1418 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1262 - accuracy: 0.9585 - val_loss: 0.1417 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1261 - accuracy: 0.9585 - val_loss: 0.1417 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9585 - val_loss: 0.1416 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1259 - accuracy: 0.9585 - val_loss: 0.1415 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 675/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1258 - accuracy: 0.9585 - val_loss: 0.1415 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 676/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1258 - accuracy: 0.9585 - val_loss: 0.1414 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 677/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1257 - accuracy: 0.9585 - val_loss: 0.1413 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1256 - accuracy: 0.9585 - val_loss: 0.1413 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 679/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1255 - accuracy: 0.9585 - val_loss: 0.1412 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1254 - accuracy: 0.9585 - val_loss: 0.1411 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1253 - accuracy: 0.9585 - val_loss: 0.1411 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 0.9585 - val_loss: 0.1410 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 683/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1252 - accuracy: 0.9585 - val_loss: 0.1410 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 684/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1251 - accuracy: 0.9585 - val_loss: 0.1410 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 0.9585 - val_loss: 0.1409 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1250 - accuracy: 0.9585 - val_loss: 0.1408 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 687/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9585 - val_loss: 0.1408 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1248 - accuracy: 0.9585 - val_loss: 0.1407 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1247 - accuracy: 0.9585 - val_loss: 0.1406 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1247 - accuracy: 0.9585 - val_loss: 0.1406 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 691/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1246 - accuracy: 0.9585 - val_loss: 0.1406 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 692/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9585 - val_loss: 0.1405 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1244 - accuracy: 0.9585 - val_loss: 0.1404 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 694/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9585 - val_loss: 0.1404 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9585 - val_loss: 0.1403 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1242 - accuracy: 0.9585 - val_loss: 0.1402 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1241 - accuracy: 0.9585 - val_loss: 0.1402 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9585 - val_loss: 0.1401 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9617 - val_loss: 0.1400 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1239 - accuracy: 0.9617 - val_loss: 0.1400 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 701/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1238 - accuracy: 0.9617 - val_loss: 0.1399 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1237 - accuracy: 0.9617 - val_loss: 0.1398 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 703/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1237 - accuracy: 0.9617 - val_loss: 0.1398 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1236 - accuracy: 0.9617 - val_loss: 0.1397 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 705/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9617 - val_loss: 0.1397 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1234 - accuracy: 0.9617 - val_loss: 0.1396 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1234 - accuracy: 0.9617 - val_loss: 0.1395 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1233 - accuracy: 0.9617 - val_loss: 0.1395 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 709/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1232 - accuracy: 0.9617 - val_loss: 0.1394 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 710/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1231 - accuracy: 0.9617 - val_loss: 0.1393 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1231 - accuracy: 0.9617 - val_loss: 0.1393 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 712/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9617 - val_loss: 0.1392 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1229 - accuracy: 0.9617 - val_loss: 0.1392 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 714/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1228 - accuracy: 0.9617 - val_loss: 0.1391 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1228 - accuracy: 0.9617 - val_loss: 0.1390 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 716/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1227 - accuracy: 0.9617 - val_loss: 0.1390 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 717/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9617 - val_loss: 0.1389 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 718/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9617 - val_loss: 0.1389 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 719/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9617 - val_loss: 0.1388 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1224 - accuracy: 0.9617 - val_loss: 0.1387 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1223 - accuracy: 0.9617 - val_loss: 0.1387 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1222 - accuracy: 0.9617 - val_loss: 0.1387 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1222 - accuracy: 0.9617 - val_loss: 0.1386 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9617 - val_loss: 0.1386 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 725/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1220 - accuracy: 0.9617 - val_loss: 0.1385 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1219 - accuracy: 0.9617 - val_loss: 0.1384 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 727/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1219 - accuracy: 0.9617 - val_loss: 0.1384 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 728/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1218 - accuracy: 0.9617 - val_loss: 0.1384 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 729/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1217 - accuracy: 0.9617 - val_loss: 0.1383 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.9617 - val_loss: 0.1383 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 731/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1216 - accuracy: 0.9617 - val_loss: 0.1382 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 732/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9617 - val_loss: 0.1382 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 733/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9617 - val_loss: 0.1381 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 734/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1213 - accuracy: 0.9617 - val_loss: 0.1380 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 735/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1213 - accuracy: 0.9617 - val_loss: 0.1380 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 736/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1212 - accuracy: 0.9617 - val_loss: 0.1379 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 737/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1211 - accuracy: 0.9617 - val_loss: 0.1379 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9617 - val_loss: 0.1378 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1210 - accuracy: 0.9617 - val_loss: 0.1378 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1209 - accuracy: 0.9617 - val_loss: 0.1377 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 741/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9617 - val_loss: 0.1376 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 742/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1208 - accuracy: 0.9617 - val_loss: 0.1376 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 743/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9617 - val_loss: 0.1375 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 744/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1206 - accuracy: 0.9617 - val_loss: 0.1374 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 745/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1206 - accuracy: 0.9617 - val_loss: 0.1374 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 746/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1205 - accuracy: 0.9617 - val_loss: 0.1373 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 747/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1204 - accuracy: 0.9617 - val_loss: 0.1372 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 748/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1203 - accuracy: 0.9617 - val_loss: 0.1372 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 749/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1203 - accuracy: 0.9617 - val_loss: 0.1371 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1202 - accuracy: 0.9617 - val_loss: 0.1371 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9617 - val_loss: 0.1370 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.9617 - val_loss: 0.1369 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1200 - accuracy: 0.9617 - val_loss: 0.1369 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 754/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.9617 - val_loss: 0.1368 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 755/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1199 - accuracy: 0.9617 - val_loss: 0.1368 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 756/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1198 - accuracy: 0.9617 - val_loss: 0.1367 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 757/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9617 - val_loss: 0.1366 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 758/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1196 - accuracy: 0.9617 - val_loss: 0.1366 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1196 - accuracy: 0.9617 - val_loss: 0.1365 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 760/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9617 - val_loss: 0.1365 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1194 - accuracy: 0.9617 - val_loss: 0.1364 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1194 - accuracy: 0.9617 - val_loss: 0.1364 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1193 - accuracy: 0.9617 - val_loss: 0.1363 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1192 - accuracy: 0.9617 - val_loss: 0.1363 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 765/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9617 - val_loss: 0.1362 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1191 - accuracy: 0.9617 - val_loss: 0.1361 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1190 - accuracy: 0.9617 - val_loss: 0.1361 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 768/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1190 - accuracy: 0.9617 - val_loss: 0.1360 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1189 - accuracy: 0.9617 - val_loss: 0.1360 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1188 - accuracy: 0.9617 - val_loss: 0.1359 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1188 - accuracy: 0.9617 - val_loss: 0.1359 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 772/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9617 - val_loss: 0.1358 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 773/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1186 - accuracy: 0.9617 - val_loss: 0.1358 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1185 - accuracy: 0.9617 - val_loss: 0.1357 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 775/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1185 - accuracy: 0.9617 - val_loss: 0.1356 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1184 - accuracy: 0.9617 - val_loss: 0.1356 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1183 - accuracy: 0.9617 - val_loss: 0.1355 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 778/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1183 - accuracy: 0.9617 - val_loss: 0.1355 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 779/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1182 - accuracy: 0.9617 - val_loss: 0.1354 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 780/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9617 - val_loss: 0.1354 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 781/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9617 - val_loss: 0.1353 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1180 - accuracy: 0.9617 - val_loss: 0.1353 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9617 - val_loss: 0.1353 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9617 - val_loss: 0.1352 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 785/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1178 - accuracy: 0.9617 - val_loss: 0.1352 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 786/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.9617 - val_loss: 0.1351 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 787/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1176 - accuracy: 0.9617 - val_loss: 0.1350 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 788/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1176 - accuracy: 0.9617 - val_loss: 0.1350 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 789/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1175 - accuracy: 0.9617 - val_loss: 0.1349 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 790/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1174 - accuracy: 0.9617 - val_loss: 0.1349 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 791/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1174 - accuracy: 0.9617 - val_loss: 0.1349 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 792/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1173 - accuracy: 0.9617 - val_loss: 0.1348 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1172 - accuracy: 0.9617 - val_loss: 0.1348 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 794/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1172 - accuracy: 0.9617 - val_loss: 0.1347 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 795/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1171 - accuracy: 0.9617 - val_loss: 0.1347 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1170 - accuracy: 0.9617 - val_loss: 0.1346 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 797/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1169 - accuracy: 0.9617 - val_loss: 0.1346 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1169 - accuracy: 0.9617 - val_loss: 0.1345 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 799/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9617 - val_loss: 0.1344 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 800/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9617 - val_loss: 0.1344 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1167 - accuracy: 0.9617 - val_loss: 0.1343 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9617 - val_loss: 0.1343 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 803/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1166 - accuracy: 0.9617 - val_loss: 0.1342 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9617 - val_loss: 0.1342 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9617 - val_loss: 0.1341 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 806/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1164 - accuracy: 0.9617 - val_loss: 0.1341 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1163 - accuracy: 0.9617 - val_loss: 0.1340 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1162 - accuracy: 0.9617 - val_loss: 0.1340 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9617 - val_loss: 0.1339 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 810/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9617 - val_loss: 0.1339 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1160 - accuracy: 0.9617 - val_loss: 0.1338 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 812/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1160 - accuracy: 0.9617 - val_loss: 0.1338 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 813/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.9617 - val_loss: 0.1337 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 814/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9617 - val_loss: 0.1337 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 815/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1158 - accuracy: 0.9617 - val_loss: 0.1336 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 816/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1157 - accuracy: 0.9617 - val_loss: 0.1336 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 817/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1157 - accuracy: 0.9617 - val_loss: 0.1335 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1156 - accuracy: 0.9617 - val_loss: 0.1335 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 819/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1155 - accuracy: 0.9617 - val_loss: 0.1334 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 820/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9617 - val_loss: 0.1334 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 821/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1154 - accuracy: 0.9617 - val_loss: 0.1333 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9617 - val_loss: 0.1333 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 823/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1153 - accuracy: 0.9617 - val_loss: 0.1332 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 824/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1152 - accuracy: 0.9617 - val_loss: 0.1331 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 825/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1151 - accuracy: 0.9617 - val_loss: 0.1331 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 826/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1151 - accuracy: 0.9617 - val_loss: 0.1330 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 827/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1150 - accuracy: 0.9617 - val_loss: 0.1330 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 828/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1149 - accuracy: 0.9617 - val_loss: 0.1329 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 829/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1149 - accuracy: 0.9617 - val_loss: 0.1328 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9617 - val_loss: 0.1328 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1147 - accuracy: 0.9617 - val_loss: 0.1327 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1147 - accuracy: 0.9617 - val_loss: 0.1327 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 833/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1146 - accuracy: 0.9617 - val_loss: 0.1326 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 834/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1145 - accuracy: 0.9617 - val_loss: 0.1326 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 835/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1145 - accuracy: 0.9617 - val_loss: 0.1325 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 836/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1144 - accuracy: 0.9617 - val_loss: 0.1325 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1144 - accuracy: 0.9617 - val_loss: 0.1324 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9617 - val_loss: 0.1324 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 839/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1142 - accuracy: 0.9617 - val_loss: 0.1323 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 840/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1142 - accuracy: 0.9617 - val_loss: 0.1323 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1141 - accuracy: 0.9617 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1140 - accuracy: 0.9617 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1140 - accuracy: 0.9617 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 844/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1139 - accuracy: 0.9617 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1139 - accuracy: 0.9617 - val_loss: 0.1321 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1138 - accuracy: 0.9617 - val_loss: 0.1321 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1137 - accuracy: 0.9617 - val_loss: 0.1320 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 848/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9617 - val_loss: 0.1320 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 849/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9617 - val_loss: 0.1319 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 850/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1136 - accuracy: 0.9617 - val_loss: 0.1319 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 851/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1135 - accuracy: 0.9617 - val_loss: 0.1318 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1134 - accuracy: 0.9617 - val_loss: 0.1318 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1134 - accuracy: 0.9617 - val_loss: 0.1317 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 854/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1133 - accuracy: 0.9617 - val_loss: 0.1317 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1132 - accuracy: 0.9617 - val_loss: 0.1316 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1132 - accuracy: 0.9617 - val_loss: 0.1316 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 857/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1131 - accuracy: 0.9617 - val_loss: 0.1316 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9617 - val_loss: 0.1315 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 859/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9617 - val_loss: 0.1315 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1129 - accuracy: 0.9617 - val_loss: 0.1314 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 861/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1128 - accuracy: 0.9617 - val_loss: 0.1314 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 862/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1128 - accuracy: 0.9617 - val_loss: 0.1313 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1127 - accuracy: 0.9617 - val_loss: 0.1313 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 864/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1127 - accuracy: 0.9617 - val_loss: 0.1312 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 865/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9617 - val_loss: 0.1312 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1126 - accuracy: 0.9617 - val_loss: 0.1311 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1125 - accuracy: 0.9617 - val_loss: 0.1311 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1124 - accuracy: 0.9617 - val_loss: 0.1310 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 869/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1124 - accuracy: 0.9617 - val_loss: 0.1310 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 870/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1123 - accuracy: 0.9617 - val_loss: 0.1310 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9617 - val_loss: 0.1309 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 872/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1122 - accuracy: 0.9617 - val_loss: 0.1309 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 873/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1121 - accuracy: 0.9617 - val_loss: 0.1308 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9617 - val_loss: 0.1308 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1120 - accuracy: 0.9617 - val_loss: 0.1307 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 876/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1119 - accuracy: 0.9617 - val_loss: 0.1307 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 877/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1119 - accuracy: 0.9617 - val_loss: 0.1306 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1118 - accuracy: 0.9617 - val_loss: 0.1306 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 879/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9617 - val_loss: 0.1305 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1117 - accuracy: 0.9617 - val_loss: 0.1305 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 881/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9617 - val_loss: 0.1304 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9617 - val_loss: 0.1304 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1115 - accuracy: 0.9617 - val_loss: 0.1304 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9617 - val_loss: 0.1303 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1114 - accuracy: 0.9617 - val_loss: 0.1303 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1113 - accuracy: 0.9617 - val_loss: 0.1302 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9617 - val_loss: 0.1302 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 888/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1112 - accuracy: 0.9617 - val_loss: 0.1301 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 889/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9617 - val_loss: 0.1301 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 890/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9617 - val_loss: 0.1300 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1110 - accuracy: 0.9617 - val_loss: 0.1300 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 892/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9617 - val_loss: 0.1299 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9617 - val_loss: 0.1299 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 894/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1109 - accuracy: 0.9617 - val_loss: 0.1299 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1108 - accuracy: 0.9617 - val_loss: 0.1298 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1108 - accuracy: 0.9617 - val_loss: 0.1298 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 897/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1107 - accuracy: 0.9617 - val_loss: 0.1297 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 0.9617 - val_loss: 0.1297 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 899/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 0.9617 - val_loss: 0.1297 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9617 - val_loss: 0.1296 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9617 - val_loss: 0.1296 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 902/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9617 - val_loss: 0.1295 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 903/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1103 - accuracy: 0.9617 - val_loss: 0.1295 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 904/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1103 - accuracy: 0.9617 - val_loss: 0.1294 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 905/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1102 - accuracy: 0.9617 - val_loss: 0.1294 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9617 - val_loss: 0.1293 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 907/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1101 - accuracy: 0.9617 - val_loss: 0.1293 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 908/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1100 - accuracy: 0.9617 - val_loss: 0.1292 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 909/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1100 - accuracy: 0.9617 - val_loss: 0.1292 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 910/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1099 - accuracy: 0.9617 - val_loss: 0.1291 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9617 - val_loss: 0.1291 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1098 - accuracy: 0.9617 - val_loss: 0.1291 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9617 - val_loss: 0.1290 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1097 - accuracy: 0.9617 - val_loss: 0.1290 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1096 - accuracy: 0.9617 - val_loss: 0.1289 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1096 - accuracy: 0.9617 - val_loss: 0.1289 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1095 - accuracy: 0.9617 - val_loss: 0.1288 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 918/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.1288 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.1288 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 920/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9617 - val_loss: 0.1287 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 921/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1093 - accuracy: 0.9617 - val_loss: 0.1287 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9617 - val_loss: 0.1287 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 923/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.9617 - val_loss: 0.1286 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 924/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1091 - accuracy: 0.9617 - val_loss: 0.1286 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1090 - accuracy: 0.9617 - val_loss: 0.1285 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9617 - val_loss: 0.1285 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 927/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1089 - accuracy: 0.9617 - val_loss: 0.1284 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1089 - accuracy: 0.9617 - val_loss: 0.1284 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9617 - val_loss: 0.1284 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 930/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1088 - accuracy: 0.9617 - val_loss: 0.1283 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 931/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1087 - accuracy: 0.9617 - val_loss: 0.1283 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1087 - accuracy: 0.9617 - val_loss: 0.1282 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 933/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1086 - accuracy: 0.9617 - val_loss: 0.1282 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 934/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9617 - val_loss: 0.1282 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1085 - accuracy: 0.9617 - val_loss: 0.1281 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 936/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9617 - val_loss: 0.1281 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 937/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1084 - accuracy: 0.9617 - val_loss: 0.1280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1083 - accuracy: 0.9617 - val_loss: 0.1280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1083 - accuracy: 0.9617 - val_loss: 0.1280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1082 - accuracy: 0.9617 - val_loss: 0.1279 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 941/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1082 - accuracy: 0.9617 - val_loss: 0.1279 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 942/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9617 - val_loss: 0.1279 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 943/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1080 - accuracy: 0.9617 - val_loss: 0.1278 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 944/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9617 - val_loss: 0.1278 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 945/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1079 - accuracy: 0.9617 - val_loss: 0.1277 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 946/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1079 - accuracy: 0.9617 - val_loss: 0.1277 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 947/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9617 - val_loss: 0.1276 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 948/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1077 - accuracy: 0.9617 - val_loss: 0.1276 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 0.9617 - val_loss: 0.1276 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1076 - accuracy: 0.9617 - val_loss: 0.1275 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9617 - val_loss: 0.1275 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 952/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9617 - val_loss: 0.1274 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 953/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9617 - val_loss: 0.1274 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 954/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9617 - val_loss: 0.1273 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 955/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1074 - accuracy: 0.9617 - val_loss: 0.1273 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1073 - accuracy: 0.9617 - val_loss: 0.1272 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 957/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1072 - accuracy: 0.9617 - val_loss: 0.1272 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9617 - val_loss: 0.1272 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1071 - accuracy: 0.9617 - val_loss: 0.1271 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1071 - accuracy: 0.9617 - val_loss: 0.1271 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 961/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9617 - val_loss: 0.1270 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9617 - val_loss: 0.1270 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1069 - accuracy: 0.9617 - val_loss: 0.1270 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1068 - accuracy: 0.9617 - val_loss: 0.1269 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1068 - accuracy: 0.9617 - val_loss: 0.1269 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 966/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9617 - val_loss: 0.1268 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1067 - accuracy: 0.9617 - val_loss: 0.1268 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 968/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1066 - accuracy: 0.9617 - val_loss: 0.1268 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9617 - val_loss: 0.1267 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 970/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1065 - accuracy: 0.9617 - val_loss: 0.1267 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 971/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1065 - accuracy: 0.9617 - val_loss: 0.1267 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 0.9617 - val_loss: 0.1266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1064 - accuracy: 0.9617 - val_loss: 0.1266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1063 - accuracy: 0.9617 - val_loss: 0.1266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 975/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9617 - val_loss: 0.1266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9617 - val_loss: 0.1265 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 977/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1061 - accuracy: 0.9617 - val_loss: 0.1265 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 978/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1061 - accuracy: 0.9617 - val_loss: 0.1265 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 979/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1060 - accuracy: 0.9617 - val_loss: 0.1264 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9617 - val_loss: 0.1264 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1059 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 982/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 983/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1058 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1058 - accuracy: 0.9617 - val_loss: 0.1262 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 985/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 987/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1056 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1055 - accuracy: 0.9617 - val_loss: 0.1262 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1055 - accuracy: 0.9617 - val_loss: 0.1262 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 990/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.9617 - val_loss: 0.1261 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.9617 - val_loss: 0.1261 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9617 - val_loss: 0.1261 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 993/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.9617 - val_loss: 0.1260 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1052 - accuracy: 0.9617 - val_loss: 0.1260 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 995/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9617 - val_loss: 0.1260 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1051 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 997/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1051 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 998/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1050 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1049 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 64)                38656     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,721\n",
      "Trainable params: 38,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Definizione del modello LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(None, x_train.shape[-1]))) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Definisco l'ottimizzatore con il learning rate iniziale\n",
    "initial_learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "# Definisco il learning rate schedule con decay lineare\n",
    "decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "decay_rate = 0.1  # Tasso di decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "# Compilazione del modello\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "# optimizer \"adam\"\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Addestramento del modello con il learning rate modificato\n",
    "history = model.fit(x_train, y_train, epochs=1000, batch_size=8, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the LSTM model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9643\n",
      "Test Loss: 0.12585870921611786\n",
      "Test Accuracy: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "# Valutazione del modello\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation on LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing LSTM with k-fold\n",
      "Loss: 0.1783, Accuracy: 91.43%\n",
      "Loss: 0.1643, Accuracy: 94.29%\n",
      "Loss: 0.1497, Accuracy: 97.14%\n",
      "Loss: 0.0685, Accuracy: 98.57%\n",
      "Loss: 0.1639, Accuracy: 92.86%\n",
      "Loss: 0.1237, Accuracy: 94.29%\n",
      "Loss: 0.1354, Accuracy: 94.29%\n",
      "Loss: 0.1707, Accuracy: 92.86%\n",
      "LSTM finished in 1178.66 sec\n",
      "\n",
      "Average accuracy: 0.9446\n",
      "Average loss: 0.1443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "\n",
    "k = 8  # numero di fold\n",
    "kf = KFold(n_splits=k, shuffle = True)\n",
    "\n",
    "# Array per memorizzare le curve di apprendimento\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "f = pd.DataFrame(columns = perfInd)\n",
    "print('Implementing LSTM with k-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(None, x_train.shape[-1]))) \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Definisco l'ottimizzatore con il learning rate iniziale\n",
    "    initial_learning_rate = 0.001\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # Definisco il learning rate schedule con decay lineare\n",
    "    decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "    decay_rate = 0.1  # Tasso di decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 1000, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Valuta il modello\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Stampa i risultati di accuracy e loss per la k-esima fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('LSTM finished in', t,'sec\\n')\n",
    "\n",
    " # Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_train_loss = np.mean(train_loss, axis=0)\n",
    "avg_train_acc = np.mean(train_acc, axis=0)\n",
    "avg_val_loss = np.mean(val_loss, axis=0)\n",
    "avg_val_acc = np.mean(train_acc, axis=0)\n",
    "print(np.shape(avg_train_loss))\n",
    "\n",
    "# Plot delle curve di apprendimento mediate sulle K fold\n",
    "\n",
    "epochs = range(1, len(train_loss[0]) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, avg_train_loss, label='Training loss')\n",
    "plt.plot(epochs, avg_val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Avg_loss')\n",
    "plt.title('Average train and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, avg_train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, avg_val_acc, label='Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Avg_accuracy')\n",
    "plt.title('Average train and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM greco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "40/40 [==============================] - 3s 15ms/step - loss: 2.6474 - accuracy: 0.5112 - val_loss: 2.6343 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.6346 - accuracy: 0.5559 - val_loss: 2.6236 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.6199 - accuracy: 0.5847 - val_loss: 2.6133 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.6143 - accuracy: 0.6038 - val_loss: 2.6029 - val_accuracy: 0.6786 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.5996 - accuracy: 0.6869 - val_loss: 2.5929 - val_accuracy: 0.7381 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5898 - accuracy: 0.7125 - val_loss: 2.5827 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5827 - accuracy: 0.7348 - val_loss: 2.5727 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.5683 - accuracy: 0.7796 - val_loss: 2.5627 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5619 - accuracy: 0.8179 - val_loss: 2.5530 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5494 - accuracy: 0.8275 - val_loss: 2.5431 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.5402 - accuracy: 0.8371 - val_loss: 2.5335 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5309 - accuracy: 0.8307 - val_loss: 2.5240 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5247 - accuracy: 0.8594 - val_loss: 2.5146 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5115 - accuracy: 0.8818 - val_loss: 2.5054 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5048 - accuracy: 0.8626 - val_loss: 2.4961 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.4933 - accuracy: 0.8850 - val_loss: 2.4869 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4855 - accuracy: 0.8722 - val_loss: 2.4779 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4745 - accuracy: 0.8754 - val_loss: 2.4691 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4609 - accuracy: 0.8914 - val_loss: 2.4603 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4616 - accuracy: 0.8786 - val_loss: 2.4514 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4461 - accuracy: 0.9010 - val_loss: 2.4426 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.4374 - accuracy: 0.8946 - val_loss: 2.4340 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.4256 - accuracy: 0.8850 - val_loss: 2.4254 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.4235 - accuracy: 0.9073 - val_loss: 2.4170 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.4140 - accuracy: 0.8946 - val_loss: 2.4087 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.3999 - accuracy: 0.8946 - val_loss: 2.4004 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3936 - accuracy: 0.9042 - val_loss: 2.3922 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3858 - accuracy: 0.9073 - val_loss: 2.3841 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3820 - accuracy: 0.8850 - val_loss: 2.3762 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3698 - accuracy: 0.9137 - val_loss: 2.3682 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.3664 - accuracy: 0.8946 - val_loss: 2.3608 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3558 - accuracy: 0.9042 - val_loss: 2.3530 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3496 - accuracy: 0.9010 - val_loss: 2.3453 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.3398 - accuracy: 0.9010 - val_loss: 2.3376 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3323 - accuracy: 0.9105 - val_loss: 2.3300 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3244 - accuracy: 0.9105 - val_loss: 2.3227 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3141 - accuracy: 0.9010 - val_loss: 2.3153 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3129 - accuracy: 0.8978 - val_loss: 2.3080 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3004 - accuracy: 0.9137 - val_loss: 2.3008 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2939 - accuracy: 0.9073 - val_loss: 2.2936 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2821 - accuracy: 0.9042 - val_loss: 2.2867 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2816 - accuracy: 0.9042 - val_loss: 2.2798 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.2682 - accuracy: 0.9105 - val_loss: 2.2727 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2653 - accuracy: 0.8946 - val_loss: 2.2658 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2560 - accuracy: 0.9201 - val_loss: 2.2589 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.2509 - accuracy: 0.9105 - val_loss: 2.2522 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.2459 - accuracy: 0.9169 - val_loss: 2.2456 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2395 - accuracy: 0.9105 - val_loss: 2.2390 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2332 - accuracy: 0.9105 - val_loss: 2.2325 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2242 - accuracy: 0.9137 - val_loss: 2.2264 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2178 - accuracy: 0.9073 - val_loss: 2.2200 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2133 - accuracy: 0.9105 - val_loss: 2.2137 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2132 - accuracy: 0.9105 - val_loss: 2.2073 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2082 - accuracy: 0.9137 - val_loss: 2.2012 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.2027 - accuracy: 0.9073 - val_loss: 2.1951 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1879 - accuracy: 0.9073 - val_loss: 2.1890 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.1863 - accuracy: 0.9105 - val_loss: 2.1831 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1825 - accuracy: 0.9201 - val_loss: 2.1771 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.1648 - accuracy: 0.9137 - val_loss: 2.1712 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1640 - accuracy: 0.9137 - val_loss: 2.1653 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.1599 - accuracy: 0.9073 - val_loss: 2.1596 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.1546 - accuracy: 0.9169 - val_loss: 2.1538 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1510 - accuracy: 0.9169 - val_loss: 2.1482 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1441 - accuracy: 0.9169 - val_loss: 2.1426 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1420 - accuracy: 0.9105 - val_loss: 2.1371 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.1272 - accuracy: 0.9201 - val_loss: 2.1318 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1259 - accuracy: 0.9137 - val_loss: 2.1266 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1266 - accuracy: 0.9137 - val_loss: 2.1212 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.1167 - accuracy: 0.9233 - val_loss: 2.1158 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1102 - accuracy: 0.9169 - val_loss: 2.1106 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1121 - accuracy: 0.9105 - val_loss: 2.1054 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1018 - accuracy: 0.9201 - val_loss: 2.1003 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0946 - accuracy: 0.9201 - val_loss: 2.0952 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.0930 - accuracy: 0.9105 - val_loss: 2.0902 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0814 - accuracy: 0.9233 - val_loss: 2.0852 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0822 - accuracy: 0.9201 - val_loss: 2.0806 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.0836 - accuracy: 0.9105 - val_loss: 2.0757 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.0718 - accuracy: 0.9201 - val_loss: 2.0708 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0695 - accuracy: 0.9137 - val_loss: 2.0662 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0601 - accuracy: 0.9169 - val_loss: 2.0614 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0527 - accuracy: 0.9265 - val_loss: 2.0567 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0595 - accuracy: 0.9201 - val_loss: 2.0522 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0445 - accuracy: 0.9233 - val_loss: 2.0476 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0393 - accuracy: 0.9169 - val_loss: 2.0430 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0414 - accuracy: 0.9137 - val_loss: 2.0384 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0353 - accuracy: 0.9169 - val_loss: 2.0339 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0325 - accuracy: 0.9201 - val_loss: 2.0295 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0321 - accuracy: 0.9201 - val_loss: 2.0251 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0238 - accuracy: 0.9201 - val_loss: 2.0207 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0145 - accuracy: 0.9233 - val_loss: 2.0163 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.0163 - accuracy: 0.9169 - val_loss: 2.0120 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0118 - accuracy: 0.9265 - val_loss: 2.0077 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9997 - accuracy: 0.9169 - val_loss: 2.0037 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0017 - accuracy: 0.9201 - val_loss: 1.9995 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9968 - accuracy: 0.9265 - val_loss: 1.9953 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9910 - accuracy: 0.9265 - val_loss: 1.9912 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9924 - accuracy: 0.9233 - val_loss: 1.9871 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9763 - accuracy: 0.9297 - val_loss: 1.9830 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9799 - accuracy: 0.9233 - val_loss: 1.9789 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9750 - accuracy: 0.9201 - val_loss: 1.9748 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9694 - accuracy: 0.9233 - val_loss: 1.9708 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9700 - accuracy: 0.9233 - val_loss: 1.9671 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9603 - accuracy: 0.9265 - val_loss: 1.9632 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9594 - accuracy: 0.9233 - val_loss: 1.9592 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.9524 - accuracy: 0.9265 - val_loss: 1.9553 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9494 - accuracy: 0.9297 - val_loss: 1.9515 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9456 - accuracy: 0.9297 - val_loss: 1.9476 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9485 - accuracy: 0.9201 - val_loss: 1.9438 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9430 - accuracy: 0.9201 - val_loss: 1.9400 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9473 - accuracy: 0.9265 - val_loss: 1.9363 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9325 - accuracy: 0.9297 - val_loss: 1.9326 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9293 - accuracy: 0.9233 - val_loss: 1.9289 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9230 - accuracy: 0.9297 - val_loss: 1.9252 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9314 - accuracy: 0.9169 - val_loss: 1.9215 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9209 - accuracy: 0.9265 - val_loss: 1.9179 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9163 - accuracy: 0.9233 - val_loss: 1.9143 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9174 - accuracy: 0.9233 - val_loss: 1.9107 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.9115 - accuracy: 0.9265 - val_loss: 1.9072 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9122 - accuracy: 0.9169 - val_loss: 1.9036 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9063 - accuracy: 0.9233 - val_loss: 1.9001 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8993 - accuracy: 0.9265 - val_loss: 1.8966 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8872 - accuracy: 0.9233 - val_loss: 1.8931 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8901 - accuracy: 0.9265 - val_loss: 1.8897 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8894 - accuracy: 0.9233 - val_loss: 1.8862 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8880 - accuracy: 0.9265 - val_loss: 1.8828 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8799 - accuracy: 0.9329 - val_loss: 1.8795 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8795 - accuracy: 0.9233 - val_loss: 1.8759 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8739 - accuracy: 0.9265 - val_loss: 1.8726 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8761 - accuracy: 0.9265 - val_loss: 1.8692 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8726 - accuracy: 0.9169 - val_loss: 1.8659 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.8596 - accuracy: 0.9297 - val_loss: 1.8626 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8665 - accuracy: 0.9297 - val_loss: 1.8593 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8620 - accuracy: 0.9201 - val_loss: 1.8561 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8511 - accuracy: 0.9233 - val_loss: 1.8528 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8562 - accuracy: 0.9233 - val_loss: 1.8495 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8463 - accuracy: 0.9265 - val_loss: 1.8462 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8410 - accuracy: 0.9329 - val_loss: 1.8431 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8400 - accuracy: 0.9297 - val_loss: 1.8398 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8348 - accuracy: 0.9265 - val_loss: 1.8365 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8308 - accuracy: 0.9297 - val_loss: 1.8333 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8279 - accuracy: 0.9329 - val_loss: 1.8302 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8286 - accuracy: 0.9297 - val_loss: 1.8270 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8288 - accuracy: 0.9297 - val_loss: 1.8239 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.8213 - accuracy: 0.9265 - val_loss: 1.8208 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8248 - accuracy: 0.9233 - val_loss: 1.8177 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8095 - accuracy: 0.9265 - val_loss: 1.8146 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8165 - accuracy: 0.9265 - val_loss: 1.8116 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8100 - accuracy: 0.9265 - val_loss: 1.8084 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8045 - accuracy: 0.9265 - val_loss: 1.8053 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7993 - accuracy: 0.9329 - val_loss: 1.8023 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8101 - accuracy: 0.9329 - val_loss: 1.7992 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8062 - accuracy: 0.9297 - val_loss: 1.7962 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7996 - accuracy: 0.9265 - val_loss: 1.7931 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7940 - accuracy: 0.9297 - val_loss: 1.7901 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7847 - accuracy: 0.9265 - val_loss: 1.7872 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7909 - accuracy: 0.9233 - val_loss: 1.7842 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.7837 - accuracy: 0.9329 - val_loss: 1.7812 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7819 - accuracy: 0.9265 - val_loss: 1.7781 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7826 - accuracy: 0.9265 - val_loss: 1.7751 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7793 - accuracy: 0.9265 - val_loss: 1.7721 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7747 - accuracy: 0.9329 - val_loss: 1.7692 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7679 - accuracy: 0.9297 - val_loss: 1.7662 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7638 - accuracy: 0.9329 - val_loss: 1.7632 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7574 - accuracy: 0.9329 - val_loss: 1.7603 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7577 - accuracy: 0.9329 - val_loss: 1.7574 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7537 - accuracy: 0.9297 - val_loss: 1.7545 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7500 - accuracy: 0.9361 - val_loss: 1.7515 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7442 - accuracy: 0.9297 - val_loss: 1.7486 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.7490 - accuracy: 0.9265 - val_loss: 1.7457 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7433 - accuracy: 0.9329 - val_loss: 1.7429 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7455 - accuracy: 0.9361 - val_loss: 1.7400 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7344 - accuracy: 0.9265 - val_loss: 1.7371 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7406 - accuracy: 0.9329 - val_loss: 1.7343 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7291 - accuracy: 0.9297 - val_loss: 1.7313 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7381 - accuracy: 0.9329 - val_loss: 1.7285 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7305 - accuracy: 0.9265 - val_loss: 1.7257 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7205 - accuracy: 0.9297 - val_loss: 1.7229 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7215 - accuracy: 0.9329 - val_loss: 1.7200 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7214 - accuracy: 0.9329 - val_loss: 1.7172 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.7227 - accuracy: 0.9329 - val_loss: 1.7145 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7162 - accuracy: 0.9329 - val_loss: 1.7116 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7101 - accuracy: 0.9361 - val_loss: 1.7089 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7034 - accuracy: 0.9361 - val_loss: 1.7061 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6981 - accuracy: 0.9361 - val_loss: 1.7033 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7005 - accuracy: 0.9361 - val_loss: 1.7006 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7036 - accuracy: 0.9329 - val_loss: 1.6979 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6932 - accuracy: 0.9361 - val_loss: 1.6951 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6910 - accuracy: 0.9361 - val_loss: 1.6924 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6911 - accuracy: 0.9297 - val_loss: 1.6897 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6901 - accuracy: 0.9361 - val_loss: 1.6869 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6806 - accuracy: 0.9361 - val_loss: 1.6842 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.6808 - accuracy: 0.9329 - val_loss: 1.6815 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6805 - accuracy: 0.9329 - val_loss: 1.6788 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6872 - accuracy: 0.9361 - val_loss: 1.6761 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6809 - accuracy: 0.9361 - val_loss: 1.6734 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6698 - accuracy: 0.9361 - val_loss: 1.6707 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6707 - accuracy: 0.9329 - val_loss: 1.6681 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6652 - accuracy: 0.9329 - val_loss: 1.6654 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6659 - accuracy: 0.9329 - val_loss: 1.6627 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6652 - accuracy: 0.9361 - val_loss: 1.6601 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6635 - accuracy: 0.9361 - val_loss: 1.6575 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6532 - accuracy: 0.9361 - val_loss: 1.6548 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6483 - accuracy: 0.9297 - val_loss: 1.6522 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.6559 - accuracy: 0.9361 - val_loss: 1.6495 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6500 - accuracy: 0.9425 - val_loss: 1.6469 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6448 - accuracy: 0.9329 - val_loss: 1.6442 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6386 - accuracy: 0.9329 - val_loss: 1.6416 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6426 - accuracy: 0.9393 - val_loss: 1.6390 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6370 - accuracy: 0.9361 - val_loss: 1.6364 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6348 - accuracy: 0.9329 - val_loss: 1.6338 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6334 - accuracy: 0.9329 - val_loss: 1.6313 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6308 - accuracy: 0.9361 - val_loss: 1.6287 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6339 - accuracy: 0.9393 - val_loss: 1.6261 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6314 - accuracy: 0.9329 - val_loss: 1.6236 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.6261 - accuracy: 0.9393 - val_loss: 1.6211 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6308 - accuracy: 0.9393 - val_loss: 1.6185 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6151 - accuracy: 0.9361 - val_loss: 1.6160 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6207 - accuracy: 0.9425 - val_loss: 1.6135 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6096 - accuracy: 0.9457 - val_loss: 1.6110 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6145 - accuracy: 0.9361 - val_loss: 1.6085 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6086 - accuracy: 0.9393 - val_loss: 1.6060 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6081 - accuracy: 0.9393 - val_loss: 1.6034 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6065 - accuracy: 0.9329 - val_loss: 1.6009 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6018 - accuracy: 0.9361 - val_loss: 1.5984 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6013 - accuracy: 0.9329 - val_loss: 1.5959 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.5932 - accuracy: 0.9393 - val_loss: 1.5934 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5934 - accuracy: 0.9329 - val_loss: 1.5909 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5893 - accuracy: 0.9297 - val_loss: 1.5884 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5868 - accuracy: 0.9425 - val_loss: 1.5859 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5825 - accuracy: 0.9361 - val_loss: 1.5834 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5815 - accuracy: 0.9393 - val_loss: 1.5809 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5833 - accuracy: 0.9361 - val_loss: 1.5784 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5733 - accuracy: 0.9457 - val_loss: 1.5759 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5751 - accuracy: 0.9393 - val_loss: 1.5735 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.5692 - accuracy: 0.9425 - val_loss: 1.5710 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5675 - accuracy: 0.9457 - val_loss: 1.5686 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5648 - accuracy: 0.9425 - val_loss: 1.5661 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5639 - accuracy: 0.9361 - val_loss: 1.5637 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5578 - accuracy: 0.9393 - val_loss: 1.5613 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5588 - accuracy: 0.9329 - val_loss: 1.5588 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5582 - accuracy: 0.9425 - val_loss: 1.5564 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5563 - accuracy: 0.9457 - val_loss: 1.5540 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5506 - accuracy: 0.9393 - val_loss: 1.5515 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5562 - accuracy: 0.9393 - val_loss: 1.5491 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5512 - accuracy: 0.9457 - val_loss: 1.5467 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.5416 - accuracy: 0.9457 - val_loss: 1.5443 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5437 - accuracy: 0.9361 - val_loss: 1.5419 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5433 - accuracy: 0.9393 - val_loss: 1.5395 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5405 - accuracy: 0.9361 - val_loss: 1.5371 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5286 - accuracy: 0.9393 - val_loss: 1.5347 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5300 - accuracy: 0.9393 - val_loss: 1.5323 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5263 - accuracy: 0.9457 - val_loss: 1.5300 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5322 - accuracy: 0.9457 - val_loss: 1.5276 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5290 - accuracy: 0.9457 - val_loss: 1.5252 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5294 - accuracy: 0.9361 - val_loss: 1.5229 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5228 - accuracy: 0.9489 - val_loss: 1.5205 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.5145 - accuracy: 0.9457 - val_loss: 1.5181 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5176 - accuracy: 0.9393 - val_loss: 1.5158 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5117 - accuracy: 0.9457 - val_loss: 1.5135 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5059 - accuracy: 0.9361 - val_loss: 1.5111 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5106 - accuracy: 0.9425 - val_loss: 1.5088 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5186 - accuracy: 0.9393 - val_loss: 1.5065 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5009 - accuracy: 0.9393 - val_loss: 1.5042 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5034 - accuracy: 0.9393 - val_loss: 1.5019 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5053 - accuracy: 0.9393 - val_loss: 1.4995 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4974 - accuracy: 0.9425 - val_loss: 1.4972 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.4873 - accuracy: 0.9457 - val_loss: 1.4949 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4993 - accuracy: 0.9457 - val_loss: 1.4926 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5003 - accuracy: 0.9425 - val_loss: 1.4903 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4862 - accuracy: 0.9393 - val_loss: 1.4879 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4865 - accuracy: 0.9393 - val_loss: 1.4857 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4786 - accuracy: 0.9489 - val_loss: 1.4834 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4747 - accuracy: 0.9521 - val_loss: 1.4811 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4821 - accuracy: 0.9425 - val_loss: 1.4788 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4755 - accuracy: 0.9393 - val_loss: 1.4765 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4822 - accuracy: 0.9425 - val_loss: 1.4742 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.4696 - accuracy: 0.9425 - val_loss: 1.4720 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4688 - accuracy: 0.9425 - val_loss: 1.4697 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4661 - accuracy: 0.9489 - val_loss: 1.4675 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4725 - accuracy: 0.9457 - val_loss: 1.4653 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4611 - accuracy: 0.9457 - val_loss: 1.4631 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4599 - accuracy: 0.9457 - val_loss: 1.4608 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4588 - accuracy: 0.9457 - val_loss: 1.4586 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4587 - accuracy: 0.9457 - val_loss: 1.4565 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4569 - accuracy: 0.9425 - val_loss: 1.4542 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4577 - accuracy: 0.9457 - val_loss: 1.4520 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.4520 - accuracy: 0.9489 - val_loss: 1.4498 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4464 - accuracy: 0.9489 - val_loss: 1.4476 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4453 - accuracy: 0.9457 - val_loss: 1.4454 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4427 - accuracy: 0.9489 - val_loss: 1.4432 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4464 - accuracy: 0.9457 - val_loss: 1.4410 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4316 - accuracy: 0.9489 - val_loss: 1.4388 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4404 - accuracy: 0.9361 - val_loss: 1.4366 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4404 - accuracy: 0.9393 - val_loss: 1.4344 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4339 - accuracy: 0.9457 - val_loss: 1.4323 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4384 - accuracy: 0.9489 - val_loss: 1.4302 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.4304 - accuracy: 0.9425 - val_loss: 1.4280 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4188 - accuracy: 0.9457 - val_loss: 1.4259 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4142 - accuracy: 0.9553 - val_loss: 1.4237 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4316 - accuracy: 0.9489 - val_loss: 1.4215 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4266 - accuracy: 0.9425 - val_loss: 1.4195 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4188 - accuracy: 0.9457 - val_loss: 1.4173 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4147 - accuracy: 0.9521 - val_loss: 1.4152 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4078 - accuracy: 0.9489 - val_loss: 1.4130 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4131 - accuracy: 0.9521 - val_loss: 1.4109 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 306/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4090 - accuracy: 0.9489 - val_loss: 1.4087 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4069 - accuracy: 0.9425 - val_loss: 1.4065 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.4025 - accuracy: 0.9489 - val_loss: 1.4044 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4048 - accuracy: 0.9457 - val_loss: 1.4023 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3983 - accuracy: 0.9489 - val_loss: 1.4002 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3956 - accuracy: 0.9425 - val_loss: 1.3981 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3948 - accuracy: 0.9457 - val_loss: 1.3960 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3922 - accuracy: 0.9457 - val_loss: 1.3939 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3911 - accuracy: 0.9521 - val_loss: 1.3918 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3925 - accuracy: 0.9521 - val_loss: 1.3897 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3858 - accuracy: 0.9457 - val_loss: 1.3876 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.3857 - accuracy: 0.9585 - val_loss: 1.3855 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 318/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3824 - accuracy: 0.9457 - val_loss: 1.3835 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3785 - accuracy: 0.9553 - val_loss: 1.3814 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3777 - accuracy: 0.9489 - val_loss: 1.3794 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3783 - accuracy: 0.9457 - val_loss: 1.3774 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3750 - accuracy: 0.9553 - val_loss: 1.3753 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3730 - accuracy: 0.9489 - val_loss: 1.3733 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3680 - accuracy: 0.9521 - val_loss: 1.3712 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 325/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3747 - accuracy: 0.9457 - val_loss: 1.3692 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.3680 - accuracy: 0.9521 - val_loss: 1.3672 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3583 - accuracy: 0.9489 - val_loss: 1.3652 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3620 - accuracy: 0.9585 - val_loss: 1.3631 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3537 - accuracy: 0.9553 - val_loss: 1.3611 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3574 - accuracy: 0.9521 - val_loss: 1.3590 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3497 - accuracy: 0.9489 - val_loss: 1.3570 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3586 - accuracy: 0.9489 - val_loss: 1.3550 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3506 - accuracy: 0.9585 - val_loss: 1.3530 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3567 - accuracy: 0.9489 - val_loss: 1.3510 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.3518 - accuracy: 0.9521 - val_loss: 1.3490 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3434 - accuracy: 0.9585 - val_loss: 1.3470 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3499 - accuracy: 0.9489 - val_loss: 1.3450 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3399 - accuracy: 0.9553 - val_loss: 1.3430 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3458 - accuracy: 0.9489 - val_loss: 1.3410 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3348 - accuracy: 0.9585 - val_loss: 1.3390 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3381 - accuracy: 0.9585 - val_loss: 1.3370 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 342/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3311 - accuracy: 0.9553 - val_loss: 1.3350 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3335 - accuracy: 0.9585 - val_loss: 1.3330 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 344/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.3295 - accuracy: 0.9585 - val_loss: 1.3311 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3354 - accuracy: 0.9521 - val_loss: 1.3291 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3254 - accuracy: 0.9553 - val_loss: 1.3271 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3333 - accuracy: 0.9521 - val_loss: 1.3252 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3253 - accuracy: 0.9553 - val_loss: 1.3232 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3252 - accuracy: 0.9489 - val_loss: 1.3213 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3168 - accuracy: 0.9553 - val_loss: 1.3193 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 351/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3143 - accuracy: 0.9553 - val_loss: 1.3174 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3136 - accuracy: 0.9553 - val_loss: 1.3155 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.3162 - accuracy: 0.9521 - val_loss: 1.3136 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3094 - accuracy: 0.9585 - val_loss: 1.3116 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 355/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3108 - accuracy: 0.9521 - val_loss: 1.3097 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3081 - accuracy: 0.9553 - val_loss: 1.3078 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3126 - accuracy: 0.9585 - val_loss: 1.3059 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3011 - accuracy: 0.9521 - val_loss: 1.3040 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2993 - accuracy: 0.9585 - val_loss: 1.3021 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2991 - accuracy: 0.9521 - val_loss: 1.3002 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 361/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2997 - accuracy: 0.9585 - val_loss: 1.2983 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 362/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2886 - accuracy: 0.9553 - val_loss: 1.2964 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2926 - accuracy: 0.9553 - val_loss: 1.2944 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2875 - accuracy: 0.9585 - val_loss: 1.2926 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2980 - accuracy: 0.9553 - val_loss: 1.2907 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2925 - accuracy: 0.9553 - val_loss: 1.2889 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2862 - accuracy: 0.9489 - val_loss: 1.2870 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 368/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2780 - accuracy: 0.9617 - val_loss: 1.2850 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2848 - accuracy: 0.9521 - val_loss: 1.2832 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 370/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2801 - accuracy: 0.9553 - val_loss: 1.2813 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2816 - accuracy: 0.9553 - val_loss: 1.2794 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2738 - accuracy: 0.9585 - val_loss: 1.2776 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2760 - accuracy: 0.9585 - val_loss: 1.2758 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2763 - accuracy: 0.9521 - val_loss: 1.2739 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2651 - accuracy: 0.9585 - val_loss: 1.2720 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2746 - accuracy: 0.9553 - val_loss: 1.2702 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2670 - accuracy: 0.9553 - val_loss: 1.2683 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2637 - accuracy: 0.9585 - val_loss: 1.2665 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2605 - accuracy: 0.9553 - val_loss: 1.2646 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2635 - accuracy: 0.9585 - val_loss: 1.2628 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 381/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2653 - accuracy: 0.9585 - val_loss: 1.2610 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2660 - accuracy: 0.9585 - val_loss: 1.2591 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2625 - accuracy: 0.9585 - val_loss: 1.2573 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2562 - accuracy: 0.9553 - val_loss: 1.2555 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.2461 - accuracy: 0.9585 - val_loss: 1.2536 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2483 - accuracy: 0.9585 - val_loss: 1.2518 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2504 - accuracy: 0.9553 - val_loss: 1.2499 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2438 - accuracy: 0.9617 - val_loss: 1.2481 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2470 - accuracy: 0.9521 - val_loss: 1.2464 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2434 - accuracy: 0.9585 - val_loss: 1.2446 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2435 - accuracy: 0.9585 - val_loss: 1.2427 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2337 - accuracy: 0.9617 - val_loss: 1.2409 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2414 - accuracy: 0.9585 - val_loss: 1.2392 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.2338 - accuracy: 0.9617 - val_loss: 1.2374 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2371 - accuracy: 0.9617 - val_loss: 1.2356 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2238 - accuracy: 0.9585 - val_loss: 1.2339 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2277 - accuracy: 0.9617 - val_loss: 1.2321 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2368 - accuracy: 0.9585 - val_loss: 1.2303 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2240 - accuracy: 0.9617 - val_loss: 1.2285 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2298 - accuracy: 0.9617 - val_loss: 1.2268 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2300 - accuracy: 0.9585 - val_loss: 1.2251 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2222 - accuracy: 0.9617 - val_loss: 1.2233 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2283 - accuracy: 0.9553 - val_loss: 1.2215 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2170 - accuracy: 0.9585 - val_loss: 1.2197 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2150 - accuracy: 0.9585 - val_loss: 1.2180 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2169 - accuracy: 0.9617 - val_loss: 1.2162 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2118 - accuracy: 0.9617 - val_loss: 1.2145 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2218 - accuracy: 0.9553 - val_loss: 1.2128 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2057 - accuracy: 0.9585 - val_loss: 1.2110 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2062 - accuracy: 0.9553 - val_loss: 1.2091 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2086 - accuracy: 0.9617 - val_loss: 1.2075 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2011 - accuracy: 0.9617 - val_loss: 1.2056 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2103 - accuracy: 0.9521 - val_loss: 1.2039 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1977 - accuracy: 0.9585 - val_loss: 1.2022 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1974 - accuracy: 0.9585 - val_loss: 1.2005 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 416/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2001 - accuracy: 0.9617 - val_loss: 1.1988 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 417/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1959 - accuracy: 0.9617 - val_loss: 1.1971 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 418/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1955 - accuracy: 0.9553 - val_loss: 1.1954 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1959 - accuracy: 0.9617 - val_loss: 1.1937 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 420/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1918 - accuracy: 0.9585 - val_loss: 1.1920 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1852 - accuracy: 0.9585 - val_loss: 1.1903 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1912 - accuracy: 0.9585 - val_loss: 1.1886 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1906 - accuracy: 0.9649 - val_loss: 1.1870 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1901 - accuracy: 0.9585 - val_loss: 1.1853 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 425/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1865 - accuracy: 0.9585 - val_loss: 1.1836 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1755 - accuracy: 0.9585 - val_loss: 1.1819 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1754 - accuracy: 0.9585 - val_loss: 1.1803 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1668 - accuracy: 0.9617 - val_loss: 1.1786 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1689 - accuracy: 0.9617 - val_loss: 1.1770 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1724 - accuracy: 0.9585 - val_loss: 1.1753 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1743 - accuracy: 0.9553 - val_loss: 1.1737 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1748 - accuracy: 0.9617 - val_loss: 1.1720 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1662 - accuracy: 0.9649 - val_loss: 1.1703 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1631 - accuracy: 0.9585 - val_loss: 1.1686 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1651 - accuracy: 0.9585 - val_loss: 1.1668 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1617 - accuracy: 0.9585 - val_loss: 1.1652 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1591 - accuracy: 0.9617 - val_loss: 1.1635 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1605 - accuracy: 0.9585 - val_loss: 1.1618 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1630 - accuracy: 0.9553 - val_loss: 1.1602 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1550 - accuracy: 0.9617 - val_loss: 1.1585 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1509 - accuracy: 0.9553 - val_loss: 1.1569 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1537 - accuracy: 0.9585 - val_loss: 1.1552 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1511 - accuracy: 0.9617 - val_loss: 1.1536 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1440 - accuracy: 0.9681 - val_loss: 1.1520 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1418 - accuracy: 0.9649 - val_loss: 1.1503 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 446/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1513 - accuracy: 0.9617 - val_loss: 1.1487 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 447/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1415 - accuracy: 0.9617 - val_loss: 1.1471 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 448/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1353 - accuracy: 0.9617 - val_loss: 1.1455 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 449/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1442 - accuracy: 0.9585 - val_loss: 1.1438 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 450/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1407 - accuracy: 0.9617 - val_loss: 1.1422 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1382 - accuracy: 0.9617 - val_loss: 1.1406 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 452/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1404 - accuracy: 0.9617 - val_loss: 1.1389 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 453/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1351 - accuracy: 0.9585 - val_loss: 1.1373 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 454/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1330 - accuracy: 0.9617 - val_loss: 1.1357 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1328 - accuracy: 0.9617 - val_loss: 1.1342 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 456/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1321 - accuracy: 0.9617 - val_loss: 1.1325 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 457/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1254 - accuracy: 0.9585 - val_loss: 1.1310 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1288 - accuracy: 0.9649 - val_loss: 1.1294 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1255 - accuracy: 0.9585 - val_loss: 1.1278 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 460/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1243 - accuracy: 0.9617 - val_loss: 1.1262 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 461/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1193 - accuracy: 0.9617 - val_loss: 1.1247 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1116 - accuracy: 0.9617 - val_loss: 1.1231 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1108 - accuracy: 0.9617 - val_loss: 1.1215 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1110 - accuracy: 0.9649 - val_loss: 1.1200 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 465/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1187 - accuracy: 0.9617 - val_loss: 1.1184 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1134 - accuracy: 0.9585 - val_loss: 1.1169 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 467/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1113 - accuracy: 0.9649 - val_loss: 1.1153 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1092 - accuracy: 0.9585 - val_loss: 1.1138 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 469/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1100 - accuracy: 0.9617 - val_loss: 1.1122 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1075 - accuracy: 0.9617 - val_loss: 1.1107 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1055 - accuracy: 0.9617 - val_loss: 1.1091 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 472/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1067 - accuracy: 0.9617 - val_loss: 1.1076 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1060 - accuracy: 0.9617 - val_loss: 1.1061 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0961 - accuracy: 0.9617 - val_loss: 1.1045 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1059 - accuracy: 0.9649 - val_loss: 1.1030 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 476/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1043 - accuracy: 0.9585 - val_loss: 1.1015 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 477/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1020 - accuracy: 0.9617 - val_loss: 1.0999 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0971 - accuracy: 0.9617 - val_loss: 1.0984 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0946 - accuracy: 0.9649 - val_loss: 1.0968 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 480/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0902 - accuracy: 0.9617 - val_loss: 1.0954 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 481/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0902 - accuracy: 0.9617 - val_loss: 1.0938 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 482/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0890 - accuracy: 0.9649 - val_loss: 1.0923 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 483/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0888 - accuracy: 0.9681 - val_loss: 1.0907 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0781 - accuracy: 0.9617 - val_loss: 1.0892 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0925 - accuracy: 0.9553 - val_loss: 1.0877 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 486/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.0816 - accuracy: 0.9617 - val_loss: 1.0862 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0884 - accuracy: 0.9553 - val_loss: 1.0846 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0811 - accuracy: 0.9585 - val_loss: 1.0831 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 489/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0709 - accuracy: 0.9617 - val_loss: 1.0817 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 490/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0769 - accuracy: 0.9617 - val_loss: 1.0802 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0715 - accuracy: 0.9649 - val_loss: 1.0786 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0766 - accuracy: 0.9649 - val_loss: 1.0771 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 493/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0769 - accuracy: 0.9649 - val_loss: 1.0756 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 494/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0737 - accuracy: 0.9553 - val_loss: 1.0741 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0685 - accuracy: 0.9649 - val_loss: 1.0727 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 496/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0687 - accuracy: 0.9617 - val_loss: 1.0713 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0631 - accuracy: 0.9617 - val_loss: 1.0699 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0627 - accuracy: 0.9585 - val_loss: 1.0683 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0693 - accuracy: 0.9585 - val_loss: 1.0669 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.0643 - accuracy: 0.9585 - val_loss: 1.0654 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 501/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0559 - accuracy: 0.9681 - val_loss: 1.0639 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 502/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0616 - accuracy: 0.9585 - val_loss: 1.0625 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 503/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0546 - accuracy: 0.9649 - val_loss: 1.0610 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 504/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0551 - accuracy: 0.9617 - val_loss: 1.0596 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0526 - accuracy: 0.9617 - val_loss: 1.0580 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 506/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0552 - accuracy: 0.9649 - val_loss: 1.0566 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0470 - accuracy: 0.9681 - val_loss: 1.0552 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 508/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0505 - accuracy: 0.9585 - val_loss: 1.0537 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 509/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0481 - accuracy: 0.9585 - val_loss: 1.0523 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 510/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0455 - accuracy: 0.9617 - val_loss: 1.0508 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0485 - accuracy: 0.9617 - val_loss: 1.0494 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 512/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0398 - accuracy: 0.9553 - val_loss: 1.0479 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 513/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0404 - accuracy: 0.9649 - val_loss: 1.0465 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 514/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.0394 - accuracy: 0.9585 - val_loss: 1.0450 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0462 - accuracy: 0.9649 - val_loss: 1.0436 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0391 - accuracy: 0.9617 - val_loss: 1.0422 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 517/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0331 - accuracy: 0.9649 - val_loss: 1.0407 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 518/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0303 - accuracy: 0.9649 - val_loss: 1.0393 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0322 - accuracy: 0.9681 - val_loss: 1.0378 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0298 - accuracy: 0.9617 - val_loss: 1.0364 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0301 - accuracy: 0.9617 - val_loss: 1.0350 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0332 - accuracy: 0.9649 - val_loss: 1.0336 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0283 - accuracy: 0.9617 - val_loss: 1.0322 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0297 - accuracy: 0.9617 - val_loss: 1.0309 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 525/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0258 - accuracy: 0.9681 - val_loss: 1.0295 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0225 - accuracy: 0.9617 - val_loss: 1.0280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0222 - accuracy: 0.9617 - val_loss: 1.0266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 528/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.0265 - accuracy: 0.9617 - val_loss: 1.0252 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 529/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0187 - accuracy: 0.9649 - val_loss: 1.0238 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 530/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0093 - accuracy: 0.9681 - val_loss: 1.0224 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0136 - accuracy: 0.9585 - val_loss: 1.0209 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 532/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0174 - accuracy: 0.9585 - val_loss: 1.0195 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0135 - accuracy: 0.9585 - val_loss: 1.0181 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 534/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0144 - accuracy: 0.9585 - val_loss: 1.0167 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 535/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0128 - accuracy: 0.9617 - val_loss: 1.0155 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 536/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0106 - accuracy: 0.9649 - val_loss: 1.0141 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9994 - accuracy: 0.9617 - val_loss: 1.0127 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0023 - accuracy: 0.9649 - val_loss: 1.0113 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0088 - accuracy: 0.9617 - val_loss: 1.0100 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0038 - accuracy: 0.9617 - val_loss: 1.0086 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0030 - accuracy: 0.9585 - val_loss: 1.0072 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0048 - accuracy: 0.9617 - val_loss: 1.0059 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 543/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0057 - accuracy: 0.9617 - val_loss: 1.0046 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0014 - accuracy: 0.9649 - val_loss: 1.0033 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0032 - accuracy: 0.9617 - val_loss: 1.0019 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9921 - accuracy: 0.9617 - val_loss: 1.0006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9929 - accuracy: 0.9681 - val_loss: 0.9992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9816 - accuracy: 0.9617 - val_loss: 0.9978 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9897 - accuracy: 0.9617 - val_loss: 0.9965 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 550/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9946 - accuracy: 0.9649 - val_loss: 0.9951 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 551/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9907 - accuracy: 0.9617 - val_loss: 0.9938 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 552/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9924 - accuracy: 0.9617 - val_loss: 0.9926 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9823 - accuracy: 0.9649 - val_loss: 0.9912 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 554/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9829 - accuracy: 0.9617 - val_loss: 0.9898 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 555/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9934 - accuracy: 0.9585 - val_loss: 0.9885 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9791 - accuracy: 0.9617 - val_loss: 0.9871 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9781 - accuracy: 0.9649 - val_loss: 0.9857 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9773 - accuracy: 0.9649 - val_loss: 0.9844 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 559/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9798 - accuracy: 0.9649 - val_loss: 0.9831 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 560/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9774 - accuracy: 0.9617 - val_loss: 0.9817 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9747 - accuracy: 0.9617 - val_loss: 0.9804 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9632 - accuracy: 0.9712 - val_loss: 0.9791 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 563/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9757 - accuracy: 0.9649 - val_loss: 0.9777 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 564/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9711 - accuracy: 0.9681 - val_loss: 0.9764 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9707 - accuracy: 0.9617 - val_loss: 0.9751 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9705 - accuracy: 0.9617 - val_loss: 0.9738 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 567/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9673 - accuracy: 0.9585 - val_loss: 0.9725 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 568/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9716 - accuracy: 0.9585 - val_loss: 0.9711 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9536 - accuracy: 0.9617 - val_loss: 0.9699 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9618 - accuracy: 0.9649 - val_loss: 0.9685 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9570 - accuracy: 0.9649 - val_loss: 0.9673 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9606 - accuracy: 0.9585 - val_loss: 0.9657 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9507 - accuracy: 0.9617 - val_loss: 0.9645 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 574/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9531 - accuracy: 0.9681 - val_loss: 0.9632 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9573 - accuracy: 0.9585 - val_loss: 0.9619 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9501 - accuracy: 0.9617 - val_loss: 0.9606 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9573 - accuracy: 0.9617 - val_loss: 0.9594 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9536 - accuracy: 0.9649 - val_loss: 0.9581 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 579/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9522 - accuracy: 0.9617 - val_loss: 0.9568 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9497 - accuracy: 0.9617 - val_loss: 0.9556 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9524 - accuracy: 0.9649 - val_loss: 0.9543 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 582/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9399 - accuracy: 0.9649 - val_loss: 0.9530 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9410 - accuracy: 0.9617 - val_loss: 0.9517 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 584/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9491 - accuracy: 0.9617 - val_loss: 0.9505 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9439 - accuracy: 0.9649 - val_loss: 0.9493 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9417 - accuracy: 0.9649 - val_loss: 0.9480 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 587/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9449 - accuracy: 0.9649 - val_loss: 0.9467 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.9346 - accuracy: 0.9617 - val_loss: 0.9454 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9386 - accuracy: 0.9617 - val_loss: 0.9441 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9361 - accuracy: 0.9617 - val_loss: 0.9428 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 591/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9389 - accuracy: 0.9649 - val_loss: 0.9415 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9289 - accuracy: 0.9617 - val_loss: 0.9403 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9251 - accuracy: 0.9681 - val_loss: 0.9391 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.9342 - accuracy: 0.9681 - val_loss: 0.9378 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 595/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9323 - accuracy: 0.9649 - val_loss: 0.9366 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9270 - accuracy: 0.9681 - val_loss: 0.9352 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9264 - accuracy: 0.9585 - val_loss: 0.9340 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 598/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9214 - accuracy: 0.9617 - val_loss: 0.9328 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 599/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9249 - accuracy: 0.9649 - val_loss: 0.9316 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 600/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.9286 - accuracy: 0.9585 - val_loss: 0.9303 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 601/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9188 - accuracy: 0.9649 - val_loss: 0.9291 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9203 - accuracy: 0.9617 - val_loss: 0.9278 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9217 - accuracy: 0.9617 - val_loss: 0.9266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9180 - accuracy: 0.9617 - val_loss: 0.9254 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9159 - accuracy: 0.9617 - val_loss: 0.9242 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9122 - accuracy: 0.9617 - val_loss: 0.9231 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9182 - accuracy: 0.9585 - val_loss: 0.9219 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 608/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9170 - accuracy: 0.9649 - val_loss: 0.9207 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 609/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9099 - accuracy: 0.9617 - val_loss: 0.9195 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9033 - accuracy: 0.9681 - val_loss: 0.9183 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9059 - accuracy: 0.9617 - val_loss: 0.9170 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9079 - accuracy: 0.9649 - val_loss: 0.9158 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9082 - accuracy: 0.9617 - val_loss: 0.9147 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9031 - accuracy: 0.9617 - val_loss: 0.9135 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 615/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8985 - accuracy: 0.9617 - val_loss: 0.9123 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9057 - accuracy: 0.9649 - val_loss: 0.9111 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 617/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8989 - accuracy: 0.9617 - val_loss: 0.9098 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 618/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8962 - accuracy: 0.9617 - val_loss: 0.9086 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 619/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.9014 - accuracy: 0.9681 - val_loss: 0.9074 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8927 - accuracy: 0.9681 - val_loss: 0.9063 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 621/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8951 - accuracy: 0.9649 - val_loss: 0.9051 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8947 - accuracy: 0.9553 - val_loss: 0.9038 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8932 - accuracy: 0.9617 - val_loss: 0.9026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8910 - accuracy: 0.9585 - val_loss: 0.9014 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8946 - accuracy: 0.9617 - val_loss: 0.9003 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 626/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8942 - accuracy: 0.9649 - val_loss: 0.8991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 627/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8840 - accuracy: 0.9617 - val_loss: 0.8979 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 628/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8833 - accuracy: 0.9617 - val_loss: 0.8967 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 629/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8868 - accuracy: 0.9649 - val_loss: 0.8955 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8841 - accuracy: 0.9617 - val_loss: 0.8942 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 631/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8849 - accuracy: 0.9681 - val_loss: 0.8929 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8878 - accuracy: 0.9617 - val_loss: 0.8918 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 633/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8762 - accuracy: 0.9649 - val_loss: 0.8907 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 634/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8742 - accuracy: 0.9649 - val_loss: 0.8895 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8833 - accuracy: 0.9617 - val_loss: 0.8883 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8817 - accuracy: 0.9712 - val_loss: 0.8872 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8765 - accuracy: 0.9617 - val_loss: 0.8860 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 638/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.8736 - accuracy: 0.9585 - val_loss: 0.8848 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 639/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8748 - accuracy: 0.9649 - val_loss: 0.8837 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 640/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8742 - accuracy: 0.9617 - val_loss: 0.8826 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 641/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8766 - accuracy: 0.9649 - val_loss: 0.8814 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 642/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8672 - accuracy: 0.9617 - val_loss: 0.8803 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 643/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8692 - accuracy: 0.9649 - val_loss: 0.8791 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8695 - accuracy: 0.9617 - val_loss: 0.8780 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 645/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8649 - accuracy: 0.9712 - val_loss: 0.8767 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8609 - accuracy: 0.9617 - val_loss: 0.8755 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8688 - accuracy: 0.9649 - val_loss: 0.8744 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 648/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8636 - accuracy: 0.9617 - val_loss: 0.8732 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8635 - accuracy: 0.9649 - val_loss: 0.8721 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8720 - accuracy: 0.9617 - val_loss: 0.8710 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8641 - accuracy: 0.9649 - val_loss: 0.8699 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 652/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8531 - accuracy: 0.9649 - val_loss: 0.8688 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8600 - accuracy: 0.9649 - val_loss: 0.8677 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8599 - accuracy: 0.9585 - val_loss: 0.8664 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 655/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8523 - accuracy: 0.9649 - val_loss: 0.8653 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8602 - accuracy: 0.9649 - val_loss: 0.8641 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 657/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8551 - accuracy: 0.9649 - val_loss: 0.8630 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 658/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8543 - accuracy: 0.9585 - val_loss: 0.8618 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 659/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8500 - accuracy: 0.9617 - val_loss: 0.8607 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 660/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8471 - accuracy: 0.9681 - val_loss: 0.8595 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 661/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8462 - accuracy: 0.9681 - val_loss: 0.8585 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.8482 - accuracy: 0.9617 - val_loss: 0.8574 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8521 - accuracy: 0.9617 - val_loss: 0.8563 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8389 - accuracy: 0.9681 - val_loss: 0.8553 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 665/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8434 - accuracy: 0.9681 - val_loss: 0.8541 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 666/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8443 - accuracy: 0.9617 - val_loss: 0.8530 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8365 - accuracy: 0.9681 - val_loss: 0.8519 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 668/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8473 - accuracy: 0.9617 - val_loss: 0.8508 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8386 - accuracy: 0.9617 - val_loss: 0.8497 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8360 - accuracy: 0.9649 - val_loss: 0.8486 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8417 - accuracy: 0.9585 - val_loss: 0.8475 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8447 - accuracy: 0.9617 - val_loss: 0.8466 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8467 - accuracy: 0.9617 - val_loss: 0.8454 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.8288 - accuracy: 0.9681 - val_loss: 0.8444 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 675/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8325 - accuracy: 0.9649 - val_loss: 0.8432 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 676/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8265 - accuracy: 0.9776 - val_loss: 0.8421 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 677/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8345 - accuracy: 0.9617 - val_loss: 0.8411 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8274 - accuracy: 0.9585 - val_loss: 0.8400 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 679/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8329 - accuracy: 0.9649 - val_loss: 0.8389 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8293 - accuracy: 0.9681 - val_loss: 0.8378 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8236 - accuracy: 0.9617 - val_loss: 0.8368 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8171 - accuracy: 0.9649 - val_loss: 0.8357 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 683/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8200 - accuracy: 0.9649 - val_loss: 0.8346 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 684/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8218 - accuracy: 0.9649 - val_loss: 0.8335 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8289 - accuracy: 0.9617 - val_loss: 0.8324 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8181 - accuracy: 0.9585 - val_loss: 0.8314 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 687/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8176 - accuracy: 0.9649 - val_loss: 0.8303 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8231 - accuracy: 0.9649 - val_loss: 0.8293 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8169 - accuracy: 0.9649 - val_loss: 0.8283 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8080 - accuracy: 0.9649 - val_loss: 0.8273 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 691/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8127 - accuracy: 0.9681 - val_loss: 0.8260 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 692/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8161 - accuracy: 0.9681 - val_loss: 0.8250 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8144 - accuracy: 0.9649 - val_loss: 0.8239 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 694/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8092 - accuracy: 0.9681 - val_loss: 0.8227 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8061 - accuracy: 0.9617 - val_loss: 0.8217 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8088 - accuracy: 0.9681 - val_loss: 0.8206 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8083 - accuracy: 0.9681 - val_loss: 0.8195 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8041 - accuracy: 0.9617 - val_loss: 0.8184 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8124 - accuracy: 0.9617 - val_loss: 0.8174 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8061 - accuracy: 0.9681 - val_loss: 0.8163 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 701/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8069 - accuracy: 0.9617 - val_loss: 0.8152 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8082 - accuracy: 0.9649 - val_loss: 0.8141 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 703/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8091 - accuracy: 0.9681 - val_loss: 0.8131 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8008 - accuracy: 0.9649 - val_loss: 0.8120 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 705/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7974 - accuracy: 0.9681 - val_loss: 0.8109 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8036 - accuracy: 0.9617 - val_loss: 0.8099 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8031 - accuracy: 0.9649 - val_loss: 0.8089 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7963 - accuracy: 0.9617 - val_loss: 0.8079 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 709/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.7968 - accuracy: 0.9649 - val_loss: 0.8068 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 710/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.8032 - accuracy: 0.9617 - val_loss: 0.8058 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.7978 - accuracy: 0.9617 - val_loss: 0.8049 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 712/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.8002 - accuracy: 0.9649 - val_loss: 0.8038 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7978 - accuracy: 0.9617 - val_loss: 0.8029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 714/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7949 - accuracy: 0.9681 - val_loss: 0.8019 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7849 - accuracy: 0.9649 - val_loss: 0.8009 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 716/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7881 - accuracy: 0.9681 - val_loss: 0.7999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 717/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7851 - accuracy: 0.9649 - val_loss: 0.7988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 718/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7803 - accuracy: 0.9617 - val_loss: 0.7978 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 719/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7844 - accuracy: 0.9617 - val_loss: 0.7968 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.7879 - accuracy: 0.9649 - val_loss: 0.7958 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7889 - accuracy: 0.9649 - val_loss: 0.7948 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7839 - accuracy: 0.9649 - val_loss: 0.7938 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7874 - accuracy: 0.9649 - val_loss: 0.7928 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7788 - accuracy: 0.9585 - val_loss: 0.7917 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 725/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7861 - accuracy: 0.9617 - val_loss: 0.7908 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7810 - accuracy: 0.9617 - val_loss: 0.7897 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 727/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7855 - accuracy: 0.9681 - val_loss: 0.7887 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 728/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7826 - accuracy: 0.9649 - val_loss: 0.7877 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 729/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7816 - accuracy: 0.9617 - val_loss: 0.7867 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7710 - accuracy: 0.9681 - val_loss: 0.7857 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 731/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7667 - accuracy: 0.9712 - val_loss: 0.7847 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 732/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7705 - accuracy: 0.9617 - val_loss: 0.7838 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 733/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7708 - accuracy: 0.9649 - val_loss: 0.7828 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 734/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7753 - accuracy: 0.9585 - val_loss: 0.7818 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 735/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.7750 - accuracy: 0.9617 - val_loss: 0.7809 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 736/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7687 - accuracy: 0.9649 - val_loss: 0.7798 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 737/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7634 - accuracy: 0.9681 - val_loss: 0.7789 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7735 - accuracy: 0.9649 - val_loss: 0.7779 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7713 - accuracy: 0.9649 - val_loss: 0.7769 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7664 - accuracy: 0.9649 - val_loss: 0.7758 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 741/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7665 - accuracy: 0.9649 - val_loss: 0.7749 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 742/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7592 - accuracy: 0.9681 - val_loss: 0.7739 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 743/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7636 - accuracy: 0.9681 - val_loss: 0.7729 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 744/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7639 - accuracy: 0.9681 - val_loss: 0.7719 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 745/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7652 - accuracy: 0.9649 - val_loss: 0.7710 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 746/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7523 - accuracy: 0.9712 - val_loss: 0.7701 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 747/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7552 - accuracy: 0.9681 - val_loss: 0.7691 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 748/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7564 - accuracy: 0.9649 - val_loss: 0.7681 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 749/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7541 - accuracy: 0.9585 - val_loss: 0.7671 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7556 - accuracy: 0.9649 - val_loss: 0.7662 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7520 - accuracy: 0.9617 - val_loss: 0.7652 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7543 - accuracy: 0.9649 - val_loss: 0.7642 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7497 - accuracy: 0.9649 - val_loss: 0.7633 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 754/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7561 - accuracy: 0.9585 - val_loss: 0.7624 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 755/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7481 - accuracy: 0.9681 - val_loss: 0.7615 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 756/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7486 - accuracy: 0.9649 - val_loss: 0.7606 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 757/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7479 - accuracy: 0.9649 - val_loss: 0.7596 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 758/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7397 - accuracy: 0.9681 - val_loss: 0.7586 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7496 - accuracy: 0.9649 - val_loss: 0.7577 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 760/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7374 - accuracy: 0.9712 - val_loss: 0.7568 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7366 - accuracy: 0.9649 - val_loss: 0.7558 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7452 - accuracy: 0.9649 - val_loss: 0.7548 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7455 - accuracy: 0.9681 - val_loss: 0.7539 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7354 - accuracy: 0.9681 - val_loss: 0.7529 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 765/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7421 - accuracy: 0.9617 - val_loss: 0.7520 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7450 - accuracy: 0.9585 - val_loss: 0.7512 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7345 - accuracy: 0.9681 - val_loss: 0.7502 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 768/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7411 - accuracy: 0.9649 - val_loss: 0.7493 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7350 - accuracy: 0.9712 - val_loss: 0.7484 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7370 - accuracy: 0.9617 - val_loss: 0.7475 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7352 - accuracy: 0.9712 - val_loss: 0.7466 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 772/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7394 - accuracy: 0.9617 - val_loss: 0.7458 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 773/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7348 - accuracy: 0.9681 - val_loss: 0.7449 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7318 - accuracy: 0.9649 - val_loss: 0.7440 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 775/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7257 - accuracy: 0.9617 - val_loss: 0.7430 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7281 - accuracy: 0.9681 - val_loss: 0.7421 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7347 - accuracy: 0.9681 - val_loss: 0.7412 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 778/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7327 - accuracy: 0.9649 - val_loss: 0.7402 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 779/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7323 - accuracy: 0.9649 - val_loss: 0.7393 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 780/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7268 - accuracy: 0.9649 - val_loss: 0.7384 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 781/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7271 - accuracy: 0.9585 - val_loss: 0.7375 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7202 - accuracy: 0.9681 - val_loss: 0.7366 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7183 - accuracy: 0.9649 - val_loss: 0.7356 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7135 - accuracy: 0.9649 - val_loss: 0.7347 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 785/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7214 - accuracy: 0.9649 - val_loss: 0.7339 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 786/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7201 - accuracy: 0.9712 - val_loss: 0.7330 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 787/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7229 - accuracy: 0.9649 - val_loss: 0.7321 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 788/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7201 - accuracy: 0.9681 - val_loss: 0.7312 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 789/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7222 - accuracy: 0.9649 - val_loss: 0.7302 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 790/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7200 - accuracy: 0.9617 - val_loss: 0.7293 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 791/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7159 - accuracy: 0.9617 - val_loss: 0.7284 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 792/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7177 - accuracy: 0.9585 - val_loss: 0.7275 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7058 - accuracy: 0.9712 - val_loss: 0.7264 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 794/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7056 - accuracy: 0.9744 - val_loss: 0.7255 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 795/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7008 - accuracy: 0.9681 - val_loss: 0.7247 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7081 - accuracy: 0.9681 - val_loss: 0.7238 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 797/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7098 - accuracy: 0.9681 - val_loss: 0.7229 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7136 - accuracy: 0.9617 - val_loss: 0.7219 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 799/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7148 - accuracy: 0.9681 - val_loss: 0.7211 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 800/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7100 - accuracy: 0.9681 - val_loss: 0.7202 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6994 - accuracy: 0.9681 - val_loss: 0.7194 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6997 - accuracy: 0.9681 - val_loss: 0.7184 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 803/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7062 - accuracy: 0.9681 - val_loss: 0.7175 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7054 - accuracy: 0.9649 - val_loss: 0.7166 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7008 - accuracy: 0.9617 - val_loss: 0.7157 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 806/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7102 - accuracy: 0.9681 - val_loss: 0.7149 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7032 - accuracy: 0.9649 - val_loss: 0.7140 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7033 - accuracy: 0.9681 - val_loss: 0.7131 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6992 - accuracy: 0.9649 - val_loss: 0.7123 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 810/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7000 - accuracy: 0.9617 - val_loss: 0.7114 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6925 - accuracy: 0.9649 - val_loss: 0.7105 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 812/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.9712 - val_loss: 0.7097 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 813/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7007 - accuracy: 0.9681 - val_loss: 0.7088 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 814/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.9585 - val_loss: 0.7079 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 815/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.9712 - val_loss: 0.7070 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 816/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6952 - accuracy: 0.9649 - val_loss: 0.7061 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 817/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.9712 - val_loss: 0.7052 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6904 - accuracy: 0.9585 - val_loss: 0.7043 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 819/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6850 - accuracy: 0.9712 - val_loss: 0.7035 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 820/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.9649 - val_loss: 0.7026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 821/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6952 - accuracy: 0.9681 - val_loss: 0.7018 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 0.9681 - val_loss: 0.7009 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 823/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.9617 - val_loss: 0.7000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 824/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6855 - accuracy: 0.9681 - val_loss: 0.6993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 825/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6883 - accuracy: 0.9585 - val_loss: 0.6985 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 826/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6875 - accuracy: 0.9744 - val_loss: 0.6976 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 827/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6852 - accuracy: 0.9617 - val_loss: 0.6966 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 828/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.9681 - val_loss: 0.6957 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 829/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6816 - accuracy: 0.9712 - val_loss: 0.6950 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6801 - accuracy: 0.9617 - val_loss: 0.6940 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6761 - accuracy: 0.9617 - val_loss: 0.6932 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6828 - accuracy: 0.9712 - val_loss: 0.6924 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 833/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6824 - accuracy: 0.9649 - val_loss: 0.6916 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 834/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6727 - accuracy: 0.9712 - val_loss: 0.6908 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 835/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6709 - accuracy: 0.9681 - val_loss: 0.6899 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 836/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6775 - accuracy: 0.9617 - val_loss: 0.6893 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6745 - accuracy: 0.9617 - val_loss: 0.6885 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6707 - accuracy: 0.9712 - val_loss: 0.6876 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 839/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6706 - accuracy: 0.9744 - val_loss: 0.6868 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 840/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6776 - accuracy: 0.9585 - val_loss: 0.6859 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6640 - accuracy: 0.9617 - val_loss: 0.6851 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6721 - accuracy: 0.9649 - val_loss: 0.6843 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6639 - accuracy: 0.9712 - val_loss: 0.6835 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 844/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6668 - accuracy: 0.9681 - val_loss: 0.6826 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6686 - accuracy: 0.9617 - val_loss: 0.6816 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6750 - accuracy: 0.9649 - val_loss: 0.6808 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6667 - accuracy: 0.9649 - val_loss: 0.6800 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 848/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6611 - accuracy: 0.9649 - val_loss: 0.6792 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 849/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6652 - accuracy: 0.9681 - val_loss: 0.6784 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 850/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6631 - accuracy: 0.9712 - val_loss: 0.6776 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 851/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6605 - accuracy: 0.9712 - val_loss: 0.6767 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6609 - accuracy: 0.9681 - val_loss: 0.6758 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6616 - accuracy: 0.9712 - val_loss: 0.6750 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 854/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6552 - accuracy: 0.9681 - val_loss: 0.6742 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6570 - accuracy: 0.9649 - val_loss: 0.6734 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6522 - accuracy: 0.9744 - val_loss: 0.6726 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 857/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6642 - accuracy: 0.9617 - val_loss: 0.6718 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6524 - accuracy: 0.9649 - val_loss: 0.6711 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 859/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6502 - accuracy: 0.9649 - val_loss: 0.6702 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6526 - accuracy: 0.9712 - val_loss: 0.6694 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 861/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6555 - accuracy: 0.9681 - val_loss: 0.6686 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 862/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6465 - accuracy: 0.9712 - val_loss: 0.6678 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6513 - accuracy: 0.9649 - val_loss: 0.6670 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 864/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6485 - accuracy: 0.9681 - val_loss: 0.6662 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 865/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.9681 - val_loss: 0.6654 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6499 - accuracy: 0.9681 - val_loss: 0.6645 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6510 - accuracy: 0.9681 - val_loss: 0.6638 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6507 - accuracy: 0.9681 - val_loss: 0.6630 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 869/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6446 - accuracy: 0.9681 - val_loss: 0.6621 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 870/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6410 - accuracy: 0.9712 - val_loss: 0.6613 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6509 - accuracy: 0.9681 - val_loss: 0.6606 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 872/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6469 - accuracy: 0.9617 - val_loss: 0.6598 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 873/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6440 - accuracy: 0.9681 - val_loss: 0.6590 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6398 - accuracy: 0.9681 - val_loss: 0.6583 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6466 - accuracy: 0.9617 - val_loss: 0.6574 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 876/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6448 - accuracy: 0.9681 - val_loss: 0.6565 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 877/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6411 - accuracy: 0.9681 - val_loss: 0.6557 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6433 - accuracy: 0.9712 - val_loss: 0.6550 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 879/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6331 - accuracy: 0.9649 - val_loss: 0.6543 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6399 - accuracy: 0.9649 - val_loss: 0.6535 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 881/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6295 - accuracy: 0.9744 - val_loss: 0.6527 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6365 - accuracy: 0.9617 - val_loss: 0.6519 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6356 - accuracy: 0.9681 - val_loss: 0.6512 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6391 - accuracy: 0.9681 - val_loss: 0.6504 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6340 - accuracy: 0.9744 - val_loss: 0.6497 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.6299 - accuracy: 0.9681 - val_loss: 0.6489 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6365 - accuracy: 0.9681 - val_loss: 0.6482 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 888/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6327 - accuracy: 0.9681 - val_loss: 0.6474 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 889/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6355 - accuracy: 0.9681 - val_loss: 0.6467 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 890/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6346 - accuracy: 0.9649 - val_loss: 0.6460 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6282 - accuracy: 0.9744 - val_loss: 0.6451 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 892/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6319 - accuracy: 0.9681 - val_loss: 0.6444 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6280 - accuracy: 0.9681 - val_loss: 0.6436 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 894/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6315 - accuracy: 0.9681 - val_loss: 0.6429 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6216 - accuracy: 0.9681 - val_loss: 0.6421 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6301 - accuracy: 0.9681 - val_loss: 0.6413 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 897/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6208 - accuracy: 0.9681 - val_loss: 0.6406 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6290 - accuracy: 0.9649 - val_loss: 0.6398 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 899/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6266 - accuracy: 0.9649 - val_loss: 0.6390 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6215 - accuracy: 0.9681 - val_loss: 0.6383 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6255 - accuracy: 0.9649 - val_loss: 0.6375 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 902/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6209 - accuracy: 0.9681 - val_loss: 0.6368 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 903/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6157 - accuracy: 0.9681 - val_loss: 0.6360 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 904/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6206 - accuracy: 0.9681 - val_loss: 0.6352 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 905/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6213 - accuracy: 0.9712 - val_loss: 0.6345 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6199 - accuracy: 0.9681 - val_loss: 0.6338 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 907/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6227 - accuracy: 0.9712 - val_loss: 0.6331 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 908/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6146 - accuracy: 0.9681 - val_loss: 0.6323 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 909/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6161 - accuracy: 0.9681 - val_loss: 0.6315 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 910/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6185 - accuracy: 0.9681 - val_loss: 0.6308 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6195 - accuracy: 0.9617 - val_loss: 0.6300 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6137 - accuracy: 0.9712 - val_loss: 0.6296 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6162 - accuracy: 0.9681 - val_loss: 0.6288 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6173 - accuracy: 0.9649 - val_loss: 0.6280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6067 - accuracy: 0.9649 - val_loss: 0.6273 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6146 - accuracy: 0.9617 - val_loss: 0.6266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5991 - accuracy: 0.9744 - val_loss: 0.6258 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 918/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6052 - accuracy: 0.9712 - val_loss: 0.6251 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6068 - accuracy: 0.9712 - val_loss: 0.6244 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 920/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6031 - accuracy: 0.9681 - val_loss: 0.6236 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 921/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6077 - accuracy: 0.9712 - val_loss: 0.6229 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6087 - accuracy: 0.9681 - val_loss: 0.6223 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 923/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6116 - accuracy: 0.9681 - val_loss: 0.6215 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 924/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6075 - accuracy: 0.9649 - val_loss: 0.6208 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6049 - accuracy: 0.9712 - val_loss: 0.6200 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6092 - accuracy: 0.9681 - val_loss: 0.6193 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 927/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6012 - accuracy: 0.9681 - val_loss: 0.6185 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6005 - accuracy: 0.9681 - val_loss: 0.6178 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6048 - accuracy: 0.9681 - val_loss: 0.6170 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 930/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6030 - accuracy: 0.9681 - val_loss: 0.6163 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 931/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6015 - accuracy: 0.9649 - val_loss: 0.6156 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5936 - accuracy: 0.9712 - val_loss: 0.6148 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 933/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6042 - accuracy: 0.9649 - val_loss: 0.6141 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 934/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.5986 - accuracy: 0.9681 - val_loss: 0.6135 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5939 - accuracy: 0.9681 - val_loss: 0.6128 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 936/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5972 - accuracy: 0.9681 - val_loss: 0.6120 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 937/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5951 - accuracy: 0.9649 - val_loss: 0.6112 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5904 - accuracy: 0.9712 - val_loss: 0.6105 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5949 - accuracy: 0.9649 - val_loss: 0.6099 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5854 - accuracy: 0.9744 - val_loss: 0.6091 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 941/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5927 - accuracy: 0.9649 - val_loss: 0.6085 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 942/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5876 - accuracy: 0.9712 - val_loss: 0.6077 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 943/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5892 - accuracy: 0.9681 - val_loss: 0.6070 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 944/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5873 - accuracy: 0.9681 - val_loss: 0.6063 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 945/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5860 - accuracy: 0.9681 - val_loss: 0.6056 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 946/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5901 - accuracy: 0.9649 - val_loss: 0.6049 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 947/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5850 - accuracy: 0.9649 - val_loss: 0.6042 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 948/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5896 - accuracy: 0.9712 - val_loss: 0.6035 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5866 - accuracy: 0.9681 - val_loss: 0.6029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5902 - accuracy: 0.9712 - val_loss: 0.6022 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5808 - accuracy: 0.9681 - val_loss: 0.6015 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 952/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5819 - accuracy: 0.9712 - val_loss: 0.6008 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 953/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5880 - accuracy: 0.9649 - val_loss: 0.6001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 954/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5865 - accuracy: 0.9712 - val_loss: 0.5994 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 955/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5892 - accuracy: 0.9649 - val_loss: 0.5986 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5869 - accuracy: 0.9681 - val_loss: 0.5982 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 957/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5830 - accuracy: 0.9712 - val_loss: 0.5975 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5766 - accuracy: 0.9744 - val_loss: 0.5968 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5812 - accuracy: 0.9681 - val_loss: 0.5962 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5797 - accuracy: 0.9649 - val_loss: 0.5955 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 961/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5712 - accuracy: 0.9681 - val_loss: 0.5948 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5779 - accuracy: 0.9744 - val_loss: 0.5941 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5854 - accuracy: 0.9681 - val_loss: 0.5934 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5785 - accuracy: 0.9712 - val_loss: 0.5927 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5771 - accuracy: 0.9712 - val_loss: 0.5919 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 966/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5684 - accuracy: 0.9712 - val_loss: 0.5912 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5747 - accuracy: 0.9681 - val_loss: 0.5906 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 968/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5671 - accuracy: 0.9712 - val_loss: 0.5898 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5748 - accuracy: 0.9681 - val_loss: 0.5892 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 970/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5727 - accuracy: 0.9681 - val_loss: 0.5885 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 971/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5753 - accuracy: 0.9649 - val_loss: 0.5883 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5632 - accuracy: 0.9712 - val_loss: 0.5874 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5768 - accuracy: 0.9712 - val_loss: 0.5867 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5727 - accuracy: 0.9712 - val_loss: 0.5861 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 975/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5714 - accuracy: 0.9681 - val_loss: 0.5855 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5667 - accuracy: 0.9649 - val_loss: 0.5845 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 977/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5697 - accuracy: 0.9681 - val_loss: 0.5838 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 978/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5775 - accuracy: 0.9681 - val_loss: 0.5834 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 979/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5623 - accuracy: 0.9681 - val_loss: 0.5828 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5627 - accuracy: 0.9617 - val_loss: 0.5821 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5663 - accuracy: 0.9712 - val_loss: 0.5814 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 982/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5624 - accuracy: 0.9712 - val_loss: 0.5808 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 983/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5569 - accuracy: 0.9681 - val_loss: 0.5800 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5647 - accuracy: 0.9712 - val_loss: 0.5793 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 985/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5643 - accuracy: 0.9681 - val_loss: 0.5786 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5713 - accuracy: 0.9712 - val_loss: 0.5779 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 987/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5574 - accuracy: 0.9744 - val_loss: 0.5772 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5560 - accuracy: 0.9712 - val_loss: 0.5766 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5488 - accuracy: 0.9744 - val_loss: 0.5759 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 990/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.5591 - accuracy: 0.9649 - val_loss: 0.5753 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5620 - accuracy: 0.9712 - val_loss: 0.5747 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5632 - accuracy: 0.9681 - val_loss: 0.5740 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 993/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5548 - accuracy: 0.9681 - val_loss: 0.5734 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "40/40 [==============================] - 1s 13ms/step - loss: 0.5628 - accuracy: 0.9681 - val_loss: 0.5727 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 995/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5608 - accuracy: 0.9712 - val_loss: 0.5720 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5570 - accuracy: 0.9712 - val_loss: 0.5714 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 997/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5625 - accuracy: 0.9649 - val_loss: 0.5707 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 998/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5442 - accuracy: 0.9681 - val_loss: 0.5699 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5513 - accuracy: 0.9776 - val_loss: 0.5692 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5562 - accuracy: 0.9617 - val_loss: 0.5686 - val_accuracy: 0.9643 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_13 (LSTM)              (None, 128)               110080    \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114,241\n",
      "Trainable params: 114,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 1s 2ms/step - loss: 0.5686 - accuracy: 0.9643\n",
      "Test Loss: 0.5685984492301941\n",
      "Test Accuracy: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "# Definisco l'ottimizzatore con il learning rate iniziale\n",
    "initial_learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "# Definisco il learning rate schedule con decay lineare\n",
    "decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "decay_rate = 0.1  # Tasso di decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "                                                             \n",
    "model = Sequential()\n",
    "lstm_units = 128\n",
    "dense_units = 32\n",
    "\n",
    "# kernel_regularizer=regularizers.l2(1e-3)\n",
    "\n",
    "model.add(LSTM(lstm_units, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(dropout_percentage))\n",
    "model.add(Dense(dense_units, activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(dropout_percentage/2))\n",
    "model.add(Dense(1, activation = 'sigmoid', kernel_regularizer=regularizers.l2(1e-3)))\n",
    "\n",
    "# Compilazione del modello\n",
    "# model.compile(optimizer = Adam(learning_rate = 1e-3), loss = loss_function, metrics = metric)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestramento del modello con il learning rate modificato\n",
    "history = model.fit(x_train, y_train, epochs=1000, batch_size=8, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Valutazione del modello\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold con early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing vanilla RNN with k-fold\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 1s 9ms/step - loss: 0.5190 - accuracy: 0.7522 - val_loss: 0.5247 - val_accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4813 - accuracy: 0.7679 - val_loss: 0.4928 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.7857 - val_loss: 0.4674 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4210 - accuracy: 0.8259 - val_loss: 0.4473 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4030 - accuracy: 0.8348 - val_loss: 0.4309 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8281 - val_loss: 0.4176 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3604 - accuracy: 0.8549 - val_loss: 0.4064 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3799 - accuracy: 0.8571 - val_loss: 0.3964 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8616 - val_loss: 0.3875 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3619 - accuracy: 0.8527 - val_loss: 0.3789 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3483 - accuracy: 0.8527 - val_loss: 0.3713 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8862 - val_loss: 0.3646 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8862 - val_loss: 0.3586 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8795 - val_loss: 0.3533 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8750 - val_loss: 0.3482 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3048 - accuracy: 0.8973 - val_loss: 0.3436 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3092 - accuracy: 0.8884 - val_loss: 0.3396 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3138 - accuracy: 0.8862 - val_loss: 0.3355 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8862 - val_loss: 0.3316 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3015 - accuracy: 0.8795 - val_loss: 0.3282 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8951 - val_loss: 0.3248 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8973 - val_loss: 0.3218 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3037 - accuracy: 0.8862 - val_loss: 0.3189 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2893 - accuracy: 0.8929 - val_loss: 0.3161 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2880 - accuracy: 0.8929 - val_loss: 0.3135 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.8862 - val_loss: 0.3115 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.9040 - val_loss: 0.3094 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.8973 - val_loss: 0.3071 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2827 - accuracy: 0.8862 - val_loss: 0.3048 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2739 - accuracy: 0.8951 - val_loss: 0.3025 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8817 - val_loss: 0.3001 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2695 - accuracy: 0.9040 - val_loss: 0.2983 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2710 - accuracy: 0.8929 - val_loss: 0.2963 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2729 - accuracy: 0.9018 - val_loss: 0.2944 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2684 - accuracy: 0.9062 - val_loss: 0.2930 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.9018 - val_loss: 0.2915 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2690 - accuracy: 0.8996 - val_loss: 0.2897 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.9040 - val_loss: 0.2881 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2602 - accuracy: 0.9129 - val_loss: 0.2867 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2484 - accuracy: 0.9129 - val_loss: 0.2851 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2505 - accuracy: 0.9129 - val_loss: 0.2836 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9085 - val_loss: 0.2820 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2630 - accuracy: 0.9085 - val_loss: 0.2804 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.9085 - val_loss: 0.2791 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2461 - accuracy: 0.9040 - val_loss: 0.2780 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.9062 - val_loss: 0.2765 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2483 - accuracy: 0.9085 - val_loss: 0.2752 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.9107 - val_loss: 0.2741 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2343 - accuracy: 0.9174 - val_loss: 0.2731 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.9085 - val_loss: 0.2718 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9062 - val_loss: 0.2706 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2462 - accuracy: 0.9107 - val_loss: 0.2694 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2310 - accuracy: 0.9174 - val_loss: 0.2681 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2320 - accuracy: 0.9219 - val_loss: 0.2670 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2424 - accuracy: 0.9152 - val_loss: 0.2660 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2431 - accuracy: 0.9152 - val_loss: 0.2650 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2307 - accuracy: 0.9085 - val_loss: 0.2639 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2303 - accuracy: 0.9196 - val_loss: 0.2630 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2330 - accuracy: 0.9152 - val_loss: 0.2621 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9219 - val_loss: 0.2610 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2434 - accuracy: 0.9062 - val_loss: 0.2599 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.9196 - val_loss: 0.2591 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2299 - accuracy: 0.9263 - val_loss: 0.2582 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.9219 - val_loss: 0.2572 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9263 - val_loss: 0.2565 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.9174 - val_loss: 0.2557 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.9107 - val_loss: 0.2548 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.9219 - val_loss: 0.2540 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2193 - accuracy: 0.9263 - val_loss: 0.2531 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9174 - val_loss: 0.2522 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9174 - val_loss: 0.2512 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.9196 - val_loss: 0.2503 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9308 - val_loss: 0.2496 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2188 - accuracy: 0.9196 - val_loss: 0.2489 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2226 - accuracy: 0.9107 - val_loss: 0.2482 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9219 - val_loss: 0.2474 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2132 - accuracy: 0.9286 - val_loss: 0.2465 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9241 - val_loss: 0.2459 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9129 - val_loss: 0.2451 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9353 - val_loss: 0.2442 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2155 - accuracy: 0.9219 - val_loss: 0.2435 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2106 - accuracy: 0.9308 - val_loss: 0.2427 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9330 - val_loss: 0.2419 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2072 - accuracy: 0.9375 - val_loss: 0.2411 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2050 - accuracy: 0.9353 - val_loss: 0.2403 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.9174 - val_loss: 0.2395 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9196 - val_loss: 0.2388 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1990 - accuracy: 0.9286 - val_loss: 0.2382 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2037 - accuracy: 0.9375 - val_loss: 0.2376 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9286 - val_loss: 0.2369 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2106 - accuracy: 0.9308 - val_loss: 0.2362 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9330 - val_loss: 0.2357 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2059 - accuracy: 0.9375 - val_loss: 0.2350 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2068 - accuracy: 0.9241 - val_loss: 0.2342 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2103 - accuracy: 0.9263 - val_loss: 0.2335 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9286 - val_loss: 0.2328 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9353 - val_loss: 0.2322 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9286 - val_loss: 0.2316 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9464 - val_loss: 0.2309 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9308 - val_loss: 0.2304 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1951 - accuracy: 0.9353 - val_loss: 0.2299 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2052 - accuracy: 0.9375 - val_loss: 0.2292 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2016 - accuracy: 0.9308 - val_loss: 0.2286 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9353 - val_loss: 0.2282 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9330 - val_loss: 0.2276 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9397 - val_loss: 0.2268 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1945 - accuracy: 0.9375 - val_loss: 0.2263 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1986 - accuracy: 0.9353 - val_loss: 0.2258 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9308 - val_loss: 0.2252 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9353 - val_loss: 0.2247 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 0.9464 - val_loss: 0.2241 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9464 - val_loss: 0.2235 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9308 - val_loss: 0.2228 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1979 - accuracy: 0.9397 - val_loss: 0.2225 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1806 - accuracy: 0.9442 - val_loss: 0.2220 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1845 - accuracy: 0.9442 - val_loss: 0.2215 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.9330 - val_loss: 0.2209 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1947 - accuracy: 0.9353 - val_loss: 0.2204 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1862 - accuracy: 0.9464 - val_loss: 0.2198 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1983 - accuracy: 0.9375 - val_loss: 0.2194 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.9330 - val_loss: 0.2189 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1904 - accuracy: 0.9375 - val_loss: 0.2184 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.9397 - val_loss: 0.2177 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9442 - val_loss: 0.2172 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1931 - accuracy: 0.9397 - val_loss: 0.2167 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.9420 - val_loss: 0.2164 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1875 - accuracy: 0.9397 - val_loss: 0.2159 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9353 - val_loss: 0.2155 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.9420 - val_loss: 0.2151 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.9375 - val_loss: 0.2146 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1835 - accuracy: 0.9330 - val_loss: 0.2141 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9442 - val_loss: 0.2136 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9420 - val_loss: 0.2132 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9420 - val_loss: 0.2126 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9442 - val_loss: 0.2122 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.9397 - val_loss: 0.2117 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1828 - accuracy: 0.9353 - val_loss: 0.2113 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9442 - val_loss: 0.2108 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1865 - accuracy: 0.9397 - val_loss: 0.2104 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9531 - val_loss: 0.2099 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9397 - val_loss: 0.2097 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.9464 - val_loss: 0.2092 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.9375 - val_loss: 0.2086 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.9464 - val_loss: 0.2081 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.9487 - val_loss: 0.2077 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.9487 - val_loss: 0.2074 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9375 - val_loss: 0.2069 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.9375 - val_loss: 0.2066 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.9420 - val_loss: 0.2062 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9554 - val_loss: 0.2059 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1892 - accuracy: 0.9442 - val_loss: 0.2056 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.9442 - val_loss: 0.2052 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9487 - val_loss: 0.2049 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1802 - accuracy: 0.9464 - val_loss: 0.2044 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9464 - val_loss: 0.2042 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9375 - val_loss: 0.2038 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.9464 - val_loss: 0.2034 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9442 - val_loss: 0.2031 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.9442 - val_loss: 0.2028 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9442 - val_loss: 0.2025 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1678 - accuracy: 0.9464 - val_loss: 0.2020 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9464 - val_loss: 0.2016 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1717 - accuracy: 0.9509 - val_loss: 0.2013 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1735 - accuracy: 0.9397 - val_loss: 0.2009 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1703 - accuracy: 0.9531 - val_loss: 0.2004 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9554 - val_loss: 0.2001 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9509 - val_loss: 0.1998 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 0.9531 - val_loss: 0.1994 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.9464 - val_loss: 0.1990 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9487 - val_loss: 0.1987 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9554 - val_loss: 0.1984 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9397 - val_loss: 0.1981 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.9420 - val_loss: 0.1978 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1770 - accuracy: 0.9397 - val_loss: 0.1975 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1776 - accuracy: 0.9420 - val_loss: 0.1971 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.9442 - val_loss: 0.1968 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1690 - accuracy: 0.9464 - val_loss: 0.1966 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9531 - val_loss: 0.1962 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1644 - accuracy: 0.9531 - val_loss: 0.1958 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.9531 - val_loss: 0.1955 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1675 - accuracy: 0.9420 - val_loss: 0.1953 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9420 - val_loss: 0.1950 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9487 - val_loss: 0.1945 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1636 - accuracy: 0.9464 - val_loss: 0.1942 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1622 - accuracy: 0.9487 - val_loss: 0.1940 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1472 - accuracy: 0.9509 - val_loss: 0.1936 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9554 - val_loss: 0.1932 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.9487 - val_loss: 0.1929 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9375 - val_loss: 0.1927 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1655 - accuracy: 0.9487 - val_loss: 0.1923 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1636 - accuracy: 0.9576 - val_loss: 0.1921 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9487 - val_loss: 0.1917 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9464 - val_loss: 0.1914 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9464 - val_loss: 0.1911 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9487 - val_loss: 0.1908 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9464 - val_loss: 0.1905 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.9509 - val_loss: 0.1902 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9487 - val_loss: 0.1898 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.9554 - val_loss: 0.1895 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1656 - accuracy: 0.9509 - val_loss: 0.1892 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Loss: 0.1892, Accuracy: 92.86%\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 1s 7ms/step - loss: 0.7091 - accuracy: 0.6451 - val_loss: 0.5647 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6337 - accuracy: 0.6830 - val_loss: 0.5085 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5980 - accuracy: 0.7098 - val_loss: 0.4673 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5537 - accuracy: 0.7500 - val_loss: 0.4369 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7746 - val_loss: 0.4142 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5003 - accuracy: 0.7612 - val_loss: 0.3952 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4707 - accuracy: 0.7924 - val_loss: 0.3797 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4561 - accuracy: 0.8080 - val_loss: 0.3671 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4459 - accuracy: 0.8103 - val_loss: 0.3567 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8036 - val_loss: 0.3484 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8192 - val_loss: 0.3417 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8304 - val_loss: 0.3359 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8259 - val_loss: 0.3305 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3762 - accuracy: 0.8326 - val_loss: 0.3260 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3596 - accuracy: 0.8549 - val_loss: 0.3223 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8638 - val_loss: 0.3188 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3598 - accuracy: 0.8638 - val_loss: 0.3159 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3477 - accuracy: 0.8638 - val_loss: 0.3131 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3505 - accuracy: 0.8527 - val_loss: 0.3107 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8594 - val_loss: 0.3084 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8728 - val_loss: 0.3063 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8750 - val_loss: 0.3045 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8795 - val_loss: 0.3028 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8817 - val_loss: 0.3012 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.2996 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8884 - val_loss: 0.2981 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8772 - val_loss: 0.2968 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8817 - val_loss: 0.2955 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8839 - val_loss: 0.2944 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2985 - accuracy: 0.8862 - val_loss: 0.2933 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2929 - accuracy: 0.8973 - val_loss: 0.2922 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3086 - accuracy: 0.8839 - val_loss: 0.2911 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.9018 - val_loss: 0.2903 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2798 - accuracy: 0.9018 - val_loss: 0.2894 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.8906 - val_loss: 0.2885 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2929 - accuracy: 0.8884 - val_loss: 0.2877 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2823 - accuracy: 0.8973 - val_loss: 0.2868 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8973 - val_loss: 0.2861 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.9040 - val_loss: 0.2854 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2750 - accuracy: 0.8996 - val_loss: 0.2847 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2760 - accuracy: 0.9040 - val_loss: 0.2841 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.9107 - val_loss: 0.2834 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2770 - accuracy: 0.8929 - val_loss: 0.2828 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2659 - accuracy: 0.9152 - val_loss: 0.2821 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.9018 - val_loss: 0.2815 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.9152 - val_loss: 0.2808 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.9107 - val_loss: 0.2803 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.9040 - val_loss: 0.2797 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2682 - accuracy: 0.8996 - val_loss: 0.2789 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2561 - accuracy: 0.9062 - val_loss: 0.2784 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2522 - accuracy: 0.9286 - val_loss: 0.2778 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9129 - val_loss: 0.2773 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.9241 - val_loss: 0.2768 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2549 - accuracy: 0.9152 - val_loss: 0.2762 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2537 - accuracy: 0.9107 - val_loss: 0.2757 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.9129 - val_loss: 0.2752 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2415 - accuracy: 0.9152 - val_loss: 0.2747 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2451 - accuracy: 0.9018 - val_loss: 0.2742 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2294 - accuracy: 0.9174 - val_loss: 0.2737 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2295 - accuracy: 0.9219 - val_loss: 0.2733 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.9219 - val_loss: 0.2729 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2403 - accuracy: 0.9241 - val_loss: 0.2726 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.9219 - val_loss: 0.2721 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.9219 - val_loss: 0.2717 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.9286 - val_loss: 0.2713 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.9174 - val_loss: 0.2709 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.9196 - val_loss: 0.2705 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2272 - accuracy: 0.9263 - val_loss: 0.2701 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.9196 - val_loss: 0.2697 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.9263 - val_loss: 0.2692 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.9286 - val_loss: 0.2689 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2215 - accuracy: 0.9308 - val_loss: 0.2686 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.9263 - val_loss: 0.2682 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2258 - accuracy: 0.9196 - val_loss: 0.2678 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2320 - accuracy: 0.9219 - val_loss: 0.2674 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.9219 - val_loss: 0.2671 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9353 - val_loss: 0.2668 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2337 - accuracy: 0.9196 - val_loss: 0.2664 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2355 - accuracy: 0.9308 - val_loss: 0.2660 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.9263 - val_loss: 0.2656 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2284 - accuracy: 0.9219 - val_loss: 0.2653 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2216 - accuracy: 0.9286 - val_loss: 0.2649 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9375 - val_loss: 0.2645 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9286 - val_loss: 0.2641 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9375 - val_loss: 0.2638 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2206 - accuracy: 0.9286 - val_loss: 0.2635 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2105 - accuracy: 0.9397 - val_loss: 0.2632 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2064 - accuracy: 0.9375 - val_loss: 0.2629 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9219 - val_loss: 0.2626 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2050 - accuracy: 0.9375 - val_loss: 0.2623 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2107 - accuracy: 0.9330 - val_loss: 0.2619 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9330 - val_loss: 0.2616 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1990 - accuracy: 0.9375 - val_loss: 0.2613 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2086 - accuracy: 0.9375 - val_loss: 0.2611 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9353 - val_loss: 0.2607 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9397 - val_loss: 0.2604 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9330 - val_loss: 0.2601 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9375 - val_loss: 0.2598 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2041 - accuracy: 0.9330 - val_loss: 0.2595 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2053 - accuracy: 0.9420 - val_loss: 0.2591 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9330 - val_loss: 0.2589 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1976 - accuracy: 0.9375 - val_loss: 0.2585 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1940 - accuracy: 0.9420 - val_loss: 0.2582 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1995 - accuracy: 0.9397 - val_loss: 0.2579 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1949 - accuracy: 0.9420 - val_loss: 0.2576 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1926 - accuracy: 0.9397 - val_loss: 0.2573 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1961 - accuracy: 0.9420 - val_loss: 0.2570 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9442 - val_loss: 0.2567 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1963 - accuracy: 0.9442 - val_loss: 0.2564 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2036 - accuracy: 0.9353 - val_loss: 0.2563 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1947 - accuracy: 0.9442 - val_loss: 0.2561 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1902 - accuracy: 0.9397 - val_loss: 0.2558 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9464 - val_loss: 0.2555 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9420 - val_loss: 0.2552 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9442 - val_loss: 0.2549 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.9420 - val_loss: 0.2546 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9554 - val_loss: 0.2543 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9397 - val_loss: 0.2540 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9509 - val_loss: 0.2537 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1880 - accuracy: 0.9442 - val_loss: 0.2535 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.9464 - val_loss: 0.2533 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1961 - accuracy: 0.9308 - val_loss: 0.2530 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1829 - accuracy: 0.9487 - val_loss: 0.2528 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1835 - accuracy: 0.9487 - val_loss: 0.2526 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1914 - accuracy: 0.9442 - val_loss: 0.2523 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9464 - val_loss: 0.2520 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9509 - val_loss: 0.2517 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9442 - val_loss: 0.2515 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.9397 - val_loss: 0.2511 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9442 - val_loss: 0.2509 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1881 - accuracy: 0.9464 - val_loss: 0.2507 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9397 - val_loss: 0.2504 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9464 - val_loss: 0.2501 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1846 - accuracy: 0.9397 - val_loss: 0.2499 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1787 - accuracy: 0.9442 - val_loss: 0.2497 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1815 - accuracy: 0.9442 - val_loss: 0.2494 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9442 - val_loss: 0.2492 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1746 - accuracy: 0.9554 - val_loss: 0.2489 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1663 - accuracy: 0.9509 - val_loss: 0.2487 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.9487 - val_loss: 0.2484 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1810 - accuracy: 0.9442 - val_loss: 0.2481 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9487 - val_loss: 0.2478 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1836 - accuracy: 0.9487 - val_loss: 0.2475 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.9442 - val_loss: 0.2472 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1706 - accuracy: 0.9487 - val_loss: 0.2469 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9487 - val_loss: 0.2467 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9487 - val_loss: 0.2465 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1762 - accuracy: 0.9464 - val_loss: 0.2463 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.9509 - val_loss: 0.2461 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1755 - accuracy: 0.9464 - val_loss: 0.2458 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1740 - accuracy: 0.9509 - val_loss: 0.2456 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.9487 - val_loss: 0.2454 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1720 - accuracy: 0.9509 - val_loss: 0.2453 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1723 - accuracy: 0.9487 - val_loss: 0.2451 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9531 - val_loss: 0.2448 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9509 - val_loss: 0.2445 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9509 - val_loss: 0.2443 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9531 - val_loss: 0.2441 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.9509 - val_loss: 0.2439 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.9464 - val_loss: 0.2437 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1788 - accuracy: 0.9509 - val_loss: 0.2434 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9464 - val_loss: 0.2432 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9420 - val_loss: 0.2431 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.9509 - val_loss: 0.2428 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1749 - accuracy: 0.9464 - val_loss: 0.2426 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9464 - val_loss: 0.2424 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9442 - val_loss: 0.2422 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1614 - accuracy: 0.9531 - val_loss: 0.2420 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9442 - val_loss: 0.2418 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.9464 - val_loss: 0.2416 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1674 - accuracy: 0.9509 - val_loss: 0.2414 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9576 - val_loss: 0.2411 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1614 - accuracy: 0.9554 - val_loss: 0.2409 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1617 - accuracy: 0.9531 - val_loss: 0.2407 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9531 - val_loss: 0.2406 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.9531 - val_loss: 0.2405 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1729 - accuracy: 0.9464 - val_loss: 0.2403 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1638 - accuracy: 0.9509 - val_loss: 0.2400 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9509 - val_loss: 0.2398 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1608 - accuracy: 0.9531 - val_loss: 0.2396 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9531 - val_loss: 0.2394 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9531 - val_loss: 0.2392 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1685 - accuracy: 0.9576 - val_loss: 0.2390 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9509 - val_loss: 0.2388 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1596 - accuracy: 0.9621 - val_loss: 0.2386 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9464 - val_loss: 0.2385 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1542 - accuracy: 0.9554 - val_loss: 0.2383 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9576 - val_loss: 0.2382 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.9554 - val_loss: 0.2380 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9554 - val_loss: 0.2378 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.9442 - val_loss: 0.2377 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1588 - accuracy: 0.9509 - val_loss: 0.2375 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9576 - val_loss: 0.2374 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.9554 - val_loss: 0.2372 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1586 - accuracy: 0.9554 - val_loss: 0.2370 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.9531 - val_loss: 0.2369 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1598 - accuracy: 0.9509 - val_loss: 0.2368 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9531 - val_loss: 0.2366 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.9643 - val_loss: 0.2365 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.9554 - val_loss: 0.2363 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Loss: 0.2363, Accuracy: 91.96%\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 2s 11ms/step - loss: 0.8080 - accuracy: 0.5000 - val_loss: 0.7664 - val_accuracy: 0.4732 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6746 - accuracy: 0.6339 - val_loss: 0.6839 - val_accuracy: 0.5804 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6316 - accuracy: 0.6719 - val_loss: 0.6205 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5786 - accuracy: 0.7232 - val_loss: 0.5730 - val_accuracy: 0.6964 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5297 - accuracy: 0.7433 - val_loss: 0.5361 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4939 - accuracy: 0.7902 - val_loss: 0.5052 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4720 - accuracy: 0.7969 - val_loss: 0.4783 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.4324 - accuracy: 0.8125 - val_loss: 0.4568 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.8125 - val_loss: 0.4382 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4098 - accuracy: 0.8170 - val_loss: 0.4220 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4068 - accuracy: 0.8438 - val_loss: 0.4088 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8482 - val_loss: 0.3966 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8616 - val_loss: 0.3858 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3625 - accuracy: 0.8594 - val_loss: 0.3768 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8527 - val_loss: 0.3684 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3505 - accuracy: 0.8683 - val_loss: 0.3612 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8661 - val_loss: 0.3548 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8683 - val_loss: 0.3488 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8817 - val_loss: 0.3429 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3313 - accuracy: 0.8906 - val_loss: 0.3377 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8795 - val_loss: 0.3327 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8750 - val_loss: 0.3282 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3135 - accuracy: 0.8839 - val_loss: 0.3241 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3089 - accuracy: 0.8728 - val_loss: 0.3203 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3016 - accuracy: 0.8951 - val_loss: 0.3165 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3088 - accuracy: 0.8973 - val_loss: 0.3132 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8906 - val_loss: 0.3099 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3104 - accuracy: 0.8839 - val_loss: 0.3069 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2929 - accuracy: 0.8906 - val_loss: 0.3042 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.8973 - val_loss: 0.3016 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2788 - accuracy: 0.8996 - val_loss: 0.2989 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2846 - accuracy: 0.8951 - val_loss: 0.2964 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2996 - accuracy: 0.8906 - val_loss: 0.2940 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2786 - accuracy: 0.9062 - val_loss: 0.2918 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2783 - accuracy: 0.9018 - val_loss: 0.2897 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2922 - accuracy: 0.8973 - val_loss: 0.2876 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8951 - val_loss: 0.2856 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8973 - val_loss: 0.2839 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2732 - accuracy: 0.9018 - val_loss: 0.2820 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.9107 - val_loss: 0.2801 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.9129 - val_loss: 0.2785 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2675 - accuracy: 0.9129 - val_loss: 0.2767 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.9107 - val_loss: 0.2751 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.9018 - val_loss: 0.2734 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2559 - accuracy: 0.9107 - val_loss: 0.2718 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2659 - accuracy: 0.9107 - val_loss: 0.2702 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2608 - accuracy: 0.9085 - val_loss: 0.2688 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2675 - accuracy: 0.8929 - val_loss: 0.2674 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2642 - accuracy: 0.8996 - val_loss: 0.2658 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2548 - accuracy: 0.9219 - val_loss: 0.2643 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2522 - accuracy: 0.9085 - val_loss: 0.2629 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2611 - accuracy: 0.9085 - val_loss: 0.2616 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.9107 - val_loss: 0.2603 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.9196 - val_loss: 0.2590 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2453 - accuracy: 0.9174 - val_loss: 0.2577 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2432 - accuracy: 0.9129 - val_loss: 0.2565 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2377 - accuracy: 0.9241 - val_loss: 0.2554 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9085 - val_loss: 0.2542 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2349 - accuracy: 0.9219 - val_loss: 0.2531 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2495 - accuracy: 0.9129 - val_loss: 0.2520 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2383 - accuracy: 0.9107 - val_loss: 0.2510 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9174 - val_loss: 0.2501 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2445 - accuracy: 0.9107 - val_loss: 0.2491 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2421 - accuracy: 0.9152 - val_loss: 0.2481 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.9174 - val_loss: 0.2471 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.9040 - val_loss: 0.2462 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.9107 - val_loss: 0.2453 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.9241 - val_loss: 0.2442 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2363 - accuracy: 0.9107 - val_loss: 0.2434 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2359 - accuracy: 0.9219 - val_loss: 0.2425 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.9152 - val_loss: 0.2416 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.9286 - val_loss: 0.2407 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2307 - accuracy: 0.9330 - val_loss: 0.2398 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2412 - accuracy: 0.9196 - val_loss: 0.2390 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.9196 - val_loss: 0.2383 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.9174 - val_loss: 0.2376 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2394 - accuracy: 0.9196 - val_loss: 0.2367 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2412 - accuracy: 0.9196 - val_loss: 0.2360 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.9241 - val_loss: 0.2352 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.9308 - val_loss: 0.2345 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.9219 - val_loss: 0.2337 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.9263 - val_loss: 0.2329 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2294 - accuracy: 0.9286 - val_loss: 0.2322 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2291 - accuracy: 0.9174 - val_loss: 0.2314 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2266 - accuracy: 0.9219 - val_loss: 0.2306 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9263 - val_loss: 0.2300 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.9174 - val_loss: 0.2293 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2167 - accuracy: 0.9286 - val_loss: 0.2286 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2095 - accuracy: 0.9330 - val_loss: 0.2279 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2231 - accuracy: 0.9241 - val_loss: 0.2273 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.9263 - val_loss: 0.2266 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2211 - accuracy: 0.9263 - val_loss: 0.2261 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9330 - val_loss: 0.2255 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2228 - accuracy: 0.9241 - val_loss: 0.2248 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2185 - accuracy: 0.9219 - val_loss: 0.2241 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.9241 - val_loss: 0.2235 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9263 - val_loss: 0.2230 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9241 - val_loss: 0.2224 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.9353 - val_loss: 0.2218 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9263 - val_loss: 0.2212 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2066 - accuracy: 0.9420 - val_loss: 0.2207 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9308 - val_loss: 0.2200 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2028 - accuracy: 0.9308 - val_loss: 0.2195 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2060 - accuracy: 0.9353 - val_loss: 0.2189 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9397 - val_loss: 0.2184 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2124 - accuracy: 0.9330 - val_loss: 0.2179 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9397 - val_loss: 0.2174 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2090 - accuracy: 0.9308 - val_loss: 0.2169 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2106 - accuracy: 0.9308 - val_loss: 0.2163 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2022 - accuracy: 0.9397 - val_loss: 0.2158 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1911 - accuracy: 0.9330 - val_loss: 0.2152 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9308 - val_loss: 0.2147 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2068 - accuracy: 0.9442 - val_loss: 0.2141 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1999 - accuracy: 0.9353 - val_loss: 0.2137 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1929 - accuracy: 0.9353 - val_loss: 0.2132 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2006 - accuracy: 0.9375 - val_loss: 0.2128 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1990 - accuracy: 0.9353 - val_loss: 0.2123 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9420 - val_loss: 0.2117 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9263 - val_loss: 0.2112 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9353 - val_loss: 0.2108 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2001 - accuracy: 0.9397 - val_loss: 0.2103 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1993 - accuracy: 0.9375 - val_loss: 0.2098 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1973 - accuracy: 0.9375 - val_loss: 0.2093 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1899 - accuracy: 0.9375 - val_loss: 0.2088 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9263 - val_loss: 0.2084 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2012 - accuracy: 0.9353 - val_loss: 0.2079 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1899 - accuracy: 0.9375 - val_loss: 0.2075 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1920 - accuracy: 0.9397 - val_loss: 0.2070 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1992 - accuracy: 0.9397 - val_loss: 0.2066 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1938 - accuracy: 0.9375 - val_loss: 0.2062 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.9375 - val_loss: 0.2057 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1942 - accuracy: 0.9442 - val_loss: 0.2054 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.9487 - val_loss: 0.2050 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1920 - accuracy: 0.9420 - val_loss: 0.2046 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9464 - val_loss: 0.2041 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.9487 - val_loss: 0.2037 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1900 - accuracy: 0.9487 - val_loss: 0.2034 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1942 - accuracy: 0.9442 - val_loss: 0.2030 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1865 - accuracy: 0.9464 - val_loss: 0.2026 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1928 - accuracy: 0.9375 - val_loss: 0.2021 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1958 - accuracy: 0.9397 - val_loss: 0.2017 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9420 - val_loss: 0.2013 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9464 - val_loss: 0.2008 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.9397 - val_loss: 0.2005 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.9442 - val_loss: 0.2001 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1804 - accuracy: 0.9487 - val_loss: 0.1997 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1891 - accuracy: 0.9375 - val_loss: 0.1994 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9420 - val_loss: 0.1990 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1763 - accuracy: 0.9442 - val_loss: 0.1987 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1892 - accuracy: 0.9420 - val_loss: 0.1984 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.9420 - val_loss: 0.1981 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9442 - val_loss: 0.1977 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1836 - accuracy: 0.9464 - val_loss: 0.1974 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9420 - val_loss: 0.1970 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.9442 - val_loss: 0.1966 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1890 - accuracy: 0.9442 - val_loss: 0.1963 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9420 - val_loss: 0.1961 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.9420 - val_loss: 0.1957 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1820 - accuracy: 0.9509 - val_loss: 0.1953 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9375 - val_loss: 0.1950 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1804 - accuracy: 0.9420 - val_loss: 0.1947 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9464 - val_loss: 0.1943 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.9487 - val_loss: 0.1940 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1907 - accuracy: 0.9420 - val_loss: 0.1936 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9420 - val_loss: 0.1933 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1709 - accuracy: 0.9442 - val_loss: 0.1930 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.9442 - val_loss: 0.1926 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.9464 - val_loss: 0.1923 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1709 - accuracy: 0.9576 - val_loss: 0.1919 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1798 - accuracy: 0.9487 - val_loss: 0.1916 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1723 - accuracy: 0.9464 - val_loss: 0.1913 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1757 - accuracy: 0.9442 - val_loss: 0.1909 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1807 - accuracy: 0.9397 - val_loss: 0.1906 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9487 - val_loss: 0.1904 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9397 - val_loss: 0.1901 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9509 - val_loss: 0.1898 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1700 - accuracy: 0.9487 - val_loss: 0.1895 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.9509 - val_loss: 0.1892 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.9509 - val_loss: 0.1889 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.9442 - val_loss: 0.1886 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.9420 - val_loss: 0.1884 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.9420 - val_loss: 0.1880 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9464 - val_loss: 0.1877 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9509 - val_loss: 0.1874 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9487 - val_loss: 0.1871 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1765 - accuracy: 0.9397 - val_loss: 0.1868 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1712 - accuracy: 0.9464 - val_loss: 0.1865 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9487 - val_loss: 0.1862 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9487 - val_loss: 0.1858 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1668 - accuracy: 0.9509 - val_loss: 0.1856 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1722 - accuracy: 0.9442 - val_loss: 0.1853 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1699 - accuracy: 0.9487 - val_loss: 0.1850 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1611 - accuracy: 0.9554 - val_loss: 0.1847 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9487 - val_loss: 0.1844 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9442 - val_loss: 0.1841 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9509 - val_loss: 0.1839 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1599 - accuracy: 0.9487 - val_loss: 0.1837 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.9464 - val_loss: 0.1834 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1738 - accuracy: 0.9509 - val_loss: 0.1831 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9531 - val_loss: 0.1828 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Loss: 0.1828, Accuracy: 92.86%\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 1s 8ms/step - loss: 0.7557 - accuracy: 0.5714 - val_loss: 0.7241 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6238 - accuracy: 0.6853 - val_loss: 0.6347 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5369 - accuracy: 0.7455 - val_loss: 0.5690 - val_accuracy: 0.6607 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5022 - accuracy: 0.7879 - val_loss: 0.5182 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4647 - accuracy: 0.8058 - val_loss: 0.4811 - val_accuracy: 0.7232 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4297 - accuracy: 0.8170 - val_loss: 0.4519 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4167 - accuracy: 0.8371 - val_loss: 0.4287 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3913 - accuracy: 0.8571 - val_loss: 0.4104 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3755 - accuracy: 0.8571 - val_loss: 0.3941 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3778 - accuracy: 0.8549 - val_loss: 0.3813 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3572 - accuracy: 0.8661 - val_loss: 0.3702 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.8817 - val_loss: 0.3608 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8661 - val_loss: 0.3524 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8817 - val_loss: 0.3453 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8839 - val_loss: 0.3390 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8884 - val_loss: 0.3336 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3272 - accuracy: 0.8705 - val_loss: 0.3286 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3172 - accuracy: 0.8862 - val_loss: 0.3240 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8772 - val_loss: 0.3202 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2944 - accuracy: 0.8839 - val_loss: 0.3161 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3095 - accuracy: 0.8795 - val_loss: 0.3126 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2961 - accuracy: 0.8929 - val_loss: 0.3097 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2970 - accuracy: 0.8884 - val_loss: 0.3070 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2965 - accuracy: 0.8929 - val_loss: 0.3042 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.8929 - val_loss: 0.3018 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2927 - accuracy: 0.8906 - val_loss: 0.2995 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2778 - accuracy: 0.8973 - val_loss: 0.2974 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.9062 - val_loss: 0.2952 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8839 - val_loss: 0.2936 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2833 - accuracy: 0.8862 - val_loss: 0.2918 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2922 - accuracy: 0.8884 - val_loss: 0.2903 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.9018 - val_loss: 0.2888 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2829 - accuracy: 0.9040 - val_loss: 0.2873 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2681 - accuracy: 0.9085 - val_loss: 0.2857 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2669 - accuracy: 0.8996 - val_loss: 0.2844 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2723 - accuracy: 0.9040 - val_loss: 0.2830 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.8951 - val_loss: 0.2818 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2599 - accuracy: 0.8951 - val_loss: 0.2807 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2590 - accuracy: 0.9107 - val_loss: 0.2797 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2680 - accuracy: 0.9085 - val_loss: 0.2786 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2550 - accuracy: 0.9107 - val_loss: 0.2776 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2630 - accuracy: 0.9107 - val_loss: 0.2764 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2544 - accuracy: 0.9062 - val_loss: 0.2753 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2606 - accuracy: 0.8973 - val_loss: 0.2742 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2665 - accuracy: 0.8996 - val_loss: 0.2732 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2475 - accuracy: 0.9085 - val_loss: 0.2724 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.9085 - val_loss: 0.2715 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2568 - accuracy: 0.9174 - val_loss: 0.2707 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2530 - accuracy: 0.9040 - val_loss: 0.2699 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2567 - accuracy: 0.9062 - val_loss: 0.2692 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.9018 - val_loss: 0.2684 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2510 - accuracy: 0.9107 - val_loss: 0.2676 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2504 - accuracy: 0.9129 - val_loss: 0.2669 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2363 - accuracy: 0.9129 - val_loss: 0.2663 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2500 - accuracy: 0.9107 - val_loss: 0.2656 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9152 - val_loss: 0.2650 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.9062 - val_loss: 0.2644 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2501 - accuracy: 0.9085 - val_loss: 0.2637 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.9085 - val_loss: 0.2631 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2340 - accuracy: 0.9040 - val_loss: 0.2624 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.9107 - val_loss: 0.2618 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9062 - val_loss: 0.2611 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.9152 - val_loss: 0.2604 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2378 - accuracy: 0.9174 - val_loss: 0.2598 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2286 - accuracy: 0.9129 - val_loss: 0.2592 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2359 - accuracy: 0.9129 - val_loss: 0.2587 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2294 - accuracy: 0.9152 - val_loss: 0.2581 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2255 - accuracy: 0.9152 - val_loss: 0.2575 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2198 - accuracy: 0.9129 - val_loss: 0.2569 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2297 - accuracy: 0.9174 - val_loss: 0.2564 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2267 - accuracy: 0.9219 - val_loss: 0.2558 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2297 - accuracy: 0.9129 - val_loss: 0.2553 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2237 - accuracy: 0.9241 - val_loss: 0.2547 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2210 - accuracy: 0.9263 - val_loss: 0.2543 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2219 - accuracy: 0.9196 - val_loss: 0.2537 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2276 - accuracy: 0.9196 - val_loss: 0.2533 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9286 - val_loss: 0.2527 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2171 - accuracy: 0.9241 - val_loss: 0.2522 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.9241 - val_loss: 0.2518 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9308 - val_loss: 0.2513 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2177 - accuracy: 0.9263 - val_loss: 0.2509 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2093 - accuracy: 0.9241 - val_loss: 0.2505 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2125 - accuracy: 0.9219 - val_loss: 0.2500 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2185 - accuracy: 0.9263 - val_loss: 0.2496 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9353 - val_loss: 0.2492 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9196 - val_loss: 0.2487 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2142 - accuracy: 0.9174 - val_loss: 0.2483 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2103 - accuracy: 0.9375 - val_loss: 0.2480 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9286 - val_loss: 0.2476 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9286 - val_loss: 0.2471 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.9308 - val_loss: 0.2468 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2173 - accuracy: 0.9353 - val_loss: 0.2463 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9353 - val_loss: 0.2459 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2095 - accuracy: 0.9263 - val_loss: 0.2455 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1998 - accuracy: 0.9286 - val_loss: 0.2452 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1945 - accuracy: 0.9308 - val_loss: 0.2448 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2024 - accuracy: 0.9308 - val_loss: 0.2445 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2016 - accuracy: 0.9353 - val_loss: 0.2441 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1953 - accuracy: 0.9330 - val_loss: 0.2438 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2016 - accuracy: 0.9330 - val_loss: 0.2435 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2084 - accuracy: 0.9353 - val_loss: 0.2432 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1973 - accuracy: 0.9375 - val_loss: 0.2429 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2020 - accuracy: 0.9330 - val_loss: 0.2425 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9330 - val_loss: 0.2422 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9241 - val_loss: 0.2418 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1928 - accuracy: 0.9308 - val_loss: 0.2415 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1935 - accuracy: 0.9397 - val_loss: 0.2412 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1990 - accuracy: 0.9308 - val_loss: 0.2409 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.9330 - val_loss: 0.2406 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2033 - accuracy: 0.9241 - val_loss: 0.2402 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9308 - val_loss: 0.2399 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1969 - accuracy: 0.9308 - val_loss: 0.2395 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9286 - val_loss: 0.2392 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1887 - accuracy: 0.9353 - val_loss: 0.2390 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9397 - val_loss: 0.2387 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.9330 - val_loss: 0.2384 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1961 - accuracy: 0.9420 - val_loss: 0.2381 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1895 - accuracy: 0.9420 - val_loss: 0.2378 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1979 - accuracy: 0.9353 - val_loss: 0.2374 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1909 - accuracy: 0.9375 - val_loss: 0.2371 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9330 - val_loss: 0.2368 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1920 - accuracy: 0.9397 - val_loss: 0.2365 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1880 - accuracy: 0.9330 - val_loss: 0.2362 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9420 - val_loss: 0.2359 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1965 - accuracy: 0.9353 - val_loss: 0.2357 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1856 - accuracy: 0.9397 - val_loss: 0.2354 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1819 - accuracy: 0.9397 - val_loss: 0.2351 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1845 - accuracy: 0.9308 - val_loss: 0.2348 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1885 - accuracy: 0.9375 - val_loss: 0.2345 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9397 - val_loss: 0.2343 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1803 - accuracy: 0.9442 - val_loss: 0.2340 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9464 - val_loss: 0.2338 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1890 - accuracy: 0.9353 - val_loss: 0.2335 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9375 - val_loss: 0.2332 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.9420 - val_loss: 0.2329 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9375 - val_loss: 0.2327 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9487 - val_loss: 0.2325 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.9442 - val_loss: 0.2322 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1783 - accuracy: 0.9397 - val_loss: 0.2319 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9420 - val_loss: 0.2317 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9487 - val_loss: 0.2315 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1782 - accuracy: 0.9375 - val_loss: 0.2313 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9442 - val_loss: 0.2311 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9330 - val_loss: 0.2309 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1809 - accuracy: 0.9375 - val_loss: 0.2307 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9375 - val_loss: 0.2304 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9420 - val_loss: 0.2302 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1745 - accuracy: 0.9442 - val_loss: 0.2299 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1617 - accuracy: 0.9487 - val_loss: 0.2297 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1836 - accuracy: 0.9464 - val_loss: 0.2295 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9442 - val_loss: 0.2293 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1697 - accuracy: 0.9442 - val_loss: 0.2290 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9442 - val_loss: 0.2288 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9487 - val_loss: 0.2286 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9442 - val_loss: 0.2284 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.9442 - val_loss: 0.2282 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1691 - accuracy: 0.9464 - val_loss: 0.2280 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.9464 - val_loss: 0.2278 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9442 - val_loss: 0.2276 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9420 - val_loss: 0.2274 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1616 - accuracy: 0.9487 - val_loss: 0.2272 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9464 - val_loss: 0.2271 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1783 - accuracy: 0.9420 - val_loss: 0.2268 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1781 - accuracy: 0.9375 - val_loss: 0.2267 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1568 - accuracy: 0.9464 - val_loss: 0.2265 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1551 - accuracy: 0.9509 - val_loss: 0.2262 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9509 - val_loss: 0.2261 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9420 - val_loss: 0.2258 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.9420 - val_loss: 0.2256 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1616 - accuracy: 0.9576 - val_loss: 0.2255 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1736 - accuracy: 0.9464 - val_loss: 0.2253 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1744 - accuracy: 0.9375 - val_loss: 0.2251 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9420 - val_loss: 0.2248 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1642 - accuracy: 0.9531 - val_loss: 0.2247 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9554 - val_loss: 0.2245 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1547 - accuracy: 0.9554 - val_loss: 0.2243 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9464 - val_loss: 0.2241 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9487 - val_loss: 0.2240 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1633 - accuracy: 0.9464 - val_loss: 0.2238 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9509 - val_loss: 0.2236 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1571 - accuracy: 0.9487 - val_loss: 0.2234 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.9531 - val_loss: 0.2233 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.9442 - val_loss: 0.2231 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9487 - val_loss: 0.2229 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1644 - accuracy: 0.9464 - val_loss: 0.2227 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9509 - val_loss: 0.2225 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1536 - accuracy: 0.9576 - val_loss: 0.2223 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1570 - accuracy: 0.9442 - val_loss: 0.2222 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1644 - accuracy: 0.9509 - val_loss: 0.2220 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1607 - accuracy: 0.9442 - val_loss: 0.2218 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9397 - val_loss: 0.2216 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.1589 - accuracy: 0.9464 - val_loss: 0.2215 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.1560 - accuracy: 0.9464 - val_loss: 0.2213 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1553 - accuracy: 0.9509 - val_loss: 0.2212 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1602 - accuracy: 0.9464 - val_loss: 0.2210 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.1590 - accuracy: 0.9464 - val_loss: 0.2208 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9509 - val_loss: 0.2207 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1521 - accuracy: 0.9464 - val_loss: 0.2205 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1590 - accuracy: 0.9554 - val_loss: 0.2204 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1498 - accuracy: 0.9509 - val_loss: 0.2203 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Loss: 0.2203, Accuracy: 93.75%\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 2s 11ms/step - loss: 0.8109 - accuracy: 0.5603 - val_loss: 0.6803 - val_accuracy: 0.6161 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.6429 - val_loss: 0.6218 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.6505 - accuracy: 0.6719 - val_loss: 0.5764 - val_accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.5860 - accuracy: 0.7009 - val_loss: 0.5419 - val_accuracy: 0.7321 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.5671 - accuracy: 0.7321 - val_loss: 0.5135 - val_accuracy: 0.7232 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.5249 - accuracy: 0.7344 - val_loss: 0.4908 - val_accuracy: 0.7321 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.4997 - accuracy: 0.7589 - val_loss: 0.4724 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.4782 - accuracy: 0.7768 - val_loss: 0.4572 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.7991 - val_loss: 0.4442 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4430 - accuracy: 0.7879 - val_loss: 0.4328 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4112 - accuracy: 0.8259 - val_loss: 0.4241 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4058 - accuracy: 0.8237 - val_loss: 0.4158 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4017 - accuracy: 0.8348 - val_loss: 0.4088 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3916 - accuracy: 0.8393 - val_loss: 0.4024 - val_accuracy: 0.7946 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4000 - accuracy: 0.8304 - val_loss: 0.3967 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3593 - accuracy: 0.8527 - val_loss: 0.3915 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3577 - accuracy: 0.8482 - val_loss: 0.3869 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3573 - accuracy: 0.8504 - val_loss: 0.3827 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3389 - accuracy: 0.8638 - val_loss: 0.3791 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3384 - accuracy: 0.8705 - val_loss: 0.3755 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3533 - accuracy: 0.8504 - val_loss: 0.3721 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3381 - accuracy: 0.8661 - val_loss: 0.3691 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3395 - accuracy: 0.8616 - val_loss: 0.3661 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3311 - accuracy: 0.8728 - val_loss: 0.3634 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3180 - accuracy: 0.8750 - val_loss: 0.3609 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3248 - accuracy: 0.8661 - val_loss: 0.3584 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8795 - val_loss: 0.3561 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8661 - val_loss: 0.3540 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3242 - accuracy: 0.8795 - val_loss: 0.3520 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3075 - accuracy: 0.8817 - val_loss: 0.3500 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3064 - accuracy: 0.8795 - val_loss: 0.3481 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3029 - accuracy: 0.8795 - val_loss: 0.3463 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2967 - accuracy: 0.8817 - val_loss: 0.3447 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2898 - accuracy: 0.8817 - val_loss: 0.3430 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2918 - accuracy: 0.8906 - val_loss: 0.3413 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3022 - accuracy: 0.8817 - val_loss: 0.3397 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8906 - val_loss: 0.3382 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.8862 - val_loss: 0.3365 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2896 - accuracy: 0.8951 - val_loss: 0.3351 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8906 - val_loss: 0.3336 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2852 - accuracy: 0.9040 - val_loss: 0.3321 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2903 - accuracy: 0.8929 - val_loss: 0.3306 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2773 - accuracy: 0.8929 - val_loss: 0.3293 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8906 - val_loss: 0.3280 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2809 - accuracy: 0.8839 - val_loss: 0.3267 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2754 - accuracy: 0.8951 - val_loss: 0.3255 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2768 - accuracy: 0.9018 - val_loss: 0.3243 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2747 - accuracy: 0.8906 - val_loss: 0.3231 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.8906 - val_loss: 0.3219 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2738 - accuracy: 0.8951 - val_loss: 0.3207 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2719 - accuracy: 0.8929 - val_loss: 0.3196 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2522 - accuracy: 0.9040 - val_loss: 0.3185 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2549 - accuracy: 0.9040 - val_loss: 0.3173 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2607 - accuracy: 0.9018 - val_loss: 0.3162 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.8884 - val_loss: 0.3151 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2662 - accuracy: 0.8996 - val_loss: 0.3140 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.9062 - val_loss: 0.3129 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2566 - accuracy: 0.8951 - val_loss: 0.3119 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2594 - accuracy: 0.9018 - val_loss: 0.3108 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2563 - accuracy: 0.9062 - val_loss: 0.3096 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2547 - accuracy: 0.9129 - val_loss: 0.3087 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2501 - accuracy: 0.9062 - val_loss: 0.3077 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2573 - accuracy: 0.8996 - val_loss: 0.3068 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2567 - accuracy: 0.9062 - val_loss: 0.3058 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2502 - accuracy: 0.8996 - val_loss: 0.3049 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2433 - accuracy: 0.9129 - val_loss: 0.3039 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2533 - accuracy: 0.9040 - val_loss: 0.3030 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2449 - accuracy: 0.9062 - val_loss: 0.3021 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9085 - val_loss: 0.3011 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2384 - accuracy: 0.9107 - val_loss: 0.3002 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2423 - accuracy: 0.9152 - val_loss: 0.2994 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.9085 - val_loss: 0.2985 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.9040 - val_loss: 0.2975 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2341 - accuracy: 0.9107 - val_loss: 0.2967 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9085 - val_loss: 0.2958 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2303 - accuracy: 0.9107 - val_loss: 0.2950 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2196 - accuracy: 0.9174 - val_loss: 0.2941 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2314 - accuracy: 0.9040 - val_loss: 0.2933 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2395 - accuracy: 0.9107 - val_loss: 0.2923 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2374 - accuracy: 0.9196 - val_loss: 0.2914 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2317 - accuracy: 0.9219 - val_loss: 0.2905 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2268 - accuracy: 0.9107 - val_loss: 0.2898 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2399 - accuracy: 0.9062 - val_loss: 0.2888 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2302 - accuracy: 0.9241 - val_loss: 0.2880 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2209 - accuracy: 0.9174 - val_loss: 0.2872 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2386 - accuracy: 0.9085 - val_loss: 0.2863 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2301 - accuracy: 0.9085 - val_loss: 0.2856 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2240 - accuracy: 0.9263 - val_loss: 0.2848 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2190 - accuracy: 0.9152 - val_loss: 0.2840 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2265 - accuracy: 0.9085 - val_loss: 0.2833 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2321 - accuracy: 0.9129 - val_loss: 0.2825 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2198 - accuracy: 0.9107 - val_loss: 0.2817 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2186 - accuracy: 0.9107 - val_loss: 0.2809 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9241 - val_loss: 0.2802 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2226 - accuracy: 0.9174 - val_loss: 0.2793 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9263 - val_loss: 0.2786 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2193 - accuracy: 0.9241 - val_loss: 0.2778 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2213 - accuracy: 0.9129 - val_loss: 0.2771 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2129 - accuracy: 0.9152 - val_loss: 0.2764 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2150 - accuracy: 0.9241 - val_loss: 0.2757 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2174 - accuracy: 0.9196 - val_loss: 0.2750 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2159 - accuracy: 0.9219 - val_loss: 0.2743 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2290 - accuracy: 0.9241 - val_loss: 0.2735 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2091 - accuracy: 0.9196 - val_loss: 0.2729 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2195 - accuracy: 0.9152 - val_loss: 0.2722 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2138 - accuracy: 0.9263 - val_loss: 0.2715 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2126 - accuracy: 0.9308 - val_loss: 0.2708 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9152 - val_loss: 0.2701 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2133 - accuracy: 0.9308 - val_loss: 0.2695 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1945 - accuracy: 0.9375 - val_loss: 0.2689 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2166 - accuracy: 0.9152 - val_loss: 0.2682 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2117 - accuracy: 0.9174 - val_loss: 0.2676 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2047 - accuracy: 0.9263 - val_loss: 0.2670 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9219 - val_loss: 0.2663 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2146 - accuracy: 0.9174 - val_loss: 0.2655 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2055 - accuracy: 0.9263 - val_loss: 0.2649 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2100 - accuracy: 0.9219 - val_loss: 0.2643 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2048 - accuracy: 0.9286 - val_loss: 0.2638 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2135 - accuracy: 0.9263 - val_loss: 0.2632 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9330 - val_loss: 0.2626 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2005 - accuracy: 0.9353 - val_loss: 0.2621 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2001 - accuracy: 0.9308 - val_loss: 0.2616 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2025 - accuracy: 0.9286 - val_loss: 0.2610 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2007 - accuracy: 0.9286 - val_loss: 0.2604 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2064 - accuracy: 0.9263 - val_loss: 0.2598 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1958 - accuracy: 0.9353 - val_loss: 0.2593 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2110 - accuracy: 0.9286 - val_loss: 0.2587 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1953 - accuracy: 0.9286 - val_loss: 0.2581 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2084 - accuracy: 0.9196 - val_loss: 0.2575 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2009 - accuracy: 0.9353 - val_loss: 0.2570 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2017 - accuracy: 0.9241 - val_loss: 0.2563 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1922 - accuracy: 0.9353 - val_loss: 0.2558 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2002 - accuracy: 0.9263 - val_loss: 0.2552 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1969 - accuracy: 0.9353 - val_loss: 0.2546 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1973 - accuracy: 0.9353 - val_loss: 0.2541 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1930 - accuracy: 0.9330 - val_loss: 0.2535 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2020 - accuracy: 0.9397 - val_loss: 0.2529 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1940 - accuracy: 0.9353 - val_loss: 0.2524 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2013 - accuracy: 0.9286 - val_loss: 0.2519 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1927 - accuracy: 0.9353 - val_loss: 0.2513 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1989 - accuracy: 0.9308 - val_loss: 0.2507 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1812 - accuracy: 0.9487 - val_loss: 0.2502 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1983 - accuracy: 0.9353 - val_loss: 0.2497 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.9397 - val_loss: 0.2492 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9263 - val_loss: 0.2486 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.9397 - val_loss: 0.2481 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9442 - val_loss: 0.2477 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1925 - accuracy: 0.9330 - val_loss: 0.2472 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1926 - accuracy: 0.9420 - val_loss: 0.2467 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1814 - accuracy: 0.9420 - val_loss: 0.2462 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.9375 - val_loss: 0.2457 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1839 - accuracy: 0.9397 - val_loss: 0.2453 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1905 - accuracy: 0.9308 - val_loss: 0.2448 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1872 - accuracy: 0.9330 - val_loss: 0.2443 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1907 - accuracy: 0.9397 - val_loss: 0.2437 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9397 - val_loss: 0.2433 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1881 - accuracy: 0.9330 - val_loss: 0.2428 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1823 - accuracy: 0.9397 - val_loss: 0.2422 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9397 - val_loss: 0.2417 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1885 - accuracy: 0.9464 - val_loss: 0.2413 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.9420 - val_loss: 0.2408 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1732 - accuracy: 0.9375 - val_loss: 0.2404 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9487 - val_loss: 0.2400 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1751 - accuracy: 0.9420 - val_loss: 0.2396 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1748 - accuracy: 0.9464 - val_loss: 0.2391 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1834 - accuracy: 0.9353 - val_loss: 0.2386 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9464 - val_loss: 0.2382 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9308 - val_loss: 0.2377 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9420 - val_loss: 0.2372 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9375 - val_loss: 0.2366 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9464 - val_loss: 0.2362 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1837 - accuracy: 0.9397 - val_loss: 0.2358 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1799 - accuracy: 0.9420 - val_loss: 0.2353 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.9531 - val_loss: 0.2350 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9308 - val_loss: 0.2345 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1762 - accuracy: 0.9464 - val_loss: 0.2341 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9442 - val_loss: 0.2337 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1875 - accuracy: 0.9330 - val_loss: 0.2333 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1695 - accuracy: 0.9464 - val_loss: 0.2328 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1842 - accuracy: 0.9353 - val_loss: 0.2324 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.1757 - accuracy: 0.9330 - val_loss: 0.2320 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1834 - accuracy: 0.9420 - val_loss: 0.2316 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1766 - accuracy: 0.9353 - val_loss: 0.2312 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1698 - accuracy: 0.9487 - val_loss: 0.2308 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1719 - accuracy: 0.9420 - val_loss: 0.2304 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1711 - accuracy: 0.9420 - val_loss: 0.2301 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1770 - accuracy: 0.9442 - val_loss: 0.2297 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1718 - accuracy: 0.9487 - val_loss: 0.2293 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1709 - accuracy: 0.9442 - val_loss: 0.2290 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1707 - accuracy: 0.9442 - val_loss: 0.2286 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1688 - accuracy: 0.9531 - val_loss: 0.2283 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9509 - val_loss: 0.2280 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1752 - accuracy: 0.9397 - val_loss: 0.2276 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1744 - accuracy: 0.9487 - val_loss: 0.2272 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9487 - val_loss: 0.2268 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9420 - val_loss: 0.2265 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1629 - accuracy: 0.9509 - val_loss: 0.2261 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9487 - val_loss: 0.2258 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9464 - val_loss: 0.2255 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1657 - accuracy: 0.9509 - val_loss: 0.2251 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Loss: 0.2251, Accuracy: 91.96%\n",
      "RNN finished in 187.83 sec\n",
      "\n",
      "Average accuracy: 0.9268\n",
      "Average loss: 0.2107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "\n",
    "k = 5  # numero di fold\n",
    "kf = KFold(n_splits=k, shuffle = True)\n",
    "\n",
    "# Array per memorizzare le curve di apprendimento\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "f = pd.DataFrame(columns = perfInd)\n",
    "print('Implementing vanilla RNN with k-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  # \"timesteps\" rappresenta il numero di istanti temporali nel segnale EEG, \"features\"  il numero di caratteristiche per ciascun istante temporale\n",
    "    model.add(Dropout(0.18))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Definisco l'ottimizzatore con il learning rate iniziale\n",
    "    initial_learning_rate = 0.001\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # Definisco il learning rate schedule con decay lineare\n",
    "    decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "    decay_rate = 0.1  # Tasso di decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 200, verbose = 1, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), early_stopping])\n",
    "    # Valuta il modello\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Stampa i risultati di accuracy e loss per la k-esima fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "    # train_loss.append(history.history['loss'])\n",
    "    # train_acc.append(history.history['accuracy'])\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('RNN finished in', t,'sec\\n')\n",
    "\n",
    " # Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the results\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modello Vanilla RNN con dropout e early stopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  # \"timesteps\" rappresenta il numero di istanti temporali nel segnale EEG, \"features\"  il numero di caratteristiche per ciascun istante temporale\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilazione del modello\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Definizione dell'early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Addestramento del modello con early stopping\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=8, validation_data=(x_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione del modello\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "test_loss = history.history['loss']\n",
    "test_acc = history.history['accuracy']\n",
    "\n",
    "epochs = range(len(test_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, test_loss, label='Test loss')\n",
    "plt.plot(epochs, test_acc, label='Test accuracy')\n",
    "plt.title('Test loss and accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 1 layer SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "43/63 [===================>..........] - ETA: 0s - loss: 0.7200 - accuracy: 0.6186\n",
      "Epoch 1: val_loss improved from inf to 0.69626, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 3s 11ms/step - loss: 0.7307 - accuracy: 0.6006 - val_loss: 0.6963 - val_accuracy: 0.6726 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.6764 - accuracy: 0.6290\n",
      "Epoch 2: val_loss improved from 0.69626 to 0.65035, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6777 - accuracy: 0.6262 - val_loss: 0.6504 - val_accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.6355 - accuracy: 0.6873\n",
      "Epoch 3: val_loss improved from 0.65035 to 0.61083, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6335 - accuracy: 0.6869 - val_loss: 0.6108 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.5972 - accuracy: 0.7556\n",
      "Epoch 4: val_loss improved from 0.61083 to 0.57649, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.7380 - val_loss: 0.5765 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.5679 - accuracy: 0.7571\n",
      "Epoch 5: val_loss improved from 0.57649 to 0.54773, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5638 - accuracy: 0.7604 - val_loss: 0.5477 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.5292 - accuracy: 0.7927\n",
      "Epoch 6: val_loss improved from 0.54773 to 0.52294, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5364 - accuracy: 0.7859 - val_loss: 0.5229 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.5182 - accuracy: 0.7927\n",
      "Epoch 7: val_loss improved from 0.52294 to 0.50111, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5127 - accuracy: 0.8019 - val_loss: 0.5011 - val_accuracy: 0.8274 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.4834 - accuracy: 0.8145\n",
      "Epoch 8: val_loss improved from 0.50111 to 0.48172, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4918 - accuracy: 0.8147 - val_loss: 0.4817 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.4777 - accuracy: 0.8269\n",
      "Epoch 9: val_loss improved from 0.48172 to 0.46419, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8371 - val_loss: 0.4642 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.4578 - accuracy: 0.8453\n",
      "Epoch 10: val_loss improved from 0.46419 to 0.44848, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4566 - accuracy: 0.8466 - val_loss: 0.4485 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.4377 - accuracy: 0.8679\n",
      "Epoch 11: val_loss improved from 0.44848 to 0.43417, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4416 - accuracy: 0.8530 - val_loss: 0.4342 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.4395 - accuracy: 0.8480\n",
      "Epoch 12: val_loss improved from 0.43417 to 0.42129, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4278 - accuracy: 0.8530 - val_loss: 0.4213 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.4244 - accuracy: 0.8490\n",
      "Epoch 13: val_loss improved from 0.42129 to 0.40980, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4153 - accuracy: 0.8626 - val_loss: 0.4098 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.3915 - accuracy: 0.8870\n",
      "Epoch 14: val_loss improved from 0.40980 to 0.39959, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4037 - accuracy: 0.8690 - val_loss: 0.3996 - val_accuracy: 0.8810 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3990 - accuracy: 0.8655\n",
      "Epoch 15: val_loss improved from 0.39959 to 0.39027, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3934 - accuracy: 0.8722 - val_loss: 0.3903 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.3943 - accuracy: 0.8694\n",
      "Epoch 16: val_loss improved from 0.39027 to 0.38172, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3841 - accuracy: 0.8786 - val_loss: 0.3817 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.3666 - accuracy: 0.8885\n",
      "Epoch 17: val_loss improved from 0.38172 to 0.37385, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3757 - accuracy: 0.8786 - val_loss: 0.3738 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3679 - accuracy: 0.8727\n",
      "Epoch 18: val_loss improved from 0.37385 to 0.36650, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3679 - accuracy: 0.8818 - val_loss: 0.3665 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3573 - accuracy: 0.8945\n",
      "Epoch 19: val_loss improved from 0.36650 to 0.35963, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3607 - accuracy: 0.8882 - val_loss: 0.3596 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3497 - accuracy: 0.8889\n",
      "Epoch 20: val_loss improved from 0.35963 to 0.35323, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3540 - accuracy: 0.8882 - val_loss: 0.3532 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3505 - accuracy: 0.8909\n",
      "Epoch 21: val_loss improved from 0.35323 to 0.34726, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3478 - accuracy: 0.8914 - val_loss: 0.3473 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3511 - accuracy: 0.8945\n",
      "Epoch 22: val_loss improved from 0.34726 to 0.34164, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3419 - accuracy: 0.8946 - val_loss: 0.3416 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3289 - accuracy: 0.8981\n",
      "Epoch 23: val_loss improved from 0.34164 to 0.33640, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3364 - accuracy: 0.8946 - val_loss: 0.3364 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.3346 - accuracy: 0.8917\n",
      "Epoch 24: val_loss improved from 0.33640 to 0.33150, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8946 - val_loss: 0.3315 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.3453 - accuracy: 0.8816\n",
      "Epoch 25: val_loss improved from 0.33150 to 0.32687, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8946 - val_loss: 0.3269 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3201 - accuracy: 0.9055\n",
      "Epoch 26: val_loss improved from 0.32687 to 0.32253, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3218 - accuracy: 0.8946 - val_loss: 0.3225 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3123 - accuracy: 0.9036\n",
      "Epoch 27: val_loss improved from 0.32253 to 0.31843, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8978 - val_loss: 0.3184 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3148 - accuracy: 0.8926\n",
      "Epoch 28: val_loss improved from 0.31843 to 0.31451, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8978 - val_loss: 0.3145 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.3107 - accuracy: 0.9040\n",
      "Epoch 29: val_loss improved from 0.31451 to 0.31081, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3094 - accuracy: 0.8978 - val_loss: 0.3108 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.3217 - accuracy: 0.8846\n",
      "Epoch 30: val_loss improved from 0.31081 to 0.30729, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.8978 - val_loss: 0.3073 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2871 - accuracy: 0.9055\n",
      "Epoch 31: val_loss improved from 0.30729 to 0.30400, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3021 - accuracy: 0.8978 - val_loss: 0.3040 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3038 - accuracy: 0.8852\n",
      "Epoch 32: val_loss improved from 0.30400 to 0.30078, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2987 - accuracy: 0.8978 - val_loss: 0.3008 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3059 - accuracy: 0.8945\n",
      "Epoch 33: val_loss improved from 0.30078 to 0.29771, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2954 - accuracy: 0.9010 - val_loss: 0.2977 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2874 - accuracy: 0.9034\n",
      "Epoch 34: val_loss improved from 0.29771 to 0.29482, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2923 - accuracy: 0.9010 - val_loss: 0.2948 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2798 - accuracy: 0.9036\n",
      "Epoch 35: val_loss improved from 0.29482 to 0.29202, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2893 - accuracy: 0.9010 - val_loss: 0.2920 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.2870 - accuracy: 0.9021\n",
      "Epoch 36: val_loss improved from 0.29202 to 0.28932, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2864 - accuracy: 0.9010 - val_loss: 0.2893 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2921 - accuracy: 0.8981\n",
      "Epoch 37: val_loss improved from 0.28932 to 0.28675, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.9042 - val_loss: 0.2867 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2771 - accuracy: 0.9091\n",
      "Epoch 38: val_loss improved from 0.28675 to 0.28429, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2809 - accuracy: 0.9042 - val_loss: 0.2843 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2755 - accuracy: 0.9074\n",
      "Epoch 39: val_loss improved from 0.28429 to 0.28192, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2783 - accuracy: 0.9042 - val_loss: 0.2819 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.2790 - accuracy: 0.9000\n",
      "Epoch 40: val_loss improved from 0.28192 to 0.27965, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2758 - accuracy: 0.9073 - val_loss: 0.2796 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2709 - accuracy: 0.9107\n",
      "Epoch 41: val_loss improved from 0.27965 to 0.27746, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2734 - accuracy: 0.9073 - val_loss: 0.2775 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2853 - accuracy: 0.8923\n",
      "Epoch 42: val_loss improved from 0.27746 to 0.27534, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2711 - accuracy: 0.9073 - val_loss: 0.2753 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2680 - accuracy: 0.9000\n",
      "Epoch 43: val_loss improved from 0.27534 to 0.27330, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2688 - accuracy: 0.9073 - val_loss: 0.2733 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2733 - accuracy: 0.9000\n",
      "Epoch 44: val_loss improved from 0.27330 to 0.27134, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2666 - accuracy: 0.9073 - val_loss: 0.2713 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2596 - accuracy: 0.9111\n",
      "Epoch 45: val_loss improved from 0.27134 to 0.26943, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.9073 - val_loss: 0.2694 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2606 - accuracy: 0.9074\n",
      "Epoch 46: val_loss improved from 0.26943 to 0.26758, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2624 - accuracy: 0.9073 - val_loss: 0.2676 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2486 - accuracy: 0.9127\n",
      "Epoch 47: val_loss improved from 0.26758 to 0.26583, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2603 - accuracy: 0.9073 - val_loss: 0.2658 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2592 - accuracy: 0.9111\n",
      "Epoch 48: val_loss improved from 0.26583 to 0.26411, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2584 - accuracy: 0.9105 - val_loss: 0.2641 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2611 - accuracy: 0.9132\n",
      "Epoch 49: val_loss improved from 0.26411 to 0.26248, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2564 - accuracy: 0.9137 - val_loss: 0.2625 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.2257 - accuracy: 0.9289\n",
      "Epoch 50: val_loss improved from 0.26248 to 0.26083, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2546 - accuracy: 0.9169 - val_loss: 0.2608 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2545 - accuracy: 0.9164\n",
      "Epoch 51: val_loss improved from 0.26083 to 0.25924, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2528 - accuracy: 0.9169 - val_loss: 0.2592 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2430 - accuracy: 0.9192\n",
      "Epoch 52: val_loss improved from 0.25924 to 0.25770, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2510 - accuracy: 0.9169 - val_loss: 0.2577 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2490 - accuracy: 0.9222\n",
      "Epoch 53: val_loss improved from 0.25770 to 0.25620, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2493 - accuracy: 0.9201 - val_loss: 0.2562 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2396 - accuracy: 0.9245\n",
      "Epoch 54: val_loss improved from 0.25620 to 0.25474, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2476 - accuracy: 0.9201 - val_loss: 0.2547 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2476 - accuracy: 0.9273\n",
      "Epoch 55: val_loss improved from 0.25474 to 0.25332, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2460 - accuracy: 0.9233 - val_loss: 0.2533 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2491 - accuracy: 0.9228\n",
      "Epoch 56: val_loss improved from 0.25332 to 0.25193, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2444 - accuracy: 0.9233 - val_loss: 0.2519 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2600 - accuracy: 0.9132\n",
      "Epoch 57: val_loss improved from 0.25193 to 0.25058, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2428 - accuracy: 0.9233 - val_loss: 0.2506 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2459 - accuracy: 0.9200\n",
      "Epoch 58: val_loss improved from 0.25058 to 0.24927, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2413 - accuracy: 0.9233 - val_loss: 0.2493 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2382 - accuracy: 0.9250\n",
      "Epoch 59: val_loss improved from 0.24927 to 0.24799, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2398 - accuracy: 0.9233 - val_loss: 0.2480 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2433 - accuracy: 0.9186\n",
      "Epoch 60: val_loss improved from 0.24799 to 0.24674, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2384 - accuracy: 0.9233 - val_loss: 0.2467 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.2239 - accuracy: 0.9304\n",
      "Epoch 61: val_loss improved from 0.24674 to 0.24552, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2370 - accuracy: 0.9233 - val_loss: 0.2455 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2435 - accuracy: 0.9132\n",
      "Epoch 62: val_loss improved from 0.24552 to 0.24433, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2356 - accuracy: 0.9233 - val_loss: 0.2443 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2362 - accuracy: 0.9245\n",
      "Epoch 63: val_loss improved from 0.24433 to 0.24316, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2342 - accuracy: 0.9233 - val_loss: 0.2432 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2274 - accuracy: 0.9269\n",
      "Epoch 64: val_loss improved from 0.24316 to 0.24202, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2329 - accuracy: 0.9233 - val_loss: 0.2420 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2218 - accuracy: 0.9345\n",
      "Epoch 65: val_loss improved from 0.24202 to 0.24091, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2316 - accuracy: 0.9233 - val_loss: 0.2409 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2299 - accuracy: 0.9222\n",
      "Epoch 66: val_loss improved from 0.24091 to 0.23983, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2303 - accuracy: 0.9233 - val_loss: 0.2398 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2120 - accuracy: 0.9286\n",
      "Epoch 67: val_loss improved from 0.23983 to 0.23876, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2290 - accuracy: 0.9233 - val_loss: 0.2388 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9258\n",
      "Epoch 68: val_loss improved from 0.23876 to 0.23773, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2278 - accuracy: 0.9265 - val_loss: 0.2377 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2375 - accuracy: 0.9208\n",
      "Epoch 69: val_loss improved from 0.23773 to 0.23670, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2266 - accuracy: 0.9265 - val_loss: 0.2367 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2280 - accuracy: 0.9245\n",
      "Epoch 70: val_loss improved from 0.23670 to 0.23571, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2254 - accuracy: 0.9297 - val_loss: 0.2357 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.2095 - accuracy: 0.9333\n",
      "Epoch 71: val_loss improved from 0.23571 to 0.23475, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2242 - accuracy: 0.9297 - val_loss: 0.2347 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2429 - accuracy: 0.9231\n",
      "Epoch 72: val_loss improved from 0.23475 to 0.23381, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.9297 - val_loss: 0.2338 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2259 - accuracy: 0.9296\n",
      "Epoch 73: val_loss improved from 0.23381 to 0.23288, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9297 - val_loss: 0.2329 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2250 - accuracy: 0.9294\n",
      "Epoch 74: val_loss improved from 0.23288 to 0.23196, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2208 - accuracy: 0.9297 - val_loss: 0.2320 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2249 - accuracy: 0.9288\n",
      "Epoch 75: val_loss improved from 0.23196 to 0.23109, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2197 - accuracy: 0.9297 - val_loss: 0.2311 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2072 - accuracy: 0.9333\n",
      "Epoch 76: val_loss improved from 0.23109 to 0.23020, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2186 - accuracy: 0.9297 - val_loss: 0.2302 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2272 - accuracy: 0.9192\n",
      "Epoch 77: val_loss improved from 0.23020 to 0.22932, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2176 - accuracy: 0.9297 - val_loss: 0.2293 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2094 - accuracy: 0.9280\n",
      "Epoch 78: val_loss improved from 0.22932 to 0.22850, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.9297 - val_loss: 0.2285 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9297\n",
      "Epoch 79: val_loss improved from 0.22850 to 0.22767, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2156 - accuracy: 0.9297 - val_loss: 0.2277 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9279\n",
      "Epoch 80: val_loss improved from 0.22767 to 0.22686, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2146 - accuracy: 0.9297 - val_loss: 0.2269 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2195 - accuracy: 0.9241\n",
      "Epoch 81: val_loss improved from 0.22686 to 0.22606, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2136 - accuracy: 0.9297 - val_loss: 0.2261 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2080 - accuracy: 0.9294\n",
      "Epoch 82: val_loss improved from 0.22606 to 0.22528, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2126 - accuracy: 0.9297 - val_loss: 0.2253 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2162 - accuracy: 0.9306\n",
      "Epoch 83: val_loss improved from 0.22528 to 0.22451, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2117 - accuracy: 0.9297 - val_loss: 0.2245 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2156 - accuracy: 0.9236\n",
      "Epoch 84: val_loss improved from 0.22451 to 0.22375, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2107 - accuracy: 0.9297 - val_loss: 0.2237 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2143 - accuracy: 0.9280\n",
      "Epoch 85: val_loss improved from 0.22375 to 0.22301, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2098 - accuracy: 0.9297 - val_loss: 0.2230 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.2016 - accuracy: 0.9292\n",
      "Epoch 86: val_loss improved from 0.22301 to 0.22227, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2089 - accuracy: 0.9297 - val_loss: 0.2223 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2177 - accuracy: 0.9250\n",
      "Epoch 87: val_loss improved from 0.22227 to 0.22154, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2080 - accuracy: 0.9297 - val_loss: 0.2215 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1988 - accuracy: 0.9321\n",
      "Epoch 88: val_loss improved from 0.22154 to 0.22086, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2071 - accuracy: 0.9297 - val_loss: 0.2209 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2151 - accuracy: 0.9265\n",
      "Epoch 89: val_loss improved from 0.22086 to 0.22016, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2063 - accuracy: 0.9297 - val_loss: 0.2202 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2118 - accuracy: 0.9241\n",
      "Epoch 90: val_loss improved from 0.22016 to 0.21947, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2054 - accuracy: 0.9297 - val_loss: 0.2195 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.2130 - accuracy: 0.9234\n",
      "Epoch 91: val_loss improved from 0.21947 to 0.21880, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2045 - accuracy: 0.9297 - val_loss: 0.2188 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2037 - accuracy: 0.9297\n",
      "Epoch 92: val_loss improved from 0.21880 to 0.21812, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2037 - accuracy: 0.9297 - val_loss: 0.2181 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2145 - accuracy: 0.9176\n",
      "Epoch 93: val_loss improved from 0.21812 to 0.21747, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2029 - accuracy: 0.9297 - val_loss: 0.2175 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2106 - accuracy: 0.9216\n",
      "Epoch 94: val_loss improved from 0.21747 to 0.21682, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2021 - accuracy: 0.9297 - val_loss: 0.2168 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1963 - accuracy: 0.9321\n",
      "Epoch 95: val_loss improved from 0.21682 to 0.21619, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2013 - accuracy: 0.9297 - val_loss: 0.2162 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1893 - accuracy: 0.9375\n",
      "Epoch 96: val_loss improved from 0.21619 to 0.21554, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.9297 - val_loss: 0.2155 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1894 - accuracy: 0.9360\n",
      "Epoch 97: val_loss improved from 0.21554 to 0.21492, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1997 - accuracy: 0.9329 - val_loss: 0.2149 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1871 - accuracy: 0.9385\n",
      "Epoch 98: val_loss improved from 0.21492 to 0.21433, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1989 - accuracy: 0.9329 - val_loss: 0.2143 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2064 - accuracy: 0.9265\n",
      "Epoch 99: val_loss improved from 0.21433 to 0.21372, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1981 - accuracy: 0.9329 - val_loss: 0.2137 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1980 - accuracy: 0.9322\n",
      "Epoch 100: val_loss improved from 0.21372 to 0.21313, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1974 - accuracy: 0.9329 - val_loss: 0.2131 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2025 - accuracy: 0.9345\n",
      "Epoch 101: val_loss improved from 0.21313 to 0.21254, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9361 - val_loss: 0.2125 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9361\n",
      "Epoch 102: val_loss improved from 0.21254 to 0.21196, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9361 - val_loss: 0.2120 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2052 - accuracy: 0.9306\n",
      "Epoch 103: val_loss improved from 0.21196 to 0.21139, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1952 - accuracy: 0.9361 - val_loss: 0.2114 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1961 - accuracy: 0.9373\n",
      "Epoch 104: val_loss improved from 0.21139 to 0.21083, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1945 - accuracy: 0.9361 - val_loss: 0.2108 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1889 - accuracy: 0.9414\n",
      "Epoch 105: val_loss improved from 0.21083 to 0.21028, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1938 - accuracy: 0.9361 - val_loss: 0.2103 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2003 - accuracy: 0.9333\n",
      "Epoch 106: val_loss improved from 0.21028 to 0.20973, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1931 - accuracy: 0.9361 - val_loss: 0.2097 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1870 - accuracy: 0.9458\n",
      "Epoch 107: val_loss improved from 0.20973 to 0.20919, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1923 - accuracy: 0.9361 - val_loss: 0.2092 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1809 - accuracy: 0.9424\n",
      "Epoch 108: val_loss improved from 0.20919 to 0.20866, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1917 - accuracy: 0.9361 - val_loss: 0.2087 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1920 - accuracy: 0.9355\n",
      "Epoch 109: val_loss improved from 0.20866 to 0.20814, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1910 - accuracy: 0.9361 - val_loss: 0.2081 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1911 - accuracy: 0.9388\n",
      "Epoch 110: val_loss improved from 0.20814 to 0.20764, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1903 - accuracy: 0.9361 - val_loss: 0.2076 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1909 - accuracy: 0.9333\n",
      "Epoch 111: val_loss improved from 0.20764 to 0.20712, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1896 - accuracy: 0.9361 - val_loss: 0.2071 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9361\n",
      "Epoch 112: val_loss improved from 0.20712 to 0.20661, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1890 - accuracy: 0.9361 - val_loss: 0.2066 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1952 - accuracy: 0.9333\n",
      "Epoch 113: val_loss improved from 0.20661 to 0.20611, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1883 - accuracy: 0.9361 - val_loss: 0.2061 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9355\n",
      "Epoch 114: val_loss improved from 0.20611 to 0.20561, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1877 - accuracy: 0.9361 - val_loss: 0.2056 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2016 - accuracy: 0.9283\n",
      "Epoch 115: val_loss improved from 0.20561 to 0.20512, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1870 - accuracy: 0.9361 - val_loss: 0.2051 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2026 - accuracy: 0.9306\n",
      "Epoch 116: val_loss improved from 0.20512 to 0.20463, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1864 - accuracy: 0.9393 - val_loss: 0.2046 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1836 - accuracy: 0.9396\n",
      "Epoch 117: val_loss improved from 0.20463 to 0.20415, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1858 - accuracy: 0.9393 - val_loss: 0.2041 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1779 - accuracy: 0.9462\n",
      "Epoch 118: val_loss improved from 0.20415 to 0.20369, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1852 - accuracy: 0.9393 - val_loss: 0.2037 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1999 - accuracy: 0.9320\n",
      "Epoch 119: val_loss improved from 0.20369 to 0.20322, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1846 - accuracy: 0.9393 - val_loss: 0.2032 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1852 - accuracy: 0.9404\n",
      "Epoch 120: val_loss improved from 0.20322 to 0.20275, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1839 - accuracy: 0.9393 - val_loss: 0.2028 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1774 - accuracy: 0.9429\n",
      "Epoch 121: val_loss improved from 0.20275 to 0.20230, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1833 - accuracy: 0.9393 - val_loss: 0.2023 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1781 - accuracy: 0.9439\n",
      "Epoch 122: val_loss improved from 0.20230 to 0.20187, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1828 - accuracy: 0.9393 - val_loss: 0.2019 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1874 - accuracy: 0.9367\n",
      "Epoch 123: val_loss improved from 0.20187 to 0.20142, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1822 - accuracy: 0.9393 - val_loss: 0.2014 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1823 - accuracy: 0.9379\n",
      "Epoch 124: val_loss improved from 0.20142 to 0.20097, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1816 - accuracy: 0.9393 - val_loss: 0.2010 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1846 - accuracy: 0.9429\n",
      "Epoch 125: val_loss improved from 0.20097 to 0.20052, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1810 - accuracy: 0.9393 - val_loss: 0.2005 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1913 - accuracy: 0.9333\n",
      "Epoch 126: val_loss improved from 0.20052 to 0.20008, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1804 - accuracy: 0.9393 - val_loss: 0.2001 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1746 - accuracy: 0.9443\n",
      "Epoch 127: val_loss improved from 0.20008 to 0.19965, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1799 - accuracy: 0.9393 - val_loss: 0.1996 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1783 - accuracy: 0.9423\n",
      "Epoch 128: val_loss improved from 0.19965 to 0.19922, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9393 - val_loss: 0.1992 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9393\n",
      "Epoch 129: val_loss improved from 0.19922 to 0.19880, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1787 - accuracy: 0.9393 - val_loss: 0.1988 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1691 - accuracy: 0.9412\n",
      "Epoch 130: val_loss improved from 0.19880 to 0.19837, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1782 - accuracy: 0.9393 - val_loss: 0.1984 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1725 - accuracy: 0.9373\n",
      "Epoch 131: val_loss improved from 0.19837 to 0.19798, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9393 - val_loss: 0.1980 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9377\n",
      "Epoch 132: val_loss improved from 0.19798 to 0.19757, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.9393 - val_loss: 0.1976 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1777 - accuracy: 0.9375\n",
      "Epoch 133: val_loss improved from 0.19757 to 0.19717, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9393 - val_loss: 0.1972 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1729 - accuracy: 0.9410\n",
      "Epoch 134: val_loss improved from 0.19717 to 0.19679, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1760 - accuracy: 0.9393 - val_loss: 0.1968 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1712 - accuracy: 0.9467\n",
      "Epoch 135: val_loss improved from 0.19679 to 0.19639, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1755 - accuracy: 0.9393 - val_loss: 0.1964 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9410\n",
      "Epoch 136: val_loss improved from 0.19639 to 0.19599, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1750 - accuracy: 0.9393 - val_loss: 0.1960 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1536 - accuracy: 0.9517\n",
      "Epoch 137: val_loss improved from 0.19599 to 0.19562, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1745 - accuracy: 0.9393 - val_loss: 0.1956 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1666 - accuracy: 0.9404\n",
      "Epoch 138: val_loss improved from 0.19562 to 0.19523, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1739 - accuracy: 0.9393 - val_loss: 0.1952 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1653 - accuracy: 0.9404\n",
      "Epoch 139: val_loss improved from 0.19523 to 0.19484, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1735 - accuracy: 0.9393 - val_loss: 0.1948 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1615 - accuracy: 0.9393\n",
      "Epoch 140: val_loss improved from 0.19484 to 0.19446, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1729 - accuracy: 0.9393 - val_loss: 0.1945 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1601 - accuracy: 0.9439\n",
      "Epoch 141: val_loss improved from 0.19446 to 0.19408, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1724 - accuracy: 0.9393 - val_loss: 0.1941 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9387\n",
      "Epoch 142: val_loss improved from 0.19408 to 0.19370, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1719 - accuracy: 0.9393 - val_loss: 0.1937 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9387\n",
      "Epoch 143: val_loss improved from 0.19370 to 0.19333, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1714 - accuracy: 0.9393 - val_loss: 0.1933 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1774 - accuracy: 0.9333\n",
      "Epoch 144: val_loss improved from 0.19333 to 0.19296, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1710 - accuracy: 0.9393 - val_loss: 0.1930 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1798 - accuracy: 0.9333\n",
      "Epoch 145: val_loss improved from 0.19296 to 0.19260, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1705 - accuracy: 0.9393 - val_loss: 0.1926 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1719 - accuracy: 0.9429\n",
      "Epoch 146: val_loss improved from 0.19260 to 0.19223, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1700 - accuracy: 0.9393 - val_loss: 0.1922 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1706 - accuracy: 0.9404\n",
      "Epoch 147: val_loss improved from 0.19223 to 0.19187, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1695 - accuracy: 0.9393 - val_loss: 0.1919 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1720 - accuracy: 0.9407\n",
      "Epoch 148: val_loss improved from 0.19187 to 0.19151, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1690 - accuracy: 0.9393 - val_loss: 0.1915 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1676 - accuracy: 0.9418\n",
      "Epoch 149: val_loss improved from 0.19151 to 0.19116, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1685 - accuracy: 0.9393 - val_loss: 0.1912 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1627 - accuracy: 0.9429\n",
      "Epoch 150: val_loss improved from 0.19116 to 0.19082, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9393 - val_loss: 0.1908 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.9387\n",
      "Epoch 151: val_loss improved from 0.19082 to 0.19047, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.9393 - val_loss: 0.1905 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1679 - accuracy: 0.9419\n",
      "Epoch 152: val_loss improved from 0.19047 to 0.19013, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.9425 - val_loss: 0.1901 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9443\n",
      "Epoch 153: val_loss improved from 0.19013 to 0.18978, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.9393 - val_loss: 0.1898 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1537 - accuracy: 0.9464\n",
      "Epoch 154: val_loss improved from 0.18978 to 0.18944, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1662 - accuracy: 0.9425 - val_loss: 0.1894 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1761 - accuracy: 0.9404\n",
      "Epoch 155: val_loss improved from 0.18944 to 0.18911, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1658 - accuracy: 0.9457 - val_loss: 0.1891 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1758 - accuracy: 0.9396\n",
      "Epoch 156: val_loss improved from 0.18911 to 0.18878, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9457 - val_loss: 0.1888 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1681 - accuracy: 0.9448\n",
      "Epoch 157: val_loss improved from 0.18878 to 0.18845, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1649 - accuracy: 0.9457 - val_loss: 0.1884 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9443\n",
      "Epoch 158: val_loss improved from 0.18845 to 0.18813, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1645 - accuracy: 0.9457 - val_loss: 0.1881 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1681 - accuracy: 0.9424\n",
      "Epoch 159: val_loss improved from 0.18813 to 0.18781, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1641 - accuracy: 0.9457 - val_loss: 0.1878 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1708 - accuracy: 0.9448\n",
      "Epoch 160: val_loss improved from 0.18781 to 0.18749, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1636 - accuracy: 0.9489 - val_loss: 0.1875 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1618 - accuracy: 0.9492\n",
      "Epoch 161: val_loss improved from 0.18749 to 0.18718, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9489 - val_loss: 0.1872 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1676 - accuracy: 0.9474\n",
      "Epoch 162: val_loss improved from 0.18718 to 0.18686, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9489 - val_loss: 0.1869 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "44/63 [===================>..........] - ETA: 0s - loss: 0.1540 - accuracy: 0.9545\n",
      "Epoch 163: val_loss improved from 0.18686 to 0.18655, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9489 - val_loss: 0.1865 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1579 - accuracy: 0.9517\n",
      "Epoch 164: val_loss improved from 0.18655 to 0.18624, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1620 - accuracy: 0.9489 - val_loss: 0.1862 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1662 - accuracy: 0.9440\n",
      "Epoch 165: val_loss improved from 0.18624 to 0.18594, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1616 - accuracy: 0.9489 - val_loss: 0.1859 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1537 - accuracy: 0.9536\n",
      "Epoch 166: val_loss improved from 0.18594 to 0.18564, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9489 - val_loss: 0.1856 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1659 - accuracy: 0.9467\n",
      "Epoch 167: val_loss improved from 0.18564 to 0.18534, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1608 - accuracy: 0.9489 - val_loss: 0.1853 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1668 - accuracy: 0.9458\n",
      "Epoch 168: val_loss improved from 0.18534 to 0.18504, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.9489 - val_loss: 0.1850 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1565 - accuracy: 0.9491\n",
      "Epoch 169: val_loss improved from 0.18504 to 0.18475, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.9489 - val_loss: 0.1848 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1664 - accuracy: 0.9455\n",
      "Epoch 170: val_loss improved from 0.18475 to 0.18445, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9489 - val_loss: 0.1845 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1622 - accuracy: 0.9474\n",
      "Epoch 171: val_loss improved from 0.18445 to 0.18417, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 0.9489 - val_loss: 0.1842 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1577 - accuracy: 0.9492\n",
      "Epoch 172: val_loss improved from 0.18417 to 0.18388, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1588 - accuracy: 0.9489 - val_loss: 0.1839 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1613 - accuracy: 0.9509\n",
      "Epoch 173: val_loss improved from 0.18388 to 0.18360, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1584 - accuracy: 0.9489 - val_loss: 0.1836 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1611 - accuracy: 0.9481\n",
      "Epoch 174: val_loss improved from 0.18360 to 0.18332, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1581 - accuracy: 0.9521 - val_loss: 0.1833 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1568 - accuracy: 0.9586\n",
      "Epoch 175: val_loss improved from 0.18332 to 0.18302, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1577 - accuracy: 0.9553 - val_loss: 0.1830 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1525 - accuracy: 0.9527\n",
      "Epoch 176: val_loss improved from 0.18302 to 0.18277, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1573 - accuracy: 0.9553 - val_loss: 0.1828 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1476 - accuracy: 0.9586\n",
      "Epoch 177: val_loss improved from 0.18277 to 0.18252, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1569 - accuracy: 0.9553 - val_loss: 0.1825 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9553\n",
      "Epoch 178: val_loss improved from 0.18252 to 0.18224, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1566 - accuracy: 0.9553 - val_loss: 0.1822 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1606 - accuracy: 0.9559\n",
      "Epoch 179: val_loss improved from 0.18224 to 0.18196, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9553 - val_loss: 0.1820 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1630 - accuracy: 0.9517\n",
      "Epoch 180: val_loss improved from 0.18196 to 0.18170, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1559 - accuracy: 0.9553 - val_loss: 0.1817 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1425 - accuracy: 0.9614\n",
      "Epoch 181: val_loss improved from 0.18170 to 0.18143, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9553 - val_loss: 0.1814 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1536 - accuracy: 0.9592\n",
      "Epoch 182: val_loss improved from 0.18143 to 0.18117, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1551 - accuracy: 0.9553 - val_loss: 0.1812 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1595 - accuracy: 0.9536\n",
      "Epoch 183: val_loss improved from 0.18117 to 0.18091, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1548 - accuracy: 0.9553 - val_loss: 0.1809 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1613 - accuracy: 0.9509\n",
      "Epoch 184: val_loss improved from 0.18091 to 0.18065, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1544 - accuracy: 0.9553 - val_loss: 0.1807 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1433 - accuracy: 0.9636\n",
      "Epoch 185: val_loss improved from 0.18065 to 0.18039, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9553 - val_loss: 0.1804 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1522 - accuracy: 0.9547\n",
      "Epoch 186: val_loss improved from 0.18039 to 0.18014, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9553 - val_loss: 0.1801 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1494 - accuracy: 0.9579\n",
      "Epoch 187: val_loss improved from 0.18014 to 0.17989, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1534 - accuracy: 0.9553 - val_loss: 0.1799 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.1640 - accuracy: 0.9511\n",
      "Epoch 188: val_loss improved from 0.17989 to 0.17964, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1531 - accuracy: 0.9553 - val_loss: 0.1796 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1602 - accuracy: 0.9536\n",
      "Epoch 189: val_loss improved from 0.17964 to 0.17939, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9553 - val_loss: 0.1794 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1378 - accuracy: 0.9623\n",
      "Epoch 190: val_loss improved from 0.17939 to 0.17914, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.9553 - val_loss: 0.1791 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1537 - accuracy: 0.9544\n",
      "Epoch 191: val_loss improved from 0.17914 to 0.17890, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9553 - val_loss: 0.1789 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1698 - accuracy: 0.9462\n",
      "Epoch 192: val_loss improved from 0.17890 to 0.17866, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9553 - val_loss: 0.1787 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1471 - accuracy: 0.9564\n",
      "Epoch 193: val_loss improved from 0.17866 to 0.17841, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1514 - accuracy: 0.9553 - val_loss: 0.1784 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9519\n",
      "Epoch 194: val_loss improved from 0.17841 to 0.17817, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1511 - accuracy: 0.9553 - val_loss: 0.1782 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1577 - accuracy: 0.9517\n",
      "Epoch 195: val_loss improved from 0.17817 to 0.17794, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9553 - val_loss: 0.1779 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1532 - accuracy: 0.9525\n",
      "Epoch 196: val_loss improved from 0.17794 to 0.17768, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.9553 - val_loss: 0.1777 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1520 - accuracy: 0.9541\n",
      "Epoch 197: val_loss improved from 0.17768 to 0.17744, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1501 - accuracy: 0.9553 - val_loss: 0.1774 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1423 - accuracy: 0.9577\n",
      "Epoch 198: val_loss improved from 0.17744 to 0.17722, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9553 - val_loss: 0.1772 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1354 - accuracy: 0.9621\n",
      "Epoch 199: val_loss improved from 0.17722 to 0.17700, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9553 - val_loss: 0.1770 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1511 - accuracy: 0.9544\n",
      "Epoch 200: val_loss improved from 0.17700 to 0.17678, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9553 - val_loss: 0.1768 - val_accuracy: 0.9464 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_5 (GRU)                 (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9464\n",
      "Test Loss: 0.1767757534980774\n",
      "Test Accuracy: 0.9464285969734192\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_SGD.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule),callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 1 layer Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.6981 - accuracy: 0.5542\n",
      "Epoch 1: val_loss improved from inf to 0.70889, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 3s 10ms/step - loss: 0.6830 - accuracy: 0.5815 - val_loss: 0.7089 - val_accuracy: 0.5476\n",
      "Epoch 2/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.6357 - accuracy: 0.6311\n",
      "Epoch 2: val_loss improved from 0.70889 to 0.64839, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6180 - accuracy: 0.6486 - val_loss: 0.6484 - val_accuracy: 0.6310\n",
      "Epoch 3/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.5851 - accuracy: 0.6981\n",
      "Epoch 3: val_loss improved from 0.64839 to 0.59753, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5635 - accuracy: 0.7284 - val_loss: 0.5975 - val_accuracy: 0.6845\n",
      "Epoch 4/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.5188 - accuracy: 0.7821\n",
      "Epoch 4: val_loss improved from 0.59753 to 0.55538, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5167 - accuracy: 0.7764 - val_loss: 0.5554 - val_accuracy: 0.7262\n",
      "Epoch 5/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.4764 - accuracy: 0.8068\n",
      "Epoch 5: val_loss improved from 0.55538 to 0.51824, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4766 - accuracy: 0.8083 - val_loss: 0.5182 - val_accuracy: 0.7798\n",
      "Epoch 6/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.4444 - accuracy: 0.8444\n",
      "Epoch 6: val_loss improved from 0.51824 to 0.48603, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4423 - accuracy: 0.8371 - val_loss: 0.4860 - val_accuracy: 0.8333\n",
      "Epoch 7/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.4201 - accuracy: 0.8690\n",
      "Epoch 7: val_loss improved from 0.48603 to 0.45910, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4124 - accuracy: 0.8722 - val_loss: 0.4591 - val_accuracy: 0.8512\n",
      "Epoch 8/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3767 - accuracy: 0.8982\n",
      "Epoch 8: val_loss improved from 0.45910 to 0.43459, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3866 - accuracy: 0.8978 - val_loss: 0.4346 - val_accuracy: 0.8631\n",
      "Epoch 9/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.3693 - accuracy: 0.8983\n",
      "Epoch 9: val_loss improved from 0.43459 to 0.41306, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3639 - accuracy: 0.9042 - val_loss: 0.4131 - val_accuracy: 0.8690\n",
      "Epoch 10/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.3488 - accuracy: 0.9067\n",
      "Epoch 10: val_loss improved from 0.41306 to 0.39407, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3441 - accuracy: 0.9073 - val_loss: 0.3941 - val_accuracy: 0.8690\n",
      "Epoch 11/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3264 - accuracy: 0.9088\n",
      "Epoch 11: val_loss improved from 0.39407 to 0.37695, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3266 - accuracy: 0.9105 - val_loss: 0.3769 - val_accuracy: 0.8690\n",
      "Epoch 12/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3088 - accuracy: 0.9283\n",
      "Epoch 12: val_loss improved from 0.37695 to 0.36161, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3109 - accuracy: 0.9233 - val_loss: 0.3616 - val_accuracy: 0.8750\n",
      "Epoch 13/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.3002 - accuracy: 0.9207\n",
      "Epoch 13: val_loss improved from 0.36161 to 0.34796, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2970 - accuracy: 0.9265 - val_loss: 0.3480 - val_accuracy: 0.8750\n",
      "Epoch 14/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2832 - accuracy: 0.9276\n",
      "Epoch 14: val_loss improved from 0.34796 to 0.33540, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2844 - accuracy: 0.9265 - val_loss: 0.3354 - val_accuracy: 0.8810\n",
      "Epoch 15/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2781 - accuracy: 0.9254\n",
      "Epoch 15: val_loss improved from 0.33540 to 0.32427, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2730 - accuracy: 0.9297 - val_loss: 0.3243 - val_accuracy: 0.8869\n",
      "Epoch 16/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2606 - accuracy: 0.9368\n",
      "Epoch 16: val_loss improved from 0.32427 to 0.31374, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2628 - accuracy: 0.9329 - val_loss: 0.3137 - val_accuracy: 0.8988\n",
      "Epoch 17/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2603 - accuracy: 0.9356\n",
      "Epoch 17: val_loss improved from 0.31374 to 0.30474, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2532 - accuracy: 0.9393 - val_loss: 0.3047 - val_accuracy: 0.8988\n",
      "Epoch 18/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2517 - accuracy: 0.9368\n",
      "Epoch 18: val_loss improved from 0.30474 to 0.29623, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2446 - accuracy: 0.9393 - val_loss: 0.2962 - val_accuracy: 0.8988\n",
      "Epoch 19/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2388 - accuracy: 0.9434\n",
      "Epoch 19: val_loss improved from 0.29623 to 0.28835, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2368 - accuracy: 0.9393 - val_loss: 0.2883 - val_accuracy: 0.9048\n",
      "Epoch 20/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2373 - accuracy: 0.9308\n",
      "Epoch 20: val_loss improved from 0.28835 to 0.28115, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2295 - accuracy: 0.9393 - val_loss: 0.2812 - val_accuracy: 0.9048\n",
      "Epoch 21/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2297 - accuracy: 0.9390\n",
      "Epoch 21: val_loss improved from 0.28115 to 0.27439, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2227 - accuracy: 0.9425 - val_loss: 0.2744 - val_accuracy: 0.9048\n",
      "Epoch 22/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2167 - accuracy: 0.9458\n",
      "Epoch 22: val_loss improved from 0.27439 to 0.26799, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.9425 - val_loss: 0.2680 - val_accuracy: 0.9048\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.9425\n",
      "Epoch 23: val_loss improved from 0.26799 to 0.26240, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2107 - accuracy: 0.9425 - val_loss: 0.2624 - val_accuracy: 0.9048\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9457\n",
      "Epoch 24: val_loss improved from 0.26240 to 0.25684, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9457 - val_loss: 0.2568 - val_accuracy: 0.9048\n",
      "Epoch 25/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2046 - accuracy: 0.9433\n",
      "Epoch 25: val_loss improved from 0.25684 to 0.25190, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2002 - accuracy: 0.9457 - val_loss: 0.2519 - val_accuracy: 0.9107\n",
      "Epoch 26/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1948 - accuracy: 0.9418\n",
      "Epoch 26: val_loss improved from 0.25190 to 0.24704, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1954 - accuracy: 0.9457 - val_loss: 0.2470 - val_accuracy: 0.9107\n",
      "Epoch 27/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1937 - accuracy: 0.9412\n",
      "Epoch 27: val_loss improved from 0.24704 to 0.24263, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1908 - accuracy: 0.9457 - val_loss: 0.2426 - val_accuracy: 0.9107\n",
      "Epoch 28/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1877 - accuracy: 0.9396\n",
      "Epoch 28: val_loss improved from 0.24263 to 0.23867, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1866 - accuracy: 0.9457 - val_loss: 0.2387 - val_accuracy: 0.9107\n",
      "Epoch 29/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1859 - accuracy: 0.9455\n",
      "Epoch 29: val_loss improved from 0.23867 to 0.23472, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1826 - accuracy: 0.9457 - val_loss: 0.2347 - val_accuracy: 0.9107\n",
      "Epoch 30/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1708 - accuracy: 0.9538\n",
      "Epoch 30: val_loss improved from 0.23472 to 0.23071, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1786 - accuracy: 0.9457 - val_loss: 0.2307 - val_accuracy: 0.9107\n",
      "Epoch 31/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1670 - accuracy: 0.9481\n",
      "Epoch 31: val_loss improved from 0.23071 to 0.22728, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9457 - val_loss: 0.2273 - val_accuracy: 0.9167\n",
      "Epoch 32/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1807 - accuracy: 0.9455\n",
      "Epoch 32: val_loss improved from 0.22728 to 0.22416, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1717 - accuracy: 0.9489 - val_loss: 0.2242 - val_accuracy: 0.9107\n",
      "Epoch 33/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9475\n",
      "Epoch 33: val_loss improved from 0.22416 to 0.22090, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1683 - accuracy: 0.9489 - val_loss: 0.2209 - val_accuracy: 0.9167\n",
      "Epoch 34/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1730 - accuracy: 0.9455\n",
      "Epoch 34: val_loss improved from 0.22090 to 0.21796, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1652 - accuracy: 0.9489 - val_loss: 0.2180 - val_accuracy: 0.9107\n",
      "Epoch 35/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1707 - accuracy: 0.9439\n",
      "Epoch 35: val_loss improved from 0.21796 to 0.21495, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9489 - val_loss: 0.2150 - val_accuracy: 0.9167\n",
      "Epoch 36/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1626 - accuracy: 0.9467\n",
      "Epoch 36: val_loss improved from 0.21495 to 0.21239, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1593 - accuracy: 0.9489 - val_loss: 0.2124 - val_accuracy: 0.9167\n",
      "Epoch 37/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1550 - accuracy: 0.9525\n",
      "Epoch 37: val_loss improved from 0.21239 to 0.20975, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9521 - val_loss: 0.2098 - val_accuracy: 0.9167\n",
      "Epoch 38/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1508 - accuracy: 0.9536\n",
      "Epoch 38: val_loss improved from 0.20975 to 0.20726, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1538 - accuracy: 0.9521 - val_loss: 0.2073 - val_accuracy: 0.9167\n",
      "Epoch 39/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1515 - accuracy: 0.9491\n",
      "Epoch 39: val_loss improved from 0.20726 to 0.20486, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9521 - val_loss: 0.2049 - val_accuracy: 0.9167\n",
      "Epoch 40/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1371 - accuracy: 0.9600\n",
      "Epoch 40: val_loss improved from 0.20486 to 0.20262, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.9553 - val_loss: 0.2026 - val_accuracy: 0.9167\n",
      "Epoch 41/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1554 - accuracy: 0.9491\n",
      "Epoch 41: val_loss improved from 0.20262 to 0.20059, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1464 - accuracy: 0.9553 - val_loss: 0.2006 - val_accuracy: 0.9167\n",
      "Epoch 42/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1408 - accuracy: 0.9615\n",
      "Epoch 42: val_loss improved from 0.20059 to 0.19838, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9585 - val_loss: 0.1984 - val_accuracy: 0.9286\n",
      "Epoch 43/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1290 - accuracy: 0.9673\n",
      "Epoch 43: val_loss improved from 0.19838 to 0.19620, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1419 - accuracy: 0.9585 - val_loss: 0.1962 - val_accuracy: 0.9286\n",
      "Epoch 44/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1353 - accuracy: 0.9586\n",
      "Epoch 44: val_loss improved from 0.19620 to 0.19457, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9585 - val_loss: 0.1946 - val_accuracy: 0.9345\n",
      "Epoch 45/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1392 - accuracy: 0.9586\n",
      "Epoch 45: val_loss improved from 0.19457 to 0.19271, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9585 - val_loss: 0.1927 - val_accuracy: 0.9345\n",
      "Epoch 46/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.9567\n",
      "Epoch 46: val_loss improved from 0.19271 to 0.19105, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1355 - accuracy: 0.9585 - val_loss: 0.1911 - val_accuracy: 0.9345\n",
      "Epoch 47/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1341 - accuracy: 0.9593\n",
      "Epoch 47: val_loss improved from 0.19105 to 0.18935, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9585 - val_loss: 0.1893 - val_accuracy: 0.9345\n",
      "Epoch 48/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1303 - accuracy: 0.9564\n",
      "Epoch 48: val_loss improved from 0.18935 to 0.18766, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9585 - val_loss: 0.1877 - val_accuracy: 0.9345\n",
      "Epoch 49/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1319 - accuracy: 0.9574\n",
      "Epoch 49: val_loss improved from 0.18766 to 0.18625, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9585 - val_loss: 0.1862 - val_accuracy: 0.9345\n",
      "Epoch 50/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1268 - accuracy: 0.9600\n",
      "Epoch 50: val_loss improved from 0.18625 to 0.18460, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1279 - accuracy: 0.9585 - val_loss: 0.1846 - val_accuracy: 0.9345\n",
      "Epoch 51/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9574\n",
      "Epoch 51: val_loss improved from 0.18460 to 0.18336, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.9585 - val_loss: 0.1834 - val_accuracy: 0.9345\n",
      "Epoch 52/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1269 - accuracy: 0.9567\n",
      "Epoch 52: val_loss improved from 0.18336 to 0.18197, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1243 - accuracy: 0.9585 - val_loss: 0.1820 - val_accuracy: 0.9345\n",
      "Epoch 53/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1271 - accuracy: 0.9547\n",
      "Epoch 53: val_loss improved from 0.18197 to 0.18069, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9585 - val_loss: 0.1807 - val_accuracy: 0.9405\n",
      "Epoch 54/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9607\n",
      "Epoch 54: val_loss improved from 0.18069 to 0.17914, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 0.9585 - val_loss: 0.1791 - val_accuracy: 0.9405\n",
      "Epoch 55/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9581\n",
      "Epoch 55: val_loss improved from 0.17914 to 0.17807, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9585 - val_loss: 0.1781 - val_accuracy: 0.9405\n",
      "Epoch 56/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9567\n",
      "Epoch 56: val_loss improved from 0.17807 to 0.17664, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9585 - val_loss: 0.1766 - val_accuracy: 0.9405\n",
      "Epoch 57/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1216 - accuracy: 0.9552\n",
      "Epoch 57: val_loss improved from 0.17664 to 0.17566, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1161 - accuracy: 0.9585 - val_loss: 0.1757 - val_accuracy: 0.9405\n",
      "Epoch 58/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9613\n",
      "Epoch 58: val_loss improved from 0.17566 to 0.17458, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9617 - val_loss: 0.1746 - val_accuracy: 0.9405\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9617\n",
      "Epoch 59: val_loss improved from 0.17458 to 0.17351, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.9617 - val_loss: 0.1735 - val_accuracy: 0.9405\n",
      "Epoch 60/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1065 - accuracy: 0.9655\n",
      "Epoch 60: val_loss improved from 0.17351 to 0.17225, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9617 - val_loss: 0.1723 - val_accuracy: 0.9405\n",
      "Epoch 61/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1100 - accuracy: 0.9569\n",
      "Epoch 61: val_loss improved from 0.17225 to 0.17136, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9617 - val_loss: 0.1714 - val_accuracy: 0.9405\n",
      "Epoch 62/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1121 - accuracy: 0.9614\n",
      "Epoch 62: val_loss improved from 0.17136 to 0.17027, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1087 - accuracy: 0.9617 - val_loss: 0.1703 - val_accuracy: 0.9405\n",
      "Epoch 63/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1104 - accuracy: 0.9621\n",
      "Epoch 63: val_loss improved from 0.17027 to 0.16936, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1073 - accuracy: 0.9617 - val_loss: 0.1694 - val_accuracy: 0.9405\n",
      "Epoch 64/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1007 - accuracy: 0.9615\n",
      "Epoch 64: val_loss improved from 0.16936 to 0.16843, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1058 - accuracy: 0.9617 - val_loss: 0.1684 - val_accuracy: 0.9405\n",
      "Epoch 65/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1070 - accuracy: 0.9627\n",
      "Epoch 65: val_loss improved from 0.16843 to 0.16741, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1045 - accuracy: 0.9649 - val_loss: 0.1674 - val_accuracy: 0.9405\n",
      "Epoch 66/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1040 - accuracy: 0.9645\n",
      "Epoch 66: val_loss improved from 0.16741 to 0.16659, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9649 - val_loss: 0.1666 - val_accuracy: 0.9405\n",
      "Epoch 67/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1039 - accuracy: 0.9633\n",
      "Epoch 67: val_loss improved from 0.16659 to 0.16575, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9649 - val_loss: 0.1658 - val_accuracy: 0.9405\n",
      "Epoch 68/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0984 - accuracy: 0.9690\n",
      "Epoch 68: val_loss improved from 0.16575 to 0.16479, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1006 - accuracy: 0.9681 - val_loss: 0.1648 - val_accuracy: 0.9405\n",
      "Epoch 69/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1020 - accuracy: 0.9667\n",
      "Epoch 69: val_loss improved from 0.16479 to 0.16413, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9681 - val_loss: 0.1641 - val_accuracy: 0.9405\n",
      "Epoch 70/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0970 - accuracy: 0.9700\n",
      "Epoch 70: val_loss improved from 0.16413 to 0.16314, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9681 - val_loss: 0.1631 - val_accuracy: 0.9405\n",
      "Epoch 71/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1004 - accuracy: 0.9640\n",
      "Epoch 71: val_loss improved from 0.16314 to 0.16235, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0967 - accuracy: 0.9681 - val_loss: 0.1624 - val_accuracy: 0.9405\n",
      "Epoch 72/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0955 - accuracy: 0.9714\n",
      "Epoch 72: val_loss improved from 0.16235 to 0.16169, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9681 - val_loss: 0.1617 - val_accuracy: 0.9405\n",
      "Epoch 73/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0908 - accuracy: 0.9667\n",
      "Epoch 73: val_loss improved from 0.16169 to 0.16080, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0943 - accuracy: 0.9681 - val_loss: 0.1608 - val_accuracy: 0.9405\n",
      "Epoch 74/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0889 - accuracy: 0.9709\n",
      "Epoch 74: val_loss improved from 0.16080 to 0.16004, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0931 - accuracy: 0.9681 - val_loss: 0.1600 - val_accuracy: 0.9405\n",
      "Epoch 75/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0920 - accuracy: 0.9680\n",
      "Epoch 75: val_loss improved from 0.16004 to 0.15933, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0920 - accuracy: 0.9681 - val_loss: 0.1593 - val_accuracy: 0.9405\n",
      "Epoch 76/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0961 - accuracy: 0.9667\n",
      "Epoch 76: val_loss improved from 0.15933 to 0.15859, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9681 - val_loss: 0.1586 - val_accuracy: 0.9405\n",
      "Epoch 77/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0965 - accuracy: 0.9660\n",
      "Epoch 77: val_loss improved from 0.15859 to 0.15789, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9681 - val_loss: 0.1579 - val_accuracy: 0.9405\n",
      "Epoch 78/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0895 - accuracy: 0.9667\n",
      "Epoch 78: val_loss improved from 0.15789 to 0.15688, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9681 - val_loss: 0.1569 - val_accuracy: 0.9405\n",
      "Epoch 79/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0899 - accuracy: 0.9667\n",
      "Epoch 79: val_loss improved from 0.15688 to 0.15645, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0876 - accuracy: 0.9681 - val_loss: 0.1565 - val_accuracy: 0.9405\n",
      "Epoch 80/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0835 - accuracy: 0.9698\n",
      "Epoch 80: val_loss improved from 0.15645 to 0.15570, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0864 - accuracy: 0.9681 - val_loss: 0.1557 - val_accuracy: 0.9405\n",
      "Epoch 81/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0859 - accuracy: 0.9660\n",
      "Epoch 81: val_loss improved from 0.15570 to 0.15527, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.9681 - val_loss: 0.1553 - val_accuracy: 0.9405\n",
      "Epoch 82/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0848 - accuracy: 0.9695\n",
      "Epoch 82: val_loss improved from 0.15527 to 0.15431, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0843 - accuracy: 0.9681 - val_loss: 0.1543 - val_accuracy: 0.9405\n",
      "Epoch 83/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0807 - accuracy: 0.9714\n",
      "Epoch 83: val_loss improved from 0.15431 to 0.15366, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0831 - accuracy: 0.9681 - val_loss: 0.1537 - val_accuracy: 0.9405\n",
      "Epoch 84/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0746 - accuracy: 0.9709\n",
      "Epoch 84: val_loss improved from 0.15366 to 0.15290, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9681 - val_loss: 0.1529 - val_accuracy: 0.9405\n",
      "Epoch 85/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0810 - accuracy: 0.9700\n",
      "Epoch 85: val_loss improved from 0.15290 to 0.15265, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.9712 - val_loss: 0.1526 - val_accuracy: 0.9405\n",
      "Epoch 86/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0837 - accuracy: 0.9709\n",
      "Epoch 86: val_loss improved from 0.15265 to 0.15160, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9712 - val_loss: 0.1516 - val_accuracy: 0.9405\n",
      "Epoch 87/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0840 - accuracy: 0.9709\n",
      "Epoch 87: val_loss improved from 0.15160 to 0.15102, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0790 - accuracy: 0.9744 - val_loss: 0.1510 - val_accuracy: 0.9405\n",
      "Epoch 88/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0837 - accuracy: 0.9745\n",
      "Epoch 88: val_loss improved from 0.15102 to 0.15058, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9776 - val_loss: 0.1506 - val_accuracy: 0.9405\n",
      "Epoch 89/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0771 - accuracy: 0.9797\n",
      "Epoch 89: val_loss improved from 0.15058 to 0.14981, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.9776 - val_loss: 0.1498 - val_accuracy: 0.9405\n",
      "Epoch 90/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0817 - accuracy: 0.9750\n",
      "Epoch 90: val_loss improved from 0.14981 to 0.14932, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0762 - accuracy: 0.9776 - val_loss: 0.1493 - val_accuracy: 0.9405\n",
      "Epoch 91/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0808 - accuracy: 0.9755\n",
      "Epoch 91: val_loss improved from 0.14932 to 0.14858, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0751 - accuracy: 0.9776 - val_loss: 0.1486 - val_accuracy: 0.9405\n",
      "Epoch 92/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0758 - accuracy: 0.9793\n",
      "Epoch 92: val_loss improved from 0.14858 to 0.14803, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9776 - val_loss: 0.1480 - val_accuracy: 0.9405\n",
      "Epoch 93/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9770\n",
      "Epoch 93: val_loss improved from 0.14803 to 0.14720, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9776 - val_loss: 0.1472 - val_accuracy: 0.9405\n",
      "Epoch 94/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0718 - accuracy: 0.9767\n",
      "Epoch 94: val_loss improved from 0.14720 to 0.14662, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.9776 - val_loss: 0.1466 - val_accuracy: 0.9405\n",
      "Epoch 95/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0729 - accuracy: 0.9797\n",
      "Epoch 95: val_loss improved from 0.14662 to 0.14619, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9808 - val_loss: 0.1462 - val_accuracy: 0.9405\n",
      "Epoch 96/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0630 - accuracy: 0.9796\n",
      "Epoch 96: val_loss improved from 0.14619 to 0.14547, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.9808 - val_loss: 0.1455 - val_accuracy: 0.9405\n",
      "Epoch 97/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0653 - accuracy: 0.9831\n",
      "Epoch 97: val_loss improved from 0.14547 to 0.14466, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.9808 - val_loss: 0.1447 - val_accuracy: 0.9405\n",
      "Epoch 98/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0776 - accuracy: 0.9804\n",
      "Epoch 98: val_loss improved from 0.14466 to 0.14420, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9840 - val_loss: 0.1442 - val_accuracy: 0.9405\n",
      "Epoch 99/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0723 - accuracy: 0.9811\n",
      "Epoch 99: val_loss improved from 0.14420 to 0.14350, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0681 - accuracy: 0.9840 - val_loss: 0.1435 - val_accuracy: 0.9405\n",
      "Epoch 100/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0706 - accuracy: 0.9828\n",
      "Epoch 100: val_loss improved from 0.14350 to 0.14280, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9840 - val_loss: 0.1428 - val_accuracy: 0.9405\n",
      "Epoch 101/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0541 - accuracy: 0.9900\n",
      "Epoch 101: val_loss improved from 0.14280 to 0.14261, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9840 - val_loss: 0.1426 - val_accuracy: 0.9405\n",
      "Epoch 102/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0724 - accuracy: 0.9815\n",
      "Epoch 102: val_loss improved from 0.14261 to 0.14175, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0658 - accuracy: 0.9840 - val_loss: 0.1418 - val_accuracy: 0.9405\n",
      "Epoch 103/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0609 - accuracy: 0.9867\n",
      "Epoch 103: val_loss improved from 0.14175 to 0.14088, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9840 - val_loss: 0.1409 - val_accuracy: 0.9405\n",
      "Epoch 104/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0701 - accuracy: 0.9840\n",
      "Epoch 104: val_loss improved from 0.14088 to 0.14044, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9840 - val_loss: 0.1404 - val_accuracy: 0.9405\n",
      "Epoch 105/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0663 - accuracy: 0.9828\n",
      "Epoch 105: val_loss improved from 0.14044 to 0.14008, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0633 - accuracy: 0.9840 - val_loss: 0.1401 - val_accuracy: 0.9405\n",
      "Epoch 106/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0574 - accuracy: 0.9893\n",
      "Epoch 106: val_loss improved from 0.14008 to 0.13903, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.9872 - val_loss: 0.1390 - val_accuracy: 0.9405\n",
      "Epoch 107/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0641 - accuracy: 0.9864\n",
      "Epoch 107: val_loss improved from 0.13903 to 0.13879, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.9872 - val_loss: 0.1388 - val_accuracy: 0.9405\n",
      "Epoch 108/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0638 - accuracy: 0.9864\n",
      "Epoch 108: val_loss improved from 0.13879 to 0.13787, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9872 - val_loss: 0.1379 - val_accuracy: 0.9405\n",
      "Epoch 109/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0575 - accuracy: 0.9898\n",
      "Epoch 109: val_loss improved from 0.13787 to 0.13704, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9872 - val_loss: 0.1370 - val_accuracy: 0.9405\n",
      "Epoch 110/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0646 - accuracy: 0.9857\n",
      "Epoch 110: val_loss improved from 0.13704 to 0.13665, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9872 - val_loss: 0.1367 - val_accuracy: 0.9405\n",
      "Epoch 111/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0561 - accuracy: 0.9895\n",
      "Epoch 111: val_loss improved from 0.13665 to 0.13579, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9872 - val_loss: 0.1358 - val_accuracy: 0.9405\n",
      "Epoch 112/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0587 - accuracy: 0.9852\n",
      "Epoch 112: val_loss improved from 0.13579 to 0.13516, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0581 - accuracy: 0.9872 - val_loss: 0.1352 - val_accuracy: 0.9405\n",
      "Epoch 113/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0595 - accuracy: 0.9867\n",
      "Epoch 113: val_loss improved from 0.13516 to 0.13459, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9872 - val_loss: 0.1346 - val_accuracy: 0.9405\n",
      "Epoch 114/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0594 - accuracy: 0.9862\n",
      "Epoch 114: val_loss improved from 0.13459 to 0.13411, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9872 - val_loss: 0.1341 - val_accuracy: 0.9405\n",
      "Epoch 115/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0553 - accuracy: 0.9869\n",
      "Epoch 115: val_loss improved from 0.13411 to 0.13362, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9872 - val_loss: 0.1336 - val_accuracy: 0.9405\n",
      "Epoch 116/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9869\n",
      "Epoch 116: val_loss improved from 0.13362 to 0.13259, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9872 - val_loss: 0.1326 - val_accuracy: 0.9405\n",
      "Epoch 117/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0547 - accuracy: 0.9862\n",
      "Epoch 117: val_loss improved from 0.13259 to 0.13168, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9872 - val_loss: 0.1317 - val_accuracy: 0.9405\n",
      "Epoch 118/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0566 - accuracy: 0.9852\n",
      "Epoch 118: val_loss improved from 0.13168 to 0.13136, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9872 - val_loss: 0.1314 - val_accuracy: 0.9405\n",
      "Epoch 119/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9902\n",
      "Epoch 119: val_loss improved from 0.13136 to 0.13049, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9872 - val_loss: 0.1305 - val_accuracy: 0.9405\n",
      "Epoch 120/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0550 - accuracy: 0.9864\n",
      "Epoch 120: val_loss improved from 0.13049 to 0.12982, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9872 - val_loss: 0.1298 - val_accuracy: 0.9405\n",
      "Epoch 121/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0528 - accuracy: 0.9864\n",
      "Epoch 121: val_loss improved from 0.12982 to 0.12958, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9872 - val_loss: 0.1296 - val_accuracy: 0.9405\n",
      "Epoch 122/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0535 - accuracy: 0.9860\n",
      "Epoch 122: val_loss improved from 0.12958 to 0.12861, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.9872 - val_loss: 0.1286 - val_accuracy: 0.9405\n",
      "Epoch 123/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0537 - accuracy: 0.9862\n",
      "Epoch 123: val_loss improved from 0.12861 to 0.12813, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.9872 - val_loss: 0.1281 - val_accuracy: 0.9405\n",
      "Epoch 124/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0524 - accuracy: 0.9864\n",
      "Epoch 124: val_loss improved from 0.12813 to 0.12732, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9872 - val_loss: 0.1273 - val_accuracy: 0.9405\n",
      "Epoch 125/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0476 - accuracy: 0.9887\n",
      "Epoch 125: val_loss improved from 0.12732 to 0.12670, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0497 - accuracy: 0.9872 - val_loss: 0.1267 - val_accuracy: 0.9405\n",
      "Epoch 126/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0506 - accuracy: 0.9846\n",
      "Epoch 126: val_loss improved from 0.12670 to 0.12598, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.9872 - val_loss: 0.1260 - val_accuracy: 0.9405\n",
      "Epoch 127/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0298 - accuracy: 0.9962\n",
      "Epoch 127: val_loss improved from 0.12598 to 0.12530, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0486 - accuracy: 0.9872 - val_loss: 0.1253 - val_accuracy: 0.9464\n",
      "Epoch 128/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0460 - accuracy: 0.9889\n",
      "Epoch 128: val_loss improved from 0.12530 to 0.12456, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.9872 - val_loss: 0.1246 - val_accuracy: 0.9464\n",
      "Epoch 129/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0460 - accuracy: 0.9893\n",
      "Epoch 129: val_loss improved from 0.12456 to 0.12392, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0472 - accuracy: 0.9872 - val_loss: 0.1239 - val_accuracy: 0.9464\n",
      "Epoch 130/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0428 - accuracy: 0.9895\n",
      "Epoch 130: val_loss improved from 0.12392 to 0.12308, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9872 - val_loss: 0.1231 - val_accuracy: 0.9524\n",
      "Epoch 131/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0431 - accuracy: 0.9891\n",
      "Epoch 131: val_loss improved from 0.12308 to 0.12279, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.9872 - val_loss: 0.1228 - val_accuracy: 0.9524\n",
      "Epoch 132/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0489 - accuracy: 0.9849\n",
      "Epoch 132: val_loss improved from 0.12279 to 0.12224, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.9872 - val_loss: 0.1222 - val_accuracy: 0.9524\n",
      "Epoch 133/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0497 - accuracy: 0.9837\n",
      "Epoch 133: val_loss improved from 0.12224 to 0.12160, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0449 - accuracy: 0.9872 - val_loss: 0.1216 - val_accuracy: 0.9524\n",
      "Epoch 134/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0370 - accuracy: 0.9933\n",
      "Epoch 134: val_loss improved from 0.12160 to 0.12057, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0442 - accuracy: 0.9872 - val_loss: 0.1206 - val_accuracy: 0.9524\n",
      "Epoch 135/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0484 - accuracy: 0.9849\n",
      "Epoch 135: val_loss improved from 0.12057 to 0.12056, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.9872 - val_loss: 0.1206 - val_accuracy: 0.9524\n",
      "Epoch 136/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0439 - accuracy: 0.9869\n",
      "Epoch 136: val_loss improved from 0.12056 to 0.11975, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 0.9872 - val_loss: 0.1197 - val_accuracy: 0.9524\n",
      "Epoch 137/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0429 - accuracy: 0.9882\n",
      "Epoch 137: val_loss improved from 0.11975 to 0.11905, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0426 - accuracy: 0.9872 - val_loss: 0.1190 - val_accuracy: 0.9524\n",
      "Epoch 138/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0433 - accuracy: 0.9898\n",
      "Epoch 138: val_loss improved from 0.11905 to 0.11853, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9904 - val_loss: 0.1185 - val_accuracy: 0.9524\n",
      "Epoch 139/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0439 - accuracy: 0.9885\n",
      "Epoch 139: val_loss improved from 0.11853 to 0.11808, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 0.9904 - val_loss: 0.1181 - val_accuracy: 0.9524\n",
      "Epoch 140/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0463 - accuracy: 0.9882\n",
      "Epoch 140: val_loss improved from 0.11808 to 0.11716, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0410 - accuracy: 0.9904 - val_loss: 0.1172 - val_accuracy: 0.9524\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9904\n",
      "Epoch 141: val_loss improved from 0.11716 to 0.11633, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 0.9904 - val_loss: 0.1163 - val_accuracy: 0.9524\n",
      "Epoch 142/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.0425 - accuracy: 0.9911\n",
      "Epoch 142: val_loss improved from 0.11633 to 0.11589, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 0.9904 - val_loss: 0.1159 - val_accuracy: 0.9524\n",
      "Epoch 143/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0411 - accuracy: 0.9882\n",
      "Epoch 143: val_loss improved from 0.11589 to 0.11547, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9904 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 144/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0408 - accuracy: 0.9895\n",
      "Epoch 144: val_loss improved from 0.11547 to 0.11499, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9904 - val_loss: 0.1150 - val_accuracy: 0.9524\n",
      "Epoch 145/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0306 - accuracy: 0.9931\n",
      "Epoch 145: val_loss improved from 0.11499 to 0.11464, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9904 - val_loss: 0.1146 - val_accuracy: 0.9524\n",
      "Epoch 146/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0397 - accuracy: 0.9897\n",
      "Epoch 146: val_loss improved from 0.11464 to 0.11361, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0379 - accuracy: 0.9904 - val_loss: 0.1136 - val_accuracy: 0.9524\n",
      "Epoch 147/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0393 - accuracy: 0.9875\n",
      "Epoch 147: val_loss improved from 0.11361 to 0.11311, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 0.9904 - val_loss: 0.1131 - val_accuracy: 0.9524\n",
      "Epoch 148/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0393 - accuracy: 0.9891\n",
      "Epoch 148: val_loss improved from 0.11311 to 0.11265, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.9904 - val_loss: 0.1127 - val_accuracy: 0.9524\n",
      "Epoch 149/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0397 - accuracy: 0.9893\n",
      "Epoch 149: val_loss improved from 0.11265 to 0.11202, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.9904 - val_loss: 0.1120 - val_accuracy: 0.9524\n",
      "Epoch 150/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0354 - accuracy: 0.9930\n",
      "Epoch 150: val_loss improved from 0.11202 to 0.11116, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 0.9904 - val_loss: 0.1112 - val_accuracy: 0.9524\n",
      "Epoch 151/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0251 - accuracy: 0.9965\n",
      "Epoch 151: val_loss improved from 0.11116 to 0.11106, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9904 - val_loss: 0.1111 - val_accuracy: 0.9524\n",
      "Epoch 152/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0364 - accuracy: 0.9897\n",
      "Epoch 152: val_loss improved from 0.11106 to 0.11027, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 0.9904 - val_loss: 0.1103 - val_accuracy: 0.9524\n",
      "Epoch 153/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 0.9895\n",
      "Epoch 153: val_loss improved from 0.11027 to 0.10990, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9904 - val_loss: 0.1099 - val_accuracy: 0.9524\n",
      "Epoch 154/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0387 - accuracy: 0.9882\n",
      "Epoch 154: val_loss improved from 0.10990 to 0.10915, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9904 - val_loss: 0.1092 - val_accuracy: 0.9524\n",
      "Epoch 155/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0276 - accuracy: 0.9920\n",
      "Epoch 155: val_loss improved from 0.10915 to 0.10909, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.9904 - val_loss: 0.1091 - val_accuracy: 0.9524\n",
      "Epoch 156/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0363 - accuracy: 0.9887\n",
      "Epoch 156: val_loss improved from 0.10909 to 0.10822, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0331 - accuracy: 0.9904 - val_loss: 0.1082 - val_accuracy: 0.9524\n",
      "Epoch 157/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0328 - accuracy: 0.9903\n",
      "Epoch 157: val_loss improved from 0.10822 to 0.10736, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0327 - accuracy: 0.9904 - val_loss: 0.1074 - val_accuracy: 0.9524\n",
      "Epoch 158/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0332 - accuracy: 0.9889\n",
      "Epoch 158: val_loss improved from 0.10736 to 0.10713, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 0.9904 - val_loss: 0.1071 - val_accuracy: 0.9524\n",
      "Epoch 159/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0303 - accuracy: 0.9927\n",
      "Epoch 159: val_loss improved from 0.10713 to 0.10637, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 0.9904 - val_loss: 0.1064 - val_accuracy: 0.9524\n",
      "Epoch 160/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0330 - accuracy: 0.9897\n",
      "Epoch 160: val_loss improved from 0.10637 to 0.10614, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.9904 - val_loss: 0.1061 - val_accuracy: 0.9583\n",
      "Epoch 161/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0307 - accuracy: 0.9889\n",
      "Epoch 161: val_loss improved from 0.10614 to 0.10582, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0309 - accuracy: 0.9904 - val_loss: 0.1058 - val_accuracy: 0.9583\n",
      "Epoch 162/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0327 - accuracy: 0.9889\n",
      "Epoch 162: val_loss improved from 0.10582 to 0.10538, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0304 - accuracy: 0.9904 - val_loss: 0.1054 - val_accuracy: 0.9583\n",
      "Epoch 163/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0272 - accuracy: 0.9929\n",
      "Epoch 163: val_loss improved from 0.10538 to 0.10462, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.1046 - val_accuracy: 0.9583\n",
      "Epoch 164/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0311 - accuracy: 0.9895\n",
      "Epoch 164: val_loss improved from 0.10462 to 0.10460, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9904 - val_loss: 0.1046 - val_accuracy: 0.9583\n",
      "Epoch 165/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0300 - accuracy: 0.9933\n",
      "Epoch 165: val_loss improved from 0.10460 to 0.10422, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 0.9936 - val_loss: 0.1042 - val_accuracy: 0.9583\n",
      "Epoch 166/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0299 - accuracy: 0.9920\n",
      "Epoch 166: val_loss improved from 0.10422 to 0.10374, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0287 - accuracy: 0.9936 - val_loss: 0.1037 - val_accuracy: 0.9583\n",
      "Epoch 167/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0294 - accuracy: 0.9932\n",
      "Epoch 167: val_loss improved from 0.10374 to 0.10318, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.9936 - val_loss: 0.1032 - val_accuracy: 0.9583\n",
      "Epoch 168/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0288 - accuracy: 0.9931\n",
      "Epoch 168: val_loss improved from 0.10318 to 0.10318, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9936 - val_loss: 0.1032 - val_accuracy: 0.9583\n",
      "Epoch 169/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0197 - accuracy: 0.9964\n",
      "Epoch 169: val_loss improved from 0.10318 to 0.10272, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9936 - val_loss: 0.1027 - val_accuracy: 0.9583\n",
      "Epoch 170/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0198 - accuracy: 0.9965\n",
      "Epoch 170: val_loss improved from 0.10272 to 0.10237, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.9936 - val_loss: 0.1024 - val_accuracy: 0.9583\n",
      "Epoch 171/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0250 - accuracy: 0.9963\n",
      "Epoch 171: val_loss improved from 0.10237 to 0.10138, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0267 - accuracy: 0.9936 - val_loss: 0.1014 - val_accuracy: 0.9583\n",
      "Epoch 172/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0279 - accuracy: 0.9925\n",
      "Epoch 172: val_loss did not improve from 0.10138\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.9936 - val_loss: 0.1017 - val_accuracy: 0.9583\n",
      "Epoch 173/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0241 - accuracy: 0.9931\n",
      "Epoch 173: val_loss improved from 0.10138 to 0.10103, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0259 - accuracy: 0.9936 - val_loss: 0.1010 - val_accuracy: 0.9583\n",
      "Epoch 174/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0271 - accuracy: 0.9929\n",
      "Epoch 174: val_loss improved from 0.10103 to 0.10097, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9936 - val_loss: 0.1010 - val_accuracy: 0.9583\n",
      "Epoch 175/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0273 - accuracy: 0.9923\n",
      "Epoch 175: val_loss improved from 0.10097 to 0.10063, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0251 - accuracy: 0.9936 - val_loss: 0.1006 - val_accuracy: 0.9583\n",
      "Epoch 176/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0249 - accuracy: 0.9935\n",
      "Epoch 176: val_loss improved from 0.10063 to 0.10010, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0248 - accuracy: 0.9936 - val_loss: 0.1001 - val_accuracy: 0.9583\n",
      "Epoch 177/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0267 - accuracy: 0.9926\n",
      "Epoch 177: val_loss improved from 0.10010 to 0.09984, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 0.9936 - val_loss: 0.0998 - val_accuracy: 0.9583\n",
      "Epoch 178/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0255 - accuracy: 0.9923\n",
      "Epoch 178: val_loss improved from 0.09984 to 0.09970, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 0.9936 - val_loss: 0.0997 - val_accuracy: 0.9583\n",
      "Epoch 179/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0262 - accuracy: 0.9918\n",
      "Epoch 179: val_loss improved from 0.09970 to 0.09934, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0236 - accuracy: 0.9936 - val_loss: 0.0993 - val_accuracy: 0.9583\n",
      "Epoch 180/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0228 - accuracy: 0.9927\n",
      "Epoch 180: val_loss improved from 0.09934 to 0.09865, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 0.9936 - val_loss: 0.0987 - val_accuracy: 0.9583\n",
      "Epoch 181/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0237 - accuracy: 0.9931\n",
      "Epoch 181: val_loss did not improve from 0.09865\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.0988 - val_accuracy: 0.9583\n",
      "Epoch 182/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0231 - accuracy: 0.9933\n",
      "Epoch 182: val_loss improved from 0.09865 to 0.09864, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0226 - accuracy: 0.9936 - val_loss: 0.0986 - val_accuracy: 0.9583\n",
      "Epoch 183/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0233 - accuracy: 0.9926\n",
      "Epoch 183: val_loss improved from 0.09864 to 0.09824, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 0.9936 - val_loss: 0.0982 - val_accuracy: 0.9583\n",
      "Epoch 184/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0211 - accuracy: 0.9962\n",
      "Epoch 184: val_loss improved from 0.09824 to 0.09780, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9936 - val_loss: 0.0978 - val_accuracy: 0.9583\n",
      "Epoch 185/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0223 - accuracy: 0.9929\n",
      "Epoch 185: val_loss did not improve from 0.09780\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.9936 - val_loss: 0.0979 - val_accuracy: 0.9583\n",
      "Epoch 186/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0217 - accuracy: 0.9934\n",
      "Epoch 186: val_loss improved from 0.09780 to 0.09754, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9936 - val_loss: 0.0975 - val_accuracy: 0.9583\n",
      "Epoch 187/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0164 - accuracy: 0.9965\n",
      "Epoch 187: val_loss improved from 0.09754 to 0.09740, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 0.0974 - val_accuracy: 0.9583\n",
      "Epoch 188/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0224 - accuracy: 0.9925\n",
      "Epoch 188: val_loss improved from 0.09740 to 0.09688, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0206 - accuracy: 0.9936 - val_loss: 0.0969 - val_accuracy: 0.9583\n",
      "Epoch 189/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0199 - accuracy: 0.9929\n",
      "Epoch 189: val_loss improved from 0.09688 to 0.09669, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.9936 - val_loss: 0.0967 - val_accuracy: 0.9583\n",
      "Epoch 190/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0195 - accuracy: 0.9931\n",
      "Epoch 190: val_loss improved from 0.09669 to 0.09648, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9936 - val_loss: 0.0965 - val_accuracy: 0.9583\n",
      "Epoch 191/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0211 - accuracy: 0.9962\n",
      "Epoch 191: val_loss did not improve from 0.09648\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.9968 - val_loss: 0.0965 - val_accuracy: 0.9583\n",
      "Epoch 192/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0192 - accuracy: 0.9962\n",
      "Epoch 192: val_loss improved from 0.09648 to 0.09615, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9968 - val_loss: 0.0961 - val_accuracy: 0.9583\n",
      "Epoch 193/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 193: val_loss did not improve from 0.09615\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9968 - val_loss: 0.0962 - val_accuracy: 0.9583\n",
      "Epoch 194/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0173 - accuracy: 0.9962\n",
      "Epoch 194: val_loss improved from 0.09615 to 0.09567, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 0.9968 - val_loss: 0.0957 - val_accuracy: 0.9643\n",
      "Epoch 195/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0196 - accuracy: 0.9960\n",
      "Epoch 195: val_loss did not improve from 0.09567\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0184 - accuracy: 0.9968 - val_loss: 0.0957 - val_accuracy: 0.9643\n",
      "Epoch 196/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0204 - accuracy: 0.9961\n",
      "Epoch 196: val_loss improved from 0.09567 to 0.09563, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 0.9968 - val_loss: 0.0956 - val_accuracy: 0.9643\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9968\n",
      "Epoch 197: val_loss improved from 0.09563 to 0.09561, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9968 - val_loss: 0.0956 - val_accuracy: 0.9643\n",
      "Epoch 198/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0178 - accuracy: 0.9967\n",
      "Epoch 198: val_loss improved from 0.09561 to 0.09547, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0174 - accuracy: 0.9968 - val_loss: 0.0955 - val_accuracy: 0.9643\n",
      "Epoch 199/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0180 - accuracy: 0.9966\n",
      "Epoch 199: val_loss improved from 0.09547 to 0.09526, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.9968 - val_loss: 0.0953 - val_accuracy: 0.9643\n",
      "Epoch 200/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0176 - accuracy: 0.9966\n",
      "Epoch 200: val_loss improved from 0.09526 to 0.09505, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.9968 - val_loss: 0.0950 - val_accuracy: 0.9643\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_1 (GRU)                 (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0950 - accuracy: 0.9643\n",
      "Test Loss: 0.09504835307598114\n",
      "Test Accuracy: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 1 layer RMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.7930 - accuracy: 0.3565\n",
      "Epoch 1: val_loss improved from inf to 0.72199, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 3s 13ms/step - loss: 0.7833 - accuracy: 0.3834 - val_loss: 0.7220 - val_accuracy: 0.5060\n",
      "Epoch 2/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.7248 - accuracy: 0.4691\n",
      "Epoch 2: val_loss improved from 0.72199 to 0.66277, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.7133 - accuracy: 0.4856 - val_loss: 0.6628 - val_accuracy: 0.6190\n",
      "Epoch 3/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.6561 - accuracy: 0.5745\n",
      "Epoch 3: val_loss improved from 0.66277 to 0.60938, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6522 - accuracy: 0.5974 - val_loss: 0.6094 - val_accuracy: 0.6845\n",
      "Epoch 4/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.5990 - accuracy: 0.7143\n",
      "Epoch 4: val_loss improved from 0.60938 to 0.56258, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5975 - accuracy: 0.7252 - val_loss: 0.5626 - val_accuracy: 0.7440\n",
      "Epoch 5/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.5410 - accuracy: 0.8038\n",
      "Epoch 5: val_loss improved from 0.56258 to 0.52054, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5488 - accuracy: 0.7891 - val_loss: 0.5205 - val_accuracy: 0.7976\n",
      "Epoch 6/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.5083 - accuracy: 0.8426\n",
      "Epoch 6: val_loss improved from 0.52054 to 0.48333, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5057 - accuracy: 0.8211 - val_loss: 0.4833 - val_accuracy: 0.8452\n",
      "Epoch 7/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.4673 - accuracy: 0.8346\n",
      "Epoch 7: val_loss improved from 0.48333 to 0.45016, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4678 - accuracy: 0.8275 - val_loss: 0.4502 - val_accuracy: 0.8690\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.8626\n",
      "Epoch 8: val_loss improved from 0.45016 to 0.42061, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4337 - accuracy: 0.8626 - val_loss: 0.4206 - val_accuracy: 0.8690\n",
      "Epoch 9/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.4006 - accuracy: 0.8746\n",
      "Epoch 9: val_loss improved from 0.42061 to 0.39449, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4036 - accuracy: 0.8754 - val_loss: 0.3945 - val_accuracy: 0.8869\n",
      "Epoch 10/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3870 - accuracy: 0.8842\n",
      "Epoch 10: val_loss improved from 0.39449 to 0.37126, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3767 - accuracy: 0.8882 - val_loss: 0.3713 - val_accuracy: 0.8929\n",
      "Epoch 11/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3447 - accuracy: 0.9088\n",
      "Epoch 11: val_loss improved from 0.37126 to 0.35038, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3529 - accuracy: 0.8978 - val_loss: 0.3504 - val_accuracy: 0.9107\n",
      "Epoch 12/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3391 - accuracy: 0.8964\n",
      "Epoch 12: val_loss improved from 0.35038 to 0.33215, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.9042 - val_loss: 0.3322 - val_accuracy: 0.9107\n",
      "Epoch 13/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3086 - accuracy: 0.9088\n",
      "Epoch 13: val_loss improved from 0.33215 to 0.31552, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.9073 - val_loss: 0.3155 - val_accuracy: 0.9167\n",
      "Epoch 14/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2956 - accuracy: 0.9207\n",
      "Epoch 14: val_loss improved from 0.31552 to 0.30078, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2964 - accuracy: 0.9169 - val_loss: 0.3008 - val_accuracy: 0.9167\n",
      "Epoch 15/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2875 - accuracy: 0.9143\n",
      "Epoch 15: val_loss improved from 0.30078 to 0.28772, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2816 - accuracy: 0.9233 - val_loss: 0.2877 - val_accuracy: 0.9286\n",
      "Epoch 16/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2667 - accuracy: 0.9259\n",
      "Epoch 16: val_loss improved from 0.28772 to 0.27548, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2679 - accuracy: 0.9297 - val_loss: 0.2755 - val_accuracy: 0.9286\n",
      "Epoch 17/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2587 - accuracy: 0.9298\n",
      "Epoch 17: val_loss improved from 0.27548 to 0.26509, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2556 - accuracy: 0.9329 - val_loss: 0.2651 - val_accuracy: 0.9286\n",
      "Epoch 18/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2381 - accuracy: 0.9407\n",
      "Epoch 18: val_loss improved from 0.26509 to 0.25548, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2447 - accuracy: 0.9361 - val_loss: 0.2555 - val_accuracy: 0.9286\n",
      "Epoch 19/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2353 - accuracy: 0.9346\n",
      "Epoch 19: val_loss improved from 0.25548 to 0.24662, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2346 - accuracy: 0.9361 - val_loss: 0.2466 - val_accuracy: 0.9286\n",
      "Epoch 20/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2207 - accuracy: 0.9385\n",
      "Epoch 20: val_loss improved from 0.24662 to 0.23855, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2254 - accuracy: 0.9329 - val_loss: 0.2385 - val_accuracy: 0.9345\n",
      "Epoch 21/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2159 - accuracy: 0.9358\n",
      "Epoch 21: val_loss improved from 0.23855 to 0.23149, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2169 - accuracy: 0.9393 - val_loss: 0.2315 - val_accuracy: 0.9405\n",
      "Epoch 22/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2061 - accuracy: 0.9474\n",
      "Epoch 22: val_loss improved from 0.23149 to 0.22489, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2094 - accuracy: 0.9425 - val_loss: 0.2249 - val_accuracy: 0.9405\n",
      "Epoch 23/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.2146 - accuracy: 0.9375\n",
      "Epoch 23: val_loss improved from 0.22489 to 0.21906, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2023 - accuracy: 0.9457 - val_loss: 0.2191 - val_accuracy: 0.9405\n",
      "Epoch 24/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1963 - accuracy: 0.9484\n",
      "Epoch 24: val_loss improved from 0.21906 to 0.21357, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1958 - accuracy: 0.9489 - val_loss: 0.2136 - val_accuracy: 0.9405\n",
      "Epoch 25/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1698 - accuracy: 0.9625\n",
      "Epoch 25: val_loss improved from 0.21357 to 0.20850, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1897 - accuracy: 0.9489 - val_loss: 0.2085 - val_accuracy: 0.9405\n",
      "Epoch 26/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.9639\n",
      "Epoch 26: val_loss improved from 0.20850 to 0.20407, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1840 - accuracy: 0.9553 - val_loss: 0.2041 - val_accuracy: 0.9405\n",
      "Epoch 27/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1708 - accuracy: 0.9469\n",
      "Epoch 27: val_loss improved from 0.20407 to 0.19986, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1787 - accuracy: 0.9521 - val_loss: 0.1999 - val_accuracy: 0.9405\n",
      "Epoch 28/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9574\n",
      "Epoch 28: val_loss improved from 0.19986 to 0.19599, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.9553 - val_loss: 0.1960 - val_accuracy: 0.9405\n",
      "Epoch 29/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9548\n",
      "Epoch 29: val_loss improved from 0.19599 to 0.19237, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1691 - accuracy: 0.9553 - val_loss: 0.1924 - val_accuracy: 0.9405\n",
      "Epoch 30/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1662 - accuracy: 0.9536\n",
      "Epoch 30: val_loss improved from 0.19237 to 0.18917, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.9553 - val_loss: 0.1892 - val_accuracy: 0.9405\n",
      "Epoch 31/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1597 - accuracy: 0.9529\n",
      "Epoch 31: val_loss improved from 0.18917 to 0.18612, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.9553 - val_loss: 0.1861 - val_accuracy: 0.9405\n",
      "Epoch 32/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1624 - accuracy: 0.9517\n",
      "Epoch 32: val_loss improved from 0.18612 to 0.18328, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.9553 - val_loss: 0.1833 - val_accuracy: 0.9405\n",
      "Epoch 33/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1570 - accuracy: 0.9533\n",
      "Epoch 33: val_loss improved from 0.18328 to 0.18075, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1532 - accuracy: 0.9553 - val_loss: 0.1807 - val_accuracy: 0.9464\n",
      "Epoch 34/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1572 - accuracy: 0.9542\n",
      "Epoch 34: val_loss improved from 0.18075 to 0.17833, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1497 - accuracy: 0.9553 - val_loss: 0.1783 - val_accuracy: 0.9464\n",
      "Epoch 35/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1370 - accuracy: 0.9593\n",
      "Epoch 35: val_loss improved from 0.17833 to 0.17603, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1465 - accuracy: 0.9553 - val_loss: 0.1760 - val_accuracy: 0.9464\n",
      "Epoch 36/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1443 - accuracy: 0.9547\n",
      "Epoch 36: val_loss improved from 0.17603 to 0.17404, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9553 - val_loss: 0.1740 - val_accuracy: 0.9464\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9553\n",
      "Epoch 37: val_loss improved from 0.17404 to 0.17197, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1406 - accuracy: 0.9553 - val_loss: 0.1720 - val_accuracy: 0.9464\n",
      "Epoch 38/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1260 - accuracy: 0.9654\n",
      "Epoch 38: val_loss improved from 0.17197 to 0.17001, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9553 - val_loss: 0.1700 - val_accuracy: 0.9524\n",
      "Epoch 39/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1400 - accuracy: 0.9538\n",
      "Epoch 39: val_loss improved from 0.17001 to 0.16826, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1353 - accuracy: 0.9553 - val_loss: 0.1683 - val_accuracy: 0.9524\n",
      "Epoch 40/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1257 - accuracy: 0.9640\n",
      "Epoch 40: val_loss improved from 0.16826 to 0.16655, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9553 - val_loss: 0.1665 - val_accuracy: 0.9524\n",
      "Epoch 41/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.1330 - accuracy: 0.9522\n",
      "Epoch 41: val_loss improved from 0.16655 to 0.16499, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9553 - val_loss: 0.1650 - val_accuracy: 0.9524\n",
      "Epoch 42/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1332 - accuracy: 0.9547\n",
      "Epoch 42: val_loss improved from 0.16499 to 0.16348, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9585 - val_loss: 0.1635 - val_accuracy: 0.9524\n",
      "Epoch 43/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1191 - accuracy: 0.9680\n",
      "Epoch 43: val_loss improved from 0.16348 to 0.16200, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1258 - accuracy: 0.9585 - val_loss: 0.1620 - val_accuracy: 0.9524\n",
      "Epoch 44/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1085 - accuracy: 0.9686\n",
      "Epoch 44: val_loss improved from 0.16200 to 0.16065, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9617 - val_loss: 0.1606 - val_accuracy: 0.9524\n",
      "Epoch 45/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9607\n",
      "Epoch 45: val_loss improved from 0.16065 to 0.15941, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1218 - accuracy: 0.9617 - val_loss: 0.1594 - val_accuracy: 0.9524\n",
      "Epoch 46/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1140 - accuracy: 0.9627\n",
      "Epoch 46: val_loss improved from 0.15941 to 0.15825, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1198 - accuracy: 0.9617 - val_loss: 0.1583 - val_accuracy: 0.9524\n",
      "Epoch 47/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1085 - accuracy: 0.9647\n",
      "Epoch 47: val_loss improved from 0.15825 to 0.15714, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9617 - val_loss: 0.1571 - val_accuracy: 0.9524\n",
      "Epoch 48/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1165 - accuracy: 0.9608\n",
      "Epoch 48: val_loss improved from 0.15714 to 0.15604, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1163 - accuracy: 0.9617 - val_loss: 0.1560 - val_accuracy: 0.9524\n",
      "Epoch 49/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9645\n",
      "Epoch 49: val_loss improved from 0.15604 to 0.15489, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1144 - accuracy: 0.9649 - val_loss: 0.1549 - val_accuracy: 0.9524\n",
      "Epoch 50/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1230 - accuracy: 0.9633\n",
      "Epoch 50: val_loss improved from 0.15489 to 0.15376, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9649 - val_loss: 0.1538 - val_accuracy: 0.9524\n",
      "Epoch 51/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1048 - accuracy: 0.9673\n",
      "Epoch 51: val_loss improved from 0.15376 to 0.15283, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9649 - val_loss: 0.1528 - val_accuracy: 0.9524\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9649\n",
      "Epoch 52: val_loss improved from 0.15283 to 0.15197, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1094 - accuracy: 0.9649 - val_loss: 0.1520 - val_accuracy: 0.9524\n",
      "Epoch 53/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1111 - accuracy: 0.9633\n",
      "Epoch 53: val_loss improved from 0.15197 to 0.15115, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9649 - val_loss: 0.1511 - val_accuracy: 0.9524\n",
      "Epoch 54/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1053 - accuracy: 0.9615\n",
      "Epoch 54: val_loss improved from 0.15115 to 0.15032, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1061 - accuracy: 0.9649 - val_loss: 0.1503 - val_accuracy: 0.9524\n",
      "Epoch 55/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0955 - accuracy: 0.9625\n",
      "Epoch 55: val_loss improved from 0.15032 to 0.14951, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1046 - accuracy: 0.9649 - val_loss: 0.1495 - val_accuracy: 0.9524\n",
      "Epoch 56/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0881 - accuracy: 0.9686\n",
      "Epoch 56: val_loss improved from 0.14951 to 0.14876, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1033 - accuracy: 0.9649 - val_loss: 0.1488 - val_accuracy: 0.9524\n",
      "Epoch 57/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1064 - accuracy: 0.9647\n",
      "Epoch 57: val_loss improved from 0.14876 to 0.14808, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1018 - accuracy: 0.9649 - val_loss: 0.1481 - val_accuracy: 0.9524\n",
      "Epoch 58/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1055 - accuracy: 0.9636\n",
      "Epoch 58: val_loss improved from 0.14808 to 0.14737, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1003 - accuracy: 0.9649 - val_loss: 0.1474 - val_accuracy: 0.9524\n",
      "Epoch 59/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0941 - accuracy: 0.9654\n",
      "Epoch 59: val_loss improved from 0.14737 to 0.14665, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9649 - val_loss: 0.1466 - val_accuracy: 0.9524\n",
      "Epoch 60/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0984 - accuracy: 0.9615\n",
      "Epoch 60: val_loss improved from 0.14665 to 0.14608, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0976 - accuracy: 0.9649 - val_loss: 0.1461 - val_accuracy: 0.9524\n",
      "Epoch 61/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0900 - accuracy: 0.9667\n",
      "Epoch 61: val_loss improved from 0.14608 to 0.14544, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9649 - val_loss: 0.1454 - val_accuracy: 0.9524\n",
      "Epoch 62/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0945 - accuracy: 0.9633\n",
      "Epoch 62: val_loss improved from 0.14544 to 0.14477, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0950 - accuracy: 0.9649 - val_loss: 0.1448 - val_accuracy: 0.9524\n",
      "Epoch 63/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0971 - accuracy: 0.9627\n",
      "Epoch 63: val_loss improved from 0.14477 to 0.14408, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0940 - accuracy: 0.9649 - val_loss: 0.1441 - val_accuracy: 0.9524\n",
      "Epoch 64/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0886 - accuracy: 0.9660\n",
      "Epoch 64: val_loss improved from 0.14408 to 0.14335, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0926 - accuracy: 0.9649 - val_loss: 0.1433 - val_accuracy: 0.9524\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9649\n",
      "Epoch 65: val_loss improved from 0.14335 to 0.14283, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0914 - accuracy: 0.9649 - val_loss: 0.1428 - val_accuracy: 0.9524\n",
      "Epoch 66/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0956 - accuracy: 0.9655\n",
      "Epoch 66: val_loss improved from 0.14283 to 0.14224, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9681 - val_loss: 0.1422 - val_accuracy: 0.9524\n",
      "Epoch 67/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0914 - accuracy: 0.9667\n",
      "Epoch 67: val_loss improved from 0.14224 to 0.14184, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0893 - accuracy: 0.9681 - val_loss: 0.1418 - val_accuracy: 0.9524\n",
      "Epoch 68/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0906 - accuracy: 0.9679\n",
      "Epoch 68: val_loss improved from 0.14184 to 0.14129, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0881 - accuracy: 0.9681 - val_loss: 0.1413 - val_accuracy: 0.9524\n",
      "Epoch 69/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.0984 - accuracy: 0.9617\n",
      "Epoch 69: val_loss improved from 0.14129 to 0.14094, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0868 - accuracy: 0.9681 - val_loss: 0.1409 - val_accuracy: 0.9524\n",
      "Epoch 70/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0820 - accuracy: 0.9704\n",
      "Epoch 70: val_loss improved from 0.14094 to 0.14066, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0859 - accuracy: 0.9681 - val_loss: 0.1407 - val_accuracy: 0.9524\n",
      "Epoch 71/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0739 - accuracy: 0.9754\n",
      "Epoch 71: val_loss improved from 0.14066 to 0.13999, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0848 - accuracy: 0.9681 - val_loss: 0.1400 - val_accuracy: 0.9524\n",
      "Epoch 72/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0861 - accuracy: 0.9667\n",
      "Epoch 72: val_loss improved from 0.13999 to 0.13964, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9681 - val_loss: 0.1396 - val_accuracy: 0.9524\n",
      "Epoch 73/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0885 - accuracy: 0.9649\n",
      "Epoch 73: val_loss improved from 0.13964 to 0.13916, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0826 - accuracy: 0.9681 - val_loss: 0.1392 - val_accuracy: 0.9524\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9681\n",
      "Epoch 74: val_loss improved from 0.13916 to 0.13862, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9681 - val_loss: 0.1386 - val_accuracy: 0.9524\n",
      "Epoch 75/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0757 - accuracy: 0.9686\n",
      "Epoch 75: val_loss improved from 0.13862 to 0.13815, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 0.9681 - val_loss: 0.1381 - val_accuracy: 0.9524\n",
      "Epoch 76/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0807 - accuracy: 0.9725\n",
      "Epoch 76: val_loss improved from 0.13815 to 0.13771, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0795 - accuracy: 0.9681 - val_loss: 0.1377 - val_accuracy: 0.9524\n",
      "Epoch 77/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0832 - accuracy: 0.9643\n",
      "Epoch 77: val_loss improved from 0.13771 to 0.13729, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0785 - accuracy: 0.9681 - val_loss: 0.1373 - val_accuracy: 0.9524\n",
      "Epoch 78/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0829 - accuracy: 0.9643\n",
      "Epoch 78: val_loss improved from 0.13729 to 0.13690, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0778 - accuracy: 0.9681 - val_loss: 0.1369 - val_accuracy: 0.9524\n",
      "Epoch 79/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0750 - accuracy: 0.9733\n",
      "Epoch 79: val_loss improved from 0.13690 to 0.13668, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0771 - accuracy: 0.9681 - val_loss: 0.1367 - val_accuracy: 0.9524\n",
      "Epoch 80/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0616 - accuracy: 0.9846\n",
      "Epoch 80: val_loss improved from 0.13668 to 0.13626, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0761 - accuracy: 0.9712 - val_loss: 0.1363 - val_accuracy: 0.9524\n",
      "Epoch 81/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9710\n",
      "Epoch 81: val_loss improved from 0.13626 to 0.13582, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0750 - accuracy: 0.9712 - val_loss: 0.1358 - val_accuracy: 0.9524\n",
      "Epoch 82/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0721 - accuracy: 0.9729\n",
      "Epoch 82: val_loss improved from 0.13582 to 0.13548, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0741 - accuracy: 0.9712 - val_loss: 0.1355 - val_accuracy: 0.9524\n",
      "Epoch 83/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0683 - accuracy: 0.9733\n",
      "Epoch 83: val_loss improved from 0.13548 to 0.13527, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0733 - accuracy: 0.9712 - val_loss: 0.1353 - val_accuracy: 0.9524\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9712\n",
      "Epoch 84: val_loss improved from 0.13527 to 0.13502, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0723 - accuracy: 0.9712 - val_loss: 0.1350 - val_accuracy: 0.9524\n",
      "Epoch 85/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0624 - accuracy: 0.9704\n",
      "Epoch 85: val_loss improved from 0.13502 to 0.13461, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0716 - accuracy: 0.9712 - val_loss: 0.1346 - val_accuracy: 0.9524\n",
      "Epoch 86/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0713 - accuracy: 0.9719\n",
      "Epoch 86: val_loss improved from 0.13461 to 0.13425, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0706 - accuracy: 0.9712 - val_loss: 0.1342 - val_accuracy: 0.9524\n",
      "Epoch 87/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9742\n",
      "Epoch 87: val_loss improved from 0.13425 to 0.13398, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0697 - accuracy: 0.9744 - val_loss: 0.1340 - val_accuracy: 0.9524\n",
      "Epoch 88/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0701 - accuracy: 0.9725\n",
      "Epoch 88: val_loss improved from 0.13398 to 0.13372, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0690 - accuracy: 0.9744 - val_loss: 0.1337 - val_accuracy: 0.9524\n",
      "Epoch 89/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0623 - accuracy: 0.9731\n",
      "Epoch 89: val_loss improved from 0.13372 to 0.13336, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0682 - accuracy: 0.9744 - val_loss: 0.1334 - val_accuracy: 0.9524\n",
      "Epoch 90/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9774\n",
      "Epoch 90: val_loss improved from 0.13336 to 0.13319, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9776 - val_loss: 0.1332 - val_accuracy: 0.9524\n",
      "Epoch 91/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0685 - accuracy: 0.9833\n",
      "Epoch 91: val_loss improved from 0.13319 to 0.13284, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9776 - val_loss: 0.1328 - val_accuracy: 0.9524\n",
      "Epoch 92/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9806\n",
      "Epoch 92: val_loss improved from 0.13284 to 0.13259, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0657 - accuracy: 0.9808 - val_loss: 0.1326 - val_accuracy: 0.9524\n",
      "Epoch 93/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0655 - accuracy: 0.9828\n",
      "Epoch 93: val_loss improved from 0.13259 to 0.13233, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0651 - accuracy: 0.9808 - val_loss: 0.1323 - val_accuracy: 0.9524\n",
      "Epoch 94/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0665 - accuracy: 0.9800\n",
      "Epoch 94: val_loss improved from 0.13233 to 0.13208, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0643 - accuracy: 0.9808 - val_loss: 0.1321 - val_accuracy: 0.9524\n",
      "Epoch 95/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0692 - accuracy: 0.9796\n",
      "Epoch 95: val_loss improved from 0.13208 to 0.13176, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0635 - accuracy: 0.9808 - val_loss: 0.1318 - val_accuracy: 0.9583\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9808\n",
      "Epoch 96: val_loss improved from 0.13176 to 0.13146, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9808 - val_loss: 0.1315 - val_accuracy: 0.9583\n",
      "Epoch 97/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0480 - accuracy: 0.9885\n",
      "Epoch 97: val_loss improved from 0.13146 to 0.13118, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0625 - accuracy: 0.9808 - val_loss: 0.1312 - val_accuracy: 0.9583\n",
      "Epoch 98/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0695 - accuracy: 0.9769\n",
      "Epoch 98: val_loss improved from 0.13118 to 0.13096, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.9808 - val_loss: 0.1310 - val_accuracy: 0.9583\n",
      "Epoch 99/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0664 - accuracy: 0.9808\n",
      "Epoch 99: val_loss improved from 0.13096 to 0.13076, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0609 - accuracy: 0.9808 - val_loss: 0.1308 - val_accuracy: 0.9583\n",
      "Epoch 100/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0635 - accuracy: 0.9808\n",
      "Epoch 100: val_loss improved from 0.13076 to 0.13060, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0601 - accuracy: 0.9808 - val_loss: 0.1306 - val_accuracy: 0.9583\n",
      "Epoch 101/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0641 - accuracy: 0.9804\n",
      "Epoch 101: val_loss improved from 0.13060 to 0.13042, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0594 - accuracy: 0.9840 - val_loss: 0.1304 - val_accuracy: 0.9583\n",
      "Epoch 102/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0615 - accuracy: 0.9808\n",
      "Epoch 102: val_loss improved from 0.13042 to 0.13020, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9808 - val_loss: 0.1302 - val_accuracy: 0.9583\n",
      "Epoch 103/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9871\n",
      "Epoch 103: val_loss improved from 0.13020 to 0.12996, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9872 - val_loss: 0.1300 - val_accuracy: 0.9583\n",
      "Epoch 104/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0579 - accuracy: 0.9882\n",
      "Epoch 104: val_loss did not improve from 0.12996\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9872 - val_loss: 0.1300 - val_accuracy: 0.9583\n",
      "Epoch 105/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0621 - accuracy: 0.9849\n",
      "Epoch 105: val_loss improved from 0.12996 to 0.12969, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9872 - val_loss: 0.1297 - val_accuracy: 0.9583\n",
      "Epoch 106/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0559 - accuracy: 0.9920\n",
      "Epoch 106: val_loss improved from 0.12969 to 0.12948, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 0.9872 - val_loss: 0.1295 - val_accuracy: 0.9583\n",
      "Epoch 107/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0580 - accuracy: 0.9862\n",
      "Epoch 107: val_loss improved from 0.12948 to 0.12938, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9872 - val_loss: 0.1294 - val_accuracy: 0.9583\n",
      "Epoch 108/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0629 - accuracy: 0.9843\n",
      "Epoch 108: val_loss improved from 0.12938 to 0.12930, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 0.9872 - val_loss: 0.1293 - val_accuracy: 0.9583\n",
      "Epoch 109/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0535 - accuracy: 0.9889\n",
      "Epoch 109: val_loss improved from 0.12930 to 0.12912, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0548 - accuracy: 0.9872 - val_loss: 0.1291 - val_accuracy: 0.9583\n",
      "Epoch 110/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0621 - accuracy: 0.9837\n",
      "Epoch 110: val_loss improved from 0.12912 to 0.12891, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0539 - accuracy: 0.9872 - val_loss: 0.1289 - val_accuracy: 0.9583\n",
      "Epoch 111/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0549 - accuracy: 0.9862\n",
      "Epoch 111: val_loss improved from 0.12891 to 0.12879, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0535 - accuracy: 0.9872 - val_loss: 0.1288 - val_accuracy: 0.9583\n",
      "Epoch 112/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0603 - accuracy: 0.9837\n",
      "Epoch 112: val_loss improved from 0.12879 to 0.12867, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 0.9872 - val_loss: 0.1287 - val_accuracy: 0.9583\n",
      "Epoch 113/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0415 - accuracy: 0.9893\n",
      "Epoch 113: val_loss improved from 0.12867 to 0.12857, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0530 - accuracy: 0.9872 - val_loss: 0.1286 - val_accuracy: 0.9583\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9872\n",
      "Epoch 114: val_loss improved from 0.12857 to 0.12851, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0521 - accuracy: 0.9872 - val_loss: 0.1285 - val_accuracy: 0.9583\n",
      "Epoch 115/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9869\n",
      "Epoch 115: val_loss improved from 0.12851 to 0.12847, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0515 - accuracy: 0.9872 - val_loss: 0.1285 - val_accuracy: 0.9583\n",
      "Epoch 116/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0494 - accuracy: 0.9887\n",
      "Epoch 116: val_loss improved from 0.12847 to 0.12844, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 0.9872 - val_loss: 0.1284 - val_accuracy: 0.9583\n",
      "Epoch 117/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0534 - accuracy: 0.9860\n",
      "Epoch 117: val_loss improved from 0.12844 to 0.12825, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9872 - val_loss: 0.1283 - val_accuracy: 0.9583\n",
      "Epoch 118/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0505 - accuracy: 0.9897\n",
      "Epoch 118: val_loss improved from 0.12825 to 0.12820, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0498 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 119/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0536 - accuracy: 0.9885\n",
      "Epoch 119: val_loss improved from 0.12820 to 0.12813, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0493 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 120/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0524 - accuracy: 0.9880\n",
      "Epoch 120: val_loss did not improve from 0.12813\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0489 - accuracy: 0.9904 - val_loss: 0.1283 - val_accuracy: 0.9583\n",
      "Epoch 121/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0512 - accuracy: 0.9893\n",
      "Epoch 121: val_loss did not improve from 0.12813\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 122/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0521 - accuracy: 0.9893\n",
      "Epoch 122: val_loss did not improve from 0.12813\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0478 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 123/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0493 - accuracy: 0.9898\n",
      "Epoch 123: val_loss improved from 0.12813 to 0.12808, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0474 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 124/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0509 - accuracy: 0.9895\n",
      "Epoch 124: val_loss improved from 0.12808 to 0.12803, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0473 - accuracy: 0.9904 - val_loss: 0.1280 - val_accuracy: 0.9583\n",
      "Epoch 125/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0393 - accuracy: 0.9964\n",
      "Epoch 125: val_loss improved from 0.12803 to 0.12784, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0468 - accuracy: 0.9904 - val_loss: 0.1278 - val_accuracy: 0.9583\n",
      "Epoch 126/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0483 - accuracy: 0.9891\n",
      "Epoch 126: val_loss did not improve from 0.12784\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0464 - accuracy: 0.9904 - val_loss: 0.1279 - val_accuracy: 0.9583\n",
      "Epoch 127/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0463 - accuracy: 0.9897\n",
      "Epoch 127: val_loss did not improve from 0.12784\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 0.9904 - val_loss: 0.1279 - val_accuracy: 0.9583\n",
      "Epoch 128/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0458 - accuracy: 0.9903\n",
      "Epoch 128: val_loss did not improve from 0.12784\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 0.9904 - val_loss: 0.1279 - val_accuracy: 0.9583\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9904\n",
      "Epoch 129: val_loss improved from 0.12784 to 0.12783, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0452 - accuracy: 0.9904 - val_loss: 0.1278 - val_accuracy: 0.9583\n",
      "Epoch 130/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0485 - accuracy: 0.9882\n",
      "Epoch 130: val_loss improved from 0.12783 to 0.12782, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0446 - accuracy: 0.9904 - val_loss: 0.1278 - val_accuracy: 0.9583\n",
      "Epoch 131/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0458 - accuracy: 0.9900\n",
      "Epoch 131: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 132/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0464 - accuracy: 0.9918\n",
      "Epoch 132: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 133/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0426 - accuracy: 0.9927\n",
      "Epoch 133: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0438 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 134/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0501 - accuracy: 0.9880\n",
      "Epoch 134: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9904 - val_loss: 0.1280 - val_accuracy: 0.9583\n",
      "Epoch 135/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0254 - accuracy: 0.9965\n",
      "Epoch 135: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0430 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 136/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 136: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0427 - accuracy: 0.9904 - val_loss: 0.1280 - val_accuracy: 0.9583\n",
      "Epoch 137/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0451 - accuracy: 0.9895\n",
      "Epoch 137: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 138/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0450 - accuracy: 0.9893\n",
      "Epoch 138: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0415 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 139/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0409 - accuracy: 0.9900\n",
      "Epoch 139: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0413 - accuracy: 0.9904 - val_loss: 0.1283 - val_accuracy: 0.9583\n",
      "Epoch 140/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0435 - accuracy: 0.9895\n",
      "Epoch 140: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 141/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0406 - accuracy: 0.9903\n",
      "Epoch 141: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 142/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0373 - accuracy: 0.9923\n",
      "Epoch 142: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9904 - val_loss: 0.1284 - val_accuracy: 0.9583\n",
      "Epoch 143/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0384 - accuracy: 0.9929\n",
      "Epoch 143: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.9904 - val_loss: 0.1286 - val_accuracy: 0.9583\n",
      "Epoch 144/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0434 - accuracy: 0.9889\n",
      "Epoch 144: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.9904 - val_loss: 0.1286 - val_accuracy: 0.9583\n",
      "Epoch 145/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0423 - accuracy: 0.9887\n",
      "Epoch 145: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9583\n",
      "Epoch 146/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0432 - accuracy: 0.9887\n",
      "Epoch 146: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9583\n",
      "Epoch 147/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0383 - accuracy: 0.9923\n",
      "Epoch 147: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9583\n",
      "Epoch 148/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0375 - accuracy: 0.9903\n",
      "Epoch 148: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0382 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9583\n",
      "Epoch 149/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0263 - accuracy: 0.9922\n",
      "Epoch 149: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9583\n",
      "Epoch 150/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0409 - accuracy: 0.9885\n",
      "Epoch 150: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9583\n",
      "Epoch 151/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0425 - accuracy: 0.9882\n",
      "Epoch 151: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0370 - accuracy: 0.9904 - val_loss: 0.1289 - val_accuracy: 0.9643\n",
      "Epoch 152/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0426 - accuracy: 0.9878\n",
      "Epoch 152: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0366 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9643\n",
      "Epoch 153/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9902\n",
      "Epoch 153: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9643\n",
      "Epoch 154/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0377 - accuracy: 0.9897\n",
      "Epoch 154: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0360 - accuracy: 0.9904 - val_loss: 0.1287 - val_accuracy: 0.9643\n",
      "Epoch 155/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0221 - accuracy: 0.9925\n",
      "Epoch 155: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9904 - val_loss: 0.1292 - val_accuracy: 0.9643\n",
      "Epoch 156/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0401 - accuracy: 0.9889\n",
      "Epoch 156: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 157/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0319 - accuracy: 0.9930\n",
      "Epoch 157: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 158/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0370 - accuracy: 0.9889\n",
      "Epoch 158: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0345 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 159/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0305 - accuracy: 0.9960\n",
      "Epoch 159: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 160/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0375 - accuracy: 0.9882\n",
      "Epoch 160: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0341 - accuracy: 0.9904 - val_loss: 0.1291 - val_accuracy: 0.9643\n",
      "Epoch 161/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0216 - accuracy: 0.9917\n",
      "Epoch 161: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0338 - accuracy: 0.9904 - val_loss: 0.1293 - val_accuracy: 0.9643\n",
      "Epoch 162/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0340 - accuracy: 0.9922\n",
      "Epoch 162: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.9904 - val_loss: 0.1293 - val_accuracy: 0.9643\n",
      "Epoch 163/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0333 - accuracy: 0.9903\n",
      "Epoch 163: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 0.9904 - val_loss: 0.1292 - val_accuracy: 0.9643\n",
      "Epoch 164/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0324 - accuracy: 0.9926\n",
      "Epoch 164: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.9904 - val_loss: 0.1293 - val_accuracy: 0.9643\n",
      "Epoch 165/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0204 - accuracy: 0.9929\n",
      "Epoch 165: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.9904 - val_loss: 0.1296 - val_accuracy: 0.9643\n",
      "Epoch 166/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0300 - accuracy: 0.9959\n",
      "Epoch 166: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0325 - accuracy: 0.9904 - val_loss: 0.1294 - val_accuracy: 0.9643\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9904\n",
      "Epoch 167: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0320 - accuracy: 0.9904 - val_loss: 0.1295 - val_accuracy: 0.9643\n",
      "Epoch 168/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0321 - accuracy: 0.9900\n",
      "Epoch 168: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0317 - accuracy: 0.9904 - val_loss: 0.1296 - val_accuracy: 0.9643\n",
      "Epoch 169/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0177 - accuracy: 0.9932\n",
      "Epoch 169: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 0.9904 - val_loss: 0.1299 - val_accuracy: 0.9643\n",
      "Epoch 170/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0341 - accuracy: 0.9891\n",
      "Epoch 170: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.1299 - val_accuracy: 0.9643\n",
      "Epoch 171/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.0345 - accuracy: 0.9915\n",
      "Epoch 171: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0308 - accuracy: 0.9904 - val_loss: 0.1300 - val_accuracy: 0.9643\n",
      "Epoch 172/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0322 - accuracy: 0.9897\n",
      "Epoch 172: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0307 - accuracy: 0.9904 - val_loss: 0.1301 - val_accuracy: 0.9643\n",
      "Epoch 173/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0152 - accuracy: 0.9962\n",
      "Epoch 173: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.1307 - val_accuracy: 0.9643\n",
      "Epoch 174/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0316 - accuracy: 0.9922\n",
      "Epoch 174: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.9904 - val_loss: 0.1306 - val_accuracy: 0.9643\n",
      "Epoch 175/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0146 - accuracy: 0.9963\n",
      "Epoch 175: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9904 - val_loss: 0.1313 - val_accuracy: 0.9643\n",
      "Epoch 176/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0322 - accuracy: 0.9889\n",
      "Epoch 176: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.1311 - val_accuracy: 0.9643\n",
      "Epoch 177/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0316 - accuracy: 0.9889\n",
      "Epoch 177: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9904 - val_loss: 0.1312 - val_accuracy: 0.9643\n",
      "Epoch 178/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0323 - accuracy: 0.9885\n",
      "Epoch 178: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.9904 - val_loss: 0.1313 - val_accuracy: 0.9643\n",
      "Epoch 179/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0309 - accuracy: 0.9920\n",
      "Epoch 179: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 0.1314 - val_accuracy: 0.9643\n",
      "Epoch 180/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0322 - accuracy: 0.9889\n",
      "Epoch 180: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 0.1314 - val_accuracy: 0.9643\n",
      "Epoch 181/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0321 - accuracy: 0.9920\n",
      "Epoch 181: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9936 - val_loss: 0.1314 - val_accuracy: 0.9643\n",
      "Epoch 182/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0311 - accuracy: 0.9918\n",
      "Epoch 182: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9936 - val_loss: 0.1318 - val_accuracy: 0.9643\n",
      "Epoch 183/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0160 - accuracy: 0.9922\n",
      "Epoch 183: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0282 - accuracy: 0.9904 - val_loss: 0.1319 - val_accuracy: 0.9643\n",
      "Epoch 184/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0314 - accuracy: 0.9923\n",
      "Epoch 184: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 0.9936 - val_loss: 0.1319 - val_accuracy: 0.9643\n",
      "Epoch 185/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0310 - accuracy: 0.9925\n",
      "Epoch 185: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0279 - accuracy: 0.9936 - val_loss: 0.1325 - val_accuracy: 0.9643\n",
      "Epoch 186/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0161 - accuracy: 0.9959\n",
      "Epoch 186: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9936 - val_loss: 0.1329 - val_accuracy: 0.9643\n",
      "Epoch 187/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0298 - accuracy: 0.9918\n",
      "Epoch 187: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0272 - accuracy: 0.9936 - val_loss: 0.1328 - val_accuracy: 0.9643\n",
      "Epoch 188/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0316 - accuracy: 0.9918\n",
      "Epoch 188: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9936 - val_loss: 0.1328 - val_accuracy: 0.9643\n",
      "Epoch 189/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0298 - accuracy: 0.9922\n",
      "Epoch 189: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9936 - val_loss: 0.1330 - val_accuracy: 0.9643\n",
      "Epoch 190/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0145 - accuracy: 0.9961\n",
      "Epoch 190: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0266 - accuracy: 0.9936 - val_loss: 0.1333 - val_accuracy: 0.9643\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9936\n",
      "Epoch 191: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.9936 - val_loss: 0.1331 - val_accuracy: 0.9643\n",
      "Epoch 192/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0298 - accuracy: 0.9925\n",
      "Epoch 192: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9936 - val_loss: 0.1331 - val_accuracy: 0.9583\n",
      "Epoch 193/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0277 - accuracy: 0.9927\n",
      "Epoch 193: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9936 - val_loss: 0.1335 - val_accuracy: 0.9583\n",
      "Epoch 194/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0293 - accuracy: 0.9922\n",
      "Epoch 194: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 0.1336 - val_accuracy: 0.9583\n",
      "Epoch 195/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9968\n",
      "Epoch 195: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 0.1338 - val_accuracy: 0.9583\n",
      "Epoch 196/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 196: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 0.1342 - val_accuracy: 0.9583\n",
      "Epoch 197/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0291 - accuracy: 0.9923\n",
      "Epoch 197: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 0.9936 - val_loss: 0.1341 - val_accuracy: 0.9583\n",
      "Epoch 198/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0255 - accuracy: 0.9959\n",
      "Epoch 198: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0251 - accuracy: 0.9936 - val_loss: 0.1343 - val_accuracy: 0.9583\n",
      "Epoch 199/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0226 - accuracy: 0.9964\n",
      "Epoch 199: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 0.9936 - val_loss: 0.1344 - val_accuracy: 0.9583\n",
      "Epoch 200/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0285 - accuracy: 0.9925\n",
      "Epoch 200: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.9936 - val_loss: 0.1348 - val_accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1xklEQVR4nO3deXwc1ZXo8d9RS619sRZvkm3JeMPGK7IhmMUQCGY1a8AhgEMCMQkhCRMCLwv4JY/5TAYmYZhACGFLMiQOkwQCxATCapYBvGCMjRe8YlmWLcna926d90eV5LbcktqypFZ3n+/n05+uunW7+nS1dOr2rapboqoYY4yJfHHhDsAYY0z/sIRujDFRwhK6McZECUvoxhgTJSyhG2NMlLCEbowxUcISuglKRF4Ukev7u244icguETl7ANarIjLBnX5YRH4cSt0+vM81IvJyX+PsYb0LRKSkv9drBl98uAMw/UdE6gNmU4AWwO/Of11Vnwp1Xap63kDUjXaqurQ/1iMihcBOIEFVfe66nwJC/g5N7LGEHkVUNa1jWkR2AV9T1Ve61hOR+I4kYYyJHtblEgM6flKLyB0iUgY8ISLDROQFESkXkSp3uiDgNW+IyNfc6SUi8raI3OfW3Ski5/WxbpGIrBSROhF5RUQeFJH/7ibuUGL8qYi8467vZRHJDVh+rYjsFpFKEflhD9vnZBEpExFPQNmlIrLenZ4nIv8rItUisk9Efiki3m7W9aSI/L+A+dvd15SKyA1d6l4gIh+KSK2I7BGRZQGLV7rP1SJSLyKf69i2Aa8/RURWiUiN+3xKqNumJyJyvPv6ahHZKCIXByw7X0Q+cde5V0S+55bnut9PtYgcFJG3RMTyyyCzDR47RgLZwDjgJpzv/gl3fizQBPyyh9efBGwBcoF/Bx4TEelD3T8AHwA5wDLg2h7eM5QYvwR8BRgOeIGOBDMV+JW7/tHu+xUQhKq+BzQAZ3VZ7x/caT/wXffzfA74PPCNHuLGjWGhG885wESga/99A3AdkAVcANwsIpe4y053n7NUNU1V/7fLurOBvwMPuJ/t58DfRSSny2c4Ytv0EnMC8Dzwsvu6bwFPichkt8pjON136cAJwGtu+b8AJUAeMAL4AWDjigwyS+ixox24W1VbVLVJVStV9S+q2qiqdcA9wBk9vH63qv5GVf3Ab4FROP+4IdcVkbHAXOAuVW1V1beB57p7wxBjfEJVt6pqE/A0MMstvwJ4QVVXqmoL8GN3G3Tnj8BiABFJB853y1DVNar6nqr6VHUX8OsgcQTzRTe+DaragLMDC/x8b6jqx6rarqrr3fcLZb3g7AA+VdXfu3H9EdgMXBRQp7tt05OTgTTg39zv6DXgBdxtA7QBU0UkQ1WrVHVtQPkoYJyqtqnqW2oDRQ06S+ixo1xVmztmRCRFRH7tdknU4vzEzwrsduiirGNCVRvdybSjrDsaOBhQBrCnu4BDjLEsYLoxIKbRget2E2pld++F0xq/TEQSgcuAtaq6241jktudUObG8a84rfXeHBYDsLvL5ztJRF53u5RqgKUhrrdj3bu7lO0G8gPmu9s2vcasqoE7v8D1Xo6zs9stIm+KyOfc8nuBbcDLIrJDRO4M7WOY/mQJPXZ0bS39CzAZOElVMzj0E7+7bpT+sA/IFpGUgLIxPdQ/lhj3Ba7bfc+c7iqr6ic4ies8Du9uAafrZjMw0Y3jB32JAafbKNAfcH6hjFHVTODhgPX21rotxemKCjQW2BtCXL2td0yX/u/O9arqKlVdhNMd8yxOyx9VrVPVf1HV8Ti/Em4Tkc8fYyzmKFlCj13pOH3S1W5/7N0D/YZui3c1sExEvG7r7qIeXnIsMf4ZuFBETnUPYP6E3v/e/wDcirPj+J8ucdQC9SIyBbg5xBieBpaIyFR3h9I1/nScXyzNIjIPZ0fSoRyni2h8N+teAUwSkS+JSLyIXAVMxekeORbv4/Ttf19EEkRkAc53tNz9zq4RkUxVbcPZJn4AEblQRCa4x0o6yv1B38EMGEvoset+IBmoAN4D/jFI73sNzoHFSuD/AX/COV8+mPvpY4yquhH4Jk6S3gdU4Ry068kfgQXAa6paEVD+PZxkWwf8xo05lBhedD/DazjdEa91qfIN4CciUgfchdvadV/biHPM4B33zJGTu6y7ErgQ51dMJfB94MIucR81VW0FLsb5pVIBPARcp6qb3SrXArvcrqelwJfd8onAK0A98L/AQ6r6xrHEYo6e2HELE04i8idgs6oO+C8EY6KdtdDNoBKRuSJynIjEuaf1LcLpizXGHCO7UtQMtpHAX3EOUJYAN6vqh+ENyZjoYF0uxhgTJazLxRhjokTYulxyc3O1sLAwXG9vjDERac2aNRWqmhdsWdgSemFhIatXrw7X2xtjTEQSka5XCHeyLhdjjIkSISV0EVkoIltEZFuwMRpEJFNEnheRj9zhNr/S/6EaY4zpSa8J3R0I6UGcK8emAovdoUkDfRP4RFVn4lxp9x/SzXjRxhhjBkYofejzgG2qugNARJbjXAzySUAdBdLdcRzSgIOA3RHHmCGmra2NkpISmpube69swiopKYmCggISEhJCfk0oCT2fw4cALcG5gUGgX+KMGleKM+DQVV2G3wRARG7CubkCY8d2HXjOGDPQSkpKSE9Pp7CwkO7vT2LCTVWprKykpKSEoqKikF8XSh96sG+969VI5wLrcMZSngX8UkQyggT5iKoWq2pxXl7Qs26MMQOoubmZnJwcS+ZDnIiQk5Nz1L+kQknoJRw+pnMBTks80FeAv6pjG87dyqccVSTGmEFhyTwy9OV7CiWhrwIminNzXy9wNUfeNuwznPssIiIjcG5KsOOoownBlrI67n1pM1UNrQOxemOMiVi9JnRV9QG3AC8Bm4CnVXWjiCwVkaVutZ8Cp4jIx8CrwB3HOi5zd3ZWNPDg69vZW900EKs3xgygyspKZs2axaxZsxg5ciT5+fmd862tPTfSVq9eza233trre5xyyin9Eusbb7zBhRde2C/rGiwhXSmqqitw7pASWPZwwHQp8IX+DS243DTnbMiD1kI3JuLk5OSwbt06AJYtW0ZaWhrf+973Opf7fD7i44OnpeLiYoqLi3t9j3fffbdfYo1EEXelaHaqk9ArG7q7yY0xJpIsWbKE2267jTPPPJM77riDDz74gFNOOYXZs2dzyimnsGXLFuDwFvOyZcu44YYbWLBgAePHj+eBBx7oXF9aWlpn/QULFnDFFVcwZcoUrrnmGjpGl12xYgVTpkzh1FNP5dZbb+21JX7w4EEuueQSZsyYwcknn8z69esBePPNNzt/YcyePZu6ujr27dvH6aefzqxZszjhhBN46623+n2bdSfixkPPSUsEoLLeWujGHIv/+/xGPimt7dd1Th2dwd0XTTvq123dupVXXnkFj8dDbW0tK1euJD4+nldeeYUf/OAH/OUvfzniNZs3b+b111+nrq6OyZMnc/PNNx9xzvaHH37Ixo0bGT16NPPnz+edd96huLiYr3/966xcuZKioiIWL17ca3x33303s2fP5tlnn+W1117juuuuY926ddx33308+OCDzJ8/n/r6epKSknjkkUc499xz+eEPf4jf76exsfGot0dfRVxCz0iKJ8EjVFqXizFR48orr8Tj8QBQU1PD9ddfz6effoqI0NbWFvQ1F1xwAYmJiSQmJjJ8+HD2799PQUHBYXXmzZvXWTZr1ix27dpFWloa48eP7zy/e/HixTzyyCM9xvf222937lTOOussKisrqampYf78+dx2221cc801XHbZZRQUFDB37lxuuOEG2trauOSSS5g1a9axbJqjEnEJXUTITvVy0FroxhyTvrSkB0pqamrn9I9//GPOPPNMnnnmGXbt2sWCBQuCviYxMbFz2uPx4PMdeXF6sDp9ualPsNeICHfeeScXXHABK1as4OSTT+aVV17h9NNPZ+XKlfz973/n2muv5fbbb+e666476vfsi4jrQwfITk20PnRjolRNTQ35+fkAPPnkk/2+/ilTprBjxw527doFwJ/+9KdeX3P66afz1FNPAU7ffG5uLhkZGWzfvp3p06dzxx13UFxczObNm9m9ezfDhw/nxhtv5Ktf/Spr167t98/QnYhroQPkpHqty8WYKPX973+f66+/np///OecddZZ/b7+5ORkHnroIRYuXEhubi7z5s3r9TXLli3jK1/5CjNmzCAlJYXf/va3ANx///28/vrreDwepk6dynnnncfy5cu59957SUhIIC0tjd/97nf9/hm6E7Z7ihYXF2tfb3Dx7eUf8uFn1az8/pn9HJUx0W3Tpk0cf/zx4Q4j7Orr60lLS0NV+eY3v8nEiRP57ne/G+6wjhDs+xKRNaoa9PzNCO1y8dp56MaYPvvNb37DrFmzmDZtGjU1NXz9618Pd0j9IiK7XHLTEqlv8dHc5icpwRPucIwxEea73/3ukGyRH6uIbaGDXS1qjDGBIjKh53RcLWqnLhpjTKfITOhpdvm/McZ0FZEJPTvVLv83xpiuIi+hl33M6A/uIYs660M3JsIsWLCAl1566bCy+++/n2984xs9vqbjFOfzzz+f6urqI+osW7aM++67r8f3fvbZZ/nkk0O3Qr7rrrt45ZVXjiL64IbSMLuRl9CrdpP4wYMUeiqosC4XYyLK4sWLWb58+WFly5cvD2mALHBGSczKyurTe3dN6D/5yU84++yz+7SuoSryEnr6KACOS26w8VyMiTBXXHEFL7zwAi0tTmNs165dlJaWcuqpp3LzzTdTXFzMtGnTuPvuu4O+vrCwkIoK594599xzD5MnT+bss8/uHGIXnHPM586dy8yZM7n88stpbGzk3Xff5bnnnuP2229n1qxZbN++nSVLlvDnP/8ZgFdffZXZs2czffp0brjhhs74CgsLufvuu5kzZw7Tp09n8+bNPX6+cA+zG3nnoaePAKDQW8vaemuhG9NnL94JZR/37zpHTofz/q3bxTk5OcybN49//OMfLFq0iOXLl3PVVVchItxzzz1kZ2fj9/v5/Oc/z/r165kxY0bQ9axZs4bly5fz4Ycf4vP5mDNnDieeeCIAl112GTfeeCMAP/rRj3jsscf41re+xcUXX8yFF17IFVdccdi6mpubWbJkCa+++iqTJk3iuuuu41e/+hXf+c53AMjNzWXt2rU89NBD3HfffTz66KPdfr5wD7MbUgtdRBaKyBYR2SYidwZZfruIrHMfG0TELyLZxxxdMGlOQi9IqKXCWujGRJzAbpfA7pann36aOXPmMHv2bDZu3HhY90hXb731FpdeeikpKSlkZGRw8cUXdy7bsGEDp512GtOnT+epp55i48aNPcazZcsWioqKmDRpEgDXX389K1eu7Fx+2WWXAXDiiSd2DujVnbfffptrr70WCD7M7gMPPEB1dTXx8fHMnTuXJ554gmXLlvHxxx+Tnp7e47pD0WsLXUQ8wIPAOUAJsEpEnlPVzq2tqvcC97r1LwK+q6oHjzm6YDwJkJLLyLhqDtQ1D8hbGBMTemhJD6RLLrmE2267jbVr19LU1MScOXPYuXMn9913H6tWrWLYsGEsWbKE5uae/79FJGj5kiVLePbZZ5k5cyZPPvkkb7zxRo/r6W08q44heLsbore3dQ3mMLuhtNDnAdtUdYeqtgLLgUU91F8M/PGYoupN+kjyqKKivhV/e3gGFzPG9E1aWhoLFizghhtu6Gyd19bWkpqaSmZmJvv37+fFF1/scR2nn346zzzzDE1NTdTV1fH88893Lqurq2PUqFG0tbV1DnkLkJ6eTl1d3RHrmjJlCrt27WLbtm0A/P73v+eMM87o02cL9zC7ofSh5wN7AuZLgJOCVRSRFGAhcEs3y28CbgIYO3bsUQV6mPSRZB0oxd+uVDW2kpuW2PtrjDFDxuLFi7nssss6u15mzpzJ7NmzmTZtGuPHj2f+/Pk9vn7OnDlcddVVzJo1i3HjxnHaaad1LvvpT3/KSSedxLhx45g+fXpnEr/66qu58cYbeeCBBzoPhgIkJSXxxBNPcOWVV+Lz+Zg7dy5Lly7t0+cK9zC7vQ6fKyJXAueq6tfc+WuBear6rSB1rwK+rKoX9fbGxzJ8Ls9+k+bNLzOl+n5W3HoaU0dn9G09xsQYGz43sgzE8LklwJiA+QKgtJu6VzPQ3S0A6SNJbKkkjnbK7UwXY4wBQkvoq4CJIlIkIl6cpP1c10oikgmcAfytf0MMIn0kon6yqeNArR0YNcYYCKEPXVV9InIL8BLgAR5X1Y0istRd/rBb9VLgZVVtGLBoO7inLo6QKmuhG3OUVLXbM0TM0NGXu8mFdGGRqq4AVnQpe7jL/JPAk0cdQV+4V4uO9dZyoNYSujGhSkpKorKykpycHEvqQ5iqUllZSVJS0lG9LvKuFIXOq0WPS6pjZ50ldGNCVVBQQElJCeXl5eEOxfQiKSmJgoKCo3pNZCZ0t8tlTEItH1hCNyZkCQkJFBUVhTsMM0Aib3AugPhESM5mlKfGrhY1xhhXZCZ0gPRRjKCKcmuhG2MMEMkJPWMUw/wVNLT6aWjpeXwFY4yJBRGc0EeT0XYAwFrpxhhDRCf0fJJaKknAx367uMgYYyI5oY8GnIuLyiyhG2NMJCf0fABGUmktdGOMIQoSemFCNftqLKEbY0wEJ3Sny2VCcq210I0xhkhO6EkZ4E23FroxxrgiN6EDZIxmtBxkvyV0Y4yJ/ISeq5Xsr2uxe4saY2JehCf0fLLayvG3K5U2LroxJsZFeEIfTXJLOfH47Fx0Y0zMi/iELih51NiBUWNMzAspoYvIQhHZIiLbROTObuosEJF1IrJRRN7s3zC74Z6LPkrs4iJjjOn1Bhci4gEeBM4BSoBVIvKcqn4SUCcLeAhYqKqficjwAYr3cFljASj0VFgL3RgT80Jpoc8DtqnqDlVtBZYDi7rU+RLwV1X9DEBVD/RvmN3IGgPA5KQqO3XRGBPzQkno+cCegPkStyzQJGCYiLwhImtE5LpgKxKRm0RktYis7pd7GnpTISWX8QmVlNY0Hfv6jDEmgoWS0IPdGrzrSd/xwInABcC5wI9FZNIRL1J9RFWLVbU4Ly/vqIMNKmssBVJBabW10I0xsS2UhF4CjAmYLwBKg9T5h6o2qGoFsBKY2T8h9iJrLCPa97Ovpol2u7jIGBPDQknoq4CJIlIkIl7gauC5LnX+BpwmIvEikgKcBGzq31C7kTWWzJYyfH4/5XZxkTEmhvV6louq+kTkFuAlwAM8rqobRWSpu/xhVd0kIv8A1gPtwKOqumEgA+80bBwebSOPGkqqmhiRkTQob2uMMUNNrwkdQFVXACu6lD3cZf5e4N7+Cy1EWeMAKJBy9lY3ceK4YYMegjHGDAWRfaUodJ6LXiDl7K2yM12MMbEr8hN6pnO8dmLiQfZWN4Y5GGOMCZ/IT+jeFEjNY6K30lroxpiYFvkJHSBrLOPiKthbbQndGBO7oiOhDytipL+MvVVNqNq56MaY2BQdCT27iMzWMlpbW6hpagt3NMYYExZRktDHE0c7+VJOifWjG2NiVNQkdIBC2U9JlZ3pYoyJTVGV0MfJfj47aAndGBOboiOhp+ZBQiqTEg6wu9ISujEmNkVHQheB7PFMSqiwFroxJmZFR0IHyC5iDGWW0I0xMSuKEvp48nz72FfVgM/fHu5ojDFm0EVRQi/Coz6Gq929yBgTm6IooXec6VLG7oMNYQ7GGGMGX/Qk9JyJAIyXfXamizEmJoV0g4uIkD4S9aYzub3UDowaY2JSSC10EVkoIltEZJuI3Blk+QIRqRGRde7jrv4PtdcgkbxJHJ9QxmfWQjfGxKBeW+gi4gEeBM4BSoBVIvKcqn7SpepbqnrhAMQYutxJFJX9k12V1odujIk9obTQ5wHbVHWHqrYCy4FFAxtWH+VOJNtfQXllOe3tNoyuMSa2hJLQ84E9AfMlbllXnxORj0TkRRGZFmxFInKTiKwWkdXl5eV9CLcXuZMByPftpbTGRl00xsSWUBK6BCnr2vxdC4xT1ZnAfwHPBluRqj6iqsWqWpyXl3dUgYYkdxIAE2Qv28ut28UYE1tCSeglwJiA+QKgNLCCqtaqar07vQJIEJHcfosyVNlFaFw8x8WVsv1A/aC/vTHGhFMoCX0VMFFEikTEC1wNPBdYQURGioi40/Pc9Vb2d7C98iRA9ngmx5exo8ISujEmtvR6louq+kTkFuAlwAM8rqobRWSpu/xh4ArgZhHxAU3A1Rqmm3tK7iSmVK3jsQPW5WKMiS0hXVjkdqOs6FL2cMD0L4Ff9m9ofTRiGqM3r2Bv+eD/QDDGmHCKnkv/OwyfShztZNTvoK7ZbhhtjIkd0ZfQRzhnTE6J28MOO9PFGBNDoi+hZ4+n3ZPEZNnDNjvTxRgTQ6Ivocd5YPgUpsbtYeuBunBHY4wxgyb6EjoQN+IEjvfsYWuZJXRjTOyIyoTOiKlkazUHykrCHYkxxgya6Ezow6cCkFn3KbV2posxJkZEZ0IfOR2AqbKbT/dbt4sxJjZEZ0JPzcWXNprpcTvZUmZnuhhjYkN0JnTAkz+bGXE72WotdGNMjIjahC6jZ1Mk+/istCzcoRhjzKCI2oTO6FkAyP71hGmcMGOMGVTRm9BHzQSgsPVTSmuawxyMMcYMvOhN6GnDaU0ZyfS4nWzYWxPuaIwxZsBFb0LHPTAqO9loCd0YEwOiO6GPKWZ83D527rErRo0x0S+qEzpjTgIgft/qMAdijDEDL6SELiILRWSLiGwTkTt7qDdXRPwickX/hXgM8ufQLh6Kmj/hQJ0dGDXGRLdeE7qIeIAHgfOAqcBiEZnaTb2f4dx7dGjwptI07HhOlE/ZWFob7miMMWZAhdJCnwdsU9UdqtoKLAcWBan3LeAvwIF+jO+YJRSezMy47az/rCLcoRhjzIAKJaHnA3sC5kvcsk4ikg9cCjxMD0TkJhFZLSKry8vLjzbWPvEWnkyaNFO5Y92gvJ8xxoRLKAldgpR1vfTyfuAOVfX3tCJVfURVi1W1OC8vL8QQj9GYeQAkl622K0aNMVEtPoQ6JcCYgPkCoLRLnWJguYgA5ALni4hPVZ/tjyCPSdZYGpJGMaPhY3ZVNlKUmxruiIwxZkCE0kJfBUwUkSIR8QJXA88FVlDVIlUtVNVC4M/AN4ZEMgcQwTd2PifHbeLD3ZXhjsYYYwZMrwldVX3ALThnr2wCnlbVjSKyVESWDnSA/SF9ypnkSB17t64LdyjGGDNgQulyQVVXACu6lAU9AKqqS449rP4VV3QaAAl73gYuDm8wxhgzQKL7StEOw8ZRkziKorq11Lf4wh2NMcYMiNhI6EBz/nxOivuE1TsG53RJY4wZbDGT0LOmn0uWNPDZhnfCHYoxxgyImEnoiZPPph3Bu+u1cIdijDEDImYSOinZlKVNZXLdBzS2Wj+6MSb6xE5CB1oLz2KGbGP91p3hDsUYY/pdTCX04XMuwCPKgXUvhjsUY4zpdzGV0FMK51ETl0nGZ6+EOxRjjOl3MZXQifNQMvxMTmz5gPIqGx/dGBNdYiuhAykzLyNdmvj0vefDHYoxxvSrmEvo44oXUk8Kns2W0I0x0SXmEnpcQiKbM05hcs1b+Ntawx2OMcb0m5hL6AD+4y8hi3p2vG+tdGNM9IjJhH786ZdRrak0r/1TuEMxxph+E5MJPSM1lQ/TF3DcwTfRlvpwh2OMMf0iJhM6gG/a5aTQTOn7fwl3KMYY0y9iNqHPnH8eJZqLb81/hzsUY4zpFyEldBFZKCJbRGSbiNwZZPkiEVkvIutEZLWInNr/ofav4RkpvJN+PuNqPkArd4Q7HGOMOWa9JnQR8QAPAucBU4HFIjK1S7VXgZmqOgu4AXi0n+McEN651+HTOCpW/ibcoRhjzDELpYU+D9imqjtUtRVYDiwKrKCq9aqq7mwqoESABcUzeV3nkLLxj+BrCXc4xhhzTEJJ6PnAnoD5ErfsMCJyqYhsBv6O00o/gojc5HbJrC4vD/+t4Ialelk/6kpSfVW0f2SnMBpjIlsoCV2ClB3RAlfVZ1R1CnAJ8NNgK1LVR1S1WFWL8/LyjirQgTL5lIvY2D6Opjd/Ae3t4Q7HGGP6LJSEXgKMCZgvAEq7q6yqK4HjRCT3GGMbFF+YNoo/xF9Kau0O2GrjpBtjIlcoCX0VMFFEikTEC1wNPBdYQUQmiIi403MAL1DZ38EOBG98HNnzvshnmkfLmz8HjYjuf2OMOUKvCV1VfcAtwEvAJuBpVd0oIktFZKlb7XJgg4iswzkj5qqAg6RD3tUnj+cx3wUk7lsNn70X7nCMMaZP4kOppKorgBVdyh4OmP4Z8LP+DW3w5GclUz7hCqp2/5XMt39B3LjPhTskY4w5ajF7pWhXXzxlMk+0fYG4T1+CkjXhDscYY46aJXTX6RPzeCnjcqrihsGL37czXowxEccSuisuTrjs5Mn8pPlq2LsaPvpjuEMyxpijYgk9wNXzxvJqwhnsSJoGryyD5ppwh2SMMSGzhB4gMzmBL3+uiO/ULkYbyuHNfw93SMYYEzJL6F3ccGoRW+Im8H7W+fD+w7B/Y7hDMsaYkFhC7yI3LZEl8wv5xv6L8Hkz4dlvgN8X7rCMMaZXltCD+MaCCWhyDv+VfDPsWwfv/CLcIRljTK8soQeRmZzAd86exH/um0rZmPPhjX+DktXhDssYY3pkCb0bXzppLOPzUrmx6ho0fRT8+QZoqg53WMYY0y1L6N1I8MTxf847no8rhBen3AO1pfD0deBvC3doxhgTlCX0Hpx9/HDmT8jhjveSqD77Ptj5JrzwXRuR0RgzJFlC74GI8K+XTsfXrnx781T09Nvhw9/D23aQ1Bgz9FhC78W4nFTuWDiZN7eW8z/p18H0K+HV/wtrngx3aMYYcxhL6CG47nOFzCvK5qcvbGLfgvtgwjnw/Lfh/V+HOzRjjOlkCT0EcXHCvVfMwNeu3P7MFvxf/G+YcqEzKuPb94c7PGOMASyhh2xcTirLLp7K29sq+PdXd8KVT8IJl8Mrd8M/77Lhdo0xYRdSQheRhSKyRUS2icidQZZfIyLr3ce7IjKz/0MNv6vmjuXLJ4/l12/u4PkN5XDZb6D4q/DOf8L/XGejMxpjwqrXhC4iHpz7hJ4HTAUWi8jULtV2Ameo6gzgp8Aj/R3oUHHXhdOYWziM2//8EZ+UNcAF/wHn/itsXgEPn2Z3OzLGhE0oLfR5wDZV3aGqrcByYFFgBVV9V1Wr3Nn3gIL+DXPo8MbH8eA1c8hK9vK1366itKYZPvdNuOEfzvnpj3/B6Vdv94c7VGNMjAkloecDewLmS9yy7nwVeDHYAhG5SURWi8jq8vLy0KMcYoanJ/HYkmLqmn1c+9j7HGxohTHzYOlKmHy+06/+6Oeh7ONwh2qMiSGhJHQJUhb0UkkRORMnod8RbLmqPqKqxapanJeXF3qUQ9C00Zk8tmQuJVVNLHniA+qa2yB5GHzxd3DF41BTAr8+A17+sfWtG2MGRSgJvQQYEzBfAJR2rSQiM4BHgUWqWtk/4Q1t84qy+dWX5/BJaS1f/e1q6lt8IOKc/fLND2DWYnj3AfjPmfDuf0Fbc7hDNsZEsVAS+ipgoogUiYgXuBp4LrCCiIwF/gpcq6pb+z/MoeusKSP4+VWzWLO7imsefZ+qhlZnQUo2LHoQbnoTRs+Bl38ED8yGdx6A5trwBm2MiUq9JnRV9QG3AC8Bm4CnVXWjiCwVkaVutbuAHOAhEVknIjE1ePjFM0fz8JdPZNO+Wq565H/ZXxvQEh89C679K1z/POQcB//8MfximpPgD+4IW8zGmOgjGqaRA4uLi3X16ujK++9ur+DG365mWKqXR68vZsrIjCMr7V3rdL988jdQP4w/E05cApPOhYTkQY/ZGBNZRGSNqhYHXWYJvX+tL6nma25/+n9cOZPzpo8KXrG2FNb+Htb+DmpLwJsGkxbC1EUw8RxL7saYoCyhD7L9tc18/fdrWLenmlvPmsC3z56EJy7YyUI456vveAM+eRY2vQBNByEh1WmxT10Ex50FSUFa+saYmGQJPQya2/z86NkN/HlNCScVZfOLq2YxOquXVrffB7vecpP789BYCeKBgmInsY8/E/JPBE/8oHwGY8zQYwk9TFSVv6zdy11/20CCJ46fXT6dhSd00wXTld8He96D7a/B9teh9ENAna6ZgmIYczKMPQkK5kJi+oB+DmPM0GEJPcx2VjRw6x8/5OO9NVwwYxR3XzSV4elJR7eSxoPOLfB2vQ2fvQ/7NwAKEge5k2DECTByuvuYAWmRfeGWMSY4S+hDQKuvnV+/uZ3/em0bSQlx/OD84/li8Rjiuutb701zLZSsgj0fQNl6Z5iBmoARGtJGwsiAJJ87GYYVQmJav3weY0x4WEIfQraX1/ODv37M+zsPckJ+Bj+6YConj8/pn5U3HnRa7mUfQ5n7XL4J2n2H6qTmwbAiJ7lnu88d8+kjnStdjTFDliX0IUZVee6jUn724mZKa5r5wtQRfO/cyUwaMQB94b4WKN8CldugahdU7YSDO6Fqt3O6pAbcmMOTCJn5kJEPmWOc6cwCyChwnjPzrb/emDCzhD5ENbf5efStHfzqje00tvm5YPoovv35iUwciMQejK/V6aY5uNNJ9NWfOYOK1e51nuv2HZ7wAZIynQSfNhxScgIe2ZCa26UsBzwJg/NZjIkRltCHuKqGVn7z1g6efHcXTW1+zjl+BF8/YzwnjssOb2B+n5PUOxJ8YLJvKHdOq2ys7Hk0ycRMJ9l3Tf5HlOU4o1UmZUJ84uB9RmMijCX0CHGwoZXH397J79/bTU1TGyeOG8Z1nxvHwhNGkhjvCXd43fO3Of33HQm+sRIaKw6VNVQ4F0w1Vh4qa2vsfn3xSU5iT8xwungCH940dzrNWX7YvLs8IQW8qc5zfKIdFzB9094OrXWHblaj7c5NbNTvlGk7tLc5v3T9Le6zO+1vc7o7/W6Zzy1ra4SmKhg3HyZ9oU9hWUKPMA0tPv5n9R4ef2cXnx1sJDvVyxUnFrB43liKclPDHV7/aG10k3zAjqCpymntBz5a6qC13nluqYeWWmc+8EBvT8RzKLl7OxJ9qvPsTQmyA/CCp+ORcJTT3SyPph2KqpPM2tvc70AgLt59eA7/rKrO9+Vvc06vBedZ4gANSJDtRz7a/dDWBL5mp36cx3mvjgTZmShbeyhzk2hgQg0sa/c57+9rhtYG56F+55dpY4XTENEBuPOYxwvzvwNn/bBPL7eEHqHa25W3t1Xwh/c/45+b9uNvV045LocvnTSWc6aOGNqt9oGk6vyDttQ5LajOZO8m/7ZGZ4fR5v6TBp1udOYDy31NAxNvXELwRB/ncZObOImwczrOva1Mx7T0UE8Olbf7DiW0zmMfAf/fh/2va/Cydp+T0Np9TtL2tx2ewEPZkXYk7Y7kHG5x8UfudOO9zs5exPlF6E1zdvAdO6eUHOeMsOQsZ77rd9Hx3cUluI2AROfXYOD6PR3lAdMJSU7D4Rh28pbQo8CB2maeXr2HP36wh73VTaQnxnPOtBFcNGM08yfk4o0PZWh70yPVQ603f2uX6WBlQaZ9Lb3X6Wgptvs5rKWKui1Wt4yu04H12g/V7agX5zmUVCTg7+Gw5CFBygPKPAkBLe74w+c7p92dUZw7BEVHou/aykachBif1KU17j+U9DsfcmRZQvKh13ZsK09il4SZEKSsyyMuuv43LKFHEX+78s62Cp7/qJR/bCyjrtlHemI8Z04ZzhemjeCMSXmkJ9mZJcZEK0voUarF5+ftTyt4aWMZr2w6wMGGVryeOE6ZkMMXpo7k7OOHMzzjKIcYMMYMaZbQY4C/XVmzu4qXN5bx0idl7Dno9AfPGpPF6RNzmT8hl9ljh1nXjDER7pgTuogsBP4T8ACPquq/dVk+BXgCmAP8UFXv622dltAHjqqyZX8d/9y4n1c3H2B9STXtCileDycVZTN/Qi6nTsxl8oh0JJrOwDAmBhxTQhcRD7AVOAcowblp9GJV/SSgznBgHHAJUGUJfWipaWrjvR2VvLOtgre3VbCjvAGA3LRE5k/IYf6EXE4qymZsdooleGOGuJ4Seih3SpgHbFPVHe7KlgOLgM6ErqoHgAMickE/xGv6WWZyAudOG8m500YCUFrdxDvbKtwEX8nf1pUCMDw9kblF2cwrzKa4cBhTRmZ0f6clY8yQE0pCzwcCxmWlBDipL28mIjcBNwGMHTu2L6sw/WB0VjJXFo/hyuIxqCpb99ezatdB57HzIH9fvw+A9MR4TiwcRvG4Ycwck8WM/CwyU+wMGmOGqlASerAmWp+OpKrqI8Aj4HS59GUdpn+JCJNHpjN5ZDpfPnkcAHurm1i182Bnkr9vS3ln/aLcVGYUZDI933lMy88kLdFuiWfMUBDKf2IJMCZgvgAoHZhwzFCQn5VM/ux8LpmdDzh98Bv21rBuTzUf7anmvR2HumlEnCR/wmgnwZ+Qn8m0/Awy7Fx4YwZdKAl9FTBRRIqAvcDVwJcGNCozpGQmJzB/gnPqY4cDtc1sKK3h45JaNpTWsGrXQZ776NB+Pj8rmUkj0pg8MoM5Y7OYNSaLvPREO+hqzAAK9bTF84H7cU5bfFxV7xGRpQCq+rCIjARWAxlAO1APTFXV2u7WaWe5RJ+K+hY27K1hY2ktW/fXsXV/PdsP1NPqd8bzyExOYNKINCYMT2N8bhrj81IZn5fGmGHJxHvs/HhjQmEXFpmwaW7z8/HeGjburWHrgXq2ltWxvbyeqsa2zjoJHmFsdgrj89wkn5vKcXlpTByRTmaydd0YE+hYT1s0ps+SEjzMLcxmbuHhN+uoamhlR0U928sb2FnRwI7yenaUN/DmlvLOFj3AiIxEJg5PpzA3hXHZqYzLSWFcTipjs1NI9sboaJPGdMMSugmLYaleTkzNPuKuTP52paSqke3l9WzdX8/W/XVsP1DPC+v3UR3Qqgcn2Y/LSaXQTfLjctykn5tiB2VNTLKEboYUT5y4yTmVs6aMOGxZdWMruysb2X2wkd0VDc5zZQNvbCnnQF3JYXWHpSQcSvKdSd+Zzkn12sFZE5UsoZuIkZXiJSvFy8wxWUcsa2z18dnBRnZVOEm+I9mv2V3F8x+V0h5wqCjF62FkZhKjMpMYkeE8j8xMZlRGUmd5tiV9E4EsoZuokOKNZ8rIDKaMzDhiWauvnZKqRnZXNrKrsoE9B5vYX9vMvpom3tteyf66Fvzth58c4I2PY2RAgh+ZmeQcuM1NIyM5nmEpXoanJ9rZOWZIsYRuop43Ps49gyYt6HJ/u1JZ38K+mmb21TRTVtPEvtpmytz5Dz+rpqym+bCDteB0D43MSCIvPZHM5AQKhiVTMCyFnFQv2alehrnP2aleMpLircVvBpwldBPzPHHC8IwkhmckMXNM8Drt7cq+2mZ2VTRQ1+zjYEMrpdVNlFY3UV7fwsGGVtbtqaamqS3o6xM8Qk5qIrnpXvLSEslNSyQ3PdGZTk8kN80pd7qVEkiwlr/pA0voxoQgLk6cIRGyknus19Dio6qxlYMNzqOqsZXK+lYqG1qpqGuhor6FivpWNu2ro7KhhTZ/8OtA0hLjyUpJYJib4LNSvGQlJzCsYzrIsnZVPHFCZnKC/RqIUZbQjelHqYnxpCbGUzAspde6qkpNUxsV9S0cqGuhsr6V6sZWqhrbqGpspcZ9rmpso6SqySlraqO3awHTEuPJTE4gLTGeUVlJZCYnkBgfR2K8h6yUBIanJzI8I4mcVC/pSQmkJ8WTneolKcHO6490ltCNCRMR6TxzZ8Lw9JBe429X6prbjkj61Y1txAn42pWSqibqmn3UNrdRWt3EzooGWn3tNLf5qW32HXEAuEN6UjwZSQmkeD2kJsaTnuQ80hLjOxN/elIC6Z3LnLI0t15GkrPjsF8H4WMJ3ZgI4ok7tBMoIvWoX+9vVyobWthf00JVYyt1zT7qmtuobGilvK6F+hYfDS0+6t1HWU1zZ52GVn+v64+PE1ITnZ1AaqKnc7pjJ5Hm/oJJS4wnNaAsJTGeNLd+qvdQPbsH7tGxhG5MDPHECcPTkxiennTUr/W3K/UtTnJ3kryP+pY299eAU17f3LFD8NPQ4qOh1dkxHKh1dxatzvLujh105fXEkZroIcV75E6i687C51caWn0Md886AkhPOvy4Q7LXg9+vpCR6SIyPvi4mS+jGmJB0HHDtjwHTWnx+Gtyk3/GroKG1y3zgjqFzZ+Cnrtn55dBZt9Xf2Y3kjY+j1dfey7s7khLiyExOICnBg9cThzc+rvMgc3ycECfOI8XrIcXrIdnrIdUbT7LXQ3KCp/M5KWA6OcFDkjcOr8fpekpPjCduEG/jaAndGDPoEuOdFnJ2qveY16WqtPjaiRMhwSPUNDm/GsC5OUtNUxvVjW1UN7XS1OonToTGVl/nsua2dtr87bT42qlqbGVfdS3tqrQr+PztNLX5aWz10xLijiJQRxdUU6ufBI+4XUvxXHPSWL522vhj/uxHvF+/r9EYYwaRiBx2hk7HMQY4/FZrx8rfrjS1+Wlq9dPsJvnA+Y7ppjY/rb522lU52NBKQ4uPZG88bf52GludXx25aYn9GNkhltCNMSYEnjghzW1hD1V2CNkYY6JESAldRBaKyBYR2SYidwZZLiLygLt8vYjM6f9QjTHG9KTXhC4iHuBB4DxgKrBYRKZ2qXYeMNF93AT8qp/jNMYY04tQWujzgG2qukNVW4HlwKIudRYBv1PHe0CWiIzq51iNMcb0IJSEng/sCZgvccuOtg4icpOIrBaR1eXl5UcbqzHGmB6EktCDnRXf9TKvUOqgqo+oarGqFufl5YUSnzHGmBCFktBLOPx0zgKgtA91jDHGDKBQEvoqYKKIFImIF7gaeK5LneeA69yzXU4GalR1Xz/Haowxpge9niGvqj4RuQV4CfAAj6vqRhFZ6i5/GFgBnA9sAxqBr/S23jVr1lSIyO4+xp0LVPTxtQNtqMZmcR2doRoXDN3YLK6j09e4xnW3QLS30fKHIBFZrarF4Y4jmKEam8V1dIZqXDB0Y7O4js5AxGVXihpjTJSwhG6MMVEiUhP6I+EOoAdDNTaL6+gM1bhg6MZmcR2dfo8rIvvQjTHGHClSW+jGGGO6sIRujDFRIuISem9D+Q5iHGNE5HUR2SQiG0Xk2275MhHZKyLr3Mf5YYhtl4h87L7/arcsW0T+KSKfus/DwhDX5IDtsk5EakXkO+HYZiLyuIgcEJENAWXdbiMR+T/u39wWETl3kOO6V0Q2u0NTPyMiWW55oYg0BWy3hwc5rm6/t8HaXj3E9qeAuHaJyDq3fFC2WQ/5YWD/xlQ1Yh44FzZtB8YDXuAjYGqYYhkFzHGn04GtOMMLLwO+F+bttAvI7VL278Cd7vSdwM+GwHdZhnORxKBvM+B0YA6wobdt5H6vHwGJQJH7N+gZxLi+AMS70z8LiKswsF4YtlfQ720wt1d3sXVZ/h/AXYO5zXrIDwP6NxZpLfRQhvIdFKq6T1XXutN1wCaCjDA5hCwCfutO/xa4JHyhAPB5YLuq9vVq4WOiqiuBg12Ku9tGi4DlqtqiqjtxroieN1hxqerLqupzZ9/DGStpUHWzvbozaNurt9hERIAvAn8cqPfvJqbu8sOA/o1FWkIPaZjewSYihcBs4H236Bb35/Hj4ejawBnp8mURWSMiN7llI9QdX8d9Hh6GuAJdzeH/ZOHeZtD9NhpKf3c3AC8GzBeJyIci8qaInBaGeIJ9b0Npe50G7FfVTwPKBnWbdckPA/o3FmkJPaRhegeTiKQBfwG+o6q1OHdrOg6YBezD+bk32Oar6hycO0l9U0ROD0MM3RJnkLeLgf9xi4bCNuvJkPi7E5EfAj7gKbdoHzBWVWcDtwF/EJGMQQypu+9tSGwv12IObzgM6jYLkh+6rRqk7Ki3WaQl9CE1TK+IJOB8WU+p6l8BVHW/qvpVtR34DQP4U7M7qlrqPh8AnnFj2C/uXaTc5wODHVeA84C1qrofhsY2c3W3jcL+dyci1wMXAteo2+nq/jyvdKfX4PS7ThqsmHr43sK+vQBEJB64DPhTR9lgbrNg+YEB/huLtIQeylC+g8Ltm3sM2KSqPw8oD7z13qXAhq6vHeC4UkUkvWMa54DaBpztdL1b7Xrgb4MZVxeHtZrCvc0CdLeNngOuFpFEESnCuXfuB4MVlIgsBO4ALlbVxoDyPHHu+YuIjHfj2jGIcXX3vYV1ewU4G9isqiUdBYO1zbrLDwz039hAH+0dgKPH5+McMd4O/DCMcZyK85NoPbDOfZwP/B742C1/Dhg1yHGNxzla/hGwsWMbATnAq8Cn7nN2mLZbClAJZAaUDfo2w9mh7APacFpHX+1pGwE/dP/mtgDnDXJc23D6Vzv+zh52617ufscfAWuBiwY5rm6/t8HaXt3F5pY/CSztUndQtlkP+WFA/8bs0n9jjIkSkdblYowxphuW0I0xJkpYQjfGmChhCd0YY6KEJXRjjIkSltCNMSZKWEI3xpgo8f8BfzUK2lhMOJEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_3 (GRU)                 (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.9583\n",
      "Test Loss: 0.13477586209774017\n",
      "Test Accuracy: 0.9583333134651184\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_RMSprop.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with RMSprop optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001) \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 layer with dropout Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.7472 - accuracy: 0.4947\n",
      "Epoch 1: val_loss improved from inf to 0.68650, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 3s 12ms/step - loss: 0.7379 - accuracy: 0.5048 - val_loss: 0.6865 - val_accuracy: 0.5655\n",
      "Epoch 2/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.7081 - accuracy: 0.5741\n",
      "Epoch 2: val_loss improved from 0.68650 to 0.64916, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.5942 - val_loss: 0.6492 - val_accuracy: 0.5833\n",
      "Epoch 3/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.7036 - accuracy: 0.5882\n",
      "Epoch 3: val_loss improved from 0.64916 to 0.61401, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6892 - accuracy: 0.6038 - val_loss: 0.6140 - val_accuracy: 0.6488\n",
      "Epoch 4/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.6343 - accuracy: 0.6467\n",
      "Epoch 4: val_loss improved from 0.61401 to 0.58325, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.6303 - accuracy: 0.6486 - val_loss: 0.5832 - val_accuracy: 0.7083\n",
      "Epoch 5/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.6231 - accuracy: 0.6755\n",
      "Epoch 5: val_loss improved from 0.58325 to 0.55584, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6218 - accuracy: 0.6741 - val_loss: 0.5558 - val_accuracy: 0.7440\n",
      "Epoch 6/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.5852 - accuracy: 0.7055\n",
      "Epoch 6: val_loss improved from 0.55584 to 0.53047, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5809 - accuracy: 0.7061 - val_loss: 0.5305 - val_accuracy: 0.7560\n",
      "Epoch 7/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.5758 - accuracy: 0.6915\n",
      "Epoch 7: val_loss improved from 0.53047 to 0.50625, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5796 - accuracy: 0.6869 - val_loss: 0.5062 - val_accuracy: 0.7560\n",
      "Epoch 8/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.5371 - accuracy: 0.7615\n",
      "Epoch 8: val_loss improved from 0.50625 to 0.48623, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5354 - accuracy: 0.7540 - val_loss: 0.4862 - val_accuracy: 0.7619\n",
      "Epoch 9/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.5477 - accuracy: 0.7127\n",
      "Epoch 9: val_loss improved from 0.48623 to 0.46637, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5501 - accuracy: 0.7188 - val_loss: 0.4664 - val_accuracy: 0.7798\n",
      "Epoch 10/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.5245 - accuracy: 0.7684\n",
      "Epoch 10: val_loss improved from 0.46637 to 0.44718, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5267 - accuracy: 0.7636 - val_loss: 0.4472 - val_accuracy: 0.8095\n",
      "Epoch 11/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.4866 - accuracy: 0.8035\n",
      "Epoch 11: val_loss improved from 0.44718 to 0.43114, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4855 - accuracy: 0.8019 - val_loss: 0.4311 - val_accuracy: 0.8452\n",
      "Epoch 12/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.5006 - accuracy: 0.7828\n",
      "Epoch 12: val_loss improved from 0.43114 to 0.41624, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4937 - accuracy: 0.7891 - val_loss: 0.4162 - val_accuracy: 0.8512\n",
      "Epoch 13/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.4865 - accuracy: 0.7630\n",
      "Epoch 13: val_loss improved from 0.41624 to 0.40184, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5007 - accuracy: 0.7572 - val_loss: 0.4018 - val_accuracy: 0.8571\n",
      "Epoch 14/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.4618 - accuracy: 0.8169\n",
      "Epoch 14: val_loss improved from 0.40184 to 0.38913, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4679 - accuracy: 0.8051 - val_loss: 0.3891 - val_accuracy: 0.8750\n",
      "Epoch 15/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.4315 - accuracy: 0.8267\n",
      "Epoch 15: val_loss improved from 0.38913 to 0.37681, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4290 - accuracy: 0.8307 - val_loss: 0.3768 - val_accuracy: 0.8750\n",
      "Epoch 16/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.4310 - accuracy: 0.8373\n",
      "Epoch 16: val_loss improved from 0.37681 to 0.36518, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4322 - accuracy: 0.8307 - val_loss: 0.3652 - val_accuracy: 0.8810\n",
      "Epoch 17/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.4120 - accuracy: 0.8218\n",
      "Epoch 17: val_loss improved from 0.36518 to 0.35487, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4104 - accuracy: 0.8211 - val_loss: 0.3549 - val_accuracy: 0.8869\n",
      "Epoch 18/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.4191 - accuracy: 0.8175\n",
      "Epoch 18: val_loss improved from 0.35487 to 0.34523, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.8147 - val_loss: 0.3452 - val_accuracy: 0.9048\n",
      "Epoch 19/200\n",
      "44/63 [===================>..........] - ETA: 0s - loss: 0.3898 - accuracy: 0.8500\n",
      "Epoch 19: val_loss improved from 0.34523 to 0.33579, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3897 - accuracy: 0.8530 - val_loss: 0.3358 - val_accuracy: 0.8988\n",
      "Epoch 20/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.3952 - accuracy: 0.8308\n",
      "Epoch 20: val_loss improved from 0.33579 to 0.32720, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3968 - accuracy: 0.8339 - val_loss: 0.3272 - val_accuracy: 0.9107\n",
      "Epoch 21/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.4076 - accuracy: 0.8528\n",
      "Epoch 21: val_loss improved from 0.32720 to 0.31847, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4151 - accuracy: 0.8435 - val_loss: 0.3185 - val_accuracy: 0.9107\n",
      "Epoch 22/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3789 - accuracy: 0.8667\n",
      "Epoch 22: val_loss improved from 0.31847 to 0.31004, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3796 - accuracy: 0.8690 - val_loss: 0.3100 - val_accuracy: 0.9167\n",
      "Epoch 23/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.3795 - accuracy: 0.8552\n",
      "Epoch 23: val_loss improved from 0.31004 to 0.30229, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3800 - accuracy: 0.8562 - val_loss: 0.3023 - val_accuracy: 0.9167\n",
      "Epoch 24/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3345 - accuracy: 0.8836\n",
      "Epoch 24: val_loss improved from 0.30229 to 0.29529, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3442 - accuracy: 0.8754 - val_loss: 0.2953 - val_accuracy: 0.9167\n",
      "Epoch 25/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.3573 - accuracy: 0.8767\n",
      "Epoch 25: val_loss improved from 0.29529 to 0.28852, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3589 - accuracy: 0.8786 - val_loss: 0.2885 - val_accuracy: 0.9286\n",
      "Epoch 26/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3525 - accuracy: 0.8702\n",
      "Epoch 26: val_loss improved from 0.28852 to 0.28146, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3505 - accuracy: 0.8658 - val_loss: 0.2815 - val_accuracy: 0.9345\n",
      "Epoch 27/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3532 - accuracy: 0.8836\n",
      "Epoch 27: val_loss improved from 0.28146 to 0.27507, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3457 - accuracy: 0.8850 - val_loss: 0.2751 - val_accuracy: 0.9345\n",
      "Epoch 28/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.3258 - accuracy: 0.8780\n",
      "Epoch 28: val_loss improved from 0.27507 to 0.26925, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.8786 - val_loss: 0.2693 - val_accuracy: 0.9405\n",
      "Epoch 29/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3302 - accuracy: 0.8821\n",
      "Epoch 29: val_loss improved from 0.26925 to 0.26378, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3383 - accuracy: 0.8754 - val_loss: 0.2638 - val_accuracy: 0.9345\n",
      "Epoch 30/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3496 - accuracy: 0.8691\n",
      "Epoch 30: val_loss improved from 0.26378 to 0.25863, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3416 - accuracy: 0.8722 - val_loss: 0.2586 - val_accuracy: 0.9405\n",
      "Epoch 31/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.3353 - accuracy: 0.8857\n",
      "Epoch 31: val_loss improved from 0.25863 to 0.25323, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3189 - accuracy: 0.8946 - val_loss: 0.2532 - val_accuracy: 0.9405\n",
      "Epoch 32/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2981 - accuracy: 0.8941\n",
      "Epoch 32: val_loss improved from 0.25323 to 0.24809, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2981 - accuracy: 0.8978 - val_loss: 0.2481 - val_accuracy: 0.9464\n",
      "Epoch 33/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3352 - accuracy: 0.8778\n",
      "Epoch 33: val_loss improved from 0.24809 to 0.24402, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3275 - accuracy: 0.8914 - val_loss: 0.2440 - val_accuracy: 0.9464\n",
      "Epoch 34/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3087 - accuracy: 0.9000\n",
      "Epoch 34: val_loss improved from 0.24402 to 0.23959, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3019 - accuracy: 0.9073 - val_loss: 0.2396 - val_accuracy: 0.9524\n",
      "Epoch 35/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2933 - accuracy: 0.8881\n",
      "Epoch 35: val_loss improved from 0.23959 to 0.23561, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2968 - accuracy: 0.8882 - val_loss: 0.2356 - val_accuracy: 0.9524\n",
      "Epoch 36/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3199 - accuracy: 0.8643\n",
      "Epoch 36: val_loss improved from 0.23561 to 0.23154, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3109 - accuracy: 0.8722 - val_loss: 0.2315 - val_accuracy: 0.9524\n",
      "Epoch 37/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2990 - accuracy: 0.9036\n",
      "Epoch 37: val_loss improved from 0.23154 to 0.22724, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3044 - accuracy: 0.8978 - val_loss: 0.2272 - val_accuracy: 0.9524\n",
      "Epoch 38/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3044 - accuracy: 0.8737\n",
      "Epoch 38: val_loss improved from 0.22724 to 0.22392, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2985 - accuracy: 0.8786 - val_loss: 0.2239 - val_accuracy: 0.9524\n",
      "Epoch 39/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2906 - accuracy: 0.8967\n",
      "Epoch 39: val_loss improved from 0.22392 to 0.22002, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2924 - accuracy: 0.8914 - val_loss: 0.2200 - val_accuracy: 0.9583\n",
      "Epoch 40/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2958 - accuracy: 0.8857\n",
      "Epoch 40: val_loss improved from 0.22002 to 0.21655, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2906 - accuracy: 0.8850 - val_loss: 0.2165 - val_accuracy: 0.9583\n",
      "Epoch 41/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2928 - accuracy: 0.9071\n",
      "Epoch 41: val_loss improved from 0.21655 to 0.21370, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2816 - accuracy: 0.9137 - val_loss: 0.2137 - val_accuracy: 0.9583\n",
      "Epoch 42/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2978 - accuracy: 0.9000\n",
      "Epoch 42: val_loss improved from 0.21370 to 0.21060, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3075 - accuracy: 0.8882 - val_loss: 0.2106 - val_accuracy: 0.9583\n",
      "Epoch 43/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2628 - accuracy: 0.9020\n",
      "Epoch 43: val_loss improved from 0.21060 to 0.20751, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2749 - accuracy: 0.9010 - val_loss: 0.2075 - val_accuracy: 0.9583\n",
      "Epoch 44/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2727 - accuracy: 0.9020\n",
      "Epoch 44: val_loss improved from 0.20751 to 0.20432, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2705 - accuracy: 0.9042 - val_loss: 0.2043 - val_accuracy: 0.9583\n",
      "Epoch 45/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2505 - accuracy: 0.9158\n",
      "Epoch 45: val_loss improved from 0.20432 to 0.20129, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2468 - accuracy: 0.9201 - val_loss: 0.2013 - val_accuracy: 0.9583\n",
      "Epoch 46/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2596 - accuracy: 0.8982\n",
      "Epoch 46: val_loss improved from 0.20129 to 0.19894, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2614 - accuracy: 0.9010 - val_loss: 0.1989 - val_accuracy: 0.9583\n",
      "Epoch 47/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2545 - accuracy: 0.9097\n",
      "Epoch 47: val_loss improved from 0.19894 to 0.19650, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2527 - accuracy: 0.9105 - val_loss: 0.1965 - val_accuracy: 0.9583\n",
      "Epoch 48/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2537 - accuracy: 0.9308\n",
      "Epoch 48: val_loss improved from 0.19650 to 0.19377, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2637 - accuracy: 0.9233 - val_loss: 0.1938 - val_accuracy: 0.9583\n",
      "Epoch 49/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2939 - accuracy: 0.8923\n",
      "Epoch 49: val_loss improved from 0.19377 to 0.19114, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2882 - accuracy: 0.8946 - val_loss: 0.1911 - val_accuracy: 0.9583\n",
      "Epoch 50/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2462 - accuracy: 0.9385\n",
      "Epoch 50: val_loss improved from 0.19114 to 0.18894, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2520 - accuracy: 0.9233 - val_loss: 0.1889 - val_accuracy: 0.9583\n",
      "Epoch 51/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.2807 - accuracy: 0.8958\n",
      "Epoch 51: val_loss improved from 0.18894 to 0.18693, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2734 - accuracy: 0.9042 - val_loss: 0.1869 - val_accuracy: 0.9583\n",
      "Epoch 52/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2702 - accuracy: 0.9019\n",
      "Epoch 52: val_loss improved from 0.18693 to 0.18475, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2630 - accuracy: 0.9073 - val_loss: 0.1847 - val_accuracy: 0.9643\n",
      "Epoch 53/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2732 - accuracy: 0.9000\n",
      "Epoch 53: val_loss improved from 0.18475 to 0.18296, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2587 - accuracy: 0.9010 - val_loss: 0.1830 - val_accuracy: 0.9702\n",
      "Epoch 54/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2591 - accuracy: 0.8939\n",
      "Epoch 54: val_loss improved from 0.18296 to 0.18125, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2418 - accuracy: 0.9073 - val_loss: 0.1812 - val_accuracy: 0.9702\n",
      "Epoch 55/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2539 - accuracy: 0.9192\n",
      "Epoch 55: val_loss improved from 0.18125 to 0.17885, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2354 - accuracy: 0.9297 - val_loss: 0.1789 - val_accuracy: 0.9702\n",
      "Epoch 56/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2238 - accuracy: 0.9368\n",
      "Epoch 56: val_loss improved from 0.17885 to 0.17681, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2310 - accuracy: 0.9297 - val_loss: 0.1768 - val_accuracy: 0.9702\n",
      "Epoch 57/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2325 - accuracy: 0.9148\n",
      "Epoch 57: val_loss improved from 0.17681 to 0.17505, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2524 - accuracy: 0.8978 - val_loss: 0.1750 - val_accuracy: 0.9643\n",
      "Epoch 58/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2149 - accuracy: 0.9255\n",
      "Epoch 58: val_loss improved from 0.17505 to 0.17357, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2113 - accuracy: 0.9265 - val_loss: 0.1736 - val_accuracy: 0.9643\n",
      "Epoch 59/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2211 - accuracy: 0.9410\n",
      "Epoch 59: val_loss improved from 0.17357 to 0.17165, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2292 - accuracy: 0.9361 - val_loss: 0.1716 - val_accuracy: 0.9643\n",
      "Epoch 60/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2191 - accuracy: 0.9357\n",
      "Epoch 60: val_loss improved from 0.17165 to 0.16972, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2211 - accuracy: 0.9361 - val_loss: 0.1697 - val_accuracy: 0.9643\n",
      "Epoch 61/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2024 - accuracy: 0.9434\n",
      "Epoch 61: val_loss improved from 0.16972 to 0.16781, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2286 - accuracy: 0.9265 - val_loss: 0.1678 - val_accuracy: 0.9643\n",
      "Epoch 62/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2289 - accuracy: 0.9176\n",
      "Epoch 62: val_loss improved from 0.16781 to 0.16627, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2229 - accuracy: 0.9233 - val_loss: 0.1663 - val_accuracy: 0.9702\n",
      "Epoch 63/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2087 - accuracy: 0.9464\n",
      "Epoch 63: val_loss improved from 0.16627 to 0.16486, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2110 - accuracy: 0.9425 - val_loss: 0.1649 - val_accuracy: 0.9702\n",
      "Epoch 64/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2353 - accuracy: 0.9222\n",
      "Epoch 64: val_loss improved from 0.16486 to 0.16348, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2242 - accuracy: 0.9265 - val_loss: 0.1635 - val_accuracy: 0.9762\n",
      "Epoch 65/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2144 - accuracy: 0.9347\n",
      "Epoch 65: val_loss improved from 0.16348 to 0.16209, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2101 - accuracy: 0.9329 - val_loss: 0.1621 - val_accuracy: 0.9762\n",
      "Epoch 66/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2187 - accuracy: 0.9259\n",
      "Epoch 66: val_loss improved from 0.16209 to 0.16067, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2124 - accuracy: 0.9329 - val_loss: 0.1607 - val_accuracy: 0.9762\n",
      "Epoch 67/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2080 - accuracy: 0.9462\n",
      "Epoch 67: val_loss improved from 0.16067 to 0.15932, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2236 - accuracy: 0.9425 - val_loss: 0.1593 - val_accuracy: 0.9762\n",
      "Epoch 68/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2102 - accuracy: 0.9467\n",
      "Epoch 68: val_loss improved from 0.15932 to 0.15792, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2129 - accuracy: 0.9457 - val_loss: 0.1579 - val_accuracy: 0.9762\n",
      "Epoch 69/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2078 - accuracy: 0.9423\n",
      "Epoch 69: val_loss improved from 0.15792 to 0.15691, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2056 - accuracy: 0.9393 - val_loss: 0.1569 - val_accuracy: 0.9762\n",
      "Epoch 70/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2151 - accuracy: 0.9296\n",
      "Epoch 70: val_loss improved from 0.15691 to 0.15554, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2207 - accuracy: 0.9265 - val_loss: 0.1555 - val_accuracy: 0.9762\n",
      "Epoch 71/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2049 - accuracy: 0.9273\n",
      "Epoch 71: val_loss improved from 0.15554 to 0.15406, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9297 - val_loss: 0.1541 - val_accuracy: 0.9762\n",
      "Epoch 72/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2220 - accuracy: 0.9333\n",
      "Epoch 72: val_loss improved from 0.15406 to 0.15300, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2126 - accuracy: 0.9329 - val_loss: 0.1530 - val_accuracy: 0.9762\n",
      "Epoch 73/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1903 - accuracy: 0.9455\n",
      "Epoch 73: val_loss improved from 0.15300 to 0.15137, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1939 - accuracy: 0.9393 - val_loss: 0.1514 - val_accuracy: 0.9762\n",
      "Epoch 74/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2070 - accuracy: 0.9222\n",
      "Epoch 74: val_loss improved from 0.15137 to 0.15024, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2104 - accuracy: 0.9201 - val_loss: 0.1502 - val_accuracy: 0.9762\n",
      "Epoch 75/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2303 - accuracy: 0.9120\n",
      "Epoch 75: val_loss improved from 0.15024 to 0.14911, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2151 - accuracy: 0.9265 - val_loss: 0.1491 - val_accuracy: 0.9762\n",
      "Epoch 76/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.1857 - accuracy: 0.9435\n",
      "Epoch 76: val_loss improved from 0.14911 to 0.14796, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1896 - accuracy: 0.9393 - val_loss: 0.1480 - val_accuracy: 0.9762\n",
      "Epoch 77/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2053 - accuracy: 0.9214\n",
      "Epoch 77: val_loss improved from 0.14796 to 0.14700, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2033 - accuracy: 0.9265 - val_loss: 0.1470 - val_accuracy: 0.9762\n",
      "Epoch 78/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1967 - accuracy: 0.9347\n",
      "Epoch 78: val_loss improved from 0.14700 to 0.14566, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1942 - accuracy: 0.9361 - val_loss: 0.1457 - val_accuracy: 0.9762\n",
      "Epoch 79/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2099 - accuracy: 0.9158\n",
      "Epoch 79: val_loss improved from 0.14566 to 0.14468, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2003 - accuracy: 0.9233 - val_loss: 0.1447 - val_accuracy: 0.9762\n",
      "Epoch 80/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2263 - accuracy: 0.9123\n",
      "Epoch 80: val_loss improved from 0.14468 to 0.14413, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2179 - accuracy: 0.9201 - val_loss: 0.1441 - val_accuracy: 0.9762\n",
      "Epoch 81/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9311\n",
      "Epoch 81: val_loss improved from 0.14413 to 0.14350, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2047 - accuracy: 0.9329 - val_loss: 0.1435 - val_accuracy: 0.9762\n",
      "Epoch 82/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1722 - accuracy: 0.9467\n",
      "Epoch 82: val_loss improved from 0.14350 to 0.14279, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1778 - accuracy: 0.9393 - val_loss: 0.1428 - val_accuracy: 0.9762\n",
      "Epoch 83/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9387\n",
      "Epoch 83: val_loss improved from 0.14279 to 0.14189, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1709 - accuracy: 0.9393 - val_loss: 0.1419 - val_accuracy: 0.9762\n",
      "Epoch 84/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1943 - accuracy: 0.9407\n",
      "Epoch 84: val_loss improved from 0.14189 to 0.14116, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9361 - val_loss: 0.1412 - val_accuracy: 0.9762\n",
      "Epoch 85/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1726 - accuracy: 0.9464\n",
      "Epoch 85: val_loss improved from 0.14116 to 0.14008, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9489 - val_loss: 0.1401 - val_accuracy: 0.9762\n",
      "Epoch 86/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1840 - accuracy: 0.9417\n",
      "Epoch 86: val_loss improved from 0.14008 to 0.13946, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1722 - accuracy: 0.9457 - val_loss: 0.1395 - val_accuracy: 0.9762\n",
      "Epoch 87/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1780 - accuracy: 0.9414\n",
      "Epoch 87: val_loss improved from 0.13946 to 0.13891, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1840 - accuracy: 0.9393 - val_loss: 0.1389 - val_accuracy: 0.9762\n",
      "Epoch 88/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9311\n",
      "Epoch 88: val_loss improved from 0.13891 to 0.13802, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1705 - accuracy: 0.9297 - val_loss: 0.1380 - val_accuracy: 0.9762\n",
      "Epoch 89/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1844 - accuracy: 0.9200\n",
      "Epoch 89: val_loss improved from 0.13802 to 0.13761, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1900 - accuracy: 0.9201 - val_loss: 0.1376 - val_accuracy: 0.9762\n",
      "Epoch 90/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1915 - accuracy: 0.9300\n",
      "Epoch 90: val_loss improved from 0.13761 to 0.13697, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9233 - val_loss: 0.1370 - val_accuracy: 0.9762\n",
      "Epoch 91/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1831 - accuracy: 0.9385\n",
      "Epoch 91: val_loss improved from 0.13697 to 0.13620, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1704 - accuracy: 0.9457 - val_loss: 0.1362 - val_accuracy: 0.9762\n",
      "Epoch 92/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1990 - accuracy: 0.9424\n",
      "Epoch 92: val_loss improved from 0.13620 to 0.13526, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1920 - accuracy: 0.9457 - val_loss: 0.1353 - val_accuracy: 0.9762\n",
      "Epoch 93/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1732 - accuracy: 0.9544\n",
      "Epoch 93: val_loss improved from 0.13526 to 0.13466, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1850 - accuracy: 0.9457 - val_loss: 0.1347 - val_accuracy: 0.9762\n",
      "Epoch 94/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9443\n",
      "Epoch 94: val_loss improved from 0.13466 to 0.13408, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1763 - accuracy: 0.9457 - val_loss: 0.1341 - val_accuracy: 0.9762\n",
      "Epoch 95/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1625 - accuracy: 0.9433\n",
      "Epoch 95: val_loss improved from 0.13408 to 0.13304, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1585 - accuracy: 0.9457 - val_loss: 0.1330 - val_accuracy: 0.9762\n",
      "Epoch 96/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1652 - accuracy: 0.9525\n",
      "Epoch 96: val_loss improved from 0.13304 to 0.13228, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1661 - accuracy: 0.9521 - val_loss: 0.1323 - val_accuracy: 0.9762\n",
      "Epoch 97/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1693 - accuracy: 0.9475\n",
      "Epoch 97: val_loss improved from 0.13228 to 0.13181, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1672 - accuracy: 0.9489 - val_loss: 0.1318 - val_accuracy: 0.9762\n",
      "Epoch 98/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1465 - accuracy: 0.9481\n",
      "Epoch 98: val_loss improved from 0.13181 to 0.13095, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1601 - accuracy: 0.9425 - val_loss: 0.1309 - val_accuracy: 0.9762\n",
      "Epoch 99/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1651 - accuracy: 0.9429\n",
      "Epoch 99: val_loss improved from 0.13095 to 0.13020, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1660 - accuracy: 0.9425 - val_loss: 0.1302 - val_accuracy: 0.9762\n",
      "Epoch 100/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1678 - accuracy: 0.9474\n",
      "Epoch 100: val_loss improved from 0.13020 to 0.12976, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1666 - accuracy: 0.9457 - val_loss: 0.1298 - val_accuracy: 0.9762\n",
      "Epoch 101/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1737 - accuracy: 0.9424\n",
      "Epoch 101: val_loss improved from 0.12976 to 0.12916, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9457 - val_loss: 0.1292 - val_accuracy: 0.9762\n",
      "Epoch 102/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1578 - accuracy: 0.9571\n",
      "Epoch 102: val_loss improved from 0.12916 to 0.12849, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1641 - accuracy: 0.9553 - val_loss: 0.1285 - val_accuracy: 0.9762\n",
      "Epoch 103/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1646 - accuracy: 0.9464\n",
      "Epoch 103: val_loss improved from 0.12849 to 0.12807, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1599 - accuracy: 0.9489 - val_loss: 0.1281 - val_accuracy: 0.9762\n",
      "Epoch 104/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1740 - accuracy: 0.9483\n",
      "Epoch 104: val_loss improved from 0.12807 to 0.12747, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1718 - accuracy: 0.9489 - val_loss: 0.1275 - val_accuracy: 0.9762\n",
      "Epoch 105/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1739 - accuracy: 0.9283\n",
      "Epoch 105: val_loss improved from 0.12747 to 0.12688, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.9393 - val_loss: 0.1269 - val_accuracy: 0.9762\n",
      "Epoch 106/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1550 - accuracy: 0.9567\n",
      "Epoch 106: val_loss improved from 0.12688 to 0.12634, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9553 - val_loss: 0.1263 - val_accuracy: 0.9762\n",
      "Epoch 107/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1412 - accuracy: 0.9536\n",
      "Epoch 107: val_loss improved from 0.12634 to 0.12572, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1533 - accuracy: 0.9489 - val_loss: 0.1257 - val_accuracy: 0.9762\n",
      "Epoch 108/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1373 - accuracy: 0.9673\n",
      "Epoch 108: val_loss improved from 0.12572 to 0.12520, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9553 - val_loss: 0.1252 - val_accuracy: 0.9762\n",
      "Epoch 109/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1626 - accuracy: 0.9382\n",
      "Epoch 109: val_loss improved from 0.12520 to 0.12498, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.9425 - val_loss: 0.1250 - val_accuracy: 0.9762\n",
      "Epoch 110/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9516\n",
      "Epoch 110: val_loss improved from 0.12498 to 0.12479, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1480 - accuracy: 0.9489 - val_loss: 0.1248 - val_accuracy: 0.9762\n",
      "Epoch 111/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9645\n",
      "Epoch 111: val_loss improved from 0.12479 to 0.12436, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1400 - accuracy: 0.9649 - val_loss: 0.1244 - val_accuracy: 0.9762\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9457\n",
      "Epoch 112: val_loss improved from 0.12436 to 0.12333, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1531 - accuracy: 0.9457 - val_loss: 0.1233 - val_accuracy: 0.9762\n",
      "Epoch 113/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1445 - accuracy: 0.9527\n",
      "Epoch 113: val_loss improved from 0.12333 to 0.12286, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1357 - accuracy: 0.9585 - val_loss: 0.1229 - val_accuracy: 0.9762\n",
      "Epoch 114/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9443\n",
      "Epoch 114: val_loss improved from 0.12286 to 0.12235, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1531 - accuracy: 0.9425 - val_loss: 0.1223 - val_accuracy: 0.9762\n",
      "Epoch 115/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9574\n",
      "Epoch 115: val_loss improved from 0.12235 to 0.12200, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1548 - accuracy: 0.9585 - val_loss: 0.1220 - val_accuracy: 0.9762\n",
      "Epoch 116/200\n",
      "42/63 [===================>..........] - ETA: 0s - loss: 0.1488 - accuracy: 0.9619\n",
      "Epoch 116: val_loss improved from 0.12200 to 0.12140, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1442 - accuracy: 0.9585 - val_loss: 0.1214 - val_accuracy: 0.9762\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9457\n",
      "Epoch 117: val_loss improved from 0.12140 to 0.12078, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1647 - accuracy: 0.9457 - val_loss: 0.1208 - val_accuracy: 0.9762\n",
      "Epoch 118/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1449 - accuracy: 0.9581\n",
      "Epoch 118: val_loss improved from 0.12078 to 0.12021, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1439 - accuracy: 0.9585 - val_loss: 0.1202 - val_accuracy: 0.9762\n",
      "Epoch 119/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9425\n",
      "Epoch 119: val_loss improved from 0.12021 to 0.12011, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9425 - val_loss: 0.1201 - val_accuracy: 0.9702\n",
      "Epoch 120/200\n",
      "43/63 [===================>..........] - ETA: 0s - loss: 0.1112 - accuracy: 0.9628\n",
      "Epoch 120: val_loss improved from 0.12011 to 0.11949, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1347 - accuracy: 0.9457 - val_loss: 0.1195 - val_accuracy: 0.9762\n",
      "Epoch 121/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1418 - accuracy: 0.9585\n",
      "Epoch 121: val_loss improved from 0.11949 to 0.11926, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1590 - accuracy: 0.9521 - val_loss: 0.1193 - val_accuracy: 0.9762\n",
      "Epoch 122/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9645\n",
      "Epoch 122: val_loss improved from 0.11926 to 0.11901, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9617 - val_loss: 0.1190 - val_accuracy: 0.9762\n",
      "Epoch 123/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1493 - accuracy: 0.9615\n",
      "Epoch 123: val_loss improved from 0.11901 to 0.11825, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1547 - accuracy: 0.9553 - val_loss: 0.1183 - val_accuracy: 0.9762\n",
      "Epoch 124/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9553\n",
      "Epoch 124: val_loss improved from 0.11825 to 0.11800, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1285 - accuracy: 0.9553 - val_loss: 0.1180 - val_accuracy: 0.9762\n",
      "Epoch 125/200\n",
      "43/63 [===================>..........] - ETA: 0s - loss: 0.1294 - accuracy: 0.9581\n",
      "Epoch 125: val_loss improved from 0.11800 to 0.11737, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1285 - accuracy: 0.9585 - val_loss: 0.1174 - val_accuracy: 0.9762\n",
      "Epoch 126/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.9574\n",
      "Epoch 126: val_loss improved from 0.11737 to 0.11690, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9585 - val_loss: 0.1169 - val_accuracy: 0.9762\n",
      "Epoch 127/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1315 - accuracy: 0.9593\n",
      "Epoch 127: val_loss improved from 0.11690 to 0.11638, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1227 - accuracy: 0.9649 - val_loss: 0.1164 - val_accuracy: 0.9762\n",
      "Epoch 128/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9585\n",
      "Epoch 128: val_loss improved from 0.11638 to 0.11609, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1340 - accuracy: 0.9585 - val_loss: 0.1161 - val_accuracy: 0.9762\n",
      "Epoch 129/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1504 - accuracy: 0.9533\n",
      "Epoch 129: val_loss improved from 0.11609 to 0.11599, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9553 - val_loss: 0.1160 - val_accuracy: 0.9762\n",
      "Epoch 130/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1316 - accuracy: 0.9547\n",
      "Epoch 130: val_loss improved from 0.11599 to 0.11569, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1333 - accuracy: 0.9553 - val_loss: 0.1157 - val_accuracy: 0.9762\n",
      "Epoch 131/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9672\n",
      "Epoch 131: val_loss improved from 0.11569 to 0.11549, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9649 - val_loss: 0.1155 - val_accuracy: 0.9762\n",
      "Epoch 132/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1241 - accuracy: 0.9679\n",
      "Epoch 132: val_loss improved from 0.11549 to 0.11525, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9712 - val_loss: 0.1152 - val_accuracy: 0.9762\n",
      "Epoch 133/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1329 - accuracy: 0.9667\n",
      "Epoch 133: val_loss improved from 0.11525 to 0.11505, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1335 - accuracy: 0.9617 - val_loss: 0.1151 - val_accuracy: 0.9762\n",
      "Epoch 134/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1463 - accuracy: 0.9418\n",
      "Epoch 134: val_loss improved from 0.11505 to 0.11503, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9489 - val_loss: 0.1150 - val_accuracy: 0.9762\n",
      "Epoch 135/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1234 - accuracy: 0.9633\n",
      "Epoch 135: val_loss improved from 0.11503 to 0.11455, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1295 - accuracy: 0.9617 - val_loss: 0.1146 - val_accuracy: 0.9762\n",
      "Epoch 136/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1372 - accuracy: 0.9593\n",
      "Epoch 136: val_loss improved from 0.11455 to 0.11429, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1326 - accuracy: 0.9617 - val_loss: 0.1143 - val_accuracy: 0.9762\n",
      "Epoch 137/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.9541\n",
      "Epoch 137: val_loss improved from 0.11429 to 0.11408, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1362 - accuracy: 0.9553 - val_loss: 0.1141 - val_accuracy: 0.9762\n",
      "Epoch 138/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1484 - accuracy: 0.9483\n",
      "Epoch 138: val_loss improved from 0.11408 to 0.11388, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1434 - accuracy: 0.9521 - val_loss: 0.1139 - val_accuracy: 0.9762\n",
      "Epoch 139/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1112 - accuracy: 0.9714\n",
      "Epoch 139: val_loss improved from 0.11388 to 0.11374, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1184 - accuracy: 0.9681 - val_loss: 0.1137 - val_accuracy: 0.9762\n",
      "Epoch 140/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1230 - accuracy: 0.9571\n",
      "Epoch 140: val_loss did not improve from 0.11374\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1321 - accuracy: 0.9521 - val_loss: 0.1138 - val_accuracy: 0.9762\n",
      "Epoch 141/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1188 - accuracy: 0.9741\n",
      "Epoch 141: val_loss improved from 0.11374 to 0.11310, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1301 - accuracy: 0.9681 - val_loss: 0.1131 - val_accuracy: 0.9762\n",
      "Epoch 142/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9672\n",
      "Epoch 142: val_loss improved from 0.11310 to 0.11261, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1315 - accuracy: 0.9681 - val_loss: 0.1126 - val_accuracy: 0.9762\n",
      "Epoch 143/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0941 - accuracy: 0.9792\n",
      "Epoch 143: val_loss improved from 0.11261 to 0.11241, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1273 - accuracy: 0.9649 - val_loss: 0.1124 - val_accuracy: 0.9762\n",
      "Epoch 144/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1151 - accuracy: 0.9765\n",
      "Epoch 144: val_loss did not improve from 0.11241\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9744 - val_loss: 0.1125 - val_accuracy: 0.9762\n",
      "Epoch 145/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1150 - accuracy: 0.9627\n",
      "Epoch 145: val_loss improved from 0.11241 to 0.11227, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9617 - val_loss: 0.1123 - val_accuracy: 0.9762\n",
      "Epoch 146/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9585\n",
      "Epoch 146: val_loss improved from 0.11227 to 0.11165, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1243 - accuracy: 0.9585 - val_loss: 0.1116 - val_accuracy: 0.9762\n",
      "Epoch 147/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1064 - accuracy: 0.9741\n",
      "Epoch 147: val_loss improved from 0.11165 to 0.11123, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1160 - accuracy: 0.9712 - val_loss: 0.1112 - val_accuracy: 0.9762\n",
      "Epoch 148/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1345 - accuracy: 0.9571\n",
      "Epoch 148: val_loss improved from 0.11123 to 0.11107, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1288 - accuracy: 0.9553 - val_loss: 0.1111 - val_accuracy: 0.9762\n",
      "Epoch 149/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1086 - accuracy: 0.9692\n",
      "Epoch 149: val_loss improved from 0.11107 to 0.11035, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1088 - accuracy: 0.9681 - val_loss: 0.1103 - val_accuracy: 0.9762\n",
      "Epoch 150/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1226 - accuracy: 0.9692\n",
      "Epoch 150: val_loss improved from 0.11035 to 0.10987, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9681 - val_loss: 0.1099 - val_accuracy: 0.9762\n",
      "Epoch 151/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1156 - accuracy: 0.9640\n",
      "Epoch 151: val_loss improved from 0.10987 to 0.10966, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1127 - accuracy: 0.9681 - val_loss: 0.1097 - val_accuracy: 0.9762\n",
      "Epoch 152/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1329 - accuracy: 0.9542\n",
      "Epoch 152: val_loss did not improve from 0.10966\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1271 - accuracy: 0.9585 - val_loss: 0.1097 - val_accuracy: 0.9762\n",
      "Epoch 153/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1193 - accuracy: 0.9649\n",
      "Epoch 153: val_loss improved from 0.10966 to 0.10957, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9681 - val_loss: 0.1096 - val_accuracy: 0.9762\n",
      "Epoch 154/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9617\n",
      "Epoch 154: val_loss did not improve from 0.10957\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1174 - accuracy: 0.9617 - val_loss: 0.1097 - val_accuracy: 0.9762\n",
      "Epoch 155/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1139 - accuracy: 0.9607\n",
      "Epoch 155: val_loss did not improve from 0.10957\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9585 - val_loss: 0.1097 - val_accuracy: 0.9762\n",
      "Epoch 156/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1260 - accuracy: 0.9448\n",
      "Epoch 156: val_loss did not improve from 0.10957\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9457 - val_loss: 0.1096 - val_accuracy: 0.9762\n",
      "Epoch 157/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1234 - accuracy: 0.9643\n",
      "Epoch 157: val_loss improved from 0.10957 to 0.10932, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1154 - accuracy: 0.9681 - val_loss: 0.1093 - val_accuracy: 0.9762\n",
      "Epoch 158/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0756 - accuracy: 0.9875\n",
      "Epoch 158: val_loss improved from 0.10932 to 0.10890, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0940 - accuracy: 0.9808 - val_loss: 0.1089 - val_accuracy: 0.9762\n",
      "Epoch 159/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1268 - accuracy: 0.9544\n",
      "Epoch 159: val_loss improved from 0.10890 to 0.10877, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1210 - accuracy: 0.9553 - val_loss: 0.1088 - val_accuracy: 0.9762\n",
      "Epoch 160/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0917 - accuracy: 0.9714\n",
      "Epoch 160: val_loss improved from 0.10877 to 0.10863, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9649 - val_loss: 0.1086 - val_accuracy: 0.9762\n",
      "Epoch 161/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1176 - accuracy: 0.9655\n",
      "Epoch 161: val_loss improved from 0.10863 to 0.10860, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1167 - accuracy: 0.9617 - val_loss: 0.1086 - val_accuracy: 0.9762\n",
      "Epoch 162/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1143 - accuracy: 0.9552\n",
      "Epoch 162: val_loss did not improve from 0.10860\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9553 - val_loss: 0.1088 - val_accuracy: 0.9762\n",
      "Epoch 163/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1102 - accuracy: 0.9679\n",
      "Epoch 163: val_loss improved from 0.10860 to 0.10824, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9681 - val_loss: 0.1082 - val_accuracy: 0.9762\n",
      "Epoch 164/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9672\n",
      "Epoch 164: val_loss improved from 0.10824 to 0.10787, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9681 - val_loss: 0.1079 - val_accuracy: 0.9762\n",
      "Epoch 165/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0790 - accuracy: 0.9849\n",
      "Epoch 165: val_loss improved from 0.10787 to 0.10723, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1042 - accuracy: 0.9744 - val_loss: 0.1072 - val_accuracy: 0.9762\n",
      "Epoch 166/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9639\n",
      "Epoch 166: val_loss improved from 0.10723 to 0.10678, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9649 - val_loss: 0.1068 - val_accuracy: 0.9762\n",
      "Epoch 167/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1196 - accuracy: 0.9679\n",
      "Epoch 167: val_loss did not improve from 0.10678\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1164 - accuracy: 0.9681 - val_loss: 0.1070 - val_accuracy: 0.9762\n",
      "Epoch 168/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1109 - accuracy: 0.9593\n",
      "Epoch 168: val_loss improved from 0.10678 to 0.10673, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.1067 - val_accuracy: 0.9762\n",
      "Epoch 169/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1086 - accuracy: 0.9600\n",
      "Epoch 169: val_loss did not improve from 0.10673\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.9617 - val_loss: 0.1068 - val_accuracy: 0.9762\n",
      "Epoch 170/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1103 - accuracy: 0.9621\n",
      "Epoch 170: val_loss improved from 0.10673 to 0.10633, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1144 - accuracy: 0.9585 - val_loss: 0.1063 - val_accuracy: 0.9762\n",
      "Epoch 171/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1025 - accuracy: 0.9698\n",
      "Epoch 171: val_loss improved from 0.10633 to 0.10587, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0929 - accuracy: 0.9744 - val_loss: 0.1059 - val_accuracy: 0.9762\n",
      "Epoch 172/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1144 - accuracy: 0.9661\n",
      "Epoch 172: val_loss improved from 0.10587 to 0.10565, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1118 - accuracy: 0.9681 - val_loss: 0.1057 - val_accuracy: 0.9762\n",
      "Epoch 173/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9581\n",
      "Epoch 173: val_loss did not improve from 0.10565\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1132 - accuracy: 0.9585 - val_loss: 0.1058 - val_accuracy: 0.9762\n",
      "Epoch 174/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1030 - accuracy: 0.9719\n",
      "Epoch 174: val_loss did not improve from 0.10565\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9712 - val_loss: 0.1058 - val_accuracy: 0.9762\n",
      "Epoch 175/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0979 - accuracy: 0.9686\n",
      "Epoch 175: val_loss improved from 0.10565 to 0.10554, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9681 - val_loss: 0.1055 - val_accuracy: 0.9762\n",
      "Epoch 176/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1108 - accuracy: 0.9709\n",
      "Epoch 176: val_loss improved from 0.10554 to 0.10547, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1020 - accuracy: 0.9744 - val_loss: 0.1055 - val_accuracy: 0.9762\n",
      "Epoch 177/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9649\n",
      "Epoch 177: val_loss did not improve from 0.10547\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9649 - val_loss: 0.1055 - val_accuracy: 0.9762\n",
      "Epoch 178/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1068 - accuracy: 0.9714\n",
      "Epoch 178: val_loss did not improve from 0.10547\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1030 - accuracy: 0.9744 - val_loss: 0.1056 - val_accuracy: 0.9762\n",
      "Epoch 179/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1145 - accuracy: 0.9736\n",
      "Epoch 179: val_loss improved from 0.10547 to 0.10517, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1021 - accuracy: 0.9776 - val_loss: 0.1052 - val_accuracy: 0.9762\n",
      "Epoch 180/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1169 - accuracy: 0.9585\n",
      "Epoch 180: val_loss improved from 0.10517 to 0.10503, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.9649 - val_loss: 0.1050 - val_accuracy: 0.9762\n",
      "Epoch 181/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0976 - accuracy: 0.9733\n",
      "Epoch 181: val_loss improved from 0.10503 to 0.10458, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.9712 - val_loss: 0.1046 - val_accuracy: 0.9762\n",
      "Epoch 182/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1261 - accuracy: 0.9640\n",
      "Epoch 182: val_loss did not improve from 0.10458\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9681 - val_loss: 0.1047 - val_accuracy: 0.9762\n",
      "Epoch 183/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1171 - accuracy: 0.9579\n",
      "Epoch 183: val_loss improved from 0.10458 to 0.10441, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9617 - val_loss: 0.1044 - val_accuracy: 0.9762\n",
      "Epoch 184/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0927 - accuracy: 0.9763\n",
      "Epoch 184: val_loss improved from 0.10441 to 0.10421, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.9744 - val_loss: 0.1042 - val_accuracy: 0.9762\n",
      "Epoch 185/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9617\n",
      "Epoch 185: val_loss improved from 0.10421 to 0.10412, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1163 - accuracy: 0.9617 - val_loss: 0.1041 - val_accuracy: 0.9762\n",
      "Epoch 186/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1134 - accuracy: 0.9667\n",
      "Epoch 186: val_loss improved from 0.10412 to 0.10404, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1189 - accuracy: 0.9617 - val_loss: 0.1040 - val_accuracy: 0.9762\n",
      "Epoch 187/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1141 - accuracy: 0.9627\n",
      "Epoch 187: val_loss did not improve from 0.10404\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1090 - accuracy: 0.9649 - val_loss: 0.1041 - val_accuracy: 0.9762\n",
      "Epoch 188/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0945 - accuracy: 0.9686\n",
      "Epoch 188: val_loss did not improve from 0.10404\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9681 - val_loss: 0.1040 - val_accuracy: 0.9762\n",
      "Epoch 189/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1046 - accuracy: 0.9673\n",
      "Epoch 189: val_loss improved from 0.10404 to 0.10362, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1005 - accuracy: 0.9712 - val_loss: 0.1036 - val_accuracy: 0.9762\n",
      "Epoch 190/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0877 - accuracy: 0.9729\n",
      "Epoch 190: val_loss improved from 0.10362 to 0.10332, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0871 - accuracy: 0.9744 - val_loss: 0.1033 - val_accuracy: 0.9762\n",
      "Epoch 191/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1031 - accuracy: 0.9765\n",
      "Epoch 191: val_loss improved from 0.10332 to 0.10300, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1013 - accuracy: 0.9776 - val_loss: 0.1030 - val_accuracy: 0.9762\n",
      "Epoch 192/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1127 - accuracy: 0.9639\n",
      "Epoch 192: val_loss did not improve from 0.10300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1125 - accuracy: 0.9649 - val_loss: 0.1032 - val_accuracy: 0.9762\n",
      "Epoch 193/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1247 - accuracy: 0.9520\n",
      "Epoch 193: val_loss did not improve from 0.10300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9585 - val_loss: 0.1031 - val_accuracy: 0.9762\n",
      "Epoch 194/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9681\n",
      "Epoch 194: val_loss did not improve from 0.10300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0871 - accuracy: 0.9681 - val_loss: 0.1033 - val_accuracy: 0.9762\n",
      "Epoch 195/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0759 - accuracy: 0.9769\n",
      "Epoch 195: val_loss improved from 0.10300 to 0.10283, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0887 - accuracy: 0.9744 - val_loss: 0.1028 - val_accuracy: 0.9762\n",
      "Epoch 196/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0863 - accuracy: 0.9709\n",
      "Epoch 196: val_loss improved from 0.10283 to 0.10228, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0848 - accuracy: 0.9712 - val_loss: 0.1023 - val_accuracy: 0.9762\n",
      "Epoch 197/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0940 - accuracy: 0.9714\n",
      "Epoch 197: val_loss improved from 0.10228 to 0.10203, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0970 - accuracy: 0.9712 - val_loss: 0.1020 - val_accuracy: 0.9762\n",
      "Epoch 198/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1026 - accuracy: 0.9667\n",
      "Epoch 198: val_loss did not improve from 0.10203\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0917 - accuracy: 0.9712 - val_loss: 0.1020 - val_accuracy: 0.9762\n",
      "Epoch 199/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1004 - accuracy: 0.9633\n",
      "Epoch 199: val_loss did not improve from 0.10203\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0979 - accuracy: 0.9649 - val_loss: 0.1022 - val_accuracy: 0.9762\n",
      "Epoch 200/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0864 - accuracy: 0.9698\n",
      "Epoch 200: val_loss did not improve from 0.10203\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_4 (GRU)                 (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1026 - accuracy: 0.9762\n",
      "Test Loss: 0.1026051715016365\n",
      "Test Accuracy: 0.976190447807312\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer with dropout_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, \n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.3,\n",
    "                     input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 layer with dropout RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.7912 - accuracy: 0.4230\n",
      "Epoch 1: val_loss improved from inf to 0.73954, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 4s 11ms/step - loss: 0.7933 - accuracy: 0.4217 - val_loss: 0.7395 - val_accuracy: 0.4940\n",
      "Epoch 2/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.7521 - accuracy: 0.4912\n",
      "Epoch 2: val_loss improved from 0.73954 to 0.68323, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.7504 - accuracy: 0.4952 - val_loss: 0.6832 - val_accuracy: 0.6131\n",
      "Epoch 3/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.7239 - accuracy: 0.5796\n",
      "Epoch 3: val_loss improved from 0.68323 to 0.63364, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.7106 - accuracy: 0.5847 - val_loss: 0.6336 - val_accuracy: 0.7202\n",
      "Epoch 4/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6787 - accuracy: 0.6422\n",
      "Epoch 4: val_loss improved from 0.63364 to 0.58684, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6787 - accuracy: 0.6422 - val_loss: 0.5868 - val_accuracy: 0.7500\n",
      "Epoch 5/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.6563 - accuracy: 0.6852\n",
      "Epoch 5: val_loss improved from 0.58684 to 0.54777, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6424 - accuracy: 0.6933 - val_loss: 0.5478 - val_accuracy: 0.7738\n",
      "Epoch 6/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.6071 - accuracy: 0.6714\n",
      "Epoch 6: val_loss improved from 0.54777 to 0.51259, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6055 - accuracy: 0.6709 - val_loss: 0.5126 - val_accuracy: 0.7917\n",
      "Epoch 7/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.5495 - accuracy: 0.7585\n",
      "Epoch 7: val_loss improved from 0.51259 to 0.48109, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5446 - accuracy: 0.7636 - val_loss: 0.4811 - val_accuracy: 0.8214\n",
      "Epoch 8/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.5379 - accuracy: 0.7615\n",
      "Epoch 8: val_loss improved from 0.48109 to 0.45189, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5296 - accuracy: 0.7636 - val_loss: 0.4519 - val_accuracy: 0.8393\n",
      "Epoch 9/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.5208 - accuracy: 0.7614\n",
      "Epoch 9: val_loss improved from 0.45189 to 0.42472, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5162 - accuracy: 0.7700 - val_loss: 0.4247 - val_accuracy: 0.8571\n",
      "Epoch 10/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.5008 - accuracy: 0.7544\n",
      "Epoch 10: val_loss improved from 0.42472 to 0.40126, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4921 - accuracy: 0.7700 - val_loss: 0.4013 - val_accuracy: 0.8750\n",
      "Epoch 11/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.4899 - accuracy: 0.7889\n",
      "Epoch 11: val_loss improved from 0.40126 to 0.37958, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.8019 - val_loss: 0.3796 - val_accuracy: 0.8869\n",
      "Epoch 12/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.4609 - accuracy: 0.8000\n",
      "Epoch 12: val_loss improved from 0.37958 to 0.36030, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4555 - accuracy: 0.8051 - val_loss: 0.3603 - val_accuracy: 0.8929\n",
      "Epoch 13/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.4337 - accuracy: 0.8296\n",
      "Epoch 13: val_loss improved from 0.36030 to 0.34379, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4420 - accuracy: 0.8307 - val_loss: 0.3438 - val_accuracy: 0.9048\n",
      "Epoch 14/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.4410 - accuracy: 0.7927\n",
      "Epoch 14: val_loss improved from 0.34379 to 0.32849, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4275 - accuracy: 0.8115 - val_loss: 0.3285 - val_accuracy: 0.9107\n",
      "Epoch 15/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3936 - accuracy: 0.8519\n",
      "Epoch 15: val_loss improved from 0.32849 to 0.31498, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4012 - accuracy: 0.8466 - val_loss: 0.3150 - val_accuracy: 0.9226\n",
      "Epoch 16/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3859 - accuracy: 0.8691\n",
      "Epoch 16: val_loss improved from 0.31498 to 0.30159, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3759 - accuracy: 0.8754 - val_loss: 0.3016 - val_accuracy: 0.9345\n",
      "Epoch 17/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3777 - accuracy: 0.8500\n",
      "Epoch 17: val_loss improved from 0.30159 to 0.28977, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3806 - accuracy: 0.8466 - val_loss: 0.2898 - val_accuracy: 0.9405\n",
      "Epoch 18/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3591 - accuracy: 0.8830\n",
      "Epoch 18: val_loss improved from 0.28977 to 0.27879, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.8786 - val_loss: 0.2788 - val_accuracy: 0.9345\n",
      "Epoch 19/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3404 - accuracy: 0.9000\n",
      "Epoch 19: val_loss improved from 0.27879 to 0.26845, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3350 - accuracy: 0.9042 - val_loss: 0.2684 - val_accuracy: 0.9405\n",
      "Epoch 20/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.3211 - accuracy: 0.9038\n",
      "Epoch 20: val_loss improved from 0.26845 to 0.25930, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3326 - accuracy: 0.9010 - val_loss: 0.2593 - val_accuracy: 0.9405\n",
      "Epoch 21/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.3336 - accuracy: 0.8745\n",
      "Epoch 21: val_loss improved from 0.25930 to 0.25159, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3373 - accuracy: 0.8754 - val_loss: 0.2516 - val_accuracy: 0.9464\n",
      "Epoch 22/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.3308 - accuracy: 0.8746\n",
      "Epoch 22: val_loss improved from 0.25159 to 0.24451, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3246 - accuracy: 0.8818 - val_loss: 0.2445 - val_accuracy: 0.9464\n",
      "Epoch 23/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3084 - accuracy: 0.8982\n",
      "Epoch 23: val_loss improved from 0.24451 to 0.23763, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3072 - accuracy: 0.9010 - val_loss: 0.2376 - val_accuracy: 0.9464\n",
      "Epoch 24/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2943 - accuracy: 0.8926\n",
      "Epoch 24: val_loss improved from 0.23763 to 0.23089, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2944 - accuracy: 0.8914 - val_loss: 0.2309 - val_accuracy: 0.9405\n",
      "Epoch 25/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.3109 - accuracy: 0.8840\n",
      "Epoch 25: val_loss improved from 0.23089 to 0.22508, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8818 - val_loss: 0.2251 - val_accuracy: 0.9405\n",
      "Epoch 26/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3049 - accuracy: 0.8964\n",
      "Epoch 26: val_loss improved from 0.22508 to 0.21959, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2973 - accuracy: 0.8978 - val_loss: 0.2196 - val_accuracy: 0.9405\n",
      "Epoch 27/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2824 - accuracy: 0.8964\n",
      "Epoch 27: val_loss improved from 0.21959 to 0.21462, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2941 - accuracy: 0.8946 - val_loss: 0.2146 - val_accuracy: 0.9405\n",
      "Epoch 28/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3228 - accuracy: 0.8679\n",
      "Epoch 28: val_loss improved from 0.21462 to 0.21060, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3140 - accuracy: 0.8722 - val_loss: 0.2106 - val_accuracy: 0.9405\n",
      "Epoch 29/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2811 - accuracy: 0.8923\n",
      "Epoch 29: val_loss improved from 0.21060 to 0.20635, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2904 - accuracy: 0.8882 - val_loss: 0.2064 - val_accuracy: 0.9464\n",
      "Epoch 30/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2990 - accuracy: 0.8846\n",
      "Epoch 30: val_loss improved from 0.20635 to 0.20246, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2877 - accuracy: 0.8850 - val_loss: 0.2025 - val_accuracy: 0.9524\n",
      "Epoch 31/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2778 - accuracy: 0.9036\n",
      "Epoch 31: val_loss improved from 0.20246 to 0.19834, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2824 - accuracy: 0.9042 - val_loss: 0.1983 - val_accuracy: 0.9524\n",
      "Epoch 32/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2706 - accuracy: 0.9000\n",
      "Epoch 32: val_loss improved from 0.19834 to 0.19515, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2692 - accuracy: 0.9010 - val_loss: 0.1951 - val_accuracy: 0.9524\n",
      "Epoch 33/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2769 - accuracy: 0.9057\n",
      "Epoch 33: val_loss improved from 0.19515 to 0.19197, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2725 - accuracy: 0.9042 - val_loss: 0.1920 - val_accuracy: 0.9524\n",
      "Epoch 34/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2781 - accuracy: 0.8877\n",
      "Epoch 34: val_loss improved from 0.19197 to 0.18841, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2756 - accuracy: 0.8882 - val_loss: 0.1884 - val_accuracy: 0.9524\n",
      "Epoch 35/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2586 - accuracy: 0.9018\n",
      "Epoch 35: val_loss improved from 0.18841 to 0.18543, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2566 - accuracy: 0.8978 - val_loss: 0.1854 - val_accuracy: 0.9524\n",
      "Epoch 36/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2685 - accuracy: 0.8926\n",
      "Epoch 36: val_loss improved from 0.18543 to 0.18277, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2634 - accuracy: 0.8978 - val_loss: 0.1828 - val_accuracy: 0.9524\n",
      "Epoch 37/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2789 - accuracy: 0.8967\n",
      "Epoch 37: val_loss improved from 0.18277 to 0.18029, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2773 - accuracy: 0.8946 - val_loss: 0.1803 - val_accuracy: 0.9524\n",
      "Epoch 38/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2645 - accuracy: 0.9091\n",
      "Epoch 38: val_loss improved from 0.18029 to 0.17768, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2656 - accuracy: 0.9073 - val_loss: 0.1777 - val_accuracy: 0.9524\n",
      "Epoch 39/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2438 - accuracy: 0.9185\n",
      "Epoch 39: val_loss improved from 0.17768 to 0.17553, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2516 - accuracy: 0.9169 - val_loss: 0.1755 - val_accuracy: 0.9524\n",
      "Epoch 40/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2341 - accuracy: 0.9267\n",
      "Epoch 40: val_loss improved from 0.17553 to 0.17288, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2287 - accuracy: 0.9297 - val_loss: 0.1729 - val_accuracy: 0.9524\n",
      "Epoch 41/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2530 - accuracy: 0.9033\n",
      "Epoch 41: val_loss improved from 0.17288 to 0.17041, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2504 - accuracy: 0.9073 - val_loss: 0.1704 - val_accuracy: 0.9524\n",
      "Epoch 42/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2328 - accuracy: 0.9138\n",
      "Epoch 42: val_loss improved from 0.17041 to 0.16851, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2256 - accuracy: 0.9169 - val_loss: 0.1685 - val_accuracy: 0.9524\n",
      "Epoch 43/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2442 - accuracy: 0.9085\n",
      "Epoch 43: val_loss improved from 0.16851 to 0.16637, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2388 - accuracy: 0.9137 - val_loss: 0.1664 - val_accuracy: 0.9524\n",
      "Epoch 44/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2268 - accuracy: 0.9167\n",
      "Epoch 44: val_loss improved from 0.16637 to 0.16439, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2267 - accuracy: 0.9169 - val_loss: 0.1644 - val_accuracy: 0.9524\n",
      "Epoch 45/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2308 - accuracy: 0.9220\n",
      "Epoch 45: val_loss improved from 0.16439 to 0.16258, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2256 - accuracy: 0.9233 - val_loss: 0.1626 - val_accuracy: 0.9524\n",
      "Epoch 46/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2336 - accuracy: 0.9233\n",
      "Epoch 46: val_loss improved from 0.16258 to 0.16111, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2336 - accuracy: 0.9233 - val_loss: 0.1611 - val_accuracy: 0.9524\n",
      "Epoch 47/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2271 - accuracy: 0.9267\n",
      "Epoch 47: val_loss improved from 0.16111 to 0.15928, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2210 - accuracy: 0.9297 - val_loss: 0.1593 - val_accuracy: 0.9524\n",
      "Epoch 48/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2190 - accuracy: 0.9246\n",
      "Epoch 48: val_loss improved from 0.15928 to 0.15742, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2204 - accuracy: 0.9233 - val_loss: 0.1574 - val_accuracy: 0.9524\n",
      "Epoch 49/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2190 - accuracy: 0.9279\n",
      "Epoch 49: val_loss improved from 0.15742 to 0.15609, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2186 - accuracy: 0.9265 - val_loss: 0.1561 - val_accuracy: 0.9524\n",
      "Epoch 50/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2292 - accuracy: 0.9143\n",
      "Epoch 50: val_loss improved from 0.15609 to 0.15467, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2227 - accuracy: 0.9201 - val_loss: 0.1547 - val_accuracy: 0.9583\n",
      "Epoch 51/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2159 - accuracy: 0.9347\n",
      "Epoch 51: val_loss improved from 0.15467 to 0.15343, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2304 - accuracy: 0.9265 - val_loss: 0.1534 - val_accuracy: 0.9583\n",
      "Epoch 52/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2113 - accuracy: 0.9193\n",
      "Epoch 52: val_loss improved from 0.15343 to 0.15220, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2102 - accuracy: 0.9169 - val_loss: 0.1522 - val_accuracy: 0.9583\n",
      "Epoch 53/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2269 - accuracy: 0.9071\n",
      "Epoch 53: val_loss improved from 0.15220 to 0.15108, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2169 - accuracy: 0.9169 - val_loss: 0.1511 - val_accuracy: 0.9583\n",
      "Epoch 54/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2336 - accuracy: 0.9137\n",
      "Epoch 54: val_loss improved from 0.15108 to 0.14962, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2142 - accuracy: 0.9233 - val_loss: 0.1496 - val_accuracy: 0.9583\n",
      "Epoch 55/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2251 - accuracy: 0.9255\n",
      "Epoch 55: val_loss improved from 0.14962 to 0.14841, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2190 - accuracy: 0.9233 - val_loss: 0.1484 - val_accuracy: 0.9583\n",
      "Epoch 56/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2283 - accuracy: 0.9193\n",
      "Epoch 56: val_loss improved from 0.14841 to 0.14739, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2291 - accuracy: 0.9169 - val_loss: 0.1474 - val_accuracy: 0.9583\n",
      "Epoch 57/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1958 - accuracy: 0.9288\n",
      "Epoch 57: val_loss improved from 0.14739 to 0.14585, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1979 - accuracy: 0.9265 - val_loss: 0.1458 - val_accuracy: 0.9583\n",
      "Epoch 58/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2049 - accuracy: 0.9241\n",
      "Epoch 58: val_loss improved from 0.14585 to 0.14473, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2059 - accuracy: 0.9201 - val_loss: 0.1447 - val_accuracy: 0.9583\n",
      "Epoch 59/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2131 - accuracy: 0.9194\n",
      "Epoch 59: val_loss improved from 0.14473 to 0.14392, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2125 - accuracy: 0.9201 - val_loss: 0.1439 - val_accuracy: 0.9583\n",
      "Epoch 60/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2095 - accuracy: 0.9148\n",
      "Epoch 60: val_loss improved from 0.14392 to 0.14298, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2060 - accuracy: 0.9169 - val_loss: 0.1430 - val_accuracy: 0.9583\n",
      "Epoch 61/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1864 - accuracy: 0.9358\n",
      "Epoch 61: val_loss improved from 0.14298 to 0.14190, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1919 - accuracy: 0.9329 - val_loss: 0.1419 - val_accuracy: 0.9583\n",
      "Epoch 62/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2070 - accuracy: 0.9276\n",
      "Epoch 62: val_loss improved from 0.14190 to 0.14077, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2025 - accuracy: 0.9297 - val_loss: 0.1408 - val_accuracy: 0.9583\n",
      "Epoch 63/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1868 - accuracy: 0.9329\n",
      "Epoch 63: val_loss improved from 0.14077 to 0.13964, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9329 - val_loss: 0.1396 - val_accuracy: 0.9583\n",
      "Epoch 64/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1995 - accuracy: 0.9201\n",
      "Epoch 64: val_loss improved from 0.13964 to 0.13844, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1995 - accuracy: 0.9201 - val_loss: 0.1384 - val_accuracy: 0.9583\n",
      "Epoch 65/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1855 - accuracy: 0.9346\n",
      "Epoch 65: val_loss improved from 0.13844 to 0.13756, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1880 - accuracy: 0.9297 - val_loss: 0.1376 - val_accuracy: 0.9583\n",
      "Epoch 66/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1894 - accuracy: 0.9265\n",
      "Epoch 66: val_loss improved from 0.13756 to 0.13681, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.9265 - val_loss: 0.1368 - val_accuracy: 0.9583\n",
      "Epoch 67/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2086 - accuracy: 0.9311\n",
      "Epoch 67: val_loss improved from 0.13681 to 0.13611, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2067 - accuracy: 0.9329 - val_loss: 0.1361 - val_accuracy: 0.9583\n",
      "Epoch 68/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9233\n",
      "Epoch 68: val_loss improved from 0.13611 to 0.13529, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2013 - accuracy: 0.9233 - val_loss: 0.1353 - val_accuracy: 0.9583\n",
      "Epoch 69/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1834 - accuracy: 0.9458\n",
      "Epoch 69: val_loss improved from 0.13529 to 0.13428, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.9489 - val_loss: 0.1343 - val_accuracy: 0.9583\n",
      "Epoch 70/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1876 - accuracy: 0.9267\n",
      "Epoch 70: val_loss improved from 0.13428 to 0.13335, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1838 - accuracy: 0.9297 - val_loss: 0.1334 - val_accuracy: 0.9583\n",
      "Epoch 71/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1725 - accuracy: 0.9434\n",
      "Epoch 71: val_loss improved from 0.13335 to 0.13237, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1902 - accuracy: 0.9361 - val_loss: 0.1324 - val_accuracy: 0.9583\n",
      "Epoch 72/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1843 - accuracy: 0.9361\n",
      "Epoch 72: val_loss improved from 0.13237 to 0.13172, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1843 - accuracy: 0.9361 - val_loss: 0.1317 - val_accuracy: 0.9583\n",
      "Epoch 73/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1797 - accuracy: 0.9346\n",
      "Epoch 73: val_loss improved from 0.13172 to 0.13095, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1763 - accuracy: 0.9393 - val_loss: 0.1309 - val_accuracy: 0.9583\n",
      "Epoch 74/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.2162 - accuracy: 0.9043\n",
      "Epoch 74: val_loss improved from 0.13095 to 0.13039, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2110 - accuracy: 0.9105 - val_loss: 0.1304 - val_accuracy: 0.9583\n",
      "Epoch 75/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1577 - accuracy: 0.9492\n",
      "Epoch 75: val_loss improved from 0.13039 to 0.12921, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1556 - accuracy: 0.9489 - val_loss: 0.1292 - val_accuracy: 0.9643\n",
      "Epoch 76/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1574 - accuracy: 0.9346\n",
      "Epoch 76: val_loss improved from 0.12921 to 0.12833, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9329 - val_loss: 0.1283 - val_accuracy: 0.9643\n",
      "Epoch 77/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1769 - accuracy: 0.9333\n",
      "Epoch 77: val_loss improved from 0.12833 to 0.12732, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1810 - accuracy: 0.9361 - val_loss: 0.1273 - val_accuracy: 0.9643\n",
      "Epoch 78/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1450 - accuracy: 0.9500\n",
      "Epoch 78: val_loss improved from 0.12732 to 0.12681, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9425 - val_loss: 0.1268 - val_accuracy: 0.9643\n",
      "Epoch 79/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1837 - accuracy: 0.9368\n",
      "Epoch 79: val_loss improved from 0.12681 to 0.12613, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1795 - accuracy: 0.9393 - val_loss: 0.1261 - val_accuracy: 0.9643\n",
      "Epoch 80/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1924 - accuracy: 0.9233\n",
      "Epoch 80: val_loss improved from 0.12613 to 0.12574, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1897 - accuracy: 0.9265 - val_loss: 0.1257 - val_accuracy: 0.9643\n",
      "Epoch 81/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1806 - accuracy: 0.9418\n",
      "Epoch 81: val_loss improved from 0.12574 to 0.12511, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1712 - accuracy: 0.9489 - val_loss: 0.1251 - val_accuracy: 0.9643\n",
      "Epoch 82/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1705 - accuracy: 0.9419\n",
      "Epoch 82: val_loss improved from 0.12511 to 0.12430, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1691 - accuracy: 0.9425 - val_loss: 0.1243 - val_accuracy: 0.9643\n",
      "Epoch 83/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1710 - accuracy: 0.9483\n",
      "Epoch 83: val_loss improved from 0.12430 to 0.12367, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1662 - accuracy: 0.9521 - val_loss: 0.1237 - val_accuracy: 0.9643\n",
      "Epoch 84/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.1641 - accuracy: 0.9422\n",
      "Epoch 84: val_loss improved from 0.12367 to 0.12306, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1754 - accuracy: 0.9297 - val_loss: 0.1231 - val_accuracy: 0.9643\n",
      "Epoch 85/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1677 - accuracy: 0.9452\n",
      "Epoch 85: val_loss improved from 0.12306 to 0.12262, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1663 - accuracy: 0.9457 - val_loss: 0.1226 - val_accuracy: 0.9643\n",
      "Epoch 86/200\n",
      "44/63 [===================>..........] - ETA: 0s - loss: 0.1847 - accuracy: 0.9318\n",
      "Epoch 86: val_loss improved from 0.12262 to 0.12185, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1629 - accuracy: 0.9489 - val_loss: 0.1218 - val_accuracy: 0.9643\n",
      "Epoch 87/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.1615 - accuracy: 0.9404\n",
      "Epoch 87: val_loss improved from 0.12185 to 0.12134, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9489 - val_loss: 0.1213 - val_accuracy: 0.9643\n",
      "Epoch 88/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1925 - accuracy: 0.9259\n",
      "Epoch 88: val_loss improved from 0.12134 to 0.12084, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1801 - accuracy: 0.9329 - val_loss: 0.1208 - val_accuracy: 0.9643\n",
      "Epoch 89/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1471 - accuracy: 0.9356\n",
      "Epoch 89: val_loss improved from 0.12084 to 0.12025, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1629 - accuracy: 0.9329 - val_loss: 0.1202 - val_accuracy: 0.9643\n",
      "Epoch 90/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1563 - accuracy: 0.9483\n",
      "Epoch 90: val_loss improved from 0.12025 to 0.11977, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9457 - val_loss: 0.1198 - val_accuracy: 0.9643\n",
      "Epoch 91/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1500 - accuracy: 0.9615\n",
      "Epoch 91: val_loss improved from 0.11977 to 0.11950, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1480 - accuracy: 0.9617 - val_loss: 0.1195 - val_accuracy: 0.9643\n",
      "Epoch 92/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1580 - accuracy: 0.9400\n",
      "Epoch 92: val_loss improved from 0.11950 to 0.11877, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9457 - val_loss: 0.1188 - val_accuracy: 0.9643\n",
      "Epoch 93/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1553 - accuracy: 0.9538\n",
      "Epoch 93: val_loss improved from 0.11877 to 0.11807, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1502 - accuracy: 0.9489 - val_loss: 0.1181 - val_accuracy: 0.9643\n",
      "Epoch 94/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1575 - accuracy: 0.9552\n",
      "Epoch 94: val_loss improved from 0.11807 to 0.11758, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1559 - accuracy: 0.9585 - val_loss: 0.1176 - val_accuracy: 0.9643\n",
      "Epoch 95/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1499 - accuracy: 0.9472\n",
      "Epoch 95: val_loss improved from 0.11758 to 0.11693, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9553 - val_loss: 0.1169 - val_accuracy: 0.9643\n",
      "Epoch 96/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1525 - accuracy: 0.9433\n",
      "Epoch 96: val_loss improved from 0.11693 to 0.11653, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1471 - accuracy: 0.9457 - val_loss: 0.1165 - val_accuracy: 0.9643\n",
      "Epoch 97/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.9323\n",
      "Epoch 97: val_loss improved from 0.11653 to 0.11618, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1696 - accuracy: 0.9329 - val_loss: 0.1162 - val_accuracy: 0.9643\n",
      "Epoch 98/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1547 - accuracy: 0.9481\n",
      "Epoch 98: val_loss improved from 0.11618 to 0.11558, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1415 - accuracy: 0.9553 - val_loss: 0.1156 - val_accuracy: 0.9643\n",
      "Epoch 99/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1445 - accuracy: 0.9577\n",
      "Epoch 99: val_loss improved from 0.11558 to 0.11481, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1575 - accuracy: 0.9457 - val_loss: 0.1148 - val_accuracy: 0.9643\n",
      "Epoch 100/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1356 - accuracy: 0.9520\n",
      "Epoch 100: val_loss improved from 0.11481 to 0.11460, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1565 - accuracy: 0.9425 - val_loss: 0.1146 - val_accuracy: 0.9643\n",
      "Epoch 101/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1585 - accuracy: 0.9396\n",
      "Epoch 101: val_loss improved from 0.11460 to 0.11421, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1466 - accuracy: 0.9457 - val_loss: 0.1142 - val_accuracy: 0.9643\n",
      "Epoch 102/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1712 - accuracy: 0.9382\n",
      "Epoch 102: val_loss improved from 0.11421 to 0.11374, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1648 - accuracy: 0.9393 - val_loss: 0.1137 - val_accuracy: 0.9643\n",
      "Epoch 103/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1527 - accuracy: 0.9615\n",
      "Epoch 103: val_loss improved from 0.11374 to 0.11328, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1519 - accuracy: 0.9553 - val_loss: 0.1133 - val_accuracy: 0.9643\n",
      "Epoch 104/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1446 - accuracy: 0.9538\n",
      "Epoch 104: val_loss improved from 0.11328 to 0.11290, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1416 - accuracy: 0.9553 - val_loss: 0.1129 - val_accuracy: 0.9643\n",
      "Epoch 105/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1445 - accuracy: 0.9600\n",
      "Epoch 105: val_loss improved from 0.11290 to 0.11253, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1540 - accuracy: 0.9553 - val_loss: 0.1125 - val_accuracy: 0.9643\n",
      "Epoch 106/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1607 - accuracy: 0.9519\n",
      "Epoch 106: val_loss improved from 0.11253 to 0.11215, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1545 - accuracy: 0.9521 - val_loss: 0.1121 - val_accuracy: 0.9643\n",
      "Epoch 107/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1487 - accuracy: 0.9491\n",
      "Epoch 107: val_loss improved from 0.11215 to 0.11175, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9553 - val_loss: 0.1118 - val_accuracy: 0.9643\n",
      "Epoch 108/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1668 - accuracy: 0.9368\n",
      "Epoch 108: val_loss improved from 0.11175 to 0.11139, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1641 - accuracy: 0.9393 - val_loss: 0.1114 - val_accuracy: 0.9643\n",
      "Epoch 109/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1328 - accuracy: 0.9500\n",
      "Epoch 109: val_loss improved from 0.11139 to 0.11094, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1434 - accuracy: 0.9489 - val_loss: 0.1109 - val_accuracy: 0.9702\n",
      "Epoch 110/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1331 - accuracy: 0.9434\n",
      "Epoch 110: val_loss improved from 0.11094 to 0.11073, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1302 - accuracy: 0.9457 - val_loss: 0.1107 - val_accuracy: 0.9702\n",
      "Epoch 111/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1662 - accuracy: 0.9400\n",
      "Epoch 111: val_loss improved from 0.11073 to 0.11045, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1623 - accuracy: 0.9425 - val_loss: 0.1105 - val_accuracy: 0.9702\n",
      "Epoch 112/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1265 - accuracy: 0.9649\n",
      "Epoch 112: val_loss improved from 0.11045 to 0.10992, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1245 - accuracy: 0.9617 - val_loss: 0.1099 - val_accuracy: 0.9702\n",
      "Epoch 113/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1523 - accuracy: 0.9393\n",
      "Epoch 113: val_loss improved from 0.10992 to 0.10952, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1523 - accuracy: 0.9393 - val_loss: 0.1095 - val_accuracy: 0.9702\n",
      "Epoch 114/200\n",
      "44/63 [===================>..........] - ETA: 0s - loss: 0.1383 - accuracy: 0.9409\n",
      "Epoch 114: val_loss improved from 0.10952 to 0.10941, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1413 - accuracy: 0.9457 - val_loss: 0.1094 - val_accuracy: 0.9702\n",
      "Epoch 115/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1239 - accuracy: 0.9636\n",
      "Epoch 115: val_loss improved from 0.10941 to 0.10906, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1153 - accuracy: 0.9681 - val_loss: 0.1091 - val_accuracy: 0.9762\n",
      "Epoch 116/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1459 - accuracy: 0.9508\n",
      "Epoch 116: val_loss improved from 0.10906 to 0.10881, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1428 - accuracy: 0.9521 - val_loss: 0.1088 - val_accuracy: 0.9702\n",
      "Epoch 117/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9585\n",
      "Epoch 117: val_loss improved from 0.10881 to 0.10865, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9585 - val_loss: 0.1086 - val_accuracy: 0.9762\n",
      "Epoch 118/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9489\n",
      "Epoch 118: val_loss improved from 0.10865 to 0.10812, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1554 - accuracy: 0.9489 - val_loss: 0.1081 - val_accuracy: 0.9762\n",
      "Epoch 119/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1246 - accuracy: 0.9509\n",
      "Epoch 119: val_loss improved from 0.10812 to 0.10756, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1416 - accuracy: 0.9489 - val_loss: 0.1076 - val_accuracy: 0.9762\n",
      "Epoch 120/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9443\n",
      "Epoch 120: val_loss improved from 0.10756 to 0.10745, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1446 - accuracy: 0.9425 - val_loss: 0.1074 - val_accuracy: 0.9762\n",
      "Epoch 121/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1523 - accuracy: 0.9529\n",
      "Epoch 121: val_loss improved from 0.10745 to 0.10730, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1380 - accuracy: 0.9585 - val_loss: 0.1073 - val_accuracy: 0.9762\n",
      "Epoch 122/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1340 - accuracy: 0.9491\n",
      "Epoch 122: val_loss improved from 0.10730 to 0.10656, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1364 - accuracy: 0.9489 - val_loss: 0.1066 - val_accuracy: 0.9762\n",
      "Epoch 123/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1311 - accuracy: 0.9600\n",
      "Epoch 123: val_loss improved from 0.10656 to 0.10650, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9617 - val_loss: 0.1065 - val_accuracy: 0.9762\n",
      "Epoch 124/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1290 - accuracy: 0.9552\n",
      "Epoch 124: val_loss improved from 0.10650 to 0.10629, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1382 - accuracy: 0.9521 - val_loss: 0.1063 - val_accuracy: 0.9762\n",
      "Epoch 125/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1348 - accuracy: 0.9520\n",
      "Epoch 125: val_loss improved from 0.10629 to 0.10608, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1336 - accuracy: 0.9553 - val_loss: 0.1061 - val_accuracy: 0.9702\n",
      "Epoch 126/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9710\n",
      "Epoch 126: val_loss improved from 0.10608 to 0.10573, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1198 - accuracy: 0.9712 - val_loss: 0.1057 - val_accuracy: 0.9702\n",
      "Epoch 127/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1390 - accuracy: 0.9490\n",
      "Epoch 127: val_loss improved from 0.10573 to 0.10524, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9585 - val_loss: 0.1052 - val_accuracy: 0.9702\n",
      "Epoch 128/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1416 - accuracy: 0.9527\n",
      "Epoch 128: val_loss improved from 0.10524 to 0.10489, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9553 - val_loss: 0.1049 - val_accuracy: 0.9702\n",
      "Epoch 129/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1523 - accuracy: 0.9388\n",
      "Epoch 129: val_loss improved from 0.10489 to 0.10430, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1466 - accuracy: 0.9425 - val_loss: 0.1043 - val_accuracy: 0.9762\n",
      "Epoch 130/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1278 - accuracy: 0.9412\n",
      "Epoch 130: val_loss improved from 0.10430 to 0.10427, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1349 - accuracy: 0.9425 - val_loss: 0.1043 - val_accuracy: 0.9702\n",
      "Epoch 131/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1398 - accuracy: 0.9536\n",
      "Epoch 131: val_loss improved from 0.10427 to 0.10416, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1380 - accuracy: 0.9553 - val_loss: 0.1042 - val_accuracy: 0.9702\n",
      "Epoch 132/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1173 - accuracy: 0.9586\n",
      "Epoch 132: val_loss improved from 0.10416 to 0.10380, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9521 - val_loss: 0.1038 - val_accuracy: 0.9702\n",
      "Epoch 133/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1331 - accuracy: 0.9544\n",
      "Epoch 133: val_loss improved from 0.10380 to 0.10369, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1339 - accuracy: 0.9521 - val_loss: 0.1037 - val_accuracy: 0.9702\n",
      "Epoch 134/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1134 - accuracy: 0.9654\n",
      "Epoch 134: val_loss improved from 0.10369 to 0.10364, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1251 - accuracy: 0.9553 - val_loss: 0.1036 - val_accuracy: 0.9702\n",
      "Epoch 135/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1039 - accuracy: 0.9684\n",
      "Epoch 135: val_loss improved from 0.10364 to 0.10352, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1140 - accuracy: 0.9617 - val_loss: 0.1035 - val_accuracy: 0.9702\n",
      "Epoch 136/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1338 - accuracy: 0.9483\n",
      "Epoch 136: val_loss improved from 0.10352 to 0.10317, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1370 - accuracy: 0.9457 - val_loss: 0.1032 - val_accuracy: 0.9702\n",
      "Epoch 137/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1252 - accuracy: 0.9455\n",
      "Epoch 137: val_loss improved from 0.10317 to 0.10248, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1199 - accuracy: 0.9489 - val_loss: 0.1025 - val_accuracy: 0.9762\n",
      "Epoch 138/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1288 - accuracy: 0.9636\n",
      "Epoch 138: val_loss improved from 0.10248 to 0.10223, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1317 - accuracy: 0.9649 - val_loss: 0.1022 - val_accuracy: 0.9762\n",
      "Epoch 139/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.1294 - accuracy: 0.9574\n",
      "Epoch 139: val_loss improved from 0.10223 to 0.10222, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9649 - val_loss: 0.1022 - val_accuracy: 0.9702\n",
      "Epoch 140/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0755 - accuracy: 0.9797\n",
      "Epoch 140: val_loss improved from 0.10222 to 0.10182, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.9744 - val_loss: 0.1018 - val_accuracy: 0.9702\n",
      "Epoch 141/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1385 - accuracy: 0.9472\n",
      "Epoch 141: val_loss improved from 0.10182 to 0.10140, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1313 - accuracy: 0.9489 - val_loss: 0.1014 - val_accuracy: 0.9821\n",
      "Epoch 142/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1291 - accuracy: 0.9556\n",
      "Epoch 142: val_loss improved from 0.10140 to 0.10116, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9521 - val_loss: 0.1012 - val_accuracy: 0.9821\n",
      "Epoch 143/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9452\n",
      "Epoch 143: val_loss improved from 0.10116 to 0.10085, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1528 - accuracy: 0.9457 - val_loss: 0.1008 - val_accuracy: 0.9821\n",
      "Epoch 144/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1128 - accuracy: 0.9607\n",
      "Epoch 144: val_loss did not improve from 0.10085\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1165 - accuracy: 0.9585 - val_loss: 0.1009 - val_accuracy: 0.9762\n",
      "Epoch 145/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.1092 - accuracy: 0.9609\n",
      "Epoch 145: val_loss improved from 0.10085 to 0.10046, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1088 - accuracy: 0.9617 - val_loss: 0.1005 - val_accuracy: 0.9762\n",
      "Epoch 146/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1097 - accuracy: 0.9533\n",
      "Epoch 146: val_loss improved from 0.10046 to 0.10032, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9553 - val_loss: 0.1003 - val_accuracy: 0.9762\n",
      "Epoch 147/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9672\n",
      "Epoch 147: val_loss improved from 0.10032 to 0.10005, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1052 - accuracy: 0.9649 - val_loss: 0.1001 - val_accuracy: 0.9762\n",
      "Epoch 148/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1255 - accuracy: 0.9593\n",
      "Epoch 148: val_loss did not improve from 0.10005\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1144 - accuracy: 0.9649 - val_loss: 0.1001 - val_accuracy: 0.9762\n",
      "Epoch 149/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1148 - accuracy: 0.9698\n",
      "Epoch 149: val_loss improved from 0.10005 to 0.09980, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1174 - accuracy: 0.9712 - val_loss: 0.0998 - val_accuracy: 0.9821\n",
      "Epoch 150/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1523 - accuracy: 0.9492\n",
      "Epoch 150: val_loss improved from 0.09980 to 0.09976, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9489 - val_loss: 0.0998 - val_accuracy: 0.9821\n",
      "Epoch 151/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1364 - accuracy: 0.9491\n",
      "Epoch 151: val_loss did not improve from 0.09976\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1253 - accuracy: 0.9553 - val_loss: 0.0998 - val_accuracy: 0.9762\n",
      "Epoch 152/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9639\n",
      "Epoch 152: val_loss did not improve from 0.09976\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1179 - accuracy: 0.9649 - val_loss: 0.1000 - val_accuracy: 0.9762\n",
      "Epoch 153/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1034 - accuracy: 0.9709\n",
      "Epoch 153: val_loss improved from 0.09976 to 0.09973, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1036 - accuracy: 0.9712 - val_loss: 0.0997 - val_accuracy: 0.9762\n",
      "Epoch 154/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.1083 - accuracy: 0.9652\n",
      "Epoch 154: val_loss improved from 0.09973 to 0.09937, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1107 - accuracy: 0.9649 - val_loss: 0.0994 - val_accuracy: 0.9762\n",
      "Epoch 155/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1097 - accuracy: 0.9704\n",
      "Epoch 155: val_loss improved from 0.09937 to 0.09927, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1129 - accuracy: 0.9681 - val_loss: 0.0993 - val_accuracy: 0.9762\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9585\n",
      "Epoch 156: val_loss improved from 0.09927 to 0.09892, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1323 - accuracy: 0.9585 - val_loss: 0.0989 - val_accuracy: 0.9762\n",
      "Epoch 157/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1166 - accuracy: 0.9667\n",
      "Epoch 157: val_loss improved from 0.09892 to 0.09885, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 1s 10ms/step - loss: 0.1147 - accuracy: 0.9681 - val_loss: 0.0989 - val_accuracy: 0.9762\n",
      "Epoch 158/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9581\n",
      "Epoch 158: val_loss improved from 0.09885 to 0.09879, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1214 - accuracy: 0.9585 - val_loss: 0.0988 - val_accuracy: 0.9762\n",
      "Epoch 159/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1131 - accuracy: 0.9593\n",
      "Epoch 159: val_loss did not improve from 0.09879\n",
      "63/63 [==============================] - 1s 11ms/step - loss: 0.1182 - accuracy: 0.9521 - val_loss: 0.0990 - val_accuracy: 0.9762\n",
      "Epoch 160/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9649\n",
      "Epoch 160: val_loss did not improve from 0.09879\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1276 - accuracy: 0.9649 - val_loss: 0.0990 - val_accuracy: 0.9762\n",
      "Epoch 161/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1285 - accuracy: 0.9556\n",
      "Epoch 161: val_loss did not improve from 0.09879\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1185 - accuracy: 0.9585 - val_loss: 0.0990 - val_accuracy: 0.9762\n",
      "Epoch 162/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0984 - accuracy: 0.9655\n",
      "Epoch 162: val_loss improved from 0.09879 to 0.09875, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0989 - accuracy: 0.9649 - val_loss: 0.0987 - val_accuracy: 0.9762\n",
      "Epoch 163/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1148 - accuracy: 0.9714\n",
      "Epoch 163: val_loss improved from 0.09875 to 0.09862, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1111 - accuracy: 0.9712 - val_loss: 0.0986 - val_accuracy: 0.9762\n",
      "Epoch 164/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1270 - accuracy: 0.9608\n",
      "Epoch 164: val_loss improved from 0.09862 to 0.09830, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1191 - accuracy: 0.9649 - val_loss: 0.0983 - val_accuracy: 0.9762\n",
      "Epoch 165/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1072 - accuracy: 0.9729\n",
      "Epoch 165: val_loss improved from 0.09830 to 0.09782, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1120 - accuracy: 0.9712 - val_loss: 0.0978 - val_accuracy: 0.9762\n",
      "Epoch 166/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1145 - accuracy: 0.9608\n",
      "Epoch 166: val_loss did not improve from 0.09782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1040 - accuracy: 0.9681 - val_loss: 0.0979 - val_accuracy: 0.9762\n",
      "Epoch 167/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9516\n",
      "Epoch 167: val_loss improved from 0.09782 to 0.09778, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1211 - accuracy: 0.9521 - val_loss: 0.0978 - val_accuracy: 0.9762\n",
      "Epoch 168/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1102 - accuracy: 0.9661\n",
      "Epoch 168: val_loss improved from 0.09778 to 0.09778, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9681 - val_loss: 0.0978 - val_accuracy: 0.9762\n",
      "Epoch 169/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0795 - accuracy: 0.9725\n",
      "Epoch 169: val_loss improved from 0.09778 to 0.09763, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9649 - val_loss: 0.0976 - val_accuracy: 0.9762\n",
      "Epoch 170/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1048 - accuracy: 0.9708\n",
      "Epoch 170: val_loss did not improve from 0.09763\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0930 - accuracy: 0.9744 - val_loss: 0.0977 - val_accuracy: 0.9762\n",
      "Epoch 171/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1052 - accuracy: 0.9673\n",
      "Epoch 171: val_loss improved from 0.09763 to 0.09724, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9617 - val_loss: 0.0972 - val_accuracy: 0.9762\n",
      "Epoch 172/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1045 - accuracy: 0.9617\n",
      "Epoch 172: val_loss improved from 0.09724 to 0.09692, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1045 - accuracy: 0.9617 - val_loss: 0.0969 - val_accuracy: 0.9762\n",
      "Epoch 173/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9742\n",
      "Epoch 173: val_loss improved from 0.09692 to 0.09679, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0927 - accuracy: 0.9744 - val_loss: 0.0968 - val_accuracy: 0.9762\n",
      "Epoch 174/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1109 - accuracy: 0.9569\n",
      "Epoch 174: val_loss improved from 0.09679 to 0.09661, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9585 - val_loss: 0.0966 - val_accuracy: 0.9762\n",
      "Epoch 175/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1095 - accuracy: 0.9519\n",
      "Epoch 175: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1059 - accuracy: 0.9553 - val_loss: 0.0969 - val_accuracy: 0.9762\n",
      "Epoch 176/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1274 - accuracy: 0.9633\n",
      "Epoch 176: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9617 - val_loss: 0.0970 - val_accuracy: 0.9762\n",
      "Epoch 177/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1082 - accuracy: 0.9564\n",
      "Epoch 177: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0989 - accuracy: 0.9617 - val_loss: 0.0971 - val_accuracy: 0.9762\n",
      "Epoch 178/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0911 - accuracy: 0.9607\n",
      "Epoch 178: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9649 - val_loss: 0.0970 - val_accuracy: 0.9762\n",
      "Epoch 179/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9581\n",
      "Epoch 179: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0920 - accuracy: 0.9585 - val_loss: 0.0968 - val_accuracy: 0.9762\n",
      "Epoch 180/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0839 - accuracy: 0.9782\n",
      "Epoch 180: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0778 - accuracy: 0.9808 - val_loss: 0.0973 - val_accuracy: 0.9762\n",
      "Epoch 181/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0932 - accuracy: 0.9607\n",
      "Epoch 181: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0883 - accuracy: 0.9649 - val_loss: 0.0971 - val_accuracy: 0.9762\n",
      "Epoch 182/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0899 - accuracy: 0.9793\n",
      "Epoch 182: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.9776 - val_loss: 0.0970 - val_accuracy: 0.9762\n",
      "Epoch 183/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1028 - accuracy: 0.9636\n",
      "Epoch 183: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1006 - accuracy: 0.9681 - val_loss: 0.0968 - val_accuracy: 0.9762\n",
      "Epoch 184/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0969 - accuracy: 0.9686\n",
      "Epoch 184: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9712 - val_loss: 0.0968 - val_accuracy: 0.9762\n",
      "Epoch 185/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0963 - accuracy: 0.9640\n",
      "Epoch 185: val_loss did not improve from 0.09661\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1062 - accuracy: 0.9649 - val_loss: 0.0968 - val_accuracy: 0.9762\n",
      "Epoch 186/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9508\n",
      "Epoch 186: val_loss improved from 0.09661 to 0.09640, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1123 - accuracy: 0.9521 - val_loss: 0.0964 - val_accuracy: 0.9762\n",
      "Epoch 187/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1005 - accuracy: 0.9705\n",
      "Epoch 187: val_loss did not improve from 0.09640\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1002 - accuracy: 0.9712 - val_loss: 0.0964 - val_accuracy: 0.9762\n",
      "Epoch 188/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1223 - accuracy: 0.9593\n",
      "Epoch 188: val_loss did not improve from 0.09640\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1198 - accuracy: 0.9585 - val_loss: 0.0965 - val_accuracy: 0.9762\n",
      "Epoch 189/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9639\n",
      "Epoch 189: val_loss did not improve from 0.09640\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9617 - val_loss: 0.0966 - val_accuracy: 0.9762\n",
      "Epoch 190/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1044 - accuracy: 0.9661\n",
      "Epoch 190: val_loss did not improve from 0.09640\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1015 - accuracy: 0.9681 - val_loss: 0.0966 - val_accuracy: 0.9762\n",
      "Epoch 191/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9581\n",
      "Epoch 191: val_loss improved from 0.09640 to 0.09594, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1080 - accuracy: 0.9585 - val_loss: 0.0959 - val_accuracy: 0.9762\n",
      "Epoch 192/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0850 - accuracy: 0.9714\n",
      "Epoch 192: val_loss did not improve from 0.09594\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9712 - val_loss: 0.0962 - val_accuracy: 0.9762\n",
      "Epoch 193/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1038 - accuracy: 0.9673\n",
      "Epoch 193: val_loss improved from 0.09594 to 0.09579, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9712 - val_loss: 0.0958 - val_accuracy: 0.9762\n",
      "Epoch 194/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1051 - accuracy: 0.9615\n",
      "Epoch 194: val_loss did not improve from 0.09579\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9617 - val_loss: 0.0959 - val_accuracy: 0.9762\n",
      "Epoch 195/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0853 - accuracy: 0.9736\n",
      "Epoch 195: val_loss improved from 0.09579 to 0.09564, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9712 - val_loss: 0.0956 - val_accuracy: 0.9762\n",
      "Epoch 196/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9613\n",
      "Epoch 196: val_loss improved from 0.09564 to 0.09558, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1134 - accuracy: 0.9617 - val_loss: 0.0956 - val_accuracy: 0.9762\n",
      "Epoch 197/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0816 - accuracy: 0.9729\n",
      "Epoch 197: val_loss improved from 0.09558 to 0.09518, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0779 - accuracy: 0.9744 - val_loss: 0.0952 - val_accuracy: 0.9762\n",
      "Epoch 198/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9617\n",
      "Epoch 198: val_loss improved from 0.09518 to 0.09480, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1022 - accuracy: 0.9617 - val_loss: 0.0948 - val_accuracy: 0.9762\n",
      "Epoch 199/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1028 - accuracy: 0.9600\n",
      "Epoch 199: val_loss did not improve from 0.09480\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1055 - accuracy: 0.9585 - val_loss: 0.0950 - val_accuracy: 0.9762\n",
      "Epoch 200/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0947 - accuracy: 0.9633\n",
      "Epoch 200: val_loss did not improve from 0.09480\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0967 - accuracy: 0.9617 - val_loss: 0.0948 - val_accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_5 (GRU)                 (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0948 - accuracy: 0.9762\n",
      "Test Loss: 0.09482745826244354\n",
      "Test Accuracy: 0.976190447807312\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer with dropout_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, \n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.3,\n",
    "                     input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.6969 - accuracy: 0.6510\n",
      "Epoch 1: val_loss improved from inf to 0.64976, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 3s 11ms/step - loss: 0.7005 - accuracy: 0.6390 - val_loss: 0.6498 - val_accuracy: 0.7381\n",
      "Epoch 2/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.6573 - accuracy: 0.6836\n",
      "Epoch 2: val_loss improved from 0.64976 to 0.61326, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6463 - accuracy: 0.6997 - val_loss: 0.6133 - val_accuracy: 0.8036\n",
      "Epoch 3/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.6434 - accuracy: 0.7226\n",
      "Epoch 3: val_loss improved from 0.61326 to 0.58001, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6465 - accuracy: 0.7188 - val_loss: 0.5800 - val_accuracy: 0.8393\n",
      "Epoch 4/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.6089 - accuracy: 0.7627\n",
      "Epoch 4: val_loss improved from 0.58001 to 0.55285, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.6009 - accuracy: 0.7700 - val_loss: 0.5528 - val_accuracy: 0.8452\n",
      "Epoch 5/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.5886 - accuracy: 0.7900\n",
      "Epoch 5: val_loss improved from 0.55285 to 0.52800, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5933 - accuracy: 0.7859 - val_loss: 0.5280 - val_accuracy: 0.8452\n",
      "Epoch 6/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.5774 - accuracy: 0.7705\n",
      "Epoch 6: val_loss improved from 0.52800 to 0.50925, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5716 - accuracy: 0.7764 - val_loss: 0.5092 - val_accuracy: 0.8631\n",
      "Epoch 7/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.5654 - accuracy: 0.7929\n",
      "Epoch 7: val_loss improved from 0.50925 to 0.49013, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.5680 - accuracy: 0.7923 - val_loss: 0.4901 - val_accuracy: 0.8750\n",
      "Epoch 8/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.5413 - accuracy: 0.8194\n",
      "Epoch 8: val_loss improved from 0.49013 to 0.47414, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5406 - accuracy: 0.8179 - val_loss: 0.4741 - val_accuracy: 0.8810\n",
      "Epoch 9/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.5068 - accuracy: 0.8484\n",
      "Epoch 9: val_loss improved from 0.47414 to 0.45953, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5074 - accuracy: 0.8466 - val_loss: 0.4595 - val_accuracy: 0.8869\n",
      "Epoch 10/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.5065 - accuracy: 0.8492\n",
      "Epoch 10: val_loss improved from 0.45953 to 0.44597, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.5052 - accuracy: 0.8530 - val_loss: 0.4460 - val_accuracy: 0.8929\n",
      "Epoch 11/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.5105 - accuracy: 0.8328\n",
      "Epoch 11: val_loss improved from 0.44597 to 0.43451, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5070 - accuracy: 0.8371 - val_loss: 0.4345 - val_accuracy: 0.8988\n",
      "Epoch 12/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.5201 - accuracy: 0.8308\n",
      "Epoch 12: val_loss improved from 0.43451 to 0.42436, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5104 - accuracy: 0.8307 - val_loss: 0.4244 - val_accuracy: 0.8988\n",
      "Epoch 13/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.4673 - accuracy: 0.8627\n",
      "Epoch 13: val_loss improved from 0.42436 to 0.41443, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4737 - accuracy: 0.8626 - val_loss: 0.4144 - val_accuracy: 0.8988\n",
      "Epoch 14/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.4711 - accuracy: 0.8577\n",
      "Epoch 14: val_loss improved from 0.41443 to 0.40427, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4707 - accuracy: 0.8530 - val_loss: 0.4043 - val_accuracy: 0.9048\n",
      "Epoch 15/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.4669 - accuracy: 0.8618\n",
      "Epoch 15: val_loss improved from 0.40427 to 0.39597, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4632 - accuracy: 0.8690 - val_loss: 0.3960 - val_accuracy: 0.9107\n",
      "Epoch 16/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.4488 - accuracy: 0.8473\n",
      "Epoch 16: val_loss improved from 0.39597 to 0.38836, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4428 - accuracy: 0.8530 - val_loss: 0.3884 - val_accuracy: 0.9107\n",
      "Epoch 17/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.4355 - accuracy: 0.8909\n",
      "Epoch 17: val_loss improved from 0.38836 to 0.38097, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4380 - accuracy: 0.8946 - val_loss: 0.3810 - val_accuracy: 0.9107\n",
      "Epoch 18/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.4628 - accuracy: 0.8302\n",
      "Epoch 18: val_loss improved from 0.38097 to 0.37427, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4567 - accuracy: 0.8339 - val_loss: 0.3743 - val_accuracy: 0.9167\n",
      "Epoch 19/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.4283 - accuracy: 0.8778\n",
      "Epoch 19: val_loss improved from 0.37427 to 0.36696, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4148 - accuracy: 0.8882 - val_loss: 0.3670 - val_accuracy: 0.9167\n",
      "Epoch 20/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.4113 - accuracy: 0.8792\n",
      "Epoch 20: val_loss improved from 0.36696 to 0.36109, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4088 - accuracy: 0.8882 - val_loss: 0.3611 - val_accuracy: 0.9167\n",
      "Epoch 21/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.4221 - accuracy: 0.8979\n",
      "Epoch 21: val_loss improved from 0.36109 to 0.35534, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4055 - accuracy: 0.8978 - val_loss: 0.3553 - val_accuracy: 0.9167\n",
      "Epoch 22/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.4288 - accuracy: 0.8571\n",
      "Epoch 22: val_loss improved from 0.35534 to 0.35003, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4302 - accuracy: 0.8626 - val_loss: 0.3500 - val_accuracy: 0.9167\n",
      "Epoch 23/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.4078 - accuracy: 0.8784\n",
      "Epoch 23: val_loss improved from 0.35003 to 0.34521, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4158 - accuracy: 0.8722 - val_loss: 0.3452 - val_accuracy: 0.9345\n",
      "Epoch 24/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.3921 - accuracy: 0.8898\n",
      "Epoch 24: val_loss improved from 0.34521 to 0.34005, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3946 - accuracy: 0.8850 - val_loss: 0.3401 - val_accuracy: 0.9345\n",
      "Epoch 25/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.3939 - accuracy: 0.8612\n",
      "Epoch 25: val_loss improved from 0.34005 to 0.33552, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4022 - accuracy: 0.8626 - val_loss: 0.3355 - val_accuracy: 0.9345\n",
      "Epoch 26/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3813 - accuracy: 0.8926\n",
      "Epoch 26: val_loss improved from 0.33552 to 0.33009, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3798 - accuracy: 0.8946 - val_loss: 0.3301 - val_accuracy: 0.9345\n",
      "Epoch 27/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3880 - accuracy: 0.8873\n",
      "Epoch 27: val_loss improved from 0.33009 to 0.32615, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3861 - accuracy: 0.8882 - val_loss: 0.3261 - val_accuracy: 0.9345\n",
      "Epoch 28/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3617 - accuracy: 0.9185\n",
      "Epoch 28: val_loss improved from 0.32615 to 0.32200, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3617 - accuracy: 0.9137 - val_loss: 0.3220 - val_accuracy: 0.9345\n",
      "Epoch 29/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3716 - accuracy: 0.8912\n",
      "Epoch 29: val_loss improved from 0.32200 to 0.31802, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3733 - accuracy: 0.8882 - val_loss: 0.3180 - val_accuracy: 0.9345\n",
      "Epoch 30/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3520 - accuracy: 0.9298\n",
      "Epoch 30: val_loss improved from 0.31802 to 0.31442, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3484 - accuracy: 0.9297 - val_loss: 0.3144 - val_accuracy: 0.9345\n",
      "Epoch 31/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.3519 - accuracy: 0.8979\n",
      "Epoch 31: val_loss improved from 0.31442 to 0.31109, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3630 - accuracy: 0.8882 - val_loss: 0.3111 - val_accuracy: 0.9345\n",
      "Epoch 32/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.3844 - accuracy: 0.8863\n",
      "Epoch 32: val_loss improved from 0.31109 to 0.30790, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3780 - accuracy: 0.8882 - val_loss: 0.3079 - val_accuracy: 0.9405\n",
      "Epoch 33/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3461 - accuracy: 0.9094\n",
      "Epoch 33: val_loss improved from 0.30790 to 0.30498, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3434 - accuracy: 0.9105 - val_loss: 0.3050 - val_accuracy: 0.9464\n",
      "Epoch 34/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.8818\n",
      "Epoch 34: val_loss improved from 0.30498 to 0.30182, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3620 - accuracy: 0.8818 - val_loss: 0.3018 - val_accuracy: 0.9464\n",
      "Epoch 35/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.3406 - accuracy: 0.9102\n",
      "Epoch 35: val_loss improved from 0.30182 to 0.29867, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3467 - accuracy: 0.9010 - val_loss: 0.2987 - val_accuracy: 0.9464\n",
      "Epoch 36/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3224 - accuracy: 0.9309\n",
      "Epoch 36: val_loss improved from 0.29867 to 0.29586, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3279 - accuracy: 0.9297 - val_loss: 0.2959 - val_accuracy: 0.9464\n",
      "Epoch 37/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.3446 - accuracy: 0.9098\n",
      "Epoch 37: val_loss improved from 0.29586 to 0.29316, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3456 - accuracy: 0.9073 - val_loss: 0.2932 - val_accuracy: 0.9464\n",
      "Epoch 38/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.3209 - accuracy: 0.9040\n",
      "Epoch 38: val_loss improved from 0.29316 to 0.29049, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3280 - accuracy: 0.9042 - val_loss: 0.2905 - val_accuracy: 0.9464\n",
      "Epoch 39/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.3438 - accuracy: 0.9059\n",
      "Epoch 39: val_loss improved from 0.29049 to 0.28799, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3460 - accuracy: 0.9010 - val_loss: 0.2880 - val_accuracy: 0.9464\n",
      "Epoch 40/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3573 - accuracy: 0.8889\n",
      "Epoch 40: val_loss improved from 0.28799 to 0.28564, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3574 - accuracy: 0.8850 - val_loss: 0.2856 - val_accuracy: 0.9464\n",
      "Epoch 41/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3298 - accuracy: 0.9088\n",
      "Epoch 41: val_loss improved from 0.28564 to 0.28307, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3244 - accuracy: 0.9137 - val_loss: 0.2831 - val_accuracy: 0.9464\n",
      "Epoch 42/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3208 - accuracy: 0.9143\n",
      "Epoch 42: val_loss improved from 0.28307 to 0.28044, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3251 - accuracy: 0.9137 - val_loss: 0.2804 - val_accuracy: 0.9464\n",
      "Epoch 43/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.3371 - accuracy: 0.9133\n",
      "Epoch 43: val_loss improved from 0.28044 to 0.27823, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3437 - accuracy: 0.9073 - val_loss: 0.2782 - val_accuracy: 0.9464\n",
      "Epoch 44/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3266 - accuracy: 0.9263\n",
      "Epoch 44: val_loss improved from 0.27823 to 0.27597, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3192 - accuracy: 0.9297 - val_loss: 0.2760 - val_accuracy: 0.9464\n",
      "Epoch 45/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.3179 - accuracy: 0.9216\n",
      "Epoch 45: val_loss improved from 0.27597 to 0.27301, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3061 - accuracy: 0.9233 - val_loss: 0.2730 - val_accuracy: 0.9464\n",
      "Epoch 46/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.3060 - accuracy: 0.9269\n",
      "Epoch 46: val_loss improved from 0.27301 to 0.27108, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3071 - accuracy: 0.9233 - val_loss: 0.2711 - val_accuracy: 0.9464\n",
      "Epoch 47/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3040 - accuracy: 0.9321\n",
      "Epoch 47: val_loss improved from 0.27108 to 0.26938, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3074 - accuracy: 0.9329 - val_loss: 0.2694 - val_accuracy: 0.9464\n",
      "Epoch 48/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2877 - accuracy: 0.9216\n",
      "Epoch 48: val_loss improved from 0.26938 to 0.26706, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2863 - accuracy: 0.9233 - val_loss: 0.2671 - val_accuracy: 0.9464\n",
      "Epoch 49/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3200 - accuracy: 0.8982\n",
      "Epoch 49: val_loss improved from 0.26706 to 0.26488, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3122 - accuracy: 0.9010 - val_loss: 0.2649 - val_accuracy: 0.9464\n",
      "Epoch 50/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2925 - accuracy: 0.9143\n",
      "Epoch 50: val_loss improved from 0.26488 to 0.26224, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2938 - accuracy: 0.9137 - val_loss: 0.2622 - val_accuracy: 0.9464\n",
      "Epoch 51/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2869 - accuracy: 0.9228\n",
      "Epoch 51: val_loss improved from 0.26224 to 0.25990, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2916 - accuracy: 0.9233 - val_loss: 0.2599 - val_accuracy: 0.9464\n",
      "Epoch 52/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3093 - accuracy: 0.9179\n",
      "Epoch 52: val_loss improved from 0.25990 to 0.25831, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3029 - accuracy: 0.9233 - val_loss: 0.2583 - val_accuracy: 0.9464\n",
      "Epoch 53/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2959 - accuracy: 0.9370\n",
      "Epoch 53: val_loss improved from 0.25831 to 0.25628, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2918 - accuracy: 0.9425 - val_loss: 0.2563 - val_accuracy: 0.9464\n",
      "Epoch 54/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2880 - accuracy: 0.9296\n",
      "Epoch 54: val_loss improved from 0.25628 to 0.25477, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2870 - accuracy: 0.9265 - val_loss: 0.2548 - val_accuracy: 0.9464\n",
      "Epoch 55/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3003 - accuracy: 0.9170\n",
      "Epoch 55: val_loss improved from 0.25477 to 0.25310, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2927 - accuracy: 0.9201 - val_loss: 0.2531 - val_accuracy: 0.9464\n",
      "Epoch 56/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.9097\n",
      "Epoch 56: val_loss improved from 0.25310 to 0.25155, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3065 - accuracy: 0.9105 - val_loss: 0.2516 - val_accuracy: 0.9464\n",
      "Epoch 57/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.3279 - accuracy: 0.9077\n",
      "Epoch 57: val_loss improved from 0.25155 to 0.25014, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3079 - accuracy: 0.9137 - val_loss: 0.2501 - val_accuracy: 0.9464\n",
      "Epoch 58/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2773 - accuracy: 0.9396\n",
      "Epoch 58: val_loss improved from 0.25014 to 0.24897, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2858 - accuracy: 0.9393 - val_loss: 0.2490 - val_accuracy: 0.9464\n",
      "Epoch 59/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2818 - accuracy: 0.9444\n",
      "Epoch 59: val_loss improved from 0.24897 to 0.24748, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.9457 - val_loss: 0.2475 - val_accuracy: 0.9464\n",
      "Epoch 60/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2797 - accuracy: 0.9245\n",
      "Epoch 60: val_loss improved from 0.24748 to 0.24608, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2824 - accuracy: 0.9265 - val_loss: 0.2461 - val_accuracy: 0.9464\n",
      "Epoch 61/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3283 - accuracy: 0.8982\n",
      "Epoch 61: val_loss improved from 0.24608 to 0.24507, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.9105 - val_loss: 0.2451 - val_accuracy: 0.9464\n",
      "Epoch 62/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2922 - accuracy: 0.9228\n",
      "Epoch 62: val_loss improved from 0.24507 to 0.24307, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2834 - accuracy: 0.9297 - val_loss: 0.2431 - val_accuracy: 0.9464\n",
      "Epoch 63/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2744 - accuracy: 0.9355\n",
      "Epoch 63: val_loss improved from 0.24307 to 0.24197, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2752 - accuracy: 0.9361 - val_loss: 0.2420 - val_accuracy: 0.9464\n",
      "Epoch 64/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2637 - accuracy: 0.9322\n",
      "Epoch 64: val_loss improved from 0.24197 to 0.24034, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2772 - accuracy: 0.9265 - val_loss: 0.2403 - val_accuracy: 0.9464\n",
      "Epoch 65/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2664 - accuracy: 0.9418\n",
      "Epoch 65: val_loss improved from 0.24034 to 0.23902, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2698 - accuracy: 0.9361 - val_loss: 0.2390 - val_accuracy: 0.9524\n",
      "Epoch 66/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2576 - accuracy: 0.9440\n",
      "Epoch 66: val_loss improved from 0.23902 to 0.23773, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2602 - accuracy: 0.9393 - val_loss: 0.2377 - val_accuracy: 0.9524\n",
      "Epoch 67/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2771 - accuracy: 0.9345\n",
      "Epoch 67: val_loss improved from 0.23773 to 0.23641, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2739 - accuracy: 0.9329 - val_loss: 0.2364 - val_accuracy: 0.9524\n",
      "Epoch 68/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2698 - accuracy: 0.9345\n",
      "Epoch 68: val_loss improved from 0.23641 to 0.23550, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2684 - accuracy: 0.9361 - val_loss: 0.2355 - val_accuracy: 0.9524\n",
      "Epoch 69/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2844 - accuracy: 0.9082\n",
      "Epoch 69: val_loss improved from 0.23550 to 0.23439, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2865 - accuracy: 0.9073 - val_loss: 0.2344 - val_accuracy: 0.9524\n",
      "Epoch 70/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2413 - accuracy: 0.9424\n",
      "Epoch 70: val_loss improved from 0.23439 to 0.23304, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2505 - accuracy: 0.9425 - val_loss: 0.2330 - val_accuracy: 0.9524\n",
      "Epoch 71/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2867 - accuracy: 0.9137\n",
      "Epoch 71: val_loss improved from 0.23304 to 0.23162, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2867 - accuracy: 0.9137 - val_loss: 0.2316 - val_accuracy: 0.9524\n",
      "Epoch 72/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2219 - accuracy: 0.9608\n",
      "Epoch 72: val_loss improved from 0.23162 to 0.23044, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2469 - accuracy: 0.9489 - val_loss: 0.2304 - val_accuracy: 0.9524\n",
      "Epoch 73/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2669 - accuracy: 0.9481\n",
      "Epoch 73: val_loss improved from 0.23044 to 0.22896, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2735 - accuracy: 0.9425 - val_loss: 0.2290 - val_accuracy: 0.9524\n",
      "Epoch 74/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2660 - accuracy: 0.9382\n",
      "Epoch 74: val_loss improved from 0.22896 to 0.22798, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2789 - accuracy: 0.9329 - val_loss: 0.2280 - val_accuracy: 0.9524\n",
      "Epoch 75/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2871 - accuracy: 0.9245\n",
      "Epoch 75: val_loss improved from 0.22798 to 0.22686, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2786 - accuracy: 0.9233 - val_loss: 0.2269 - val_accuracy: 0.9524\n",
      "Epoch 76/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2459 - accuracy: 0.9509\n",
      "Epoch 76: val_loss improved from 0.22686 to 0.22610, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2567 - accuracy: 0.9425 - val_loss: 0.2261 - val_accuracy: 0.9524\n",
      "Epoch 77/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2592 - accuracy: 0.9475\n",
      "Epoch 77: val_loss improved from 0.22610 to 0.22494, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2577 - accuracy: 0.9489 - val_loss: 0.2249 - val_accuracy: 0.9524\n",
      "Epoch 78/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2515 - accuracy: 0.9536\n",
      "Epoch 78: val_loss improved from 0.22494 to 0.22349, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2462 - accuracy: 0.9521 - val_loss: 0.2235 - val_accuracy: 0.9524\n",
      "Epoch 79/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2604 - accuracy: 0.9320\n",
      "Epoch 79: val_loss improved from 0.22349 to 0.22248, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2634 - accuracy: 0.9329 - val_loss: 0.2225 - val_accuracy: 0.9583\n",
      "Epoch 80/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.2491 - accuracy: 0.9319\n",
      "Epoch 80: val_loss improved from 0.22248 to 0.22158, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2560 - accuracy: 0.9361 - val_loss: 0.2216 - val_accuracy: 0.9524\n",
      "Epoch 81/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.9452\n",
      "Epoch 81: val_loss improved from 0.22158 to 0.22041, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2601 - accuracy: 0.9457 - val_loss: 0.2204 - val_accuracy: 0.9524\n",
      "Epoch 82/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2546 - accuracy: 0.9434\n",
      "Epoch 82: val_loss improved from 0.22041 to 0.21951, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2606 - accuracy: 0.9329 - val_loss: 0.2195 - val_accuracy: 0.9583\n",
      "Epoch 83/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2518 - accuracy: 0.9448\n",
      "Epoch 83: val_loss improved from 0.21951 to 0.21848, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2463 - accuracy: 0.9457 - val_loss: 0.2185 - val_accuracy: 0.9583\n",
      "Epoch 84/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2219 - accuracy: 0.9614\n",
      "Epoch 84: val_loss improved from 0.21848 to 0.21737, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2329 - accuracy: 0.9521 - val_loss: 0.2174 - val_accuracy: 0.9583\n",
      "Epoch 85/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2524 - accuracy: 0.9347\n",
      "Epoch 85: val_loss improved from 0.21737 to 0.21652, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2406 - accuracy: 0.9425 - val_loss: 0.2165 - val_accuracy: 0.9583\n",
      "Epoch 86/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.2624 - accuracy: 0.9375\n",
      "Epoch 86: val_loss improved from 0.21652 to 0.21531, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2507 - accuracy: 0.9393 - val_loss: 0.2153 - val_accuracy: 0.9583\n",
      "Epoch 87/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2477 - accuracy: 0.9296\n",
      "Epoch 87: val_loss improved from 0.21531 to 0.21446, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2513 - accuracy: 0.9265 - val_loss: 0.2145 - val_accuracy: 0.9583\n",
      "Epoch 88/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2482 - accuracy: 0.9333\n",
      "Epoch 88: val_loss improved from 0.21446 to 0.21362, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2414 - accuracy: 0.9361 - val_loss: 0.2136 - val_accuracy: 0.9583\n",
      "Epoch 89/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2534 - accuracy: 0.9333\n",
      "Epoch 89: val_loss improved from 0.21362 to 0.21304, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2480 - accuracy: 0.9393 - val_loss: 0.2130 - val_accuracy: 0.9583\n",
      "Epoch 90/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2507 - accuracy: 0.9393\n",
      "Epoch 90: val_loss improved from 0.21304 to 0.21224, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2469 - accuracy: 0.9393 - val_loss: 0.2122 - val_accuracy: 0.9583\n",
      "Epoch 91/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2324 - accuracy: 0.9536\n",
      "Epoch 91: val_loss improved from 0.21224 to 0.21130, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2376 - accuracy: 0.9553 - val_loss: 0.2113 - val_accuracy: 0.9583\n",
      "Epoch 92/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2479 - accuracy: 0.9388\n",
      "Epoch 92: val_loss improved from 0.21130 to 0.21075, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2319 - accuracy: 0.9489 - val_loss: 0.2107 - val_accuracy: 0.9583\n",
      "Epoch 93/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2216 - accuracy: 0.9519\n",
      "Epoch 93: val_loss improved from 0.21075 to 0.21032, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.9489 - val_loss: 0.2103 - val_accuracy: 0.9583\n",
      "Epoch 94/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2458 - accuracy: 0.9577\n",
      "Epoch 94: val_loss improved from 0.21032 to 0.20982, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2504 - accuracy: 0.9553 - val_loss: 0.2098 - val_accuracy: 0.9583\n",
      "Epoch 95/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2461 - accuracy: 0.9404\n",
      "Epoch 95: val_loss improved from 0.20982 to 0.20888, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2401 - accuracy: 0.9457 - val_loss: 0.2089 - val_accuracy: 0.9583\n",
      "Epoch 96/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.1865 - accuracy: 0.9745\n",
      "Epoch 96: val_loss improved from 0.20888 to 0.20836, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2262 - accuracy: 0.9553 - val_loss: 0.2084 - val_accuracy: 0.9583\n",
      "Epoch 97/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2225 - accuracy: 0.9472\n",
      "Epoch 97: val_loss improved from 0.20836 to 0.20721, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2289 - accuracy: 0.9457 - val_loss: 0.2072 - val_accuracy: 0.9583\n",
      "Epoch 98/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2272 - accuracy: 0.9536\n",
      "Epoch 98: val_loss improved from 0.20721 to 0.20634, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2347 - accuracy: 0.9489 - val_loss: 0.2063 - val_accuracy: 0.9643\n",
      "Epoch 99/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2451 - accuracy: 0.9407\n",
      "Epoch 99: val_loss improved from 0.20634 to 0.20576, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2341 - accuracy: 0.9489 - val_loss: 0.2058 - val_accuracy: 0.9643\n",
      "Epoch 100/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2082 - accuracy: 0.9444\n",
      "Epoch 100: val_loss improved from 0.20576 to 0.20490, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2164 - accuracy: 0.9457 - val_loss: 0.2049 - val_accuracy: 0.9643\n",
      "Epoch 101/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2226 - accuracy: 0.9480\n",
      "Epoch 101: val_loss improved from 0.20490 to 0.20410, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2212 - accuracy: 0.9489 - val_loss: 0.2041 - val_accuracy: 0.9643\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9553\n",
      "Epoch 102: val_loss improved from 0.20410 to 0.20333, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2194 - accuracy: 0.9553 - val_loss: 0.2033 - val_accuracy: 0.9643\n",
      "Epoch 103/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2198 - accuracy: 0.9483\n",
      "Epoch 103: val_loss improved from 0.20333 to 0.20270, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2153 - accuracy: 0.9489 - val_loss: 0.2027 - val_accuracy: 0.9643\n",
      "Epoch 104/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9387\n",
      "Epoch 104: val_loss improved from 0.20270 to 0.20222, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2358 - accuracy: 0.9393 - val_loss: 0.2022 - val_accuracy: 0.9643\n",
      "Epoch 105/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2051 - accuracy: 0.9592\n",
      "Epoch 105: val_loss improved from 0.20222 to 0.20156, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2144 - accuracy: 0.9521 - val_loss: 0.2016 - val_accuracy: 0.9643\n",
      "Epoch 106/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2119 - accuracy: 0.9414\n",
      "Epoch 106: val_loss improved from 0.20156 to 0.20133, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2093 - accuracy: 0.9425 - val_loss: 0.2013 - val_accuracy: 0.9643\n",
      "Epoch 107/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9645\n",
      "Epoch 107: val_loss improved from 0.20133 to 0.20058, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2139 - accuracy: 0.9649 - val_loss: 0.2006 - val_accuracy: 0.9643\n",
      "Epoch 108/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2220 - accuracy: 0.9541\n",
      "Epoch 108: val_loss improved from 0.20058 to 0.20027, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2188 - accuracy: 0.9553 - val_loss: 0.2003 - val_accuracy: 0.9643\n",
      "Epoch 109/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2272 - accuracy: 0.9519\n",
      "Epoch 109: val_loss improved from 0.20027 to 0.19958, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2232 - accuracy: 0.9489 - val_loss: 0.1996 - val_accuracy: 0.9643\n",
      "Epoch 110/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2158 - accuracy: 0.9517\n",
      "Epoch 110: val_loss improved from 0.19958 to 0.19945, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2204 - accuracy: 0.9489 - val_loss: 0.1995 - val_accuracy: 0.9643\n",
      "Epoch 111/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2092 - accuracy: 0.9553\n",
      "Epoch 111: val_loss improved from 0.19945 to 0.19912, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2092 - accuracy: 0.9553 - val_loss: 0.1991 - val_accuracy: 0.9643\n",
      "Epoch 112/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2264 - accuracy: 0.9525\n",
      "Epoch 112: val_loss improved from 0.19912 to 0.19864, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2277 - accuracy: 0.9457 - val_loss: 0.1986 - val_accuracy: 0.9643\n",
      "Epoch 113/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1998 - accuracy: 0.9745\n",
      "Epoch 113: val_loss improved from 0.19864 to 0.19809, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2094 - accuracy: 0.9712 - val_loss: 0.1981 - val_accuracy: 0.9643\n",
      "Epoch 114/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2230 - accuracy: 0.9490\n",
      "Epoch 114: val_loss improved from 0.19809 to 0.19743, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2115 - accuracy: 0.9553 - val_loss: 0.1974 - val_accuracy: 0.9643\n",
      "Epoch 115/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2079 - accuracy: 0.9500\n",
      "Epoch 115: val_loss improved from 0.19743 to 0.19703, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2064 - accuracy: 0.9521 - val_loss: 0.1970 - val_accuracy: 0.9643\n",
      "Epoch 116/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2023 - accuracy: 0.9698\n",
      "Epoch 116: val_loss improved from 0.19703 to 0.19625, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1967 - accuracy: 0.9712 - val_loss: 0.1963 - val_accuracy: 0.9643\n",
      "Epoch 117/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2368 - accuracy: 0.9382\n",
      "Epoch 117: val_loss improved from 0.19625 to 0.19577, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2241 - accuracy: 0.9457 - val_loss: 0.1958 - val_accuracy: 0.9643\n",
      "Epoch 118/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2108 - accuracy: 0.9484\n",
      "Epoch 118: val_loss improved from 0.19577 to 0.19501, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2108 - accuracy: 0.9489 - val_loss: 0.1950 - val_accuracy: 0.9643\n",
      "Epoch 119/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2108 - accuracy: 0.9527\n",
      "Epoch 119: val_loss improved from 0.19501 to 0.19403, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2058 - accuracy: 0.9521 - val_loss: 0.1940 - val_accuracy: 0.9643\n",
      "Epoch 120/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2036 - accuracy: 0.9556\n",
      "Epoch 120: val_loss improved from 0.19403 to 0.19365, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2013 - accuracy: 0.9585 - val_loss: 0.1936 - val_accuracy: 0.9643\n",
      "Epoch 121/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1879 - accuracy: 0.9623\n",
      "Epoch 121: val_loss improved from 0.19365 to 0.19352, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9617 - val_loss: 0.1935 - val_accuracy: 0.9643\n",
      "Epoch 122/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1965 - accuracy: 0.9673\n",
      "Epoch 122: val_loss improved from 0.19352 to 0.19300, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2016 - accuracy: 0.9617 - val_loss: 0.1930 - val_accuracy: 0.9643\n",
      "Epoch 123/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2168 - accuracy: 0.9429\n",
      "Epoch 123: val_loss improved from 0.19300 to 0.19232, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2135 - accuracy: 0.9457 - val_loss: 0.1923 - val_accuracy: 0.9643\n",
      "Epoch 124/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2062 - accuracy: 0.9509\n",
      "Epoch 124: val_loss improved from 0.19232 to 0.19192, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2111 - accuracy: 0.9521 - val_loss: 0.1919 - val_accuracy: 0.9643\n",
      "Epoch 125/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2080 - accuracy: 0.9564\n",
      "Epoch 125: val_loss improved from 0.19192 to 0.19120, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2022 - accuracy: 0.9585 - val_loss: 0.1912 - val_accuracy: 0.9643\n",
      "Epoch 126/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2016 - accuracy: 0.9614\n",
      "Epoch 126: val_loss improved from 0.19120 to 0.19114, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.2005 - accuracy: 0.9617 - val_loss: 0.1911 - val_accuracy: 0.9643\n",
      "Epoch 127/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2200 - accuracy: 0.9592\n",
      "Epoch 127: val_loss improved from 0.19114 to 0.19062, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2082 - accuracy: 0.9617 - val_loss: 0.1906 - val_accuracy: 0.9702\n",
      "Epoch 128/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2010 - accuracy: 0.9633\n",
      "Epoch 128: val_loss improved from 0.19062 to 0.18997, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1980 - accuracy: 0.9649 - val_loss: 0.1900 - val_accuracy: 0.9702\n",
      "Epoch 129/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1979 - accuracy: 0.9765\n",
      "Epoch 129: val_loss improved from 0.18997 to 0.18963, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1881 - accuracy: 0.9808 - val_loss: 0.1896 - val_accuracy: 0.9702\n",
      "Epoch 130/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1931 - accuracy: 0.9614\n",
      "Epoch 130: val_loss improved from 0.18963 to 0.18905, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1907 - accuracy: 0.9649 - val_loss: 0.1890 - val_accuracy: 0.9702\n",
      "Epoch 131/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1984 - accuracy: 0.9614\n",
      "Epoch 131: val_loss improved from 0.18905 to 0.18891, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9649 - val_loss: 0.1889 - val_accuracy: 0.9702\n",
      "Epoch 132/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2073 - accuracy: 0.9679\n",
      "Epoch 132: val_loss improved from 0.18891 to 0.18836, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2038 - accuracy: 0.9712 - val_loss: 0.1884 - val_accuracy: 0.9702\n",
      "Epoch 133/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2246 - accuracy: 0.9368\n",
      "Epoch 133: val_loss improved from 0.18836 to 0.18780, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.9425 - val_loss: 0.1878 - val_accuracy: 0.9702\n",
      "Epoch 134/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1820 - accuracy: 0.9700\n",
      "Epoch 134: val_loss improved from 0.18780 to 0.18764, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1922 - accuracy: 0.9649 - val_loss: 0.1876 - val_accuracy: 0.9702\n",
      "Epoch 135/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1705 - accuracy: 0.9789\n",
      "Epoch 135: val_loss improved from 0.18764 to 0.18742, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1830 - accuracy: 0.9712 - val_loss: 0.1874 - val_accuracy: 0.9702\n",
      "Epoch 136/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9553\n",
      "Epoch 136: val_loss improved from 0.18742 to 0.18699, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1916 - accuracy: 0.9553 - val_loss: 0.1870 - val_accuracy: 0.9702\n",
      "Epoch 137/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1827 - accuracy: 0.9661\n",
      "Epoch 137: val_loss improved from 0.18699 to 0.18634, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9681 - val_loss: 0.1863 - val_accuracy: 0.9702\n",
      "Epoch 138/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1945 - accuracy: 0.9698\n",
      "Epoch 138: val_loss improved from 0.18634 to 0.18544, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1967 - accuracy: 0.9649 - val_loss: 0.1854 - val_accuracy: 0.9702\n",
      "Epoch 139/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1707 - accuracy: 0.9673\n",
      "Epoch 139: val_loss improved from 0.18544 to 0.18500, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1834 - accuracy: 0.9553 - val_loss: 0.1850 - val_accuracy: 0.9702\n",
      "Epoch 140/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1759 - accuracy: 0.9741\n",
      "Epoch 140: val_loss did not improve from 0.18500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1898 - accuracy: 0.9681 - val_loss: 0.1850 - val_accuracy: 0.9702\n",
      "Epoch 141/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2015 - accuracy: 0.9608\n",
      "Epoch 141: val_loss did not improve from 0.18500\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1950 - accuracy: 0.9617 - val_loss: 0.1850 - val_accuracy: 0.9702\n",
      "Epoch 142/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1703 - accuracy: 0.9736\n",
      "Epoch 142: val_loss improved from 0.18500 to 0.18463, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1882 - accuracy: 0.9681 - val_loss: 0.1846 - val_accuracy: 0.9702\n",
      "Epoch 143/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1940 - accuracy: 0.9556\n",
      "Epoch 143: val_loss improved from 0.18463 to 0.18369, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1920 - accuracy: 0.9585 - val_loss: 0.1837 - val_accuracy: 0.9702\n",
      "Epoch 144/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1932 - accuracy: 0.9577\n",
      "Epoch 144: val_loss improved from 0.18369 to 0.18323, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1980 - accuracy: 0.9585 - val_loss: 0.1832 - val_accuracy: 0.9702\n",
      "Epoch 145/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2028 - accuracy: 0.9547\n",
      "Epoch 145: val_loss improved from 0.18323 to 0.18317, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1979 - accuracy: 0.9585 - val_loss: 0.1832 - val_accuracy: 0.9702\n",
      "Epoch 146/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1758 - accuracy: 0.9704\n",
      "Epoch 146: val_loss improved from 0.18317 to 0.18297, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.9681 - val_loss: 0.1830 - val_accuracy: 0.9702\n",
      "Epoch 147/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2009 - accuracy: 0.9623\n",
      "Epoch 147: val_loss improved from 0.18297 to 0.18257, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2066 - accuracy: 0.9585 - val_loss: 0.1826 - val_accuracy: 0.9702\n",
      "Epoch 148/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1844 - accuracy: 0.9667\n",
      "Epoch 148: val_loss improved from 0.18257 to 0.18220, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1815 - accuracy: 0.9681 - val_loss: 0.1822 - val_accuracy: 0.9702\n",
      "Epoch 149/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1906 - accuracy: 0.9639\n",
      "Epoch 149: val_loss improved from 0.18220 to 0.18164, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1888 - accuracy: 0.9649 - val_loss: 0.1816 - val_accuracy: 0.9702\n",
      "Epoch 150/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1790 - accuracy: 0.9667\n",
      "Epoch 150: val_loss improved from 0.18164 to 0.18068, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1838 - accuracy: 0.9617 - val_loss: 0.1807 - val_accuracy: 0.9702\n",
      "Epoch 151/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2117 - accuracy: 0.9585\n",
      "Epoch 151: val_loss improved from 0.18068 to 0.17991, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2053 - accuracy: 0.9585 - val_loss: 0.1799 - val_accuracy: 0.9702\n",
      "Epoch 152/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1637 - accuracy: 0.9704\n",
      "Epoch 152: val_loss did not improve from 0.17991\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1685 - accuracy: 0.9681 - val_loss: 0.1800 - val_accuracy: 0.9702\n",
      "Epoch 153/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1748 - accuracy: 0.9733\n",
      "Epoch 153: val_loss improved from 0.17991 to 0.17983, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1778 - accuracy: 0.9712 - val_loss: 0.1798 - val_accuracy: 0.9702\n",
      "Epoch 154/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1961 - accuracy: 0.9560\n",
      "Epoch 154: val_loss improved from 0.17983 to 0.17946, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1845 - accuracy: 0.9649 - val_loss: 0.1795 - val_accuracy: 0.9702\n",
      "Epoch 155/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1690 - accuracy: 0.9667\n",
      "Epoch 155: val_loss improved from 0.17946 to 0.17907, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1715 - accuracy: 0.9649 - val_loss: 0.1791 - val_accuracy: 0.9702\n",
      "Epoch 156/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9681\n",
      "Epoch 156: val_loss improved from 0.17907 to 0.17863, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1852 - accuracy: 0.9681 - val_loss: 0.1786 - val_accuracy: 0.9702\n",
      "Epoch 157/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1943 - accuracy: 0.9509\n",
      "Epoch 157: val_loss did not improve from 0.17863\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9489 - val_loss: 0.1787 - val_accuracy: 0.9702\n",
      "Epoch 158/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1688 - accuracy: 0.9724\n",
      "Epoch 158: val_loss improved from 0.17863 to 0.17860, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1786 - accuracy: 0.9617 - val_loss: 0.1786 - val_accuracy: 0.9702\n",
      "Epoch 159/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1601 - accuracy: 0.9750\n",
      "Epoch 159: val_loss improved from 0.17860 to 0.17797, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9649 - val_loss: 0.1780 - val_accuracy: 0.9702\n",
      "Epoch 160/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1814 - accuracy: 0.9667\n",
      "Epoch 160: val_loss improved from 0.17797 to 0.17762, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1750 - accuracy: 0.9681 - val_loss: 0.1776 - val_accuracy: 0.9702\n",
      "Epoch 161/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1722 - accuracy: 0.9579\n",
      "Epoch 161: val_loss improved from 0.17762 to 0.17735, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1679 - accuracy: 0.9617 - val_loss: 0.1774 - val_accuracy: 0.9702\n",
      "Epoch 162/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1711 - accuracy: 0.9600\n",
      "Epoch 162: val_loss improved from 0.17735 to 0.17697, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1725 - accuracy: 0.9617 - val_loss: 0.1770 - val_accuracy: 0.9702\n",
      "Epoch 163/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1871 - accuracy: 0.9592\n",
      "Epoch 163: val_loss did not improve from 0.17697\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1750 - accuracy: 0.9649 - val_loss: 0.1771 - val_accuracy: 0.9702\n",
      "Epoch 164/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1594 - accuracy: 0.9731\n",
      "Epoch 164: val_loss improved from 0.17697 to 0.17665, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.9712 - val_loss: 0.1766 - val_accuracy: 0.9702\n",
      "Epoch 165/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1771 - accuracy: 0.9630\n",
      "Epoch 165: val_loss improved from 0.17665 to 0.17656, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1804 - accuracy: 0.9649 - val_loss: 0.1766 - val_accuracy: 0.9702\n",
      "Epoch 166/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1861 - accuracy: 0.9600\n",
      "Epoch 166: val_loss improved from 0.17656 to 0.17569, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1876 - accuracy: 0.9585 - val_loss: 0.1757 - val_accuracy: 0.9702\n",
      "Epoch 167/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1624 - accuracy: 0.9709\n",
      "Epoch 167: val_loss improved from 0.17569 to 0.17555, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1636 - accuracy: 0.9712 - val_loss: 0.1755 - val_accuracy: 0.9702\n",
      "Epoch 168/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1586 - accuracy: 0.9741\n",
      "Epoch 168: val_loss improved from 0.17555 to 0.17541, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1701 - accuracy: 0.9681 - val_loss: 0.1754 - val_accuracy: 0.9702\n",
      "Epoch 169/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9623\n",
      "Epoch 169: val_loss improved from 0.17541 to 0.17528, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1708 - accuracy: 0.9681 - val_loss: 0.1753 - val_accuracy: 0.9702\n",
      "Epoch 170/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1774 - accuracy: 0.9647\n",
      "Epoch 170: val_loss improved from 0.17528 to 0.17486, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1727 - accuracy: 0.9649 - val_loss: 0.1749 - val_accuracy: 0.9702\n",
      "Epoch 171/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.1923 - accuracy: 0.9617\n",
      "Epoch 171: val_loss did not improve from 0.17486\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1796 - accuracy: 0.9681 - val_loss: 0.1751 - val_accuracy: 0.9702\n",
      "Epoch 172/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1632 - accuracy: 0.9745\n",
      "Epoch 172: val_loss improved from 0.17486 to 0.17448, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1559 - accuracy: 0.9776 - val_loss: 0.1745 - val_accuracy: 0.9702\n",
      "Epoch 173/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1732 - accuracy: 0.9600\n",
      "Epoch 173: val_loss did not improve from 0.17448\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1789 - accuracy: 0.9553 - val_loss: 0.1746 - val_accuracy: 0.9702\n",
      "Epoch 174/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1677 - accuracy: 0.9698\n",
      "Epoch 174: val_loss did not improve from 0.17448\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1751 - accuracy: 0.9649 - val_loss: 0.1747 - val_accuracy: 0.9702\n",
      "Epoch 175/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1637 - accuracy: 0.9733\n",
      "Epoch 175: val_loss improved from 0.17448 to 0.17423, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1663 - accuracy: 0.9712 - val_loss: 0.1742 - val_accuracy: 0.9702\n",
      "Epoch 176/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1691 - accuracy: 0.9700\n",
      "Epoch 176: val_loss improved from 0.17423 to 0.17368, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1728 - accuracy: 0.9681 - val_loss: 0.1737 - val_accuracy: 0.9702\n",
      "Epoch 177/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1540 - accuracy: 0.9815\n",
      "Epoch 177: val_loss improved from 0.17368 to 0.17276, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1550 - accuracy: 0.9808 - val_loss: 0.1728 - val_accuracy: 0.9702\n",
      "Epoch 178/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1684 - accuracy: 0.9704\n",
      "Epoch 178: val_loss did not improve from 0.17276\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1697 - accuracy: 0.9681 - val_loss: 0.1728 - val_accuracy: 0.9702\n",
      "Epoch 179/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1659 - accuracy: 0.9736\n",
      "Epoch 179: val_loss did not improve from 0.17276\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9776 - val_loss: 0.1730 - val_accuracy: 0.9702\n",
      "Epoch 180/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1639 - accuracy: 0.9655\n",
      "Epoch 180: val_loss did not improve from 0.17276\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1621 - accuracy: 0.9681 - val_loss: 0.1729 - val_accuracy: 0.9702\n",
      "Epoch 181/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1571 - accuracy: 0.9774\n",
      "Epoch 181: val_loss improved from 0.17276 to 0.17249, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9776 - val_loss: 0.1725 - val_accuracy: 0.9702\n",
      "Epoch 182/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1768 - accuracy: 0.9672\n",
      "Epoch 182: val_loss did not improve from 0.17249\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1746 - accuracy: 0.9681 - val_loss: 0.1726 - val_accuracy: 0.9702\n",
      "Epoch 183/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1681 - accuracy: 0.9745\n",
      "Epoch 183: val_loss improved from 0.17249 to 0.17236, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9776 - val_loss: 0.1724 - val_accuracy: 0.9702\n",
      "Epoch 184/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1570 - accuracy: 0.9672\n",
      "Epoch 184: val_loss did not improve from 0.17236\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9681 - val_loss: 0.1725 - val_accuracy: 0.9702\n",
      "Epoch 185/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1653 - accuracy: 0.9733\n",
      "Epoch 185: val_loss did not improve from 0.17236\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9744 - val_loss: 0.1725 - val_accuracy: 0.9702\n",
      "Epoch 186/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1581 - accuracy: 0.9789\n",
      "Epoch 186: val_loss improved from 0.17236 to 0.17233, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1543 - accuracy: 0.9808 - val_loss: 0.1723 - val_accuracy: 0.9702\n",
      "Epoch 187/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1945 - accuracy: 0.9439\n",
      "Epoch 187: val_loss improved from 0.17233 to 0.17207, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9489 - val_loss: 0.1721 - val_accuracy: 0.9702\n",
      "Epoch 188/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1628 - accuracy: 0.9608\n",
      "Epoch 188: val_loss improved from 0.17207 to 0.17154, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1597 - accuracy: 0.9649 - val_loss: 0.1715 - val_accuracy: 0.9702\n",
      "Epoch 189/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1547 - accuracy: 0.9692\n",
      "Epoch 189: val_loss did not improve from 0.17154\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1659 - accuracy: 0.9681 - val_loss: 0.1717 - val_accuracy: 0.9702\n",
      "Epoch 190/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1749 - accuracy: 0.9708\n",
      "Epoch 190: val_loss improved from 0.17154 to 0.17153, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1628 - accuracy: 0.9776 - val_loss: 0.1715 - val_accuracy: 0.9702\n",
      "Epoch 191/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1473 - accuracy: 0.9828\n",
      "Epoch 191: val_loss improved from 0.17153 to 0.17083, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9808 - val_loss: 0.1708 - val_accuracy: 0.9702\n",
      "Epoch 192/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1658 - accuracy: 0.9724\n",
      "Epoch 192: val_loss improved from 0.17083 to 0.17063, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1708 - accuracy: 0.9649 - val_loss: 0.1706 - val_accuracy: 0.9702\n",
      "Epoch 193/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1533 - accuracy: 0.9633\n",
      "Epoch 193: val_loss improved from 0.17063 to 0.17022, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1509 - accuracy: 0.9649 - val_loss: 0.1702 - val_accuracy: 0.9702\n",
      "Epoch 194/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1755 - accuracy: 0.9552\n",
      "Epoch 194: val_loss improved from 0.17022 to 0.16924, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9585 - val_loss: 0.1692 - val_accuracy: 0.9702\n",
      "Epoch 195/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1529 - accuracy: 0.9786\n",
      "Epoch 195: val_loss improved from 0.16924 to 0.16910, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9808 - val_loss: 0.1691 - val_accuracy: 0.9702\n",
      "Epoch 196/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1703 - accuracy: 0.9608\n",
      "Epoch 196: val_loss improved from 0.16910 to 0.16883, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1600 - accuracy: 0.9681 - val_loss: 0.1688 - val_accuracy: 0.9702\n",
      "Epoch 197/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1658 - accuracy: 0.9692\n",
      "Epoch 197: val_loss did not improve from 0.16883\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1636 - accuracy: 0.9712 - val_loss: 0.1689 - val_accuracy: 0.9702\n",
      "Epoch 198/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1380 - accuracy: 0.9808\n",
      "Epoch 198: val_loss did not improve from 0.16883\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1514 - accuracy: 0.9808 - val_loss: 0.1689 - val_accuracy: 0.9702\n",
      "Epoch 199/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1667 - accuracy: 0.9709\n",
      "Epoch 199: val_loss improved from 0.16883 to 0.16797, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1598 - accuracy: 0.9744 - val_loss: 0.1680 - val_accuracy: 0.9702\n",
      "Epoch 200/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1539 - accuracy: 0.9745\n",
      "Epoch 200: val_loss improved from 0.16797 to 0.16790, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9744 - val_loss: 0.1679 - val_accuracy: 0.9702\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_22 (GRU)                (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1679 - accuracy: 0.9702\n",
      "Test Loss: 0.16790375113487244\n",
      "Test Accuracy: 0.9702380895614624\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer with dropout_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, \n",
    "                     dropout=0.3,\n",
    "                     recurrent_dropout=0.3,\n",
    "                     input_shape=(None, x_train.shape[-1]),\n",
    "                     activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 2 layers SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1054, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 543, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1174, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 650, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1200, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1247, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 232, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable gru_10/gru_cell_10/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.SGD.'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\Main-Mari_3 copy.ipynb Cell 67\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y123sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y123sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Training of the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y123sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x_train, y_train, batch_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(x_val, y_val), callbacks\u001b[39m=\u001b[39;49m[tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mLearningRateScheduler(lr_schedule), callbacks_list])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y123sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m plot_2(history)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y123sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Evaluation of the model on the testing set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file4pmd6tp8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: in user code:\n\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1054, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 543, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1174, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 650, in apply_gradients\n        iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1200, in _internal_apply_gradients\n        return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n        distribution.extended.update(\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 1247, in apply_grad_to_update_var  **\n        return self._update_step(grad, var)\n    File \"c:\\Users\\irene\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer.py\", line 232, in _update_step\n        raise KeyError(\n\n    KeyError: 'The optimizer cannot recognize variable gru_10/gru_cell_10/kernel:0. This usually means you are trying to call the optimizer to update different parts of the model separately. Please call `optimizer.build(variables)` with the full list of trainable variables before the training loop or use legacy optimizer `tf.keras.optimizers.legacy.SGD.'\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_2 layer_SGD.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1]),return_sequences=True))\n",
    "model.add(layers.GRU(16))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "#optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=600, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM + GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.6814 - accuracy: 0.6407\n",
      "Epoch 1: val_loss improved from inf to 0.67694, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 5s 21ms/step - loss: 0.6804 - accuracy: 0.6454 - val_loss: 0.6769 - val_accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.6791 - accuracy: 0.6594\n",
      "Epoch 2: val_loss improved from 0.67694 to 0.67378, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6771 - accuracy: 0.6709 - val_loss: 0.6738 - val_accuracy: 0.7202 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.6745 - accuracy: 0.6860\n",
      "Epoch 3: val_loss improved from 0.67378 to 0.67064, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6738 - accuracy: 0.6933 - val_loss: 0.6706 - val_accuracy: 0.7321 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.6677 - accuracy: 0.7358\n",
      "Epoch 4: val_loss improved from 0.67064 to 0.66750, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6704 - accuracy: 0.7220 - val_loss: 0.6675 - val_accuracy: 0.7381 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.6658 - accuracy: 0.7276\n",
      "Epoch 5: val_loss improved from 0.66750 to 0.66435, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6671 - accuracy: 0.7284 - val_loss: 0.6644 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.6627 - accuracy: 0.7433\n",
      "Epoch 6: val_loss improved from 0.66435 to 0.66128, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6637 - accuracy: 0.7348 - val_loss: 0.6613 - val_accuracy: 0.7560 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.6588 - accuracy: 0.7520\n",
      "Epoch 7: val_loss improved from 0.66128 to 0.65817, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6603 - accuracy: 0.7412 - val_loss: 0.6582 - val_accuracy: 0.7619 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.6567 - accuracy: 0.7516\n",
      "Epoch 8: val_loss improved from 0.65817 to 0.65508, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6569 - accuracy: 0.7540 - val_loss: 0.6551 - val_accuracy: 0.7619 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.6536 - accuracy: 0.7756\n",
      "Epoch 9: val_loss improved from 0.65508 to 0.65198, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6536 - accuracy: 0.7764 - val_loss: 0.6520 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.6491 - accuracy: 0.7889\n",
      "Epoch 10: val_loss improved from 0.65198 to 0.64894, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6502 - accuracy: 0.7796 - val_loss: 0.6489 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.6472 - accuracy: 0.7917\n",
      "Epoch 11: val_loss improved from 0.64894 to 0.64586, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6470 - accuracy: 0.7827 - val_loss: 0.6459 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.6452 - accuracy: 0.7846\n",
      "Epoch 12: val_loss improved from 0.64586 to 0.64278, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6438 - accuracy: 0.7923 - val_loss: 0.6428 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.6417 - accuracy: 0.7955\n",
      "Epoch 13: val_loss improved from 0.64278 to 0.63974, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6406 - accuracy: 0.7987 - val_loss: 0.6397 - val_accuracy: 0.7976 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.6372 - accuracy: 0.8074\n",
      "Epoch 14: val_loss improved from 0.63974 to 0.63675, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.6374 - accuracy: 0.8083 - val_loss: 0.6367 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.6357 - accuracy: 0.8056\n",
      "Epoch 15: val_loss improved from 0.63675 to 0.63375, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6343 - accuracy: 0.8115 - val_loss: 0.6337 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.6335 - accuracy: 0.8000\n",
      "Epoch 16: val_loss improved from 0.63375 to 0.63072, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.6312 - accuracy: 0.8115 - val_loss: 0.6307 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.6260 - accuracy: 0.8415\n",
      "Epoch 17: val_loss improved from 0.63072 to 0.62765, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6280 - accuracy: 0.8211 - val_loss: 0.6277 - val_accuracy: 0.8274 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.6264 - accuracy: 0.8130\n",
      "Epoch 18: val_loss improved from 0.62765 to 0.62468, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6248 - accuracy: 0.8243 - val_loss: 0.6247 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.6220 - accuracy: 0.8214\n",
      "Epoch 19: val_loss improved from 0.62468 to 0.62171, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6217 - accuracy: 0.8275 - val_loss: 0.6217 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.6186 - accuracy: 0.8339\n",
      "Epoch 20: val_loss improved from 0.62171 to 0.61870, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6186 - accuracy: 0.8339 - val_loss: 0.6187 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.6150 - accuracy: 0.8472\n",
      "Epoch 21: val_loss improved from 0.61870 to 0.61571, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.6155 - accuracy: 0.8403 - val_loss: 0.6157 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.6126 - accuracy: 0.8429\n",
      "Epoch 22: val_loss improved from 0.61571 to 0.61274, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6125 - accuracy: 0.8435 - val_loss: 0.6127 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.6075 - accuracy: 0.8492\n",
      "Epoch 23: val_loss improved from 0.61274 to 0.60975, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6094 - accuracy: 0.8435 - val_loss: 0.6098 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.6063 - accuracy: 0.8429\n",
      "Epoch 24: val_loss improved from 0.60975 to 0.60677, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6064 - accuracy: 0.8435 - val_loss: 0.6068 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.6057 - accuracy: 0.8537\n",
      "Epoch 25: val_loss improved from 0.60677 to 0.60378, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6034 - accuracy: 0.8466 - val_loss: 0.6038 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.5992 - accuracy: 0.8497\n",
      "Epoch 26: val_loss improved from 0.60378 to 0.60091, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.6003 - accuracy: 0.8466 - val_loss: 0.6009 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.6014 - accuracy: 0.8293\n",
      "Epoch 27: val_loss improved from 0.60091 to 0.59804, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5974 - accuracy: 0.8466 - val_loss: 0.5980 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5934 - accuracy: 0.8492\n",
      "Epoch 28: val_loss improved from 0.59804 to 0.59507, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.5944 - accuracy: 0.8498 - val_loss: 0.5951 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.5916 - accuracy: 0.8494\n",
      "Epoch 29: val_loss improved from 0.59507 to 0.59206, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.5914 - accuracy: 0.8498 - val_loss: 0.5921 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.5868 - accuracy: 0.8537\n",
      "Epoch 30: val_loss improved from 0.59206 to 0.58912, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5884 - accuracy: 0.8530 - val_loss: 0.5891 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5837 - accuracy: 0.8611\n",
      "Epoch 31: val_loss improved from 0.58912 to 0.58612, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5854 - accuracy: 0.8562 - val_loss: 0.5861 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.5823 - accuracy: 0.8562\n",
      "Epoch 32: val_loss improved from 0.58612 to 0.58307, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5823 - accuracy: 0.8562 - val_loss: 0.5831 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5760 - accuracy: 0.8798\n",
      "Epoch 33: val_loss improved from 0.58307 to 0.58013, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5792 - accuracy: 0.8594 - val_loss: 0.5801 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.5727 - accuracy: 0.8750\n",
      "Epoch 34: val_loss improved from 0.58013 to 0.57707, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5762 - accuracy: 0.8626 - val_loss: 0.5771 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5682 - accuracy: 0.8682\n",
      "Epoch 35: val_loss improved from 0.57707 to 0.57402, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.5731 - accuracy: 0.8594 - val_loss: 0.5740 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.5668 - accuracy: 0.8699\n",
      "Epoch 36: val_loss improved from 0.57402 to 0.57112, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5700 - accuracy: 0.8658 - val_loss: 0.5711 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5703 - accuracy: 0.8527\n",
      "Epoch 37: val_loss improved from 0.57112 to 0.56804, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5670 - accuracy: 0.8658 - val_loss: 0.5680 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5674 - accuracy: 0.8566\n",
      "Epoch 38: val_loss improved from 0.56804 to 0.56492, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5639 - accuracy: 0.8690 - val_loss: 0.5649 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5548 - accuracy: 0.8915\n",
      "Epoch 39: val_loss improved from 0.56492 to 0.56183, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5608 - accuracy: 0.8690 - val_loss: 0.5618 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5568 - accuracy: 0.8849\n",
      "Epoch 40: val_loss improved from 0.56183 to 0.55884, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5576 - accuracy: 0.8722 - val_loss: 0.5588 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5579 - accuracy: 0.8690\n",
      "Epoch 41: val_loss improved from 0.55884 to 0.55568, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5546 - accuracy: 0.8754 - val_loss: 0.5557 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5501 - accuracy: 0.8837\n",
      "Epoch 42: val_loss improved from 0.55568 to 0.55253, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5514 - accuracy: 0.8786 - val_loss: 0.5525 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5499 - accuracy: 0.8810\n",
      "Epoch 43: val_loss improved from 0.55253 to 0.54934, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5482 - accuracy: 0.8818 - val_loss: 0.5493 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5422 - accuracy: 0.8810\n",
      "Epoch 44: val_loss improved from 0.54934 to 0.54616, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5450 - accuracy: 0.8850 - val_loss: 0.5462 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5408 - accuracy: 0.8798\n",
      "Epoch 45: val_loss improved from 0.54616 to 0.54295, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5418 - accuracy: 0.8850 - val_loss: 0.5430 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5450 - accuracy: 0.8721\n",
      "Epoch 46: val_loss improved from 0.54295 to 0.53976, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5386 - accuracy: 0.8850 - val_loss: 0.5398 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.5336 - accuracy: 0.8958\n",
      "Epoch 47: val_loss improved from 0.53976 to 0.53654, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5354 - accuracy: 0.8882 - val_loss: 0.5365 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5331 - accuracy: 0.9087\n",
      "Epoch 48: val_loss improved from 0.53654 to 0.53332, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5321 - accuracy: 0.8882 - val_loss: 0.5333 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.5255 - accuracy: 0.9024\n",
      "Epoch 49: val_loss improved from 0.53332 to 0.53013, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5289 - accuracy: 0.8914 - val_loss: 0.5301 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.5242 - accuracy: 0.9000\n",
      "Epoch 50: val_loss improved from 0.53013 to 0.52699, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5257 - accuracy: 0.8946 - val_loss: 0.5270 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.5201 - accuracy: 0.9031\n",
      "Epoch 51: val_loss improved from 0.52699 to 0.52374, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5226 - accuracy: 0.8978 - val_loss: 0.5237 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5231 - accuracy: 0.8929\n",
      "Epoch 52: val_loss improved from 0.52374 to 0.52048, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5194 - accuracy: 0.8978 - val_loss: 0.5205 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.5150 - accuracy: 0.9024\n",
      "Epoch 53: val_loss improved from 0.52048 to 0.51720, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5161 - accuracy: 0.8978 - val_loss: 0.5172 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.5134 - accuracy: 0.8924\n",
      "Epoch 54: val_loss improved from 0.51720 to 0.51394, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5129 - accuracy: 0.8978 - val_loss: 0.5139 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.5139 - accuracy: 0.8912\n",
      "Epoch 55: val_loss improved from 0.51394 to 0.51068, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.5096 - accuracy: 0.8978 - val_loss: 0.5107 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.5100 - accuracy: 0.8929\n",
      "Epoch 56: val_loss improved from 0.51068 to 0.50740, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.5064 - accuracy: 0.8978 - val_loss: 0.5074 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.4996 - accuracy: 0.9074\n",
      "Epoch 57: val_loss improved from 0.50740 to 0.50411, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.5032 - accuracy: 0.8978 - val_loss: 0.5041 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.5000 - accuracy: 0.8974\n",
      "Epoch 58: val_loss improved from 0.50411 to 0.50084, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4999 - accuracy: 0.8978 - val_loss: 0.5008 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.4966 - accuracy: 0.8986\n",
      "Epoch 59: val_loss improved from 0.50084 to 0.49754, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4967 - accuracy: 0.8978 - val_loss: 0.4975 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.4934 - accuracy: 0.8974\n",
      "Epoch 60: val_loss improved from 0.49754 to 0.49423, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4934 - accuracy: 0.8978 - val_loss: 0.4942 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.8978\n",
      "Epoch 61: val_loss improved from 0.49423 to 0.49094, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4902 - accuracy: 0.8978 - val_loss: 0.4909 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.4776 - accuracy: 0.9125\n",
      "Epoch 62: val_loss improved from 0.49094 to 0.48764, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4869 - accuracy: 0.8978 - val_loss: 0.4876 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.4718 - accuracy: 0.9292\n",
      "Epoch 63: val_loss improved from 0.48764 to 0.48433, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4836 - accuracy: 0.9010 - val_loss: 0.4843 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4771 - accuracy: 0.9048\n",
      "Epoch 64: val_loss improved from 0.48433 to 0.48101, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4804 - accuracy: 0.9010 - val_loss: 0.4810 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4790 - accuracy: 0.9008\n",
      "Epoch 65: val_loss improved from 0.48101 to 0.47773, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4771 - accuracy: 0.9010 - val_loss: 0.4777 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.4726 - accuracy: 0.9103\n",
      "Epoch 66: val_loss improved from 0.47773 to 0.47455, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4738 - accuracy: 0.9073 - val_loss: 0.4746 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.4730 - accuracy: 0.9042\n",
      "Epoch 67: val_loss improved from 0.47455 to 0.47127, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4707 - accuracy: 0.9073 - val_loss: 0.4713 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4646 - accuracy: 0.9048\n",
      "Epoch 68: val_loss improved from 0.47127 to 0.46795, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4674 - accuracy: 0.9073 - val_loss: 0.4679 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.4676 - accuracy: 0.9031\n",
      "Epoch 69: val_loss improved from 0.46795 to 0.46465, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4642 - accuracy: 0.9073 - val_loss: 0.4646 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.4652 - accuracy: 0.9024\n",
      "Epoch 70: val_loss improved from 0.46465 to 0.46135, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4609 - accuracy: 0.9073 - val_loss: 0.4613 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.4464 - accuracy: 0.9225\n",
      "Epoch 71: val_loss improved from 0.46135 to 0.45805, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4577 - accuracy: 0.9073 - val_loss: 0.4580 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4562 - accuracy: 0.9048\n",
      "Epoch 72: val_loss improved from 0.45805 to 0.45478, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4544 - accuracy: 0.9073 - val_loss: 0.4548 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.4516 - accuracy: 0.9048\n",
      "Epoch 73: val_loss improved from 0.45478 to 0.45152, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4512 - accuracy: 0.9073 - val_loss: 0.4515 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.4480 - accuracy: 0.9073\n",
      "Epoch 74: val_loss improved from 0.45152 to 0.44825, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4480 - accuracy: 0.9073 - val_loss: 0.4483 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4470 - accuracy: 0.9087\n",
      "Epoch 75: val_loss improved from 0.44825 to 0.44499, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4448 - accuracy: 0.9073 - val_loss: 0.4450 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4380 - accuracy: 0.9167\n",
      "Epoch 76: val_loss improved from 0.44499 to 0.44175, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4416 - accuracy: 0.9073 - val_loss: 0.4417 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.4424 - accuracy: 0.9065\n",
      "Epoch 77: val_loss improved from 0.44175 to 0.43851, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4384 - accuracy: 0.9073 - val_loss: 0.4385 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4377 - accuracy: 0.9167\n",
      "Epoch 78: val_loss improved from 0.43851 to 0.43529, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4352 - accuracy: 0.9073 - val_loss: 0.4353 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.4372 - accuracy: 0.8992\n",
      "Epoch 79: val_loss improved from 0.43529 to 0.43210, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.4321 - accuracy: 0.9073 - val_loss: 0.4321 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4319 - accuracy: 0.8968\n",
      "Epoch 80: val_loss improved from 0.43210 to 0.42895, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4289 - accuracy: 0.9073 - val_loss: 0.4289 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4244 - accuracy: 0.9048\n",
      "Epoch 81: val_loss improved from 0.42895 to 0.42578, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4258 - accuracy: 0.9073 - val_loss: 0.4258 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.4245 - accuracy: 0.9065\n",
      "Epoch 82: val_loss improved from 0.42578 to 0.42263, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4227 - accuracy: 0.9105 - val_loss: 0.4226 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4203 - accuracy: 0.9087\n",
      "Epoch 83: val_loss improved from 0.42263 to 0.41948, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4196 - accuracy: 0.9105 - val_loss: 0.4195 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.4267 - accuracy: 0.9008\n",
      "Epoch 84: val_loss improved from 0.41948 to 0.41633, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4165 - accuracy: 0.9137 - val_loss: 0.4163 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.4129 - accuracy: 0.9118\n",
      "Epoch 85: val_loss improved from 0.41633 to 0.41321, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4134 - accuracy: 0.9137 - val_loss: 0.4132 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.4056 - accuracy: 0.9186\n",
      "Epoch 86: val_loss improved from 0.41321 to 0.41011, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4103 - accuracy: 0.9137 - val_loss: 0.4101 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.4019 - accuracy: 0.9250\n",
      "Epoch 87: val_loss improved from 0.41011 to 0.40702, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4073 - accuracy: 0.9137 - val_loss: 0.4070 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.4007 - accuracy: 0.9183\n",
      "Epoch 88: val_loss improved from 0.40702 to 0.40419, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4043 - accuracy: 0.9137 - val_loss: 0.4042 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.3931 - accuracy: 0.9206\n",
      "Epoch 89: val_loss improved from 0.40419 to 0.40113, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4015 - accuracy: 0.9137 - val_loss: 0.4011 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.9137\n",
      "Epoch 90: val_loss improved from 0.40113 to 0.39810, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3985 - accuracy: 0.9137 - val_loss: 0.3981 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.3945 - accuracy: 0.9167\n",
      "Epoch 91: val_loss improved from 0.39810 to 0.39511, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3955 - accuracy: 0.9137 - val_loss: 0.3951 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.9137\n",
      "Epoch 92: val_loss improved from 0.39511 to 0.39210, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3926 - accuracy: 0.9137 - val_loss: 0.3921 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.3897 - accuracy: 0.9137\n",
      "Epoch 93: val_loss improved from 0.39210 to 0.38916, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3897 - accuracy: 0.9137 - val_loss: 0.3892 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.3866 - accuracy: 0.9208\n",
      "Epoch 94: val_loss improved from 0.38916 to 0.38618, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3868 - accuracy: 0.9137 - val_loss: 0.3862 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.3831 - accuracy: 0.9135\n",
      "Epoch 95: val_loss improved from 0.38618 to 0.38330, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3839 - accuracy: 0.9137 - val_loss: 0.3833 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.3766 - accuracy: 0.9255\n",
      "Epoch 96: val_loss improved from 0.38330 to 0.38040, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3811 - accuracy: 0.9137 - val_loss: 0.3804 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.9135\n",
      "Epoch 97: val_loss improved from 0.38040 to 0.37754, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3783 - accuracy: 0.9137 - val_loss: 0.3775 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.3787 - accuracy: 0.9022\n",
      "Epoch 98: val_loss improved from 0.37754 to 0.37472, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3755 - accuracy: 0.9137 - val_loss: 0.3747 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.3727 - accuracy: 0.9133\n",
      "Epoch 99: val_loss improved from 0.37472 to 0.37189, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3727 - accuracy: 0.9137 - val_loss: 0.3719 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.3700 - accuracy: 0.9137\n",
      "Epoch 100: val_loss improved from 0.37189 to 0.36912, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3700 - accuracy: 0.9137 - val_loss: 0.3691 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.3703 - accuracy: 0.9070\n",
      "Epoch 101: val_loss improved from 0.36912 to 0.36635, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3673 - accuracy: 0.9169 - val_loss: 0.3663 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.3624 - accuracy: 0.9199\n",
      "Epoch 102: val_loss improved from 0.36635 to 0.36373, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3646 - accuracy: 0.9169 - val_loss: 0.3637 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.3609 - accuracy: 0.9184\n",
      "Epoch 103: val_loss improved from 0.36373 to 0.36100, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3619 - accuracy: 0.9169 - val_loss: 0.3610 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.3593 - accuracy: 0.9169\n",
      "Epoch 104: val_loss improved from 0.36100 to 0.35835, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3593 - accuracy: 0.9169 - val_loss: 0.3583 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.3569 - accuracy: 0.9167\n",
      "Epoch 105: val_loss improved from 0.35835 to 0.35567, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3567 - accuracy: 0.9169 - val_loss: 0.3557 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.3522 - accuracy: 0.9201\n",
      "Epoch 106: val_loss improved from 0.35567 to 0.35304, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3541 - accuracy: 0.9169 - val_loss: 0.3530 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.3509 - accuracy: 0.9183\n",
      "Epoch 107: val_loss improved from 0.35304 to 0.35043, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3516 - accuracy: 0.9169 - val_loss: 0.3504 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.3452 - accuracy: 0.9187\n",
      "Epoch 108: val_loss improved from 0.35043 to 0.34784, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3490 - accuracy: 0.9169 - val_loss: 0.3478 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.3492 - accuracy: 0.9167\n",
      "Epoch 109: val_loss improved from 0.34784 to 0.34529, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3465 - accuracy: 0.9169 - val_loss: 0.3453 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.3464 - accuracy: 0.9150\n",
      "Epoch 110: val_loss improved from 0.34529 to 0.34277, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3440 - accuracy: 0.9169 - val_loss: 0.3428 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.3367 - accuracy: 0.9233\n",
      "Epoch 111: val_loss improved from 0.34277 to 0.34029, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3416 - accuracy: 0.9169 - val_loss: 0.3403 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.3361 - accuracy: 0.9200\n",
      "Epoch 112: val_loss improved from 0.34029 to 0.33781, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3392 - accuracy: 0.9169 - val_loss: 0.3378 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.3400 - accuracy: 0.9130\n",
      "Epoch 113: val_loss improved from 0.33781 to 0.33557, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3368 - accuracy: 0.9169 - val_loss: 0.3356 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.3258 - accuracy: 0.9222\n",
      "Epoch 114: val_loss improved from 0.33557 to 0.33315, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.3346 - accuracy: 0.9169 - val_loss: 0.3331 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.3322 - accuracy: 0.9169\n",
      "Epoch 115: val_loss improved from 0.33315 to 0.33098, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3322 - accuracy: 0.9169 - val_loss: 0.3310 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.3296 - accuracy: 0.9183\n",
      "Epoch 116: val_loss improved from 0.33098 to 0.32860, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3301 - accuracy: 0.9169 - val_loss: 0.3286 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.3236 - accuracy: 0.9228\n",
      "Epoch 117: val_loss improved from 0.32860 to 0.32627, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3278 - accuracy: 0.9169 - val_loss: 0.3263 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.3255 - accuracy: 0.9169\n",
      "Epoch 118: val_loss improved from 0.32627 to 0.32396, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3255 - accuracy: 0.9169 - val_loss: 0.3240 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.3326 - accuracy: 0.9087\n",
      "Epoch 119: val_loss improved from 0.32396 to 0.32167, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3233 - accuracy: 0.9169 - val_loss: 0.3217 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.3331 - accuracy: 0.9146\n",
      "Epoch 120: val_loss improved from 0.32167 to 0.31942, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3210 - accuracy: 0.9169 - val_loss: 0.3194 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.3094 - accuracy: 0.9255\n",
      "Epoch 121: val_loss improved from 0.31942 to 0.31719, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3189 - accuracy: 0.9169 - val_loss: 0.3172 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.9169\n",
      "Epoch 122: val_loss improved from 0.31719 to 0.31499, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3167 - accuracy: 0.9169 - val_loss: 0.3150 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.3148 - accuracy: 0.9167\n",
      "Epoch 123: val_loss improved from 0.31499 to 0.31280, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.3146 - accuracy: 0.9169 - val_loss: 0.3128 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.3188 - accuracy: 0.9147\n",
      "Epoch 124: val_loss improved from 0.31280 to 0.31064, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3125 - accuracy: 0.9169 - val_loss: 0.3106 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.3103 - accuracy: 0.9184\n",
      "Epoch 125: val_loss improved from 0.31064 to 0.30851, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3104 - accuracy: 0.9169 - val_loss: 0.3085 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.3138 - accuracy: 0.9129\n",
      "Epoch 126: val_loss improved from 0.30851 to 0.30642, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3083 - accuracy: 0.9169 - val_loss: 0.3064 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.2975 - accuracy: 0.9239\n",
      "Epoch 127: val_loss improved from 0.30642 to 0.30436, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3063 - accuracy: 0.9169 - val_loss: 0.3044 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.9167\n",
      "Epoch 128: val_loss improved from 0.30436 to 0.30232, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3044 - accuracy: 0.9169 - val_loss: 0.3023 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.3026 - accuracy: 0.9167\n",
      "Epoch 129: val_loss improved from 0.30232 to 0.30031, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3024 - accuracy: 0.9169 - val_loss: 0.3003 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.3091 - accuracy: 0.9078\n",
      "Epoch 130: val_loss improved from 0.30031 to 0.29832, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.3005 - accuracy: 0.9169 - val_loss: 0.2983 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.3117 - accuracy: 0.9065\n",
      "Epoch 131: val_loss improved from 0.29832 to 0.29635, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2985 - accuracy: 0.9169 - val_loss: 0.2963 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.2976 - accuracy: 0.9150\n",
      "Epoch 132: val_loss improved from 0.29635 to 0.29439, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2966 - accuracy: 0.9169 - val_loss: 0.2944 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.2961 - accuracy: 0.9148\n",
      "Epoch 133: val_loss improved from 0.29439 to 0.29248, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2947 - accuracy: 0.9169 - val_loss: 0.2925 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.9167\n",
      "Epoch 134: val_loss improved from 0.29248 to 0.29060, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2929 - accuracy: 0.9169 - val_loss: 0.2906 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.2898 - accuracy: 0.9218\n",
      "Epoch 135: val_loss improved from 0.29060 to 0.28875, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2911 - accuracy: 0.9169 - val_loss: 0.2887 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.2941 - accuracy: 0.9113\n",
      "Epoch 136: val_loss improved from 0.28875 to 0.28692, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2893 - accuracy: 0.9169 - val_loss: 0.2869 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.2856 - accuracy: 0.9184\n",
      "Epoch 137: val_loss improved from 0.28692 to 0.28511, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2876 - accuracy: 0.9169 - val_loss: 0.2851 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2852 - accuracy: 0.9216\n",
      "Epoch 138: val_loss improved from 0.28511 to 0.28333, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2859 - accuracy: 0.9201 - val_loss: 0.2833 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.2841 - accuracy: 0.9201\n",
      "Epoch 139: val_loss improved from 0.28333 to 0.28157, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2841 - accuracy: 0.9201 - val_loss: 0.2816 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2912 - accuracy: 0.9091\n",
      "Epoch 140: val_loss improved from 0.28157 to 0.27986, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2825 - accuracy: 0.9201 - val_loss: 0.2799 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.2789 - accuracy: 0.9233\n",
      "Epoch 141: val_loss improved from 0.27986 to 0.27816, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2808 - accuracy: 0.9201 - val_loss: 0.2782 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.2776 - accuracy: 0.9186\n",
      "Epoch 142: val_loss improved from 0.27816 to 0.27660, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2791 - accuracy: 0.9233 - val_loss: 0.2766 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.2792 - accuracy: 0.9220\n",
      "Epoch 143: val_loss improved from 0.27660 to 0.27494, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2776 - accuracy: 0.9233 - val_loss: 0.2749 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.2560 - accuracy: 0.9348\n",
      "Epoch 144: val_loss improved from 0.27494 to 0.27330, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2760 - accuracy: 0.9233 - val_loss: 0.2733 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2763 - accuracy: 0.9216\n",
      "Epoch 145: val_loss improved from 0.27330 to 0.27168, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2744 - accuracy: 0.9233 - val_loss: 0.2717 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.2713 - accuracy: 0.9309\n",
      "Epoch 146: val_loss improved from 0.27168 to 0.27008, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2729 - accuracy: 0.9233 - val_loss: 0.2701 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2737 - accuracy: 0.9216\n",
      "Epoch 147: val_loss improved from 0.27008 to 0.26851, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2713 - accuracy: 0.9233 - val_loss: 0.2685 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.9233\n",
      "Epoch 148: val_loss improved from 0.26851 to 0.26696, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2698 - accuracy: 0.9233 - val_loss: 0.2670 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2654 - accuracy: 0.9248\n",
      "Epoch 149: val_loss improved from 0.26696 to 0.26543, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2683 - accuracy: 0.9233 - val_loss: 0.2654 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.2686 - accuracy: 0.9252\n",
      "Epoch 150: val_loss improved from 0.26543 to 0.26392, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2669 - accuracy: 0.9265 - val_loss: 0.2639 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2658 - accuracy: 0.9263\n",
      "Epoch 151: val_loss improved from 0.26392 to 0.26242, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2654 - accuracy: 0.9265 - val_loss: 0.2624 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2645 - accuracy: 0.9263\n",
      "Epoch 152: val_loss improved from 0.26242 to 0.26095, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2640 - accuracy: 0.9265 - val_loss: 0.2610 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2662 - accuracy: 0.9286\n",
      "Epoch 153: val_loss improved from 0.26095 to 0.25951, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2625 - accuracy: 0.9265 - val_loss: 0.2595 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2569 - accuracy: 0.9281\n",
      "Epoch 154: val_loss improved from 0.25951 to 0.25809, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2612 - accuracy: 0.9265 - val_loss: 0.2581 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.9263\n",
      "Epoch 155: val_loss improved from 0.25809 to 0.25669, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2597 - accuracy: 0.9265 - val_loss: 0.2567 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2570 - accuracy: 0.9280\n",
      "Epoch 156: val_loss improved from 0.25669 to 0.25529, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2584 - accuracy: 0.9265 - val_loss: 0.2553 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.2440 - accuracy: 0.9333\n",
      "Epoch 157: val_loss improved from 0.25529 to 0.25391, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2570 - accuracy: 0.9265 - val_loss: 0.2539 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2520 - accuracy: 0.9281\n",
      "Epoch 158: val_loss improved from 0.25391 to 0.25254, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2557 - accuracy: 0.9265 - val_loss: 0.2525 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.2505 - accuracy: 0.9271\n",
      "Epoch 159: val_loss improved from 0.25254 to 0.25120, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2544 - accuracy: 0.9265 - val_loss: 0.2512 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.2603 - accuracy: 0.9184\n",
      "Epoch 160: val_loss improved from 0.25120 to 0.24988, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2531 - accuracy: 0.9265 - val_loss: 0.2499 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2537 - accuracy: 0.9248\n",
      "Epoch 161: val_loss improved from 0.24988 to 0.24858, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2518 - accuracy: 0.9265 - val_loss: 0.2486 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.9265\n",
      "Epoch 162: val_loss improved from 0.24858 to 0.24730, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.2506 - accuracy: 0.9265 - val_loss: 0.2473 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.2542 - accuracy: 0.9236\n",
      "Epoch 163: val_loss improved from 0.24730 to 0.24604, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2493 - accuracy: 0.9265 - val_loss: 0.2460 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.2427 - accuracy: 0.9362\n",
      "Epoch 164: val_loss improved from 0.24604 to 0.24480, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.2481 - accuracy: 0.9265 - val_loss: 0.2448 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2525 - accuracy: 0.9242\n",
      "Epoch 165: val_loss improved from 0.24480 to 0.24356, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.2469 - accuracy: 0.9265 - val_loss: 0.2436 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.2558 - accuracy: 0.9203\n",
      "Epoch 166: val_loss improved from 0.24356 to 0.24233, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.2457 - accuracy: 0.9265 - val_loss: 0.2423 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2441 - accuracy: 0.9281\n",
      "Epoch 167: val_loss improved from 0.24233 to 0.24112, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.2445 - accuracy: 0.9265 - val_loss: 0.2411 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2437 - accuracy: 0.9263\n",
      "Epoch 168: val_loss improved from 0.24112 to 0.23994, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2433 - accuracy: 0.9265 - val_loss: 0.2399 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.2423 - accuracy: 0.9267\n",
      "Epoch 169: val_loss improved from 0.23994 to 0.23877, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2422 - accuracy: 0.9265 - val_loss: 0.2388 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2414 - accuracy: 0.9263\n",
      "Epoch 170: val_loss improved from 0.23877 to 0.23762, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2410 - accuracy: 0.9265 - val_loss: 0.2376 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2369 - accuracy: 0.9242\n",
      "Epoch 171: val_loss improved from 0.23762 to 0.23646, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2399 - accuracy: 0.9265 - val_loss: 0.2365 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.9263\n",
      "Epoch 172: val_loss improved from 0.23646 to 0.23535, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.2388 - accuracy: 0.9265 - val_loss: 0.2354 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9263\n",
      "Epoch 173: val_loss improved from 0.23535 to 0.23424, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2377 - accuracy: 0.9265 - val_loss: 0.2342 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.2377 - accuracy: 0.9268\n",
      "Epoch 174: val_loss improved from 0.23424 to 0.23315, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2366 - accuracy: 0.9265 - val_loss: 0.2332 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.2356 - accuracy: 0.9265\n",
      "Epoch 175: val_loss improved from 0.23315 to 0.23207, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2356 - accuracy: 0.9265 - val_loss: 0.2321 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2402 - accuracy: 0.9246\n",
      "Epoch 176: val_loss improved from 0.23207 to 0.23101, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2345 - accuracy: 0.9265 - val_loss: 0.2310 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2383 - accuracy: 0.9206\n",
      "Epoch 177: val_loss improved from 0.23101 to 0.22995, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2335 - accuracy: 0.9265 - val_loss: 0.2299 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2499 - accuracy: 0.9127\n",
      "Epoch 178: val_loss improved from 0.22995 to 0.22891, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2325 - accuracy: 0.9265 - val_loss: 0.2289 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2465 - accuracy: 0.9127\n",
      "Epoch 179: val_loss improved from 0.22891 to 0.22793, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2314 - accuracy: 0.9265 - val_loss: 0.2279 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.2164 - accuracy: 0.9390\n",
      "Epoch 180: val_loss improved from 0.22793 to 0.22693, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2304 - accuracy: 0.9265 - val_loss: 0.2269 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.2316 - accuracy: 0.9248\n",
      "Epoch 181: val_loss improved from 0.22693 to 0.22592, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2295 - accuracy: 0.9265 - val_loss: 0.2259 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2277 - accuracy: 0.9325\n",
      "Epoch 182: val_loss improved from 0.22592 to 0.22493, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2285 - accuracy: 0.9265 - val_loss: 0.2249 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2157 - accuracy: 0.9365\n",
      "Epoch 183: val_loss improved from 0.22493 to 0.22410, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2275 - accuracy: 0.9265 - val_loss: 0.2241 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2327 - accuracy: 0.9205\n",
      "Epoch 184: val_loss improved from 0.22410 to 0.22313, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2266 - accuracy: 0.9265 - val_loss: 0.2231 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.2257 - accuracy: 0.9265\n",
      "Epoch 185: val_loss improved from 0.22313 to 0.22218, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2257 - accuracy: 0.9265 - val_loss: 0.2222 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2274 - accuracy: 0.9286\n",
      "Epoch 186: val_loss improved from 0.22218 to 0.22124, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2247 - accuracy: 0.9265 - val_loss: 0.2212 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.2226 - accuracy: 0.9296\n",
      "Epoch 187: val_loss improved from 0.22124 to 0.22032, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2238 - accuracy: 0.9265 - val_loss: 0.2203 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2266 - accuracy: 0.9242\n",
      "Epoch 188: val_loss improved from 0.22032 to 0.21954, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2229 - accuracy: 0.9265 - val_loss: 0.2195 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.2285 - accuracy: 0.9252\n",
      "Epoch 189: val_loss improved from 0.21954 to 0.21862, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2221 - accuracy: 0.9297 - val_loss: 0.2186 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.2184 - accuracy: 0.9302\n",
      "Epoch 190: val_loss improved from 0.21862 to 0.21773, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2212 - accuracy: 0.9297 - val_loss: 0.2177 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.2288 - accuracy: 0.9264\n",
      "Epoch 191: val_loss improved from 0.21773 to 0.21685, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2203 - accuracy: 0.9297 - val_loss: 0.2168 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.2072 - accuracy: 0.9407\n",
      "Epoch 192: val_loss improved from 0.21685 to 0.21612, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2195 - accuracy: 0.9329 - val_loss: 0.2161 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.2148 - accuracy: 0.9350\n",
      "Epoch 193: val_loss improved from 0.21612 to 0.21525, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2187 - accuracy: 0.9329 - val_loss: 0.2153 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.2271 - accuracy: 0.9264\n",
      "Epoch 194: val_loss improved from 0.21525 to 0.21440, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2178 - accuracy: 0.9329 - val_loss: 0.2144 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2150 - accuracy: 0.9318\n",
      "Epoch 195: val_loss improved from 0.21440 to 0.21355, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2170 - accuracy: 0.9329 - val_loss: 0.2135 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.2171 - accuracy: 0.9292\n",
      "Epoch 196: val_loss improved from 0.21355 to 0.21270, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2161 - accuracy: 0.9329 - val_loss: 0.2127 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.2071 - accuracy: 0.9390\n",
      "Epoch 197: val_loss improved from 0.21270 to 0.21187, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2153 - accuracy: 0.9329 - val_loss: 0.2119 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.2149 - accuracy: 0.9333\n",
      "Epoch 198: val_loss improved from 0.21187 to 0.21104, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2145 - accuracy: 0.9329 - val_loss: 0.2110 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2048 - accuracy: 0.9356\n",
      "Epoch 199: val_loss improved from 0.21104 to 0.21024, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2137 - accuracy: 0.9329 - val_loss: 0.2102 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.2129 - accuracy: 0.9329\n",
      "Epoch 200: val_loss improved from 0.21024 to 0.20943, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2129 - accuracy: 0.9329 - val_loss: 0.2094 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.2132 - accuracy: 0.9268\n",
      "Epoch 201: val_loss improved from 0.20943 to 0.20864, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2121 - accuracy: 0.9329 - val_loss: 0.2086 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.2095 - accuracy: 0.9268\n",
      "Epoch 202: val_loss improved from 0.20864 to 0.20799, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2113 - accuracy: 0.9329 - val_loss: 0.2080 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2226 - accuracy: 0.9242\n",
      "Epoch 203: val_loss improved from 0.20799 to 0.20721, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2106 - accuracy: 0.9329 - val_loss: 0.2072 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.2081 - accuracy: 0.9375\n",
      "Epoch 204: val_loss improved from 0.20721 to 0.20643, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2098 - accuracy: 0.9329 - val_loss: 0.2064 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2159 - accuracy: 0.9280\n",
      "Epoch 205: val_loss improved from 0.20643 to 0.20567, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2090 - accuracy: 0.9329 - val_loss: 0.2057 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2119 - accuracy: 0.9325\n",
      "Epoch 206: val_loss improved from 0.20567 to 0.20491, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2083 - accuracy: 0.9329 - val_loss: 0.2049 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.2105 - accuracy: 0.9320\n",
      "Epoch 207: val_loss improved from 0.20491 to 0.20417, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2075 - accuracy: 0.9329 - val_loss: 0.2042 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1980 - accuracy: 0.9394\n",
      "Epoch 208: val_loss improved from 0.20417 to 0.20344, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2068 - accuracy: 0.9329 - val_loss: 0.2034 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1887 - accuracy: 0.9444\n",
      "Epoch 209: val_loss improved from 0.20344 to 0.20271, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2060 - accuracy: 0.9329 - val_loss: 0.2027 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1955 - accuracy: 0.9394\n",
      "Epoch 210: val_loss improved from 0.20271 to 0.20199, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2053 - accuracy: 0.9329 - val_loss: 0.2020 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.2046 - accuracy: 0.9329\n",
      "Epoch 211: val_loss improved from 0.20199 to 0.20137, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2046 - accuracy: 0.9329 - val_loss: 0.2014 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.2138 - accuracy: 0.9264\n",
      "Epoch 212: val_loss improved from 0.20137 to 0.20066, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.2040 - accuracy: 0.9329 - val_loss: 0.2007 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1982 - accuracy: 0.9318\n",
      "Epoch 213: val_loss improved from 0.20066 to 0.19996, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2033 - accuracy: 0.9329 - val_loss: 0.2000 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.2026 - accuracy: 0.9329\n",
      "Epoch 214: val_loss improved from 0.19996 to 0.19925, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2026 - accuracy: 0.9329 - val_loss: 0.1993 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9327\n",
      "Epoch 215: val_loss improved from 0.19925 to 0.19857, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2019 - accuracy: 0.9329 - val_loss: 0.1986 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1948 - accuracy: 0.9419\n",
      "Epoch 216: val_loss improved from 0.19857 to 0.19789, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2012 - accuracy: 0.9329 - val_loss: 0.1979 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1846 - accuracy: 0.9419\n",
      "Epoch 217: val_loss improved from 0.19789 to 0.19722, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2005 - accuracy: 0.9329 - val_loss: 0.1972 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2084 - accuracy: 0.9286\n",
      "Epoch 218: val_loss improved from 0.19722 to 0.19656, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1998 - accuracy: 0.9329 - val_loss: 0.1966 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.2005 - accuracy: 0.9341\n",
      "Epoch 219: val_loss improved from 0.19656 to 0.19591, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1992 - accuracy: 0.9329 - val_loss: 0.1959 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "39/53 [=====================>........] - ETA: 0s - loss: 0.2066 - accuracy: 0.9316\n",
      "Epoch 220: val_loss improved from 0.19591 to 0.19530, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1985 - accuracy: 0.9329 - val_loss: 0.1953 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.9329\n",
      "Epoch 221: val_loss improved from 0.19530 to 0.19465, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1978 - accuracy: 0.9329 - val_loss: 0.1946 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1942 - accuracy: 0.9302\n",
      "Epoch 222: val_loss improved from 0.19465 to 0.19402, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1972 - accuracy: 0.9329 - val_loss: 0.1940 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1949 - accuracy: 0.9380\n",
      "Epoch 223: val_loss improved from 0.19402 to 0.19342, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9329 - val_loss: 0.1934 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1933 - accuracy: 0.9333\n",
      "Epoch 224: val_loss improved from 0.19342 to 0.19279, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1959 - accuracy: 0.9329 - val_loss: 0.1928 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.2019 - accuracy: 0.9309\n",
      "Epoch 225: val_loss improved from 0.19279 to 0.19217, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1953 - accuracy: 0.9329 - val_loss: 0.1922 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1935 - accuracy: 0.9268\n",
      "Epoch 226: val_loss improved from 0.19217 to 0.19156, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1947 - accuracy: 0.9329 - val_loss: 0.1916 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1952 - accuracy: 0.9333\n",
      "Epoch 227: val_loss improved from 0.19156 to 0.19095, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1940 - accuracy: 0.9329 - val_loss: 0.1910 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1892 - accuracy: 0.9405\n",
      "Epoch 228: val_loss improved from 0.19095 to 0.19034, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1934 - accuracy: 0.9329 - val_loss: 0.1903 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1839 - accuracy: 0.9375\n",
      "Epoch 229: val_loss improved from 0.19034 to 0.18976, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1928 - accuracy: 0.9329 - val_loss: 0.1898 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.2044 - accuracy: 0.9280\n",
      "Epoch 230: val_loss improved from 0.18976 to 0.18914, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1922 - accuracy: 0.9329 - val_loss: 0.1891 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1770 - accuracy: 0.9444\n",
      "Epoch 231: val_loss improved from 0.18914 to 0.18856, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1916 - accuracy: 0.9329 - val_loss: 0.1886 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1825 - accuracy: 0.9348\n",
      "Epoch 232: val_loss improved from 0.18856 to 0.18798, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1910 - accuracy: 0.9329 - val_loss: 0.1880 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1943 - accuracy: 0.9325\n",
      "Epoch 233: val_loss improved from 0.18798 to 0.18740, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1904 - accuracy: 0.9329 - val_loss: 0.1874 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1844 - accuracy: 0.9346\n",
      "Epoch 234: val_loss improved from 0.18740 to 0.18684, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1899 - accuracy: 0.9329 - val_loss: 0.1868 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1893 - accuracy: 0.9329\n",
      "Epoch 235: val_loss improved from 0.18684 to 0.18628, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1893 - accuracy: 0.9329 - val_loss: 0.1863 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1841 - accuracy: 0.9367\n",
      "Epoch 236: val_loss improved from 0.18628 to 0.18573, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1887 - accuracy: 0.9329 - val_loss: 0.1857 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.2101 - accuracy: 0.9206\n",
      "Epoch 237: val_loss improved from 0.18573 to 0.18519, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1881 - accuracy: 0.9329 - val_loss: 0.1852 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1957 - accuracy: 0.9264\n",
      "Epoch 238: val_loss improved from 0.18519 to 0.18465, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1876 - accuracy: 0.9329 - val_loss: 0.1846 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1835 - accuracy: 0.9431\n",
      "Epoch 239: val_loss improved from 0.18465 to 0.18412, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1870 - accuracy: 0.9361 - val_loss: 0.1841 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1860 - accuracy: 0.9412\n",
      "Epoch 240: val_loss improved from 0.18412 to 0.18358, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1865 - accuracy: 0.9393 - val_loss: 0.1836 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1940 - accuracy: 0.9341\n",
      "Epoch 241: val_loss improved from 0.18358 to 0.18306, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1859 - accuracy: 0.9393 - val_loss: 0.1831 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1958 - accuracy: 0.9356\n",
      "Epoch 242: val_loss improved from 0.18306 to 0.18254, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1854 - accuracy: 0.9393 - val_loss: 0.1825 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1849 - accuracy: 0.9393\n",
      "Epoch 243: val_loss improved from 0.18254 to 0.18202, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1849 - accuracy: 0.9393 - val_loss: 0.1820 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1792 - accuracy: 0.9390\n",
      "Epoch 244: val_loss improved from 0.18202 to 0.18151, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1843 - accuracy: 0.9393 - val_loss: 0.1815 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1958 - accuracy: 0.9302\n",
      "Epoch 245: val_loss improved from 0.18151 to 0.18101, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1838 - accuracy: 0.9393 - val_loss: 0.1810 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1914 - accuracy: 0.9333\n",
      "Epoch 246: val_loss improved from 0.18101 to 0.18046, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1833 - accuracy: 0.9393 - val_loss: 0.1805 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1711 - accuracy: 0.9405\n",
      "Epoch 247: val_loss improved from 0.18046 to 0.17996, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1827 - accuracy: 0.9393 - val_loss: 0.1800 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9393\n",
      "Epoch 248: val_loss improved from 0.17996 to 0.17947, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1822 - accuracy: 0.9393 - val_loss: 0.1795 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1960 - accuracy: 0.9264\n",
      "Epoch 249: val_loss improved from 0.17947 to 0.17896, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1817 - accuracy: 0.9393 - val_loss: 0.1790 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1655 - accuracy: 0.9484\n",
      "Epoch 250: val_loss improved from 0.17896 to 0.17848, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9393 - val_loss: 0.1785 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9393\n",
      "Epoch 251: val_loss improved from 0.17848 to 0.17800, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1807 - accuracy: 0.9393 - val_loss: 0.1780 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1865 - accuracy: 0.9350\n",
      "Epoch 252: val_loss improved from 0.17800 to 0.17753, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1803 - accuracy: 0.9393 - val_loss: 0.1775 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1888 - accuracy: 0.9394\n",
      "Epoch 253: val_loss improved from 0.17753 to 0.17708, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1798 - accuracy: 0.9393 - val_loss: 0.1771 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1791 - accuracy: 0.9419\n",
      "Epoch 254: val_loss improved from 0.17708 to 0.17661, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1792 - accuracy: 0.9425 - val_loss: 0.1766 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1811 - accuracy: 0.9422\n",
      "Epoch 255: val_loss improved from 0.17661 to 0.17614, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1787 - accuracy: 0.9425 - val_loss: 0.1761 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1789 - accuracy: 0.9481\n",
      "Epoch 256: val_loss improved from 0.17614 to 0.17568, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1782 - accuracy: 0.9489 - val_loss: 0.1757 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1861 - accuracy: 0.9432\n",
      "Epoch 257: val_loss improved from 0.17568 to 0.17522, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1778 - accuracy: 0.9489 - val_loss: 0.1752 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1734 - accuracy: 0.9484\n",
      "Epoch 258: val_loss improved from 0.17522 to 0.17478, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1773 - accuracy: 0.9489 - val_loss: 0.1748 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1845 - accuracy: 0.9458\n",
      "Epoch 259: val_loss improved from 0.17478 to 0.17432, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1768 - accuracy: 0.9489 - val_loss: 0.1743 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1746 - accuracy: 0.9484\n",
      "Epoch 260: val_loss improved from 0.17432 to 0.17387, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1764 - accuracy: 0.9489 - val_loss: 0.1739 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9487\n",
      "Epoch 261: val_loss improved from 0.17387 to 0.17344, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1759 - accuracy: 0.9489 - val_loss: 0.1734 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1849 - accuracy: 0.9457\n",
      "Epoch 262: val_loss improved from 0.17344 to 0.17300, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1754 - accuracy: 0.9489 - val_loss: 0.1730 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1685 - accuracy: 0.9593\n",
      "Epoch 263: val_loss improved from 0.17300 to 0.17256, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1750 - accuracy: 0.9489 - val_loss: 0.1726 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1750 - accuracy: 0.9496\n",
      "Epoch 264: val_loss improved from 0.17256 to 0.17217, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1746 - accuracy: 0.9489 - val_loss: 0.1722 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1778 - accuracy: 0.9431\n",
      "Epoch 265: val_loss improved from 0.17217 to 0.17175, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1741 - accuracy: 0.9489 - val_loss: 0.1717 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1549 - accuracy: 0.9556\n",
      "Epoch 266: val_loss improved from 0.17175 to 0.17133, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1737 - accuracy: 0.9489 - val_loss: 0.1713 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1514 - accuracy: 0.9625\n",
      "Epoch 267: val_loss improved from 0.17133 to 0.17091, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1732 - accuracy: 0.9489 - val_loss: 0.1709 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1657 - accuracy: 0.9524\n",
      "Epoch 268: val_loss improved from 0.17091 to 0.17049, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1728 - accuracy: 0.9489 - val_loss: 0.1705 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1700 - accuracy: 0.9457\n",
      "Epoch 269: val_loss improved from 0.17049 to 0.17008, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1723 - accuracy: 0.9489 - val_loss: 0.1701 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9487\n",
      "Epoch 270: val_loss improved from 0.17008 to 0.16967, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1719 - accuracy: 0.9489 - val_loss: 0.1697 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1877 - accuracy: 0.9394\n",
      "Epoch 271: val_loss improved from 0.16967 to 0.16926, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1715 - accuracy: 0.9489 - val_loss: 0.1693 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9489\n",
      "Epoch 272: val_loss improved from 0.16926 to 0.16886, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1711 - accuracy: 0.9489 - val_loss: 0.1689 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1700 - accuracy: 0.9496\n",
      "Epoch 273: val_loss improved from 0.16886 to 0.16846, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1706 - accuracy: 0.9489 - val_loss: 0.1685 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1613 - accuracy: 0.9514\n",
      "Epoch 274: val_loss improved from 0.16846 to 0.16806, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1702 - accuracy: 0.9489 - val_loss: 0.1681 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1720 - accuracy: 0.9444\n",
      "Epoch 275: val_loss improved from 0.16806 to 0.16767, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1698 - accuracy: 0.9489 - val_loss: 0.1677 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1589 - accuracy: 0.9593\n",
      "Epoch 276: val_loss improved from 0.16767 to 0.16728, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1694 - accuracy: 0.9489 - val_loss: 0.1673 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1689 - accuracy: 0.9477\n",
      "Epoch 277: val_loss improved from 0.16728 to 0.16690, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1690 - accuracy: 0.9489 - val_loss: 0.1669 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1625 - accuracy: 0.9535\n",
      "Epoch 278: val_loss improved from 0.16690 to 0.16652, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1686 - accuracy: 0.9489 - val_loss: 0.1665 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1703 - accuracy: 0.9496\n",
      "Epoch 279: val_loss improved from 0.16652 to 0.16614, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1682 - accuracy: 0.9489 - val_loss: 0.1661 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1678 - accuracy: 0.9553\n",
      "Epoch 280: val_loss improved from 0.16614 to 0.16577, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1678 - accuracy: 0.9489 - val_loss: 0.1658 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9489\n",
      "Epoch 281: val_loss improved from 0.16577 to 0.16538, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1674 - accuracy: 0.9489 - val_loss: 0.1654 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1666 - accuracy: 0.9496\n",
      "Epoch 282: val_loss improved from 0.16538 to 0.16501, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.9489 - val_loss: 0.1650 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "39/53 [=====================>........] - ETA: 0s - loss: 0.1614 - accuracy: 0.9530\n",
      "Epoch 283: val_loss improved from 0.16501 to 0.16464, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1666 - accuracy: 0.9489 - val_loss: 0.1646 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1719 - accuracy: 0.9444\n",
      "Epoch 284: val_loss improved from 0.16464 to 0.16428, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1662 - accuracy: 0.9489 - val_loss: 0.1643 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1690 - accuracy: 0.9467\n",
      "Epoch 285: val_loss improved from 0.16428 to 0.16391, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1658 - accuracy: 0.9489 - val_loss: 0.1639 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1681 - accuracy: 0.9484\n",
      "Epoch 286: val_loss improved from 0.16391 to 0.16355, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1654 - accuracy: 0.9489 - val_loss: 0.1635 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1794 - accuracy: 0.9405\n",
      "Epoch 287: val_loss improved from 0.16355 to 0.16319, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1650 - accuracy: 0.9489 - val_loss: 0.1632 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1679 - accuracy: 0.9467\n",
      "Epoch 288: val_loss improved from 0.16319 to 0.16282, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1647 - accuracy: 0.9489 - val_loss: 0.1628 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1646 - accuracy: 0.9457\n",
      "Epoch 289: val_loss improved from 0.16282 to 0.16251, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1643 - accuracy: 0.9489 - val_loss: 0.1625 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1662 - accuracy: 0.9458\n",
      "Epoch 290: val_loss improved from 0.16251 to 0.16215, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1639 - accuracy: 0.9489 - val_loss: 0.1621 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1592 - accuracy: 0.9472\n",
      "Epoch 291: val_loss improved from 0.16215 to 0.16180, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1635 - accuracy: 0.9489 - val_loss: 0.1618 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1582 - accuracy: 0.9524\n",
      "Epoch 292: val_loss improved from 0.16180 to 0.16145, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1631 - accuracy: 0.9489 - val_loss: 0.1615 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1523 - accuracy: 0.9545\n",
      "Epoch 293: val_loss improved from 0.16145 to 0.16113, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1628 - accuracy: 0.9489 - val_loss: 0.1611 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1739 - accuracy: 0.9390\n",
      "Epoch 294: val_loss improved from 0.16113 to 0.16079, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1624 - accuracy: 0.9489 - val_loss: 0.1608 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1708 - accuracy: 0.9457\n",
      "Epoch 295: val_loss improved from 0.16079 to 0.16045, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1620 - accuracy: 0.9489 - val_loss: 0.1604 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1657 - accuracy: 0.9467\n",
      "Epoch 296: val_loss improved from 0.16045 to 0.16011, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1616 - accuracy: 0.9489 - val_loss: 0.1601 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9487\n",
      "Epoch 297: val_loss improved from 0.16011 to 0.15978, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1613 - accuracy: 0.9489 - val_loss: 0.1598 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1614 - accuracy: 0.9487\n",
      "Epoch 298: val_loss improved from 0.15978 to 0.15945, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1609 - accuracy: 0.9489 - val_loss: 0.1595 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1715 - accuracy: 0.9457\n",
      "Epoch 299: val_loss improved from 0.15945 to 0.15912, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1606 - accuracy: 0.9489 - val_loss: 0.1591 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1572 - accuracy: 0.9524\n",
      "Epoch 300: val_loss improved from 0.15912 to 0.15880, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1602 - accuracy: 0.9489 - val_loss: 0.1588 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1539 - accuracy: 0.9519\n",
      "Epoch 301: val_loss improved from 0.15880 to 0.15852, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1599 - accuracy: 0.9489 - val_loss: 0.1585 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1538 - accuracy: 0.9524\n",
      "Epoch 302: val_loss improved from 0.15852 to 0.15819, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1595 - accuracy: 0.9489 - val_loss: 0.1582 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1611 - accuracy: 0.9477\n",
      "Epoch 303: val_loss improved from 0.15819 to 0.15788, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1592 - accuracy: 0.9489 - val_loss: 0.1579 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1616 - accuracy: 0.9484\n",
      "Epoch 304: val_loss improved from 0.15788 to 0.15755, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1589 - accuracy: 0.9489 - val_loss: 0.1576 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1534 - accuracy: 0.9533\n",
      "Epoch 305: val_loss improved from 0.15755 to 0.15724, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1585 - accuracy: 0.9489 - val_loss: 0.1572 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 306/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1635 - accuracy: 0.9419\n",
      "Epoch 306: val_loss improved from 0.15724 to 0.15693, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1582 - accuracy: 0.9489 - val_loss: 0.1569 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1661 - accuracy: 0.9524\n",
      "Epoch 307: val_loss improved from 0.15693 to 0.15662, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1578 - accuracy: 0.9489 - val_loss: 0.1566 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1591 - accuracy: 0.9490\n",
      "Epoch 308: val_loss improved from 0.15662 to 0.15630, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 0.9489 - val_loss: 0.1563 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1581 - accuracy: 0.9484\n",
      "Epoch 309: val_loss improved from 0.15630 to 0.15600, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1572 - accuracy: 0.9489 - val_loss: 0.1560 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1499 - accuracy: 0.9484\n",
      "Epoch 310: val_loss improved from 0.15600 to 0.15569, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1569 - accuracy: 0.9489 - val_loss: 0.1557 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1699 - accuracy: 0.9419\n",
      "Epoch 311: val_loss improved from 0.15569 to 0.15539, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1565 - accuracy: 0.9489 - val_loss: 0.1554 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1442 - accuracy: 0.9583\n",
      "Epoch 312: val_loss improved from 0.15539 to 0.15509, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1562 - accuracy: 0.9489 - val_loss: 0.1551 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1453 - accuracy: 0.9583\n",
      "Epoch 313: val_loss improved from 0.15509 to 0.15479, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1559 - accuracy: 0.9489 - val_loss: 0.1548 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1519 - accuracy: 0.9496\n",
      "Epoch 314: val_loss improved from 0.15479 to 0.15450, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1555 - accuracy: 0.9489 - val_loss: 0.1545 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1558 - accuracy: 0.9432\n",
      "Epoch 315: val_loss improved from 0.15450 to 0.15417, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.9489 - val_loss: 0.1542 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1507 - accuracy: 0.9484\n",
      "Epoch 316: val_loss improved from 0.15417 to 0.15385, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1549 - accuracy: 0.9489 - val_loss: 0.1539 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1435 - accuracy: 0.9603\n",
      "Epoch 317: val_loss improved from 0.15385 to 0.15356, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1546 - accuracy: 0.9489 - val_loss: 0.1536 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 318/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1325 - accuracy: 0.9593\n",
      "Epoch 318: val_loss improved from 0.15356 to 0.15328, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1543 - accuracy: 0.9489 - val_loss: 0.1533 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1521 - accuracy: 0.9484\n",
      "Epoch 319: val_loss improved from 0.15328 to 0.15299, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1540 - accuracy: 0.9521 - val_loss: 0.1530 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1731 - accuracy: 0.9375\n",
      "Epoch 320: val_loss improved from 0.15299 to 0.15270, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1537 - accuracy: 0.9521 - val_loss: 0.1527 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1538 - accuracy: 0.9519\n",
      "Epoch 321: val_loss improved from 0.15270 to 0.15242, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1534 - accuracy: 0.9521 - val_loss: 0.1524 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1515 - accuracy: 0.9545\n",
      "Epoch 322: val_loss improved from 0.15242 to 0.15215, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1531 - accuracy: 0.9521 - val_loss: 0.1521 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1548 - accuracy: 0.9510\n",
      "Epoch 323: val_loss improved from 0.15215 to 0.15187, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1528 - accuracy: 0.9521 - val_loss: 0.1519 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1563 - accuracy: 0.9496\n",
      "Epoch 324: val_loss improved from 0.15187 to 0.15159, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1525 - accuracy: 0.9521 - val_loss: 0.1516 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 325/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1313 - accuracy: 0.9690\n",
      "Epoch 325: val_loss improved from 0.15159 to 0.15132, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1522 - accuracy: 0.9521 - val_loss: 0.1513 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1542 - accuracy: 0.9500\n",
      "Epoch 326: val_loss improved from 0.15132 to 0.15103, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1519 - accuracy: 0.9521 - val_loss: 0.1510 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1525 - accuracy: 0.9524\n",
      "Epoch 327: val_loss improved from 0.15103 to 0.15076, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1516 - accuracy: 0.9521 - val_loss: 0.1508 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1470 - accuracy: 0.9553\n",
      "Epoch 328: val_loss improved from 0.15076 to 0.15049, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1513 - accuracy: 0.9521 - val_loss: 0.1505 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1513 - accuracy: 0.9519\n",
      "Epoch 329: val_loss improved from 0.15049 to 0.15023, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1510 - accuracy: 0.9521 - val_loss: 0.1502 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1237 - accuracy: 0.9722\n",
      "Epoch 330: val_loss improved from 0.15023 to 0.14997, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1507 - accuracy: 0.9521 - val_loss: 0.1500 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1480 - accuracy: 0.9508\n",
      "Epoch 331: val_loss improved from 0.14997 to 0.14971, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1504 - accuracy: 0.9521 - val_loss: 0.1497 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1443 - accuracy: 0.9563\n",
      "Epoch 332: val_loss improved from 0.14971 to 0.14945, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1501 - accuracy: 0.9521 - val_loss: 0.1494 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1483 - accuracy: 0.9479\n",
      "Epoch 333: val_loss improved from 0.14945 to 0.14919, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1498 - accuracy: 0.9489 - val_loss: 0.1492 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1575 - accuracy: 0.9444\n",
      "Epoch 334: val_loss improved from 0.14919 to 0.14893, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1495 - accuracy: 0.9489 - val_loss: 0.1489 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1492 - accuracy: 0.9489\n",
      "Epoch 335: val_loss improved from 0.14893 to 0.14868, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1492 - accuracy: 0.9489 - val_loss: 0.1487 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1567 - accuracy: 0.9431\n",
      "Epoch 336: val_loss improved from 0.14868 to 0.14842, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1490 - accuracy: 0.9489 - val_loss: 0.1484 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1412 - accuracy: 0.9565\n",
      "Epoch 337: val_loss improved from 0.14842 to 0.14816, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1487 - accuracy: 0.9489 - val_loss: 0.1482 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1486 - accuracy: 0.9487\n",
      "Epoch 338: val_loss improved from 0.14816 to 0.14791, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1484 - accuracy: 0.9489 - val_loss: 0.1479 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1456 - accuracy: 0.9529\n",
      "Epoch 339: val_loss improved from 0.14791 to 0.14767, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1481 - accuracy: 0.9489 - val_loss: 0.1477 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1516 - accuracy: 0.9467\n",
      "Epoch 340: val_loss improved from 0.14767 to 0.14742, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1478 - accuracy: 0.9489 - val_loss: 0.1474 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9487\n",
      "Epoch 341: val_loss improved from 0.14742 to 0.14717, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1476 - accuracy: 0.9489 - val_loss: 0.1472 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 342/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1563 - accuracy: 0.9405\n",
      "Epoch 342: val_loss improved from 0.14717 to 0.14693, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1473 - accuracy: 0.9489 - val_loss: 0.1469 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1486 - accuracy: 0.9479\n",
      "Epoch 343: val_loss improved from 0.14693 to 0.14675, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1470 - accuracy: 0.9489 - val_loss: 0.1468 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 344/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1613 - accuracy: 0.9405\n",
      "Epoch 344: val_loss improved from 0.14675 to 0.14651, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1467 - accuracy: 0.9489 - val_loss: 0.1465 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1456 - accuracy: 0.9524\n",
      "Epoch 345: val_loss improved from 0.14651 to 0.14627, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1465 - accuracy: 0.9489 - val_loss: 0.1463 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1478 - accuracy: 0.9477\n",
      "Epoch 346: val_loss improved from 0.14627 to 0.14603, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1462 - accuracy: 0.9489 - val_loss: 0.1460 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1502 - accuracy: 0.9444\n",
      "Epoch 347: val_loss improved from 0.14603 to 0.14578, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1459 - accuracy: 0.9489 - val_loss: 0.1458 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1457 - accuracy: 0.9489\n",
      "Epoch 348: val_loss improved from 0.14578 to 0.14555, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1457 - accuracy: 0.9489 - val_loss: 0.1455 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.9487\n",
      "Epoch 349: val_loss improved from 0.14555 to 0.14531, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1454 - accuracy: 0.9489 - val_loss: 0.1453 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1280 - accuracy: 0.9542\n",
      "Epoch 350: val_loss improved from 0.14531 to 0.14506, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9489 - val_loss: 0.1451 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 351/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1316 - accuracy: 0.9574\n",
      "Epoch 351: val_loss improved from 0.14506 to 0.14481, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1449 - accuracy: 0.9489 - val_loss: 0.1448 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1487 - accuracy: 0.9468\n",
      "Epoch 352: val_loss improved from 0.14481 to 0.14458, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1446 - accuracy: 0.9489 - val_loss: 0.1446 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1606 - accuracy: 0.9419\n",
      "Epoch 353: val_loss improved from 0.14458 to 0.14436, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1444 - accuracy: 0.9489 - val_loss: 0.1444 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1553 - accuracy: 0.9405\n",
      "Epoch 354: val_loss improved from 0.14436 to 0.14413, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1441 - accuracy: 0.9489 - val_loss: 0.1441 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 355/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.9567\n",
      "Epoch 355: val_loss improved from 0.14413 to 0.14390, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1438 - accuracy: 0.9521 - val_loss: 0.1439 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1507 - accuracy: 0.9500\n",
      "Epoch 356: val_loss improved from 0.14390 to 0.14367, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1436 - accuracy: 0.9521 - val_loss: 0.1437 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1378 - accuracy: 0.9574\n",
      "Epoch 357: val_loss improved from 0.14367 to 0.14345, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1433 - accuracy: 0.9521 - val_loss: 0.1434 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1433 - accuracy: 0.9524\n",
      "Epoch 358: val_loss improved from 0.14345 to 0.14323, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1431 - accuracy: 0.9521 - val_loss: 0.1432 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1392 - accuracy: 0.9625\n",
      "Epoch 359: val_loss improved from 0.14323 to 0.14300, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1428 - accuracy: 0.9521 - val_loss: 0.1430 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1318 - accuracy: 0.9667\n",
      "Epoch 360: val_loss improved from 0.14300 to 0.14279, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1426 - accuracy: 0.9521 - val_loss: 0.1428 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 361/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1390 - accuracy: 0.9545\n",
      "Epoch 361: val_loss improved from 0.14279 to 0.14257, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1423 - accuracy: 0.9521 - val_loss: 0.1426 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 362/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1424 - accuracy: 0.9519\n",
      "Epoch 362: val_loss improved from 0.14257 to 0.14236, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1421 - accuracy: 0.9521 - val_loss: 0.1424 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1340 - accuracy: 0.9545\n",
      "Epoch 363: val_loss improved from 0.14236 to 0.14214, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1419 - accuracy: 0.9521 - val_loss: 0.1421 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1363 - accuracy: 0.9612\n",
      "Epoch 364: val_loss improved from 0.14214 to 0.14192, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1416 - accuracy: 0.9521 - val_loss: 0.1419 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1432 - accuracy: 0.9524\n",
      "Epoch 365: val_loss improved from 0.14192 to 0.14171, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1414 - accuracy: 0.9521 - val_loss: 0.1417 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1276 - accuracy: 0.9612\n",
      "Epoch 366: val_loss improved from 0.14171 to 0.14149, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1411 - accuracy: 0.9521 - val_loss: 0.1415 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1496 - accuracy: 0.9524\n",
      "Epoch 367: val_loss improved from 0.14149 to 0.14129, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1409 - accuracy: 0.9521 - val_loss: 0.1413 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 368/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1436 - accuracy: 0.9500\n",
      "Epoch 368: val_loss improved from 0.14129 to 0.14107, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1407 - accuracy: 0.9521 - val_loss: 0.1411 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1365 - accuracy: 0.9563\n",
      "Epoch 369: val_loss improved from 0.14107 to 0.14086, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1404 - accuracy: 0.9521 - val_loss: 0.1409 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 370/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1150 - accuracy: 0.9603\n",
      "Epoch 370: val_loss improved from 0.14086 to 0.14066, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1402 - accuracy: 0.9521 - val_loss: 0.1407 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1417 - accuracy: 0.9500\n",
      "Epoch 371: val_loss improved from 0.14066 to 0.14045, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9521 - val_loss: 0.1404 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1148 - accuracy: 0.9643\n",
      "Epoch 372: val_loss improved from 0.14045 to 0.14024, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1397 - accuracy: 0.9521 - val_loss: 0.1402 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1260 - accuracy: 0.9535\n",
      "Epoch 373: val_loss improved from 0.14024 to 0.14004, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9521 - val_loss: 0.1400 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1423 - accuracy: 0.9500\n",
      "Epoch 374: val_loss improved from 0.14004 to 0.13984, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1392 - accuracy: 0.9521 - val_loss: 0.1398 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1216 - accuracy: 0.9659\n",
      "Epoch 375: val_loss improved from 0.13984 to 0.13964, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1390 - accuracy: 0.9553 - val_loss: 0.1396 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1328 - accuracy: 0.9612\n",
      "Epoch 376: val_loss improved from 0.13964 to 0.13944, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1388 - accuracy: 0.9553 - val_loss: 0.1394 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1328 - accuracy: 0.9574\n",
      "Epoch 377: val_loss improved from 0.13944 to 0.13924, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1386 - accuracy: 0.9553 - val_loss: 0.1392 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1535 - accuracy: 0.9444\n",
      "Epoch 378: val_loss improved from 0.13924 to 0.13905, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1383 - accuracy: 0.9553 - val_loss: 0.1390 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1412 - accuracy: 0.9500\n",
      "Epoch 379: val_loss improved from 0.13905 to 0.13885, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1381 - accuracy: 0.9553 - val_loss: 0.1388 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1443 - accuracy: 0.9524\n",
      "Epoch 380: val_loss improved from 0.13885 to 0.13865, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1379 - accuracy: 0.9553 - val_loss: 0.1387 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 381/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1336 - accuracy: 0.9612\n",
      "Epoch 381: val_loss improved from 0.13865 to 0.13846, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9553 - val_loss: 0.1385 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1177 - accuracy: 0.9690\n",
      "Epoch 382: val_loss improved from 0.13846 to 0.13836, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1375 - accuracy: 0.9553 - val_loss: 0.1384 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1395 - accuracy: 0.9533\n",
      "Epoch 383: val_loss improved from 0.13836 to 0.13817, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1372 - accuracy: 0.9553 - val_loss: 0.1382 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1225 - accuracy: 0.9625\n",
      "Epoch 384: val_loss improved from 0.13817 to 0.13797, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1370 - accuracy: 0.9553 - val_loss: 0.1380 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1322 - accuracy: 0.9574\n",
      "Epoch 385: val_loss improved from 0.13797 to 0.13777, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1368 - accuracy: 0.9553 - val_loss: 0.1378 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1427 - accuracy: 0.9504\n",
      "Epoch 386: val_loss improved from 0.13777 to 0.13758, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1365 - accuracy: 0.9553 - val_loss: 0.1376 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1473 - accuracy: 0.9472\n",
      "Epoch 387: val_loss improved from 0.13758 to 0.13740, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1363 - accuracy: 0.9553 - val_loss: 0.1374 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1339 - accuracy: 0.9553\n",
      "Epoch 388: val_loss improved from 0.13740 to 0.13719, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1361 - accuracy: 0.9553 - val_loss: 0.1372 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1400 - accuracy: 0.9514\n",
      "Epoch 389: val_loss improved from 0.13719 to 0.13701, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1359 - accuracy: 0.9553 - val_loss: 0.1370 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9553\n",
      "Epoch 390: val_loss improved from 0.13701 to 0.13683, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1357 - accuracy: 0.9553 - val_loss: 0.1368 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9553\n",
      "Epoch 391: val_loss improved from 0.13683 to 0.13664, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9553 - val_loss: 0.1366 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1425 - accuracy: 0.9500\n",
      "Epoch 392: val_loss improved from 0.13664 to 0.13646, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1352 - accuracy: 0.9553 - val_loss: 0.1365 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1294 - accuracy: 0.9563\n",
      "Epoch 393: val_loss improved from 0.13646 to 0.13628, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1350 - accuracy: 0.9553 - val_loss: 0.1363 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1285 - accuracy: 0.9583\n",
      "Epoch 394: val_loss improved from 0.13628 to 0.13609, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9553 - val_loss: 0.1361 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1119 - accuracy: 0.9675\n",
      "Epoch 395: val_loss improved from 0.13609 to 0.13591, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1346 - accuracy: 0.9553 - val_loss: 0.1359 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1225 - accuracy: 0.9612\n",
      "Epoch 396: val_loss improved from 0.13591 to 0.13573, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1344 - accuracy: 0.9553 - val_loss: 0.1357 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1346 - accuracy: 0.9567\n",
      "Epoch 397: val_loss improved from 0.13573 to 0.13555, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1342 - accuracy: 0.9553 - val_loss: 0.1356 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1243 - accuracy: 0.9553\n",
      "Epoch 398: val_loss improved from 0.13555 to 0.13537, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1340 - accuracy: 0.9553 - val_loss: 0.1354 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1358 - accuracy: 0.9545\n",
      "Epoch 399: val_loss improved from 0.13537 to 0.13519, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1338 - accuracy: 0.9553 - val_loss: 0.1352 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1223 - accuracy: 0.9645\n",
      "Epoch 400: val_loss improved from 0.13519 to 0.13503, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1336 - accuracy: 0.9553 - val_loss: 0.1350 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1201 - accuracy: 0.9593\n",
      "Epoch 401: val_loss improved from 0.13503 to 0.13486, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1334 - accuracy: 0.9553 - val_loss: 0.1349 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1452 - accuracy: 0.9472\n",
      "Epoch 402: val_loss improved from 0.13486 to 0.13468, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1332 - accuracy: 0.9553 - val_loss: 0.1347 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9553\n",
      "Epoch 403: val_loss improved from 0.13468 to 0.13450, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1330 - accuracy: 0.9553 - val_loss: 0.1345 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1334 - accuracy: 0.9593\n",
      "Epoch 404: val_loss improved from 0.13450 to 0.13437, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1328 - accuracy: 0.9553 - val_loss: 0.1344 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1346 - accuracy: 0.9542\n",
      "Epoch 405: val_loss improved from 0.13437 to 0.13420, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1326 - accuracy: 0.9553 - val_loss: 0.1342 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "39/53 [=====================>........] - ETA: 0s - loss: 0.1352 - accuracy: 0.9573\n",
      "Epoch 406: val_loss improved from 0.13420 to 0.13403, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1324 - accuracy: 0.9553 - val_loss: 0.1340 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1343 - accuracy: 0.9545\n",
      "Epoch 407: val_loss improved from 0.13403 to 0.13386, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9553 - val_loss: 0.1339 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1237 - accuracy: 0.9583\n",
      "Epoch 408: val_loss improved from 0.13386 to 0.13369, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1320 - accuracy: 0.9553 - val_loss: 0.1337 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1040 - accuracy: 0.9643\n",
      "Epoch 409: val_loss improved from 0.13369 to 0.13351, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1318 - accuracy: 0.9553 - val_loss: 0.1335 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1383 - accuracy: 0.9574\n",
      "Epoch 410: val_loss improved from 0.13351 to 0.13333, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1316 - accuracy: 0.9553 - val_loss: 0.1333 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1351 - accuracy: 0.9519\n",
      "Epoch 411: val_loss improved from 0.13333 to 0.13317, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1314 - accuracy: 0.9553 - val_loss: 0.1332 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9551\n",
      "Epoch 412: val_loss improved from 0.13317 to 0.13297, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1312 - accuracy: 0.9553 - val_loss: 0.1330 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1447 - accuracy: 0.9542\n",
      "Epoch 413: val_loss improved from 0.13297 to 0.13281, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1310 - accuracy: 0.9553 - val_loss: 0.1328 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1346 - accuracy: 0.9533\n",
      "Epoch 414: val_loss improved from 0.13281 to 0.13264, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9553 - val_loss: 0.1326 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1317 - accuracy: 0.9593\n",
      "Epoch 415: val_loss improved from 0.13264 to 0.13248, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1306 - accuracy: 0.9553 - val_loss: 0.1325 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 416/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1493 - accuracy: 0.9431\n",
      "Epoch 416: val_loss improved from 0.13248 to 0.13231, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9553 - val_loss: 0.1323 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 417/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1359 - accuracy: 0.9524\n",
      "Epoch 417: val_loss improved from 0.13231 to 0.13215, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9553 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 418/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1290 - accuracy: 0.9535\n",
      "Epoch 418: val_loss improved from 0.13215 to 0.13199, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1300 - accuracy: 0.9553 - val_loss: 0.1320 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1188 - accuracy: 0.9563\n",
      "Epoch 419: val_loss improved from 0.13199 to 0.13183, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1299 - accuracy: 0.9553 - val_loss: 0.1318 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 420/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1215 - accuracy: 0.9659\n",
      "Epoch 420: val_loss improved from 0.13183 to 0.13166, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1297 - accuracy: 0.9553 - val_loss: 0.1317 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1352 - accuracy: 0.9535\n",
      "Epoch 421: val_loss improved from 0.13166 to 0.13151, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1295 - accuracy: 0.9553 - val_loss: 0.1315 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1316 - accuracy: 0.9558\n",
      "Epoch 422: val_loss improved from 0.13151 to 0.13135, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1293 - accuracy: 0.9553 - val_loss: 0.1313 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1208 - accuracy: 0.9583\n",
      "Epoch 423: val_loss improved from 0.13135 to 0.13118, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9553 - val_loss: 0.1312 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9553\n",
      "Epoch 424: val_loss improved from 0.13118 to 0.13102, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1289 - accuracy: 0.9553 - val_loss: 0.1310 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 425/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1253 - accuracy: 0.9549\n",
      "Epoch 425: val_loss improved from 0.13102 to 0.13086, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1287 - accuracy: 0.9553 - val_loss: 0.1309 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1228 - accuracy: 0.9583\n",
      "Epoch 426: val_loss improved from 0.13086 to 0.13071, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.9553 - val_loss: 0.1307 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1359 - accuracy: 0.9484\n",
      "Epoch 427: val_loss improved from 0.13071 to 0.13055, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1284 - accuracy: 0.9553 - val_loss: 0.1305 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1282 - accuracy: 0.9585\n",
      "Epoch 428: val_loss improved from 0.13055 to 0.13040, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1282 - accuracy: 0.9585 - val_loss: 0.1304 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9583\n",
      "Epoch 429: val_loss improved from 0.13040 to 0.13025, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9585 - val_loss: 0.1302 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1167 - accuracy: 0.9643\n",
      "Epoch 430: val_loss improved from 0.13025 to 0.13009, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1278 - accuracy: 0.9585 - val_loss: 0.1301 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1242 - accuracy: 0.9592\n",
      "Epoch 431: val_loss improved from 0.13009 to 0.12994, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1277 - accuracy: 0.9585 - val_loss: 0.1299 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1149 - accuracy: 0.9675\n",
      "Epoch 432: val_loss improved from 0.12994 to 0.12979, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1275 - accuracy: 0.9585 - val_loss: 0.1298 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1310 - accuracy: 0.9535\n",
      "Epoch 433: val_loss improved from 0.12979 to 0.12965, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1273 - accuracy: 0.9585 - val_loss: 0.1296 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1325 - accuracy: 0.9539\n",
      "Epoch 434: val_loss improved from 0.12965 to 0.12950, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1271 - accuracy: 0.9585 - val_loss: 0.1295 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1196 - accuracy: 0.9621\n",
      "Epoch 435: val_loss improved from 0.12950 to 0.12934, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1269 - accuracy: 0.9585 - val_loss: 0.1293 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1132 - accuracy: 0.9681\n",
      "Epoch 436: val_loss improved from 0.12934 to 0.12919, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1268 - accuracy: 0.9585 - val_loss: 0.1292 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1343 - accuracy: 0.9553\n",
      "Epoch 437: val_loss improved from 0.12919 to 0.12904, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1266 - accuracy: 0.9585 - val_loss: 0.1290 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1215 - accuracy: 0.9626\n",
      "Epoch 438: val_loss improved from 0.12904 to 0.12890, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1264 - accuracy: 0.9585 - val_loss: 0.1289 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1149 - accuracy: 0.9626\n",
      "Epoch 439: val_loss improved from 0.12890 to 0.12875, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1262 - accuracy: 0.9585 - val_loss: 0.1288 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9583\n",
      "Epoch 440: val_loss improved from 0.12875 to 0.12861, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1261 - accuracy: 0.9585 - val_loss: 0.1286 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1209 - accuracy: 0.9601\n",
      "Epoch 441: val_loss improved from 0.12861 to 0.12846, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1259 - accuracy: 0.9585 - val_loss: 0.1285 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1366 - accuracy: 0.9553\n",
      "Epoch 442: val_loss improved from 0.12846 to 0.12832, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1257 - accuracy: 0.9585 - val_loss: 0.1283 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9585\n",
      "Epoch 443: val_loss improved from 0.12832 to 0.12818, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1255 - accuracy: 0.9585 - val_loss: 0.1282 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1241 - accuracy: 0.9574\n",
      "Epoch 444: val_loss improved from 0.12818 to 0.12811, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1254 - accuracy: 0.9585 - val_loss: 0.1281 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1273 - accuracy: 0.9575\n",
      "Epoch 445: val_loss improved from 0.12811 to 0.12796, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1252 - accuracy: 0.9585 - val_loss: 0.1280 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 446/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1369 - accuracy: 0.9512\n",
      "Epoch 446: val_loss improved from 0.12796 to 0.12778, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1250 - accuracy: 0.9585 - val_loss: 0.1278 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 447/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1352 - accuracy: 0.9519\n",
      "Epoch 447: val_loss improved from 0.12778 to 0.12764, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1249 - accuracy: 0.9585 - val_loss: 0.1276 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 448/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.0989 - accuracy: 0.9708\n",
      "Epoch 448: val_loss improved from 0.12764 to 0.12749, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 0.9585 - val_loss: 0.1275 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 449/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1314 - accuracy: 0.9539\n",
      "Epoch 449: val_loss improved from 0.12749 to 0.12735, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1246 - accuracy: 0.9585 - val_loss: 0.1273 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 450/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9583\n",
      "Epoch 450: val_loss improved from 0.12735 to 0.12721, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1244 - accuracy: 0.9585 - val_loss: 0.1272 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1186 - accuracy: 0.9608\n",
      "Epoch 451: val_loss improved from 0.12721 to 0.12707, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1242 - accuracy: 0.9585 - val_loss: 0.1271 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 452/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1305 - accuracy: 0.9593\n",
      "Epoch 452: val_loss improved from 0.12707 to 0.12693, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1240 - accuracy: 0.9585 - val_loss: 0.1269 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 453/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9585\n",
      "Epoch 453: val_loss improved from 0.12693 to 0.12678, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1239 - accuracy: 0.9585 - val_loss: 0.1268 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 454/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1195 - accuracy: 0.9618\n",
      "Epoch 454: val_loss improved from 0.12678 to 0.12664, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1237 - accuracy: 0.9585 - val_loss: 0.1266 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1184 - accuracy: 0.9583\n",
      "Epoch 455: val_loss improved from 0.12664 to 0.12650, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1236 - accuracy: 0.9585 - val_loss: 0.1265 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 456/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1359 - accuracy: 0.9553\n",
      "Epoch 456: val_loss improved from 0.12650 to 0.12636, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1234 - accuracy: 0.9585 - val_loss: 0.1264 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 457/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1379 - accuracy: 0.9512\n",
      "Epoch 457: val_loss improved from 0.12636 to 0.12623, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1232 - accuracy: 0.9585 - val_loss: 0.1262 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1045 - accuracy: 0.9643\n",
      "Epoch 458: val_loss improved from 0.12623 to 0.12609, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1231 - accuracy: 0.9585 - val_loss: 0.1261 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9583\n",
      "Epoch 459: val_loss improved from 0.12609 to 0.12599, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1229 - accuracy: 0.9585 - val_loss: 0.1260 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 460/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9585\n",
      "Epoch 460: val_loss improved from 0.12599 to 0.12586, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1227 - accuracy: 0.9585 - val_loss: 0.1259 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 461/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1176 - accuracy: 0.9634\n",
      "Epoch 461: val_loss improved from 0.12586 to 0.12572, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1226 - accuracy: 0.9585 - val_loss: 0.1257 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "39/53 [=====================>........] - ETA: 0s - loss: 0.1180 - accuracy: 0.9615\n",
      "Epoch 462: val_loss improved from 0.12572 to 0.12559, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1224 - accuracy: 0.9585 - val_loss: 0.1256 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1132 - accuracy: 0.9634\n",
      "Epoch 463: val_loss improved from 0.12559 to 0.12547, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1222 - accuracy: 0.9585 - val_loss: 0.1255 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1280 - accuracy: 0.9549\n",
      "Epoch 464: val_loss improved from 0.12547 to 0.12533, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1221 - accuracy: 0.9585 - val_loss: 0.1253 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 465/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1200 - accuracy: 0.9574\n",
      "Epoch 465: val_loss improved from 0.12533 to 0.12532, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1219 - accuracy: 0.9585 - val_loss: 0.1253 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1064 - accuracy: 0.9683\n",
      "Epoch 466: val_loss improved from 0.12532 to 0.12519, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1218 - accuracy: 0.9585 - val_loss: 0.1252 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 467/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9600\n",
      "Epoch 467: val_loss improved from 0.12519 to 0.12506, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1216 - accuracy: 0.9617 - val_loss: 0.1251 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1383 - accuracy: 0.9553\n",
      "Epoch 468: val_loss improved from 0.12506 to 0.12493, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1215 - accuracy: 0.9617 - val_loss: 0.1249 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 469/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1112 - accuracy: 0.9660\n",
      "Epoch 469: val_loss improved from 0.12493 to 0.12481, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1213 - accuracy: 0.9617 - val_loss: 0.1248 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1158 - accuracy: 0.9625\n",
      "Epoch 470: val_loss improved from 0.12481 to 0.12469, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1211 - accuracy: 0.9617 - val_loss: 0.1247 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1204 - accuracy: 0.9603\n",
      "Epoch 471: val_loss improved from 0.12469 to 0.12456, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1210 - accuracy: 0.9617 - val_loss: 0.1246 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 472/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9617\n",
      "Epoch 472: val_loss improved from 0.12456 to 0.12443, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1208 - accuracy: 0.9617 - val_loss: 0.1244 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1239 - accuracy: 0.9603\n",
      "Epoch 473: val_loss improved from 0.12443 to 0.12431, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9617 - val_loss: 0.1243 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1050 - accuracy: 0.9710\n",
      "Epoch 474: val_loss improved from 0.12431 to 0.12418, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1205 - accuracy: 0.9617 - val_loss: 0.1242 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9615\n",
      "Epoch 475: val_loss improved from 0.12418 to 0.12405, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1204 - accuracy: 0.9617 - val_loss: 0.1241 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 476/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9608\n",
      "Epoch 476: val_loss improved from 0.12405 to 0.12393, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1202 - accuracy: 0.9617 - val_loss: 0.1239 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 477/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1264 - accuracy: 0.9603\n",
      "Epoch 477: val_loss improved from 0.12393 to 0.12381, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.9617 - val_loss: 0.1238 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1218 - accuracy: 0.9608\n",
      "Epoch 478: val_loss improved from 0.12381 to 0.12368, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1199 - accuracy: 0.9617 - val_loss: 0.1237 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0985 - accuracy: 0.9688\n",
      "Epoch 479: val_loss improved from 0.12368 to 0.12356, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1198 - accuracy: 0.9617 - val_loss: 0.1236 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 480/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1151 - accuracy: 0.9634\n",
      "Epoch 480: val_loss improved from 0.12356 to 0.12344, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1196 - accuracy: 0.9617 - val_loss: 0.1234 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 481/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1169 - accuracy: 0.9641\n",
      "Epoch 481: val_loss improved from 0.12344 to 0.12331, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1195 - accuracy: 0.9617 - val_loss: 0.1233 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 482/1000\n",
      "39/53 [=====================>........] - ETA: 0s - loss: 0.1186 - accuracy: 0.9573\n",
      "Epoch 482: val_loss improved from 0.12331 to 0.12320, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1193 - accuracy: 0.9617 - val_loss: 0.1232 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 483/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1287 - accuracy: 0.9553\n",
      "Epoch 483: val_loss improved from 0.12320 to 0.12308, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1192 - accuracy: 0.9617 - val_loss: 0.1231 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9615\n",
      "Epoch 484: val_loss improved from 0.12308 to 0.12296, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1190 - accuracy: 0.9617 - val_loss: 0.1230 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1204 - accuracy: 0.9608\n",
      "Epoch 485: val_loss improved from 0.12296 to 0.12284, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1189 - accuracy: 0.9617 - val_loss: 0.1228 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 486/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9615\n",
      "Epoch 486: val_loss improved from 0.12284 to 0.12272, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.1187 - accuracy: 0.9617 - val_loss: 0.1227 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1197 - accuracy: 0.9600\n",
      "Epoch 487: val_loss improved from 0.12272 to 0.12260, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1185 - accuracy: 0.9617 - val_loss: 0.1226 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1208 - accuracy: 0.9600\n",
      "Epoch 488: val_loss improved from 0.12260 to 0.12248, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1184 - accuracy: 0.9617 - val_loss: 0.1225 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 489/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1024 - accuracy: 0.9675\n",
      "Epoch 489: val_loss improved from 0.12248 to 0.12234, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1182 - accuracy: 0.9617 - val_loss: 0.1223 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 490/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1235 - accuracy: 0.9593\n",
      "Epoch 490: val_loss improved from 0.12234 to 0.12224, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1181 - accuracy: 0.9617 - val_loss: 0.1222 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1206 - accuracy: 0.9600\n",
      "Epoch 491: val_loss improved from 0.12224 to 0.12212, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1180 - accuracy: 0.9617 - val_loss: 0.1221 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1241 - accuracy: 0.9553\n",
      "Epoch 492: val_loss improved from 0.12212 to 0.12200, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1178 - accuracy: 0.9617 - val_loss: 0.1220 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 493/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9617\n",
      "Epoch 493: val_loss improved from 0.12200 to 0.12189, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1177 - accuracy: 0.9617 - val_loss: 0.1219 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 494/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1192 - accuracy: 0.9608\n",
      "Epoch 494: val_loss improved from 0.12189 to 0.12176, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1175 - accuracy: 0.9617 - val_loss: 0.1218 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1100 - accuracy: 0.9593\n",
      "Epoch 495: val_loss improved from 0.12176 to 0.12165, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1174 - accuracy: 0.9617 - val_loss: 0.1216 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 496/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1126 - accuracy: 0.9653\n",
      "Epoch 496: val_loss improved from 0.12165 to 0.12153, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1173 - accuracy: 0.9617 - val_loss: 0.1215 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1153 - accuracy: 0.9583\n",
      "Epoch 497: val_loss improved from 0.12153 to 0.12141, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1171 - accuracy: 0.9617 - val_loss: 0.1214 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1105 - accuracy: 0.9651\n",
      "Epoch 498: val_loss improved from 0.12141 to 0.12130, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1170 - accuracy: 0.9617 - val_loss: 0.1213 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1229 - accuracy: 0.9574\n",
      "Epoch 499: val_loss improved from 0.12130 to 0.12119, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1168 - accuracy: 0.9617 - val_loss: 0.1212 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1216 - accuracy: 0.9583\n",
      "Epoch 500: val_loss improved from 0.12119 to 0.12107, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1167 - accuracy: 0.9617 - val_loss: 0.1211 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 501/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9615\n",
      "Epoch 501: val_loss improved from 0.12107 to 0.12096, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1165 - accuracy: 0.9617 - val_loss: 0.1210 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 502/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1291 - accuracy: 0.9574\n",
      "Epoch 502: val_loss improved from 0.12096 to 0.12084, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1164 - accuracy: 0.9617 - val_loss: 0.1208 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 503/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9633\n",
      "Epoch 503: val_loss improved from 0.12084 to 0.12073, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9617 - val_loss: 0.1207 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 504/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1238 - accuracy: 0.9593\n",
      "Epoch 504: val_loss improved from 0.12073 to 0.12062, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1161 - accuracy: 0.9617 - val_loss: 0.1206 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9615\n",
      "Epoch 505: val_loss improved from 0.12062 to 0.12049, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1160 - accuracy: 0.9617 - val_loss: 0.1205 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 506/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9617\n",
      "Epoch 506: val_loss improved from 0.12049 to 0.12041, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1158 - accuracy: 0.9617 - val_loss: 0.1204 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1290 - accuracy: 0.9574\n",
      "Epoch 507: val_loss improved from 0.12041 to 0.12030, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1156 - accuracy: 0.9649 - val_loss: 0.1203 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 508/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1245 - accuracy: 0.9563\n",
      "Epoch 508: val_loss improved from 0.12030 to 0.12019, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1155 - accuracy: 0.9617 - val_loss: 0.1202 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 509/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1056 - accuracy: 0.9667\n",
      "Epoch 509: val_loss improved from 0.12019 to 0.12008, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9617 - val_loss: 0.1201 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 510/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1152 - accuracy: 0.9643\n",
      "Epoch 510: val_loss improved from 0.12008 to 0.11999, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1152 - accuracy: 0.9617 - val_loss: 0.1200 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "39/53 [=====================>........] - ETA: 0s - loss: 0.1017 - accuracy: 0.9658\n",
      "Epoch 511: val_loss improved from 0.11999 to 0.11988, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1151 - accuracy: 0.9649 - val_loss: 0.1199 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 512/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1111 - accuracy: 0.9673\n",
      "Epoch 512: val_loss improved from 0.11988 to 0.11977, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1149 - accuracy: 0.9649 - val_loss: 0.1198 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 513/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1242 - accuracy: 0.9583\n",
      "Epoch 513: val_loss improved from 0.11977 to 0.11966, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1148 - accuracy: 0.9649 - val_loss: 0.1197 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 514/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1199 - accuracy: 0.9626\n",
      "Epoch 514: val_loss improved from 0.11966 to 0.11955, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1147 - accuracy: 0.9649 - val_loss: 0.1196 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0960 - accuracy: 0.9762\n",
      "Epoch 515: val_loss improved from 0.11955 to 0.11944, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1145 - accuracy: 0.9649 - val_loss: 0.1194 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1145 - accuracy: 0.9647\n",
      "Epoch 516: val_loss improved from 0.11944 to 0.11932, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1144 - accuracy: 0.9649 - val_loss: 0.1193 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 517/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9649\n",
      "Epoch 517: val_loss improved from 0.11932 to 0.11921, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1142 - accuracy: 0.9649 - val_loss: 0.1192 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 518/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1146 - accuracy: 0.9659\n",
      "Epoch 518: val_loss improved from 0.11921 to 0.11910, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1141 - accuracy: 0.9649 - val_loss: 0.1191 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1165 - accuracy: 0.9634\n",
      "Epoch 519: val_loss improved from 0.11910 to 0.11900, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1140 - accuracy: 0.9649 - val_loss: 0.1190 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9649\n",
      "Epoch 520: val_loss improved from 0.11900 to 0.11890, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1138 - accuracy: 0.9649 - val_loss: 0.1189 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1042 - accuracy: 0.9735\n",
      "Epoch 521: val_loss improved from 0.11890 to 0.11879, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1137 - accuracy: 0.9649 - val_loss: 0.1188 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1127 - accuracy: 0.9634\n",
      "Epoch 522: val_loss improved from 0.11879 to 0.11869, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1135 - accuracy: 0.9649 - val_loss: 0.1187 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1216 - accuracy: 0.9601\n",
      "Epoch 523: val_loss improved from 0.11869 to 0.11858, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1134 - accuracy: 0.9649 - val_loss: 0.1186 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9649\n",
      "Epoch 524: val_loss improved from 0.11858 to 0.11848, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1133 - accuracy: 0.9649 - val_loss: 0.1185 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 525/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1225 - accuracy: 0.9593\n",
      "Epoch 525: val_loss improved from 0.11848 to 0.11838, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1131 - accuracy: 0.9649 - val_loss: 0.1184 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1130 - accuracy: 0.9649\n",
      "Epoch 526: val_loss improved from 0.11838 to 0.11827, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9649 - val_loss: 0.1183 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1108 - accuracy: 0.9653\n",
      "Epoch 527: val_loss improved from 0.11827 to 0.11817, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1129 - accuracy: 0.9649 - val_loss: 0.1182 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 528/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9649\n",
      "Epoch 528: val_loss improved from 0.11817 to 0.11807, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1127 - accuracy: 0.9649 - val_loss: 0.1181 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 529/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1176 - accuracy: 0.9626\n",
      "Epoch 529: val_loss improved from 0.11807 to 0.11797, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1126 - accuracy: 0.9649 - val_loss: 0.1180 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 530/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9649\n",
      "Epoch 530: val_loss improved from 0.11797 to 0.11787, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1125 - accuracy: 0.9649 - val_loss: 0.1179 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1041 - accuracy: 0.9690\n",
      "Epoch 531: val_loss improved from 0.11787 to 0.11777, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1123 - accuracy: 0.9649 - val_loss: 0.1178 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 532/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1252 - accuracy: 0.9593\n",
      "Epoch 532: val_loss improved from 0.11777 to 0.11767, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1122 - accuracy: 0.9649 - val_loss: 0.1177 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1115 - accuracy: 0.9630\n",
      "Epoch 533: val_loss improved from 0.11767 to 0.11758, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1121 - accuracy: 0.9649 - val_loss: 0.1176 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 534/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1236 - accuracy: 0.9542\n",
      "Epoch 534: val_loss improved from 0.11758 to 0.11748, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1119 - accuracy: 0.9649 - val_loss: 0.1175 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 535/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1016 - accuracy: 0.9715\n",
      "Epoch 535: val_loss improved from 0.11748 to 0.11738, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1118 - accuracy: 0.9649 - val_loss: 0.1174 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 536/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1044 - accuracy: 0.9660\n",
      "Epoch 536: val_loss improved from 0.11738 to 0.11728, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1116 - accuracy: 0.9649 - val_loss: 0.1173 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9649\n",
      "Epoch 537: val_loss improved from 0.11728 to 0.11719, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1115 - accuracy: 0.9649 - val_loss: 0.1172 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1165 - accuracy: 0.9626\n",
      "Epoch 538: val_loss improved from 0.11719 to 0.11709, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1114 - accuracy: 0.9649 - val_loss: 0.1171 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1105 - accuracy: 0.9643\n",
      "Epoch 539: val_loss improved from 0.11709 to 0.11700, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1113 - accuracy: 0.9649 - val_loss: 0.1170 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1060 - accuracy: 0.9653\n",
      "Epoch 540: val_loss improved from 0.11700 to 0.11690, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1111 - accuracy: 0.9649 - val_loss: 0.1169 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1132 - accuracy: 0.9675\n",
      "Epoch 541: val_loss improved from 0.11690 to 0.11681, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1110 - accuracy: 0.9649 - val_loss: 0.1168 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1201 - accuracy: 0.9583\n",
      "Epoch 542: val_loss improved from 0.11681 to 0.11671, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9649 - val_loss: 0.1167 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 543/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1117 - accuracy: 0.9667\n",
      "Epoch 543: val_loss improved from 0.11671 to 0.11661, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1107 - accuracy: 0.9649 - val_loss: 0.1166 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1213 - accuracy: 0.9563\n",
      "Epoch 544: val_loss improved from 0.11661 to 0.11651, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1106 - accuracy: 0.9649 - val_loss: 0.1165 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1227 - accuracy: 0.9603\n",
      "Epoch 545: val_loss improved from 0.11651 to 0.11642, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1105 - accuracy: 0.9649 - val_loss: 0.1164 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1058 - accuracy: 0.9750\n",
      "Epoch 546: val_loss improved from 0.11642 to 0.11633, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1103 - accuracy: 0.9649 - val_loss: 0.1163 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1144 - accuracy: 0.9625\n",
      "Epoch 547: val_loss improved from 0.11633 to 0.11624, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1102 - accuracy: 0.9649 - val_loss: 0.1162 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1206 - accuracy: 0.9574\n",
      "Epoch 548: val_loss improved from 0.11624 to 0.11615, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9649 - val_loss: 0.1161 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1124 - accuracy: 0.9593\n",
      "Epoch 549: val_loss improved from 0.11615 to 0.11605, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1099 - accuracy: 0.9649 - val_loss: 0.1161 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 550/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1148 - accuracy: 0.9638\n",
      "Epoch 550: val_loss improved from 0.11605 to 0.11596, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1098 - accuracy: 0.9649 - val_loss: 0.1160 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 551/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1022 - accuracy: 0.9683\n",
      "Epoch 551: val_loss improved from 0.11596 to 0.11587, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1097 - accuracy: 0.9649 - val_loss: 0.1159 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 552/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1091 - accuracy: 0.9626\n",
      "Epoch 552: val_loss improved from 0.11587 to 0.11575, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1096 - accuracy: 0.9649 - val_loss: 0.1157 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0989 - accuracy: 0.9729\n",
      "Epoch 553: val_loss improved from 0.11575 to 0.11566, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1094 - accuracy: 0.9649 - val_loss: 0.1157 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 554/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1069 - accuracy: 0.9681\n",
      "Epoch 554: val_loss improved from 0.11566 to 0.11557, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1093 - accuracy: 0.9649 - val_loss: 0.1156 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 555/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1104 - accuracy: 0.9634\n",
      "Epoch 555: val_loss improved from 0.11557 to 0.11548, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1092 - accuracy: 0.9649 - val_loss: 0.1155 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1110 - accuracy: 0.9667\n",
      "Epoch 556: val_loss improved from 0.11548 to 0.11539, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1090 - accuracy: 0.9649 - val_loss: 0.1154 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1152 - accuracy: 0.9601\n",
      "Epoch 557: val_loss improved from 0.11539 to 0.11530, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1089 - accuracy: 0.9649 - val_loss: 0.1153 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1132 - accuracy: 0.9651\n",
      "Epoch 558: val_loss improved from 0.11530 to 0.11521, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9681 - val_loss: 0.1152 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 559/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0874 - accuracy: 0.9752\n",
      "Epoch 559: val_loss improved from 0.11521 to 0.11512, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1087 - accuracy: 0.9681 - val_loss: 0.1151 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 560/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9681\n",
      "Epoch 560: val_loss improved from 0.11512 to 0.11504, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1085 - accuracy: 0.9681 - val_loss: 0.1150 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1029 - accuracy: 0.9694\n",
      "Epoch 561: val_loss improved from 0.11504 to 0.11494, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1084 - accuracy: 0.9681 - val_loss: 0.1149 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1118 - accuracy: 0.9651\n",
      "Epoch 562: val_loss improved from 0.11494 to 0.11485, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1083 - accuracy: 0.9681 - val_loss: 0.1149 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 563/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1059 - accuracy: 0.9700\n",
      "Epoch 563: val_loss improved from 0.11485 to 0.11476, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1081 - accuracy: 0.9681 - val_loss: 0.1148 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 564/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9681\n",
      "Epoch 564: val_loss improved from 0.11476 to 0.11467, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1080 - accuracy: 0.9681 - val_loss: 0.1147 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1035 - accuracy: 0.9694\n",
      "Epoch 565: val_loss improved from 0.11467 to 0.11459, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1079 - accuracy: 0.9681 - val_loss: 0.1146 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1087 - accuracy: 0.9643\n",
      "Epoch 566: val_loss improved from 0.11459 to 0.11451, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1078 - accuracy: 0.9681 - val_loss: 0.1145 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 567/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1112 - accuracy: 0.9667\n",
      "Epoch 567: val_loss improved from 0.11451 to 0.11442, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1077 - accuracy: 0.9681 - val_loss: 0.1144 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 568/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1161 - accuracy: 0.9612\n",
      "Epoch 568: val_loss improved from 0.11442 to 0.11434, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1075 - accuracy: 0.9681 - val_loss: 0.1143 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1103 - accuracy: 0.9681\n",
      "Epoch 569: val_loss improved from 0.11434 to 0.11424, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1074 - accuracy: 0.9681 - val_loss: 0.1142 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1076 - accuracy: 0.9679\n",
      "Epoch 570: val_loss improved from 0.11424 to 0.11416, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1073 - accuracy: 0.9681 - val_loss: 0.1142 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1091 - accuracy: 0.9681\n",
      "Epoch 571: val_loss improved from 0.11416 to 0.11407, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1072 - accuracy: 0.9681 - val_loss: 0.1141 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.1082 - accuracy: 0.9675\n",
      "Epoch 572: val_loss improved from 0.11407 to 0.11399, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1070 - accuracy: 0.9681 - val_loss: 0.1140 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1105 - accuracy: 0.9645\n",
      "Epoch 573: val_loss improved from 0.11399 to 0.11390, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1069 - accuracy: 0.9681 - val_loss: 0.1139 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 574/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1090 - accuracy: 0.9643\n",
      "Epoch 574: val_loss improved from 0.11390 to 0.11380, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1068 - accuracy: 0.9681 - val_loss: 0.1138 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1069 - accuracy: 0.9700\n",
      "Epoch 575: val_loss improved from 0.11380 to 0.11372, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1066 - accuracy: 0.9681 - val_loss: 0.1137 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0795 - accuracy: 0.9802\n",
      "Epoch 576: val_loss improved from 0.11372 to 0.11364, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1065 - accuracy: 0.9681 - val_loss: 0.1136 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1097 - accuracy: 0.9660\n",
      "Epoch 577: val_loss improved from 0.11364 to 0.11356, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1064 - accuracy: 0.9681 - val_loss: 0.1136 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1066 - accuracy: 0.9679\n",
      "Epoch 578: val_loss improved from 0.11356 to 0.11348, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1063 - accuracy: 0.9681 - val_loss: 0.1135 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 579/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1048 - accuracy: 0.9694\n",
      "Epoch 579: val_loss improved from 0.11348 to 0.11339, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1061 - accuracy: 0.9681 - val_loss: 0.1134 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1095 - accuracy: 0.9651\n",
      "Epoch 580: val_loss improved from 0.11339 to 0.11331, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9681 - val_loss: 0.1133 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1105 - accuracy: 0.9660\n",
      "Epoch 581: val_loss improved from 0.11331 to 0.11324, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1059 - accuracy: 0.9681 - val_loss: 0.1132 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 582/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9679\n",
      "Epoch 582: val_loss improved from 0.11324 to 0.11316, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1058 - accuracy: 0.9681 - val_loss: 0.1132 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0965 - accuracy: 0.9710\n",
      "Epoch 583: val_loss improved from 0.11316 to 0.11308, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1057 - accuracy: 0.9681 - val_loss: 0.1131 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 584/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1116 - accuracy: 0.9659\n",
      "Epoch 584: val_loss improved from 0.11308 to 0.11300, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1055 - accuracy: 0.9681 - val_loss: 0.1130 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1030 - accuracy: 0.9688\n",
      "Epoch 585: val_loss improved from 0.11300 to 0.11293, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1054 - accuracy: 0.9681 - val_loss: 0.1129 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.0993 - accuracy: 0.9750\n",
      "Epoch 586: val_loss improved from 0.11293 to 0.11285, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1053 - accuracy: 0.9681 - val_loss: 0.1128 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 587/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9681\n",
      "Epoch 587: val_loss improved from 0.11285 to 0.11276, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1052 - accuracy: 0.9681 - val_loss: 0.1128 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1015 - accuracy: 0.9683\n",
      "Epoch 588: val_loss improved from 0.11276 to 0.11268, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1051 - accuracy: 0.9681 - val_loss: 0.1127 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1130 - accuracy: 0.9638\n",
      "Epoch 589: val_loss improved from 0.11268 to 0.11261, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1049 - accuracy: 0.9681 - val_loss: 0.1126 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1201 - accuracy: 0.9603\n",
      "Epoch 590: val_loss improved from 0.11261 to 0.11253, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1048 - accuracy: 0.9681 - val_loss: 0.1125 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 591/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.1056 - accuracy: 0.9673\n",
      "Epoch 591: val_loss improved from 0.11253 to 0.11245, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1047 - accuracy: 0.9681 - val_loss: 0.1125 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.0961 - accuracy: 0.9708\n",
      "Epoch 592: val_loss improved from 0.11245 to 0.11238, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1046 - accuracy: 0.9681 - val_loss: 0.1124 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0983 - accuracy: 0.9716\n",
      "Epoch 593: val_loss improved from 0.11238 to 0.11230, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1045 - accuracy: 0.9681 - val_loss: 0.1123 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1106 - accuracy: 0.9651\n",
      "Epoch 594: val_loss improved from 0.11230 to 0.11230, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1043 - accuracy: 0.9681 - val_loss: 0.1123 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 595/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0979 - accuracy: 0.9700\n",
      "Epoch 595: val_loss improved from 0.11230 to 0.11221, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1042 - accuracy: 0.9681 - val_loss: 0.1122 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1150 - accuracy: 0.9667\n",
      "Epoch 596: val_loss improved from 0.11221 to 0.11213, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1041 - accuracy: 0.9681 - val_loss: 0.1121 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1063 - accuracy: 0.9667\n",
      "Epoch 597: val_loss improved from 0.11213 to 0.11206, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9681 - val_loss: 0.1121 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 598/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.1155 - accuracy: 0.9612\n",
      "Epoch 598: val_loss improved from 0.11206 to 0.11199, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1038 - accuracy: 0.9681 - val_loss: 0.1120 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 599/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1027 - accuracy: 0.9694\n",
      "Epoch 599: val_loss improved from 0.11199 to 0.11192, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1037 - accuracy: 0.9681 - val_loss: 0.1119 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 600/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1047 - accuracy: 0.9643\n",
      "Epoch 600: val_loss improved from 0.11192 to 0.11184, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1036 - accuracy: 0.9681 - val_loss: 0.1118 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 601/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1048 - accuracy: 0.9667\n",
      "Epoch 601: val_loss improved from 0.11184 to 0.11176, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1035 - accuracy: 0.9681 - val_loss: 0.1118 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1111 - accuracy: 0.9643\n",
      "Epoch 602: val_loss improved from 0.11176 to 0.11169, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1034 - accuracy: 0.9681 - val_loss: 0.1117 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1073 - accuracy: 0.9667\n",
      "Epoch 603: val_loss improved from 0.11169 to 0.11161, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1032 - accuracy: 0.9681 - val_loss: 0.1116 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0903 - accuracy: 0.9715\n",
      "Epoch 604: val_loss improved from 0.11161 to 0.11154, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1031 - accuracy: 0.9681 - val_loss: 0.1115 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1065 - accuracy: 0.9667\n",
      "Epoch 605: val_loss improved from 0.11154 to 0.11147, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1030 - accuracy: 0.9681 - val_loss: 0.1115 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1014 - accuracy: 0.9667\n",
      "Epoch 606: val_loss improved from 0.11147 to 0.11140, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1029 - accuracy: 0.9681 - val_loss: 0.1114 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1049 - accuracy: 0.9660\n",
      "Epoch 607: val_loss improved from 0.11140 to 0.11133, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1028 - accuracy: 0.9681 - val_loss: 0.1113 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 608/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0787 - accuracy: 0.9797\n",
      "Epoch 608: val_loss improved from 0.11133 to 0.11125, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1026 - accuracy: 0.9681 - val_loss: 0.1113 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 609/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1085 - accuracy: 0.9653\n",
      "Epoch 609: val_loss improved from 0.11125 to 0.11118, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1025 - accuracy: 0.9681 - val_loss: 0.1112 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1056 - accuracy: 0.9643\n",
      "Epoch 610: val_loss improved from 0.11118 to 0.11110, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1024 - accuracy: 0.9681 - val_loss: 0.1111 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0988 - accuracy: 0.9694\n",
      "Epoch 611: val_loss improved from 0.11110 to 0.11103, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1023 - accuracy: 0.9681 - val_loss: 0.1110 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0914 - accuracy: 0.9729\n",
      "Epoch 612: val_loss improved from 0.11103 to 0.11096, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1022 - accuracy: 0.9681 - val_loss: 0.1110 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1086 - accuracy: 0.9659\n",
      "Epoch 613: val_loss did not improve from 0.11096\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1020 - accuracy: 0.9681 - val_loss: 0.1110 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9681\n",
      "Epoch 614: val_loss did not improve from 0.11096\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1019 - accuracy: 0.9681 - val_loss: 0.1110 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 615/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0999 - accuracy: 0.9690\n",
      "Epoch 615: val_loss improved from 0.11096 to 0.11089, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1018 - accuracy: 0.9681 - val_loss: 0.1109 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1033 - accuracy: 0.9667\n",
      "Epoch 616: val_loss improved from 0.11089 to 0.11082, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1017 - accuracy: 0.9681 - val_loss: 0.1108 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 617/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1033 - accuracy: 0.9667\n",
      "Epoch 617: val_loss did not improve from 0.11082\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1016 - accuracy: 0.9681 - val_loss: 0.1109 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 618/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1016 - accuracy: 0.9667\n",
      "Epoch 618: val_loss did not improve from 0.11082\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1015 - accuracy: 0.9681 - val_loss: 0.1108 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 619/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1035 - accuracy: 0.9667\n",
      "Epoch 619: val_loss improved from 0.11082 to 0.11075, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1013 - accuracy: 0.9681 - val_loss: 0.1107 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1014 - accuracy: 0.9679\n",
      "Epoch 620: val_loss improved from 0.11075 to 0.11067, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1012 - accuracy: 0.9681 - val_loss: 0.1107 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 621/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9679\n",
      "Epoch 621: val_loss improved from 0.11067 to 0.11059, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1011 - accuracy: 0.9681 - val_loss: 0.1106 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.1056 - accuracy: 0.9653\n",
      "Epoch 622: val_loss improved from 0.11059 to 0.11052, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1010 - accuracy: 0.9681 - val_loss: 0.1105 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1039 - accuracy: 0.9667\n",
      "Epoch 623: val_loss improved from 0.11052 to 0.11044, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1009 - accuracy: 0.9681 - val_loss: 0.1104 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1060 - accuracy: 0.9667\n",
      "Epoch 624: val_loss improved from 0.11044 to 0.11037, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1008 - accuracy: 0.9681 - val_loss: 0.1104 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9681\n",
      "Epoch 625: val_loss improved from 0.11037 to 0.11031, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1007 - accuracy: 0.9681 - val_loss: 0.1103 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 626/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0968 - accuracy: 0.9683\n",
      "Epoch 626: val_loss improved from 0.11031 to 0.11024, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1005 - accuracy: 0.9681 - val_loss: 0.1102 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 627/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.1007 - accuracy: 0.9679\n",
      "Epoch 627: val_loss improved from 0.11024 to 0.11017, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1005 - accuracy: 0.9681 - val_loss: 0.1102 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 628/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0916 - accuracy: 0.9700\n",
      "Epoch 628: val_loss improved from 0.11017 to 0.11010, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1003 - accuracy: 0.9681 - val_loss: 0.1101 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 629/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.1024 - accuracy: 0.9681\n",
      "Epoch 629: val_loss improved from 0.11010 to 0.11003, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1002 - accuracy: 0.9681 - val_loss: 0.1100 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.0922 - accuracy: 0.9708\n",
      "Epoch 630: val_loss did not improve from 0.11003\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1001 - accuracy: 0.9681 - val_loss: 0.1101 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 631/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1001 - accuracy: 0.9697\n",
      "Epoch 631: val_loss improved from 0.11003 to 0.10999, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1000 - accuracy: 0.9712 - val_loss: 0.1100 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0954 - accuracy: 0.9733\n",
      "Epoch 632: val_loss improved from 0.10999 to 0.10992, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0999 - accuracy: 0.9712 - val_loss: 0.1099 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 633/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0984 - accuracy: 0.9728\n",
      "Epoch 633: val_loss improved from 0.10992 to 0.10985, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0998 - accuracy: 0.9712 - val_loss: 0.1099 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 634/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9712\n",
      "Epoch 634: val_loss improved from 0.10985 to 0.10977, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0997 - accuracy: 0.9712 - val_loss: 0.1098 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1085 - accuracy: 0.9674\n",
      "Epoch 635: val_loss improved from 0.10977 to 0.10970, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0996 - accuracy: 0.9712 - val_loss: 0.1097 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0980 - accuracy: 0.9728\n",
      "Epoch 636: val_loss improved from 0.10970 to 0.10963, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0994 - accuracy: 0.9712 - val_loss: 0.1096 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0990 - accuracy: 0.9710\n",
      "Epoch 637: val_loss improved from 0.10963 to 0.10956, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0993 - accuracy: 0.9712 - val_loss: 0.1096 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 638/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0979 - accuracy: 0.9739\n",
      "Epoch 638: val_loss improved from 0.10956 to 0.10949, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0992 - accuracy: 0.9712 - val_loss: 0.1095 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 639/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.1014 - accuracy: 0.9700\n",
      "Epoch 639: val_loss improved from 0.10949 to 0.10942, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0991 - accuracy: 0.9712 - val_loss: 0.1094 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 640/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9712\n",
      "Epoch 640: val_loss improved from 0.10942 to 0.10935, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0990 - accuracy: 0.9712 - val_loss: 0.1094 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 641/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9712\n",
      "Epoch 641: val_loss improved from 0.10935 to 0.10925, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0989 - accuracy: 0.9712 - val_loss: 0.1093 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 642/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0896 - accuracy: 0.9762\n",
      "Epoch 642: val_loss improved from 0.10925 to 0.10918, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0988 - accuracy: 0.9712 - val_loss: 0.1092 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 643/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0836 - accuracy: 0.9811\n",
      "Epoch 643: val_loss improved from 0.10918 to 0.10911, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0986 - accuracy: 0.9712 - val_loss: 0.1091 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0996 - accuracy: 0.9694\n",
      "Epoch 644: val_loss improved from 0.10911 to 0.10904, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0985 - accuracy: 0.9712 - val_loss: 0.1090 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 645/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1031 - accuracy: 0.9694\n",
      "Epoch 645: val_loss improved from 0.10904 to 0.10898, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0984 - accuracy: 0.9712 - val_loss: 0.1090 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0806 - accuracy: 0.9757\n",
      "Epoch 646: val_loss improved from 0.10898 to 0.10891, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0983 - accuracy: 0.9712 - val_loss: 0.1089 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0954 - accuracy: 0.9739\n",
      "Epoch 647: val_loss improved from 0.10891 to 0.10885, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0982 - accuracy: 0.9712 - val_loss: 0.1088 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 648/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1040 - accuracy: 0.9704\n",
      "Epoch 648: val_loss improved from 0.10885 to 0.10879, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0981 - accuracy: 0.9712 - val_loss: 0.1088 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0983 - accuracy: 0.9712\n",
      "Epoch 649: val_loss improved from 0.10879 to 0.10873, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0980 - accuracy: 0.9712 - val_loss: 0.1087 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0903 - accuracy: 0.9729\n",
      "Epoch 650: val_loss improved from 0.10873 to 0.10867, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0979 - accuracy: 0.9712 - val_loss: 0.1087 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0989 - accuracy: 0.9716\n",
      "Epoch 651: val_loss improved from 0.10867 to 0.10860, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0978 - accuracy: 0.9712 - val_loss: 0.1086 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 652/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0940 - accuracy: 0.9733\n",
      "Epoch 652: val_loss improved from 0.10860 to 0.10854, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0977 - accuracy: 0.9712 - val_loss: 0.1085 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0822 - accuracy: 0.9746\n",
      "Epoch 653: val_loss improved from 0.10854 to 0.10847, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0975 - accuracy: 0.9712 - val_loss: 0.1085 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0976 - accuracy: 0.9706\n",
      "Epoch 654: val_loss improved from 0.10847 to 0.10841, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0974 - accuracy: 0.9712 - val_loss: 0.1084 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 655/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0887 - accuracy: 0.9739\n",
      "Epoch 655: val_loss improved from 0.10841 to 0.10836, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0973 - accuracy: 0.9712 - val_loss: 0.1084 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0975 - accuracy: 0.9712\n",
      "Epoch 656: val_loss improved from 0.10836 to 0.10829, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0972 - accuracy: 0.9712 - val_loss: 0.1083 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 657/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9712\n",
      "Epoch 657: val_loss improved from 0.10829 to 0.10823, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0971 - accuracy: 0.9712 - val_loss: 0.1082 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 658/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0976 - accuracy: 0.9710\n",
      "Epoch 658: val_loss improved from 0.10823 to 0.10817, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0970 - accuracy: 0.9712 - val_loss: 0.1082 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 659/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0987 - accuracy: 0.9706\n",
      "Epoch 659: val_loss improved from 0.10817 to 0.10811, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0969 - accuracy: 0.9712 - val_loss: 0.1081 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 660/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0928 - accuracy: 0.9704\n",
      "Epoch 660: val_loss improved from 0.10811 to 0.10806, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0968 - accuracy: 0.9712 - val_loss: 0.1081 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 661/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9712\n",
      "Epoch 661: val_loss improved from 0.10806 to 0.10800, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0967 - accuracy: 0.9712 - val_loss: 0.1080 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.1017 - accuracy: 0.9694\n",
      "Epoch 662: val_loss improved from 0.10800 to 0.10794, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0966 - accuracy: 0.9712 - val_loss: 0.1079 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0930 - accuracy: 0.9722\n",
      "Epoch 663: val_loss did not improve from 0.10794\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0965 - accuracy: 0.9712 - val_loss: 0.1081 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9712\n",
      "Epoch 664: val_loss did not improve from 0.10794\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0964 - accuracy: 0.9712 - val_loss: 0.1080 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 665/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0966 - accuracy: 0.9741\n",
      "Epoch 665: val_loss improved from 0.10794 to 0.10793, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0963 - accuracy: 0.9712 - val_loss: 0.1079 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 666/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0985 - accuracy: 0.9694\n",
      "Epoch 666: val_loss improved from 0.10793 to 0.10787, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0962 - accuracy: 0.9712 - val_loss: 0.1079 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0934 - accuracy: 0.9733\n",
      "Epoch 667: val_loss improved from 0.10787 to 0.10783, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0961 - accuracy: 0.9712 - val_loss: 0.1078 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 668/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0902 - accuracy: 0.9741\n",
      "Epoch 668: val_loss improved from 0.10783 to 0.10777, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0960 - accuracy: 0.9712 - val_loss: 0.1078 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0999 - accuracy: 0.9681\n",
      "Epoch 669: val_loss improved from 0.10777 to 0.10771, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0959 - accuracy: 0.9712 - val_loss: 0.1077 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.1035 - accuracy: 0.9674\n",
      "Epoch 670: val_loss improved from 0.10771 to 0.10764, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0957 - accuracy: 0.9712 - val_loss: 0.1076 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9712\n",
      "Epoch 671: val_loss improved from 0.10764 to 0.10758, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0956 - accuracy: 0.9712 - val_loss: 0.1076 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0831 - accuracy: 0.9752\n",
      "Epoch 672: val_loss improved from 0.10758 to 0.10752, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0955 - accuracy: 0.9712 - val_loss: 0.1075 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0982 - accuracy: 0.9700\n",
      "Epoch 673: val_loss improved from 0.10752 to 0.10747, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0954 - accuracy: 0.9712 - val_loss: 0.1075 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.1011 - accuracy: 0.9704\n",
      "Epoch 674: val_loss improved from 0.10747 to 0.10741, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0953 - accuracy: 0.9712 - val_loss: 0.1074 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 675/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0824 - accuracy: 0.9762\n",
      "Epoch 675: val_loss improved from 0.10741 to 0.10735, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0952 - accuracy: 0.9712 - val_loss: 0.1073 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 676/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0963 - accuracy: 0.9700\n",
      "Epoch 676: val_loss improved from 0.10735 to 0.10727, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0951 - accuracy: 0.9712 - val_loss: 0.1073 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 677/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0958 - accuracy: 0.9683\n",
      "Epoch 677: val_loss improved from 0.10727 to 0.10720, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0950 - accuracy: 0.9712 - val_loss: 0.1072 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0961 - accuracy: 0.9700\n",
      "Epoch 678: val_loss improved from 0.10720 to 0.10715, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0949 - accuracy: 0.9712 - val_loss: 0.1071 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 679/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1034 - accuracy: 0.9659\n",
      "Epoch 679: val_loss improved from 0.10715 to 0.10709, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0948 - accuracy: 0.9712 - val_loss: 0.1071 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0991 - accuracy: 0.9688\n",
      "Epoch 680: val_loss improved from 0.10709 to 0.10704, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0947 - accuracy: 0.9712 - val_loss: 0.1070 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0904 - accuracy: 0.9729\n",
      "Epoch 681: val_loss improved from 0.10704 to 0.10699, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0946 - accuracy: 0.9712 - val_loss: 0.1070 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0927 - accuracy: 0.9739\n",
      "Epoch 682: val_loss improved from 0.10699 to 0.10693, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0945 - accuracy: 0.9712 - val_loss: 0.1069 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 683/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9712\n",
      "Epoch 683: val_loss improved from 0.10693 to 0.10688, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0944 - accuracy: 0.9712 - val_loss: 0.1069 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 684/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0900 - accuracy: 0.9752\n",
      "Epoch 684: val_loss improved from 0.10688 to 0.10682, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0943 - accuracy: 0.9712 - val_loss: 0.1068 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9712\n",
      "Epoch 685: val_loss improved from 0.10682 to 0.10677, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0942 - accuracy: 0.9712 - val_loss: 0.1068 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0859 - accuracy: 0.9752\n",
      "Epoch 686: val_loss improved from 0.10677 to 0.10672, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0941 - accuracy: 0.9712 - val_loss: 0.1067 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 687/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9712\n",
      "Epoch 687: val_loss improved from 0.10672 to 0.10667, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0940 - accuracy: 0.9712 - val_loss: 0.1067 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0980 - accuracy: 0.9690\n",
      "Epoch 688: val_loss improved from 0.10667 to 0.10662, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0939 - accuracy: 0.9712 - val_loss: 0.1066 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.1007 - accuracy: 0.9708\n",
      "Epoch 689: val_loss improved from 0.10662 to 0.10655, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0938 - accuracy: 0.9712 - val_loss: 0.1065 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0836 - accuracy: 0.9715\n",
      "Epoch 690: val_loss improved from 0.10655 to 0.10650, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0937 - accuracy: 0.9712 - val_loss: 0.1065 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 691/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0980 - accuracy: 0.9688\n",
      "Epoch 691: val_loss improved from 0.10650 to 0.10645, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0936 - accuracy: 0.9712 - val_loss: 0.1064 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 692/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.0779 - accuracy: 0.9750\n",
      "Epoch 692: val_loss improved from 0.10645 to 0.10636, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0935 - accuracy: 0.9712 - val_loss: 0.1064 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0878 - accuracy: 0.9746\n",
      "Epoch 693: val_loss improved from 0.10636 to 0.10631, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0934 - accuracy: 0.9712 - val_loss: 0.1063 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 694/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9712\n",
      "Epoch 694: val_loss improved from 0.10631 to 0.10625, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0933 - accuracy: 0.9712 - val_loss: 0.1062 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0934 - accuracy: 0.9712\n",
      "Epoch 695: val_loss improved from 0.10625 to 0.10620, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0932 - accuracy: 0.9712 - val_loss: 0.1062 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.0904 - accuracy: 0.9750\n",
      "Epoch 696: val_loss improved from 0.10620 to 0.10615, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0931 - accuracy: 0.9712 - val_loss: 0.1062 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0816 - accuracy: 0.9739\n",
      "Epoch 697: val_loss improved from 0.10615 to 0.10611, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0930 - accuracy: 0.9712 - val_loss: 0.1061 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.1035 - accuracy: 0.9643\n",
      "Epoch 698: val_loss improved from 0.10611 to 0.10607, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0929 - accuracy: 0.9712 - val_loss: 0.1061 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0902 - accuracy: 0.9715\n",
      "Epoch 699: val_loss improved from 0.10607 to 0.10602, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0928 - accuracy: 0.9712 - val_loss: 0.1060 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0930 - accuracy: 0.9706\n",
      "Epoch 700: val_loss improved from 0.10602 to 0.10597, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0927 - accuracy: 0.9712 - val_loss: 0.1060 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 701/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9712\n",
      "Epoch 701: val_loss improved from 0.10597 to 0.10591, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0926 - accuracy: 0.9712 - val_loss: 0.1059 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0938 - accuracy: 0.9700\n",
      "Epoch 702: val_loss improved from 0.10591 to 0.10585, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0925 - accuracy: 0.9712 - val_loss: 0.1059 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 703/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0937 - accuracy: 0.9706\n",
      "Epoch 703: val_loss improved from 0.10585 to 0.10579, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0924 - accuracy: 0.9712 - val_loss: 0.1058 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9712\n",
      "Epoch 704: val_loss improved from 0.10579 to 0.10575, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0923 - accuracy: 0.9712 - val_loss: 0.1057 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 705/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0925 - accuracy: 0.9728\n",
      "Epoch 705: val_loss improved from 0.10575 to 0.10571, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0922 - accuracy: 0.9712 - val_loss: 0.1057 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9712\n",
      "Epoch 706: val_loss improved from 0.10571 to 0.10566, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0921 - accuracy: 0.9712 - val_loss: 0.1057 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0932 - accuracy: 0.9706\n",
      "Epoch 707: val_loss improved from 0.10566 to 0.10561, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0920 - accuracy: 0.9712 - val_loss: 0.1056 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0904 - accuracy: 0.9728\n",
      "Epoch 708: val_loss improved from 0.10561 to 0.10542, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0919 - accuracy: 0.9712 - val_loss: 0.1054 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 709/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0936 - accuracy: 0.9700\n",
      "Epoch 709: val_loss improved from 0.10542 to 0.10537, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0918 - accuracy: 0.9712 - val_loss: 0.1054 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 710/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0847 - accuracy: 0.9746\n",
      "Epoch 710: val_loss improved from 0.10537 to 0.10533, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0917 - accuracy: 0.9712 - val_loss: 0.1053 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9712\n",
      "Epoch 711: val_loss improved from 0.10533 to 0.10529, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0916 - accuracy: 0.9712 - val_loss: 0.1053 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 712/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9712\n",
      "Epoch 712: val_loss improved from 0.10529 to 0.10524, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0915 - accuracy: 0.9712 - val_loss: 0.1052 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0929 - accuracy: 0.9700\n",
      "Epoch 713: val_loss improved from 0.10524 to 0.10520, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0914 - accuracy: 0.9712 - val_loss: 0.1052 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 714/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0900 - accuracy: 0.9683\n",
      "Epoch 714: val_loss improved from 0.10520 to 0.10516, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0913 - accuracy: 0.9712 - val_loss: 0.1052 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0915 - accuracy: 0.9712\n",
      "Epoch 715: val_loss improved from 0.10516 to 0.10511, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0912 - accuracy: 0.9712 - val_loss: 0.1051 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 716/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9712\n",
      "Epoch 716: val_loss improved from 0.10511 to 0.10507, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0911 - accuracy: 0.9712 - val_loss: 0.1051 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 717/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9712\n",
      "Epoch 717: val_loss improved from 0.10507 to 0.10503, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0910 - accuracy: 0.9712 - val_loss: 0.1050 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 718/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1004 - accuracy: 0.9659\n",
      "Epoch 718: val_loss improved from 0.10503 to 0.10498, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0909 - accuracy: 0.9712 - val_loss: 0.1050 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 719/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0902 - accuracy: 0.9706\n",
      "Epoch 719: val_loss improved from 0.10498 to 0.10494, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0908 - accuracy: 0.9712 - val_loss: 0.1049 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0948 - accuracy: 0.9688\n",
      "Epoch 720: val_loss improved from 0.10494 to 0.10489, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0907 - accuracy: 0.9712 - val_loss: 0.1049 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0851 - accuracy: 0.9756\n",
      "Epoch 721: val_loss improved from 0.10489 to 0.10485, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0906 - accuracy: 0.9712 - val_loss: 0.1048 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0871 - accuracy: 0.9675\n",
      "Epoch 722: val_loss improved from 0.10485 to 0.10481, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0905 - accuracy: 0.9712 - val_loss: 0.1048 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.1019 - accuracy: 0.9659\n",
      "Epoch 723: val_loss improved from 0.10481 to 0.10476, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0904 - accuracy: 0.9712 - val_loss: 0.1048 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0958 - accuracy: 0.9681\n",
      "Epoch 724: val_loss improved from 0.10476 to 0.10472, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0903 - accuracy: 0.9712 - val_loss: 0.1047 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 725/1000\n",
      "40/53 [=====================>........] - ETA: 0s - loss: 0.0980 - accuracy: 0.9708\n",
      "Epoch 725: val_loss improved from 0.10472 to 0.10469, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0902 - accuracy: 0.9712 - val_loss: 0.1047 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0964 - accuracy: 0.9674\n",
      "Epoch 726: val_loss improved from 0.10469 to 0.10464, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0901 - accuracy: 0.9712 - val_loss: 0.1046 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 727/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0918 - accuracy: 0.9706\n",
      "Epoch 727: val_loss improved from 0.10464 to 0.10460, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0900 - accuracy: 0.9712 - val_loss: 0.1046 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 728/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0938 - accuracy: 0.9704\n",
      "Epoch 728: val_loss improved from 0.10460 to 0.10456, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0900 - accuracy: 0.9712 - val_loss: 0.1046 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 729/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0806 - accuracy: 0.9757\n",
      "Epoch 729: val_loss improved from 0.10456 to 0.10453, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0899 - accuracy: 0.9712 - val_loss: 0.1045 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0997 - accuracy: 0.9667\n",
      "Epoch 730: val_loss improved from 0.10453 to 0.10449, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0898 - accuracy: 0.9712 - val_loss: 0.1045 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 731/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0945 - accuracy: 0.9681\n",
      "Epoch 731: val_loss improved from 0.10449 to 0.10444, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0897 - accuracy: 0.9712 - val_loss: 0.1044 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 732/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0912 - accuracy: 0.9706\n",
      "Epoch 732: val_loss improved from 0.10444 to 0.10440, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0896 - accuracy: 0.9712 - val_loss: 0.1044 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 733/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9712\n",
      "Epoch 733: val_loss improved from 0.10440 to 0.10436, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0895 - accuracy: 0.9712 - val_loss: 0.1044 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 734/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0782 - accuracy: 0.9735\n",
      "Epoch 734: val_loss improved from 0.10436 to 0.10433, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0894 - accuracy: 0.9712 - val_loss: 0.1043 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 735/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0880 - accuracy: 0.9739\n",
      "Epoch 735: val_loss improved from 0.10433 to 0.10429, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0893 - accuracy: 0.9712 - val_loss: 0.1043 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 736/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0905 - accuracy: 0.9706\n",
      "Epoch 736: val_loss improved from 0.10429 to 0.10427, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0892 - accuracy: 0.9712 - val_loss: 0.1043 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 737/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0916 - accuracy: 0.9700\n",
      "Epoch 737: val_loss improved from 0.10427 to 0.10423, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0891 - accuracy: 0.9712 - val_loss: 0.1042 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0876 - accuracy: 0.9710\n",
      "Epoch 738: val_loss improved from 0.10423 to 0.10420, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0890 - accuracy: 0.9712 - val_loss: 0.1042 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0873 - accuracy: 0.9710\n",
      "Epoch 739: val_loss improved from 0.10420 to 0.10415, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0889 - accuracy: 0.9712 - val_loss: 0.1042 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0935 - accuracy: 0.9694\n",
      "Epoch 740: val_loss improved from 0.10415 to 0.10411, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0888 - accuracy: 0.9712 - val_loss: 0.1041 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 741/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0889 - accuracy: 0.9712\n",
      "Epoch 741: val_loss improved from 0.10411 to 0.10409, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0887 - accuracy: 0.9712 - val_loss: 0.1041 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 742/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0903 - accuracy: 0.9706\n",
      "Epoch 742: val_loss improved from 0.10409 to 0.10404, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0886 - accuracy: 0.9712 - val_loss: 0.1040 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 743/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0887 - accuracy: 0.9712\n",
      "Epoch 743: val_loss improved from 0.10404 to 0.10401, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0885 - accuracy: 0.9712 - val_loss: 0.1040 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 744/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0927 - accuracy: 0.9659\n",
      "Epoch 744: val_loss improved from 0.10401 to 0.10397, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0884 - accuracy: 0.9712 - val_loss: 0.1040 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 745/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9712\n",
      "Epoch 745: val_loss improved from 0.10397 to 0.10393, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0884 - accuracy: 0.9712 - val_loss: 0.1039 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 746/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0899 - accuracy: 0.9706\n",
      "Epoch 746: val_loss improved from 0.10393 to 0.10389, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0883 - accuracy: 0.9712 - val_loss: 0.1039 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 747/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0865 - accuracy: 0.9739\n",
      "Epoch 747: val_loss improved from 0.10389 to 0.10385, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0882 - accuracy: 0.9712 - val_loss: 0.1038 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 748/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9712\n",
      "Epoch 748: val_loss improved from 0.10385 to 0.10381, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0881 - accuracy: 0.9712 - val_loss: 0.1038 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 749/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0906 - accuracy: 0.9700\n",
      "Epoch 749: val_loss improved from 0.10381 to 0.10377, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0880 - accuracy: 0.9712 - val_loss: 0.1038 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0790 - accuracy: 0.9773\n",
      "Epoch 750: val_loss improved from 0.10377 to 0.10374, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0879 - accuracy: 0.9712 - val_loss: 0.1037 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9712\n",
      "Epoch 751: val_loss improved from 0.10374 to 0.10371, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0878 - accuracy: 0.9712 - val_loss: 0.1037 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0984 - accuracy: 0.9683\n",
      "Epoch 752: val_loss improved from 0.10371 to 0.10367, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0877 - accuracy: 0.9712 - val_loss: 0.1037 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9712\n",
      "Epoch 753: val_loss improved from 0.10367 to 0.10364, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0876 - accuracy: 0.9712 - val_loss: 0.1036 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 754/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0864 - accuracy: 0.9706\n",
      "Epoch 754: val_loss improved from 0.10364 to 0.10361, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0875 - accuracy: 0.9712 - val_loss: 0.1036 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 755/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0845 - accuracy: 0.9757\n",
      "Epoch 755: val_loss improved from 0.10361 to 0.10354, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0874 - accuracy: 0.9712 - val_loss: 0.1035 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 756/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0895 - accuracy: 0.9688\n",
      "Epoch 756: val_loss improved from 0.10354 to 0.10352, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0873 - accuracy: 0.9712 - val_loss: 0.1035 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 757/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0900 - accuracy: 0.9716\n",
      "Epoch 757: val_loss improved from 0.10352 to 0.10350, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0872 - accuracy: 0.9712 - val_loss: 0.1035 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 758/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9712\n",
      "Epoch 758: val_loss improved from 0.10350 to 0.10342, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0871 - accuracy: 0.9712 - val_loss: 0.1034 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9712\n",
      "Epoch 759: val_loss improved from 0.10342 to 0.10338, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0871 - accuracy: 0.9712 - val_loss: 0.1034 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 760/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0880 - accuracy: 0.9706\n",
      "Epoch 760: val_loss improved from 0.10338 to 0.10334, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0870 - accuracy: 0.9712 - val_loss: 0.1033 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0986 - accuracy: 0.9634\n",
      "Epoch 761: val_loss improved from 0.10334 to 0.10327, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0869 - accuracy: 0.9712 - val_loss: 0.1033 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0875 - accuracy: 0.9706\n",
      "Epoch 762: val_loss improved from 0.10327 to 0.10324, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0868 - accuracy: 0.9712 - val_loss: 0.1032 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9712\n",
      "Epoch 763: val_loss improved from 0.10324 to 0.10321, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0867 - accuracy: 0.9712 - val_loss: 0.1032 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0886 - accuracy: 0.9694\n",
      "Epoch 764: val_loss improved from 0.10321 to 0.10318, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0866 - accuracy: 0.9712 - val_loss: 0.1032 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 765/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9712\n",
      "Epoch 765: val_loss improved from 0.10318 to 0.10314, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0865 - accuracy: 0.9712 - val_loss: 0.1031 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0904 - accuracy: 0.9688\n",
      "Epoch 766: val_loss improved from 0.10314 to 0.10311, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0864 - accuracy: 0.9712 - val_loss: 0.1031 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0808 - accuracy: 0.9710\n",
      "Epoch 767: val_loss improved from 0.10311 to 0.10310, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0863 - accuracy: 0.9712 - val_loss: 0.1031 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 768/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0839 - accuracy: 0.9733\n",
      "Epoch 768: val_loss improved from 0.10310 to 0.10307, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0863 - accuracy: 0.9712 - val_loss: 0.1031 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9700\n",
      "Epoch 769: val_loss improved from 0.10307 to 0.10304, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0862 - accuracy: 0.9712 - val_loss: 0.1030 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0863 - accuracy: 0.9712\n",
      "Epoch 770: val_loss improved from 0.10304 to 0.10301, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0861 - accuracy: 0.9712 - val_loss: 0.1030 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0877 - accuracy: 0.9704\n",
      "Epoch 771: val_loss improved from 0.10301 to 0.10298, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0860 - accuracy: 0.9712 - val_loss: 0.1030 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 772/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0864 - accuracy: 0.9706\n",
      "Epoch 772: val_loss improved from 0.10298 to 0.10295, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0859 - accuracy: 0.9712 - val_loss: 0.1029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 773/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0955 - accuracy: 0.9643\n",
      "Epoch 773: val_loss improved from 0.10295 to 0.10292, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0858 - accuracy: 0.9712 - val_loss: 0.1029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0789 - accuracy: 0.9728\n",
      "Epoch 774: val_loss improved from 0.10292 to 0.10279, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0857 - accuracy: 0.9712 - val_loss: 0.1028 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 775/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0905 - accuracy: 0.9674\n",
      "Epoch 775: val_loss improved from 0.10279 to 0.10277, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0856 - accuracy: 0.9712 - val_loss: 0.1028 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9712\n",
      "Epoch 776: val_loss improved from 0.10277 to 0.10274, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0856 - accuracy: 0.9712 - val_loss: 0.1027 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0876 - accuracy: 0.9716\n",
      "Epoch 777: val_loss improved from 0.10274 to 0.10271, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0855 - accuracy: 0.9712 - val_loss: 0.1027 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 778/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0887 - accuracy: 0.9688\n",
      "Epoch 778: val_loss improved from 0.10271 to 0.10269, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0854 - accuracy: 0.9712 - val_loss: 0.1027 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 779/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0810 - accuracy: 0.9735\n",
      "Epoch 779: val_loss improved from 0.10269 to 0.10267, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0853 - accuracy: 0.9712 - val_loss: 0.1027 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 780/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0894 - accuracy: 0.9688\n",
      "Epoch 780: val_loss improved from 0.10267 to 0.10264, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0852 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 781/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0876 - accuracy: 0.9700\n",
      "Epoch 781: val_loss improved from 0.10264 to 0.10261, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0851 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0796 - accuracy: 0.9716\n",
      "Epoch 782: val_loss improved from 0.10261 to 0.10258, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0850 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9712\n",
      "Epoch 783: val_loss improved from 0.10258 to 0.10256, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0849 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0860 - accuracy: 0.9706\n",
      "Epoch 784: val_loss improved from 0.10256 to 0.10253, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0848 - accuracy: 0.9712 - val_loss: 0.1025 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 785/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0883 - accuracy: 0.9690\n",
      "Epoch 785: val_loss improved from 0.10253 to 0.10250, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0847 - accuracy: 0.9712 - val_loss: 0.1025 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 786/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0888 - accuracy: 0.9659\n",
      "Epoch 786: val_loss improved from 0.10250 to 0.10247, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0847 - accuracy: 0.9712 - val_loss: 0.1025 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 787/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0760 - accuracy: 0.9752\n",
      "Epoch 787: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0846 - accuracy: 0.9712 - val_loss: 0.1027 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 788/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0771 - accuracy: 0.9783\n",
      "Epoch 788: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0845 - accuracy: 0.9712 - val_loss: 0.1029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 789/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0831 - accuracy: 0.9722\n",
      "Epoch 789: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0844 - accuracy: 0.9712 - val_loss: 0.1029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 790/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0898 - accuracy: 0.9688\n",
      "Epoch 790: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0843 - accuracy: 0.9712 - val_loss: 0.1029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 791/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0871 - accuracy: 0.9735\n",
      "Epoch 791: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0842 - accuracy: 0.9712 - val_loss: 0.1028 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 792/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0857 - accuracy: 0.9706\n",
      "Epoch 792: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0842 - accuracy: 0.9712 - val_loss: 0.1028 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9712\n",
      "Epoch 793: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0841 - accuracy: 0.9712 - val_loss: 0.1027 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 794/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0848 - accuracy: 0.9704\n",
      "Epoch 794: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0840 - accuracy: 0.9712 - val_loss: 0.1027 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 795/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9712\n",
      "Epoch 795: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0839 - accuracy: 0.9712 - val_loss: 0.1027 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0838 - accuracy: 0.9728\n",
      "Epoch 796: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0838 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 797/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9712\n",
      "Epoch 797: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0837 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0838 - accuracy: 0.9690\n",
      "Epoch 798: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0836 - accuracy: 0.9712 - val_loss: 0.1025 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 799/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0890 - accuracy: 0.9674\n",
      "Epoch 799: val_loss did not improve from 0.10247\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0836 - accuracy: 0.9712 - val_loss: 0.1025 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 800/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0826 - accuracy: 0.9733\n",
      "Epoch 800: val_loss improved from 0.10247 to 0.10246, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0835 - accuracy: 0.9712 - val_loss: 0.1025 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0783 - accuracy: 0.9767\n",
      "Epoch 801: val_loss improved from 0.10246 to 0.10243, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.0834 - accuracy: 0.9712 - val_loss: 0.1024 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0849 - accuracy: 0.9706\n",
      "Epoch 802: val_loss improved from 0.10243 to 0.10239, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0833 - accuracy: 0.9712 - val_loss: 0.1024 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 803/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0781 - accuracy: 0.9757\n",
      "Epoch 803: val_loss improved from 0.10239 to 0.10236, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0832 - accuracy: 0.9712 - val_loss: 0.1024 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0789 - accuracy: 0.9728\n",
      "Epoch 804: val_loss did not improve from 0.10236\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0831 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0853 - accuracy: 0.9694\n",
      "Epoch 805: val_loss did not improve from 0.10236\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0831 - accuracy: 0.9712 - val_loss: 0.1026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 806/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0854 - accuracy: 0.9674\n",
      "Epoch 806: val_loss did not improve from 0.10236\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0830 - accuracy: 0.9712 - val_loss: 0.1025 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9712\n",
      "Epoch 807: val_loss did not improve from 0.10236\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0829 - accuracy: 0.9712 - val_loss: 0.1025 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0842 - accuracy: 0.9706\n",
      "Epoch 808: val_loss did not improve from 0.10236\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0828 - accuracy: 0.9712 - val_loss: 0.1024 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0860 - accuracy: 0.9710\n",
      "Epoch 809: val_loss did not improve from 0.10236\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0827 - accuracy: 0.9712 - val_loss: 0.1024 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 810/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9712\n",
      "Epoch 810: val_loss improved from 0.10236 to 0.10235, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0826 - accuracy: 0.9712 - val_loss: 0.1024 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0841 - accuracy: 0.9706\n",
      "Epoch 811: val_loss improved from 0.10235 to 0.10231, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0826 - accuracy: 0.9712 - val_loss: 0.1023 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 812/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0862 - accuracy: 0.9688\n",
      "Epoch 812: val_loss improved from 0.10231 to 0.10228, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0825 - accuracy: 0.9712 - val_loss: 0.1023 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 813/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0869 - accuracy: 0.9688\n",
      "Epoch 813: val_loss improved from 0.10228 to 0.10223, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0824 - accuracy: 0.9712 - val_loss: 0.1022 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 814/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9712\n",
      "Epoch 814: val_loss improved from 0.10223 to 0.10220, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0823 - accuracy: 0.9712 - val_loss: 0.1022 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 815/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0742 - accuracy: 0.9778\n",
      "Epoch 815: val_loss improved from 0.10220 to 0.10215, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0822 - accuracy: 0.9712 - val_loss: 0.1022 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 816/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0927 - accuracy: 0.9643\n",
      "Epoch 816: val_loss improved from 0.10215 to 0.10211, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0821 - accuracy: 0.9712 - val_loss: 0.1021 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 817/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0772 - accuracy: 0.9729\n",
      "Epoch 817: val_loss improved from 0.10211 to 0.10207, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0821 - accuracy: 0.9712 - val_loss: 0.1021 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0867 - accuracy: 0.9697\n",
      "Epoch 818: val_loss improved from 0.10207 to 0.10204, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0820 - accuracy: 0.9712 - val_loss: 0.1020 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 819/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0810 - accuracy: 0.9722\n",
      "Epoch 819: val_loss improved from 0.10204 to 0.10201, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0819 - accuracy: 0.9712 - val_loss: 0.1020 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 820/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0752 - accuracy: 0.9739\n",
      "Epoch 820: val_loss did not improve from 0.10201\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0818 - accuracy: 0.9712 - val_loss: 0.1022 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 821/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0832 - accuracy: 0.9739\n",
      "Epoch 821: val_loss did not improve from 0.10201\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0817 - accuracy: 0.9744 - val_loss: 0.1021 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0830 - accuracy: 0.9739\n",
      "Epoch 822: val_loss did not improve from 0.10201\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0816 - accuracy: 0.9744 - val_loss: 0.1021 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 823/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0817 - accuracy: 0.9744\n",
      "Epoch 823: val_loss did not improve from 0.10201\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0816 - accuracy: 0.9744 - val_loss: 0.1020 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 824/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0867 - accuracy: 0.9756\n",
      "Epoch 824: val_loss improved from 0.10201 to 0.10200, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0815 - accuracy: 0.9776 - val_loss: 0.1020 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 825/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0852 - accuracy: 0.9762\n",
      "Epoch 825: val_loss improved from 0.10200 to 0.10196, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0814 - accuracy: 0.9776 - val_loss: 0.1020 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 826/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0835 - accuracy: 0.9783\n",
      "Epoch 826: val_loss improved from 0.10196 to 0.10191, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0813 - accuracy: 0.9776 - val_loss: 0.1019 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 827/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0805 - accuracy: 0.9815\n",
      "Epoch 827: val_loss improved from 0.10191 to 0.10188, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0813 - accuracy: 0.9776 - val_loss: 0.1019 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 828/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0847 - accuracy: 0.9746\n",
      "Epoch 828: val_loss improved from 0.10188 to 0.10185, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0812 - accuracy: 0.9776 - val_loss: 0.1018 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 829/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0772 - accuracy: 0.9787\n",
      "Epoch 829: val_loss improved from 0.10185 to 0.10181, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0811 - accuracy: 0.9776 - val_loss: 0.1018 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0739 - accuracy: 0.9806\n",
      "Epoch 830: val_loss improved from 0.10181 to 0.10178, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0810 - accuracy: 0.9776 - val_loss: 0.1018 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0811 - accuracy: 0.9776\n",
      "Epoch 831: val_loss improved from 0.10178 to 0.10174, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0809 - accuracy: 0.9776 - val_loss: 0.1017 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9776\n",
      "Epoch 832: val_loss improved from 0.10174 to 0.10170, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0809 - accuracy: 0.9776 - val_loss: 0.1017 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 833/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0743 - accuracy: 0.9811\n",
      "Epoch 833: val_loss improved from 0.10170 to 0.10169, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0808 - accuracy: 0.9776 - val_loss: 0.1017 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 834/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0866 - accuracy: 0.9762\n",
      "Epoch 834: val_loss improved from 0.10169 to 0.10165, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0807 - accuracy: 0.9776 - val_loss: 0.1017 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 835/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0820 - accuracy: 0.9783\n",
      "Epoch 835: val_loss improved from 0.10165 to 0.10163, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0806 - accuracy: 0.9776 - val_loss: 0.1016 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 836/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0846 - accuracy: 0.9752\n",
      "Epoch 836: val_loss improved from 0.10163 to 0.10159, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0805 - accuracy: 0.9776 - val_loss: 0.1016 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0716 - accuracy: 0.9796\n",
      "Epoch 837: val_loss improved from 0.10159 to 0.10157, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0805 - accuracy: 0.9776 - val_loss: 0.1016 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "41/53 [======================>.......] - ETA: 0s - loss: 0.0776 - accuracy: 0.9797\n",
      "Epoch 838: val_loss improved from 0.10157 to 0.10153, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0804 - accuracy: 0.9776 - val_loss: 0.1015 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 839/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0735 - accuracy: 0.9804\n",
      "Epoch 839: val_loss improved from 0.10153 to 0.10146, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0803 - accuracy: 0.9776 - val_loss: 0.1015 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 840/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0884 - accuracy: 0.9722\n",
      "Epoch 840: val_loss improved from 0.10146 to 0.10142, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0803 - accuracy: 0.9776 - val_loss: 0.1014 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0781 - accuracy: 0.9804\n",
      "Epoch 841: val_loss did not improve from 0.10142\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0802 - accuracy: 0.9776 - val_loss: 0.1016 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0799 - accuracy: 0.9771\n",
      "Epoch 842: val_loss did not improve from 0.10142\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0801 - accuracy: 0.9776 - val_loss: 0.1015 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0797 - accuracy: 0.9771\n",
      "Epoch 843: val_loss did not improve from 0.10142\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0800 - accuracy: 0.9776 - val_loss: 0.1014 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 844/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0829 - accuracy: 0.9762\n",
      "Epoch 844: val_loss improved from 0.10142 to 0.10140, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0799 - accuracy: 0.9776 - val_loss: 0.1014 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0860 - accuracy: 0.9773\n",
      "Epoch 845: val_loss improved from 0.10140 to 0.10137, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0799 - accuracy: 0.9776 - val_loss: 0.1014 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0822 - accuracy: 0.9767\n",
      "Epoch 846: val_loss improved from 0.10137 to 0.10133, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0798 - accuracy: 0.9776 - val_loss: 0.1013 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0908 - accuracy: 0.9729\n",
      "Epoch 847: val_loss improved from 0.10133 to 0.10131, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0797 - accuracy: 0.9776 - val_loss: 0.1013 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 848/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0833 - accuracy: 0.9752\n",
      "Epoch 848: val_loss improved from 0.10131 to 0.10126, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0796 - accuracy: 0.9776 - val_loss: 0.1013 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 849/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9776\n",
      "Epoch 849: val_loss improved from 0.10126 to 0.10123, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0796 - accuracy: 0.9776 - val_loss: 0.1012 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 850/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0800 - accuracy: 0.9771\n",
      "Epoch 850: val_loss improved from 0.10123 to 0.10120, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0795 - accuracy: 0.9776 - val_loss: 0.1012 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 851/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0814 - accuracy: 0.9762\n",
      "Epoch 851: val_loss improved from 0.10120 to 0.10115, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0794 - accuracy: 0.9776 - val_loss: 0.1011 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0671 - accuracy: 0.9806\n",
      "Epoch 852: val_loss improved from 0.10115 to 0.10112, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0793 - accuracy: 0.9776 - val_loss: 0.1011 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0851 - accuracy: 0.9746\n",
      "Epoch 853: val_loss improved from 0.10112 to 0.10109, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0792 - accuracy: 0.9776 - val_loss: 0.1011 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 854/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0772 - accuracy: 0.9787\n",
      "Epoch 854: val_loss improved from 0.10109 to 0.10106, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0792 - accuracy: 0.9776 - val_loss: 0.1011 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0799 - accuracy: 0.9771\n",
      "Epoch 855: val_loss improved from 0.10106 to 0.10104, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0791 - accuracy: 0.9776 - val_loss: 0.1010 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0796 - accuracy: 0.9783\n",
      "Epoch 856: val_loss improved from 0.10104 to 0.10100, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0790 - accuracy: 0.9776 - val_loss: 0.1010 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 857/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0812 - accuracy: 0.9767\n",
      "Epoch 857: val_loss improved from 0.10100 to 0.10098, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0789 - accuracy: 0.9776 - val_loss: 0.1010 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0778 - accuracy: 0.9800\n",
      "Epoch 858: val_loss improved from 0.10098 to 0.10095, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0789 - accuracy: 0.9776 - val_loss: 0.1010 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 859/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0877 - accuracy: 0.9735\n",
      "Epoch 859: val_loss improved from 0.10095 to 0.10093, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0788 - accuracy: 0.9776 - val_loss: 0.1009 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0853 - accuracy: 0.9741\n",
      "Epoch 860: val_loss improved from 0.10093 to 0.10090, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0787 - accuracy: 0.9776 - val_loss: 0.1009 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 861/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9776\n",
      "Epoch 861: val_loss improved from 0.10090 to 0.10087, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0786 - accuracy: 0.9776 - val_loss: 0.1009 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 862/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0787 - accuracy: 0.9776\n",
      "Epoch 862: val_loss improved from 0.10087 to 0.10083, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0786 - accuracy: 0.9776 - val_loss: 0.1008 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0761 - accuracy: 0.9778\n",
      "Epoch 863: val_loss improved from 0.10083 to 0.10081, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0785 - accuracy: 0.9776 - val_loss: 0.1008 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 864/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9776\n",
      "Epoch 864: val_loss improved from 0.10081 to 0.10078, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0784 - accuracy: 0.9776 - val_loss: 0.1008 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 865/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0779 - accuracy: 0.9771\n",
      "Epoch 865: val_loss improved from 0.10078 to 0.10076, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0783 - accuracy: 0.9776 - val_loss: 0.1008 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0785 - accuracy: 0.9776\n",
      "Epoch 866: val_loss improved from 0.10076 to 0.10074, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0783 - accuracy: 0.9776 - val_loss: 0.1007 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9776\n",
      "Epoch 867: val_loss improved from 0.10074 to 0.10071, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0782 - accuracy: 0.9776 - val_loss: 0.1007 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0794 - accuracy: 0.9771\n",
      "Epoch 868: val_loss improved from 0.10071 to 0.10068, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0781 - accuracy: 0.9776 - val_loss: 0.1007 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 869/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0814 - accuracy: 0.9762\n",
      "Epoch 869: val_loss improved from 0.10068 to 0.10066, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0780 - accuracy: 0.9776 - val_loss: 0.1007 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 870/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0796 - accuracy: 0.9752\n",
      "Epoch 870: val_loss improved from 0.10066 to 0.10062, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0780 - accuracy: 0.9776 - val_loss: 0.1006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9776\n",
      "Epoch 871: val_loss improved from 0.10062 to 0.10061, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0779 - accuracy: 0.9776 - val_loss: 0.1006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 872/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0766 - accuracy: 0.9800\n",
      "Epoch 872: val_loss improved from 0.10061 to 0.10059, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0778 - accuracy: 0.9776 - val_loss: 0.1006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 873/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9776\n",
      "Epoch 873: val_loss improved from 0.10059 to 0.10057, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0777 - accuracy: 0.9776 - val_loss: 0.1006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0789 - accuracy: 0.9762\n",
      "Epoch 874: val_loss did not improve from 0.10057\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0777 - accuracy: 0.9776 - val_loss: 0.1007 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9776\n",
      "Epoch 875: val_loss did not improve from 0.10057\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0776 - accuracy: 0.9776 - val_loss: 0.1007 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 876/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0748 - accuracy: 0.9800\n",
      "Epoch 876: val_loss did not improve from 0.10057\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0775 - accuracy: 0.9776 - val_loss: 0.1006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 877/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0802 - accuracy: 0.9762\n",
      "Epoch 877: val_loss did not improve from 0.10057\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0774 - accuracy: 0.9776 - val_loss: 0.1006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0793 - accuracy: 0.9762\n",
      "Epoch 878: val_loss did not improve from 0.10057\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0774 - accuracy: 0.9776 - val_loss: 0.1006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 879/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0775 - accuracy: 0.9776\n",
      "Epoch 879: val_loss improved from 0.10057 to 0.10055, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0773 - accuracy: 0.9776 - val_loss: 0.1005 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0779 - accuracy: 0.9771\n",
      "Epoch 880: val_loss improved from 0.10055 to 0.10053, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0772 - accuracy: 0.9776 - val_loss: 0.1005 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 881/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0793 - accuracy: 0.9762\n",
      "Epoch 881: val_loss improved from 0.10053 to 0.10052, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0772 - accuracy: 0.9776 - val_loss: 0.1005 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0699 - accuracy: 0.9783\n",
      "Epoch 882: val_loss improved from 0.10052 to 0.10042, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0771 - accuracy: 0.9776 - val_loss: 0.1004 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0673 - accuracy: 0.9811\n",
      "Epoch 883: val_loss improved from 0.10042 to 0.10034, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0770 - accuracy: 0.9776 - val_loss: 0.1003 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0767 - accuracy: 0.9792\n",
      "Epoch 884: val_loss improved from 0.10034 to 0.10030, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0769 - accuracy: 0.9776 - val_loss: 0.1003 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0834 - accuracy: 0.9746\n",
      "Epoch 885: val_loss improved from 0.10030 to 0.10029, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0769 - accuracy: 0.9776 - val_loss: 0.1003 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9776\n",
      "Epoch 886: val_loss improved from 0.10029 to 0.10027, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0768 - accuracy: 0.9776 - val_loss: 0.1003 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0719 - accuracy: 0.9811\n",
      "Epoch 887: val_loss improved from 0.10027 to 0.10026, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0767 - accuracy: 0.9776 - val_loss: 0.1003 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 888/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0803 - accuracy: 0.9773\n",
      "Epoch 888: val_loss improved from 0.10026 to 0.10022, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0766 - accuracy: 0.9776 - val_loss: 0.1002 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 889/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0790 - accuracy: 0.9767\n",
      "Epoch 889: val_loss improved from 0.10022 to 0.10020, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0766 - accuracy: 0.9776 - val_loss: 0.1002 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 890/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0798 - accuracy: 0.9778\n",
      "Epoch 890: val_loss improved from 0.10020 to 0.10018, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0765 - accuracy: 0.9776 - val_loss: 0.1002 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0677 - accuracy: 0.9773\n",
      "Epoch 891: val_loss improved from 0.10018 to 0.10016, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0764 - accuracy: 0.9776 - val_loss: 0.1002 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 892/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0753 - accuracy: 0.9767\n",
      "Epoch 892: val_loss improved from 0.10016 to 0.10014, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0764 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9776\n",
      "Epoch 893: val_loss improved from 0.10014 to 0.10012, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0763 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 894/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0565 - accuracy: 0.9841\n",
      "Epoch 894: val_loss improved from 0.10012 to 0.10011, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0762 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0812 - accuracy: 0.9767\n",
      "Epoch 895: val_loss improved from 0.10011 to 0.10009, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0762 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0706 - accuracy: 0.9783\n",
      "Epoch 896: val_loss improved from 0.10009 to 0.10008, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0761 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 897/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0803 - accuracy: 0.9757\n",
      "Epoch 897: val_loss improved from 0.10008 to 0.10007, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0760 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0781 - accuracy: 0.9746\n",
      "Epoch 898: val_loss did not improve from 0.10007\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 899/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0731 - accuracy: 0.9778\n",
      "Epoch 899: val_loss did not improve from 0.10007\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0759 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0761 - accuracy: 0.9767\n",
      "Epoch 900: val_loss did not improve from 0.10007\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0758 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0805 - accuracy: 0.9752\n",
      "Epoch 901: val_loss improved from 0.10007 to 0.10006, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0757 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 902/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0670 - accuracy: 0.9830\n",
      "Epoch 902: val_loss improved from 0.10006 to 0.09996, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0757 - accuracy: 0.9776 - val_loss: 0.1000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 903/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0740 - accuracy: 0.9792\n",
      "Epoch 903: val_loss improved from 0.09996 to 0.09994, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0756 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 904/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0805 - accuracy: 0.9778\n",
      "Epoch 904: val_loss improved from 0.09994 to 0.09992, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0755 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 905/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0756 - accuracy: 0.9787\n",
      "Epoch 905: val_loss improved from 0.09992 to 0.09990, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0755 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0769 - accuracy: 0.9771\n",
      "Epoch 906: val_loss improved from 0.09990 to 0.09988, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0754 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 907/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0723 - accuracy: 0.9852\n",
      "Epoch 907: val_loss improved from 0.09988 to 0.09986, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0753 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 908/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0729 - accuracy: 0.9804\n",
      "Epoch 908: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0753 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 909/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9776\n",
      "Epoch 909: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0752 - accuracy: 0.9776 - val_loss: 0.1000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 910/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0763 - accuracy: 0.9771\n",
      "Epoch 910: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0751 - accuracy: 0.9776 - val_loss: 0.1000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0753 - accuracy: 0.9776\n",
      "Epoch 911: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.9776 - val_loss: 0.1000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9776\n",
      "Epoch 912: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0750 - accuracy: 0.9776 - val_loss: 0.1000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9776\n",
      "Epoch 913: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0749 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0678 - accuracy: 0.9792\n",
      "Epoch 914: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9776\n",
      "Epoch 915: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0748 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0745 - accuracy: 0.9776\n",
      "Epoch 916: val_loss improved from 0.09986 to 0.09986, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0747 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0769 - accuracy: 0.9773\n",
      "Epoch 917: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0746 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 918/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0775 - accuracy: 0.9757\n",
      "Epoch 918: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0746 - accuracy: 0.9776 - val_loss: 0.1001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0754 - accuracy: 0.9762\n",
      "Epoch 919: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0745 - accuracy: 0.9776 - val_loss: 0.1000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 920/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9776\n",
      "Epoch 920: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0744 - accuracy: 0.9776 - val_loss: 0.1000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 921/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0771 - accuracy: 0.9762\n",
      "Epoch 921: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0744 - accuracy: 0.9776 - val_loss: 0.1000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0778 - accuracy: 0.9757\n",
      "Epoch 922: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0743 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 923/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9776\n",
      "Epoch 923: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0742 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 924/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0661 - accuracy: 0.9811\n",
      "Epoch 924: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0742 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0740 - accuracy: 0.9767\n",
      "Epoch 925: val_loss did not improve from 0.09986\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.9776 - val_loss: 0.0999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0743 - accuracy: 0.9776\n",
      "Epoch 926: val_loss improved from 0.09986 to 0.09984, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0740 - accuracy: 0.9776 - val_loss: 0.0998 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 927/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0740 - accuracy: 0.9783\n",
      "Epoch 927: val_loss improved from 0.09984 to 0.09978, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0740 - accuracy: 0.9776 - val_loss: 0.0998 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0728 - accuracy: 0.9792\n",
      "Epoch 928: val_loss improved from 0.09978 to 0.09976, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0739 - accuracy: 0.9776 - val_loss: 0.0998 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9776\n",
      "Epoch 929: val_loss improved from 0.09976 to 0.09965, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0738 - accuracy: 0.9776 - val_loss: 0.0997 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 930/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0749 - accuracy: 0.9757\n",
      "Epoch 930: val_loss improved from 0.09965 to 0.09963, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0738 - accuracy: 0.9776 - val_loss: 0.0996 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 931/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9776\n",
      "Epoch 931: val_loss improved from 0.09963 to 0.09961, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0737 - accuracy: 0.9776 - val_loss: 0.0996 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0751 - accuracy: 0.9771\n",
      "Epoch 932: val_loss improved from 0.09961 to 0.09959, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0736 - accuracy: 0.9776 - val_loss: 0.0996 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 933/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0688 - accuracy: 0.9796\n",
      "Epoch 933: val_loss improved from 0.09959 to 0.09958, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0736 - accuracy: 0.9776 - val_loss: 0.0996 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 934/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0670 - accuracy: 0.9826\n",
      "Epoch 934: val_loss improved from 0.09958 to 0.09956, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0735 - accuracy: 0.9776 - val_loss: 0.0996 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0811 - accuracy: 0.9741\n",
      "Epoch 935: val_loss improved from 0.09956 to 0.09953, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0734 - accuracy: 0.9776 - val_loss: 0.0995 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 936/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0757 - accuracy: 0.9752\n",
      "Epoch 936: val_loss improved from 0.09953 to 0.09951, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0734 - accuracy: 0.9776 - val_loss: 0.0995 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 937/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0606 - accuracy: 0.9826\n",
      "Epoch 937: val_loss improved from 0.09951 to 0.09950, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0733 - accuracy: 0.9776 - val_loss: 0.0995 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0603 - accuracy: 0.9787\n",
      "Epoch 938: val_loss improved from 0.09950 to 0.09948, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0732 - accuracy: 0.9776 - val_loss: 0.0995 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0662 - accuracy: 0.9804\n",
      "Epoch 939: val_loss improved from 0.09948 to 0.09947, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0732 - accuracy: 0.9776 - val_loss: 0.0995 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9776\n",
      "Epoch 940: val_loss improved from 0.09947 to 0.09945, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0731 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 941/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0754 - accuracy: 0.9802\n",
      "Epoch 941: val_loss improved from 0.09945 to 0.09936, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0730 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 942/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0784 - accuracy: 0.9746\n",
      "Epoch 942: val_loss improved from 0.09936 to 0.09935, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0730 - accuracy: 0.9776 - val_loss: 0.0993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 943/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0741 - accuracy: 0.9792\n",
      "Epoch 943: val_loss improved from 0.09935 to 0.09931, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0729 - accuracy: 0.9776 - val_loss: 0.0993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 944/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0731 - accuracy: 0.9778\n",
      "Epoch 944: val_loss improved from 0.09931 to 0.09930, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0729 - accuracy: 0.9776 - val_loss: 0.0993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 945/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0772 - accuracy: 0.9752\n",
      "Epoch 945: val_loss improved from 0.09930 to 0.09929, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0728 - accuracy: 0.9776 - val_loss: 0.0993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 946/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0676 - accuracy: 0.9792\n",
      "Epoch 946: val_loss improved from 0.09929 to 0.09921, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0727 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 947/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0721 - accuracy: 0.9771\n",
      "Epoch 947: val_loss improved from 0.09921 to 0.09919, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0727 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 948/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9776\n",
      "Epoch 948: val_loss did not improve from 0.09919\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0726 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0754 - accuracy: 0.9741\n",
      "Epoch 949: val_loss improved from 0.09919 to 0.09919, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0725 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0594 - accuracy: 0.9802\n",
      "Epoch 950: val_loss improved from 0.09919 to 0.09918, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0725 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0724 - accuracy: 0.9767\n",
      "Epoch 951: val_loss improved from 0.09918 to 0.09916, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0724 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 952/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0730 - accuracy: 0.9767\n",
      "Epoch 952: val_loss improved from 0.09916 to 0.09915, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0723 - accuracy: 0.9776 - val_loss: 0.0991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 953/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9776\n",
      "Epoch 953: val_loss improved from 0.09915 to 0.09914, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0723 - accuracy: 0.9776 - val_loss: 0.0991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 954/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0666 - accuracy: 0.9800\n",
      "Epoch 954: val_loss improved from 0.09914 to 0.09912, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0722 - accuracy: 0.9776 - val_loss: 0.0991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 955/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0668 - accuracy: 0.9796\n",
      "Epoch 955: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0722 - accuracy: 0.9776 - val_loss: 0.0995 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0622 - accuracy: 0.9815\n",
      "Epoch 956: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0721 - accuracy: 0.9776 - val_loss: 0.0995 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 957/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0718 - accuracy: 0.9787\n",
      "Epoch 957: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0720 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0744 - accuracy: 0.9762\n",
      "Epoch 958: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0719 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0723 - accuracy: 0.9767\n",
      "Epoch 959: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0719 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0753 - accuracy: 0.9746\n",
      "Epoch 960: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0718 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 961/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0601 - accuracy: 0.9806\n",
      "Epoch 961: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0718 - accuracy: 0.9776 - val_loss: 0.0993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0770 - accuracy: 0.9741\n",
      "Epoch 962: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0717 - accuracy: 0.9776 - val_loss: 0.0993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9776\n",
      "Epoch 963: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0716 - accuracy: 0.9776 - val_loss: 0.0993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9776\n",
      "Epoch 964: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0716 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9776\n",
      "Epoch 965: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0715 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 966/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0728 - accuracy: 0.9771\n",
      "Epoch 966: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0715 - accuracy: 0.9776 - val_loss: 0.0992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0722 - accuracy: 0.9771\n",
      "Epoch 967: val_loss did not improve from 0.09912\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0714 - accuracy: 0.9776 - val_loss: 0.0991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 968/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0656 - accuracy: 0.9762\n",
      "Epoch 968: val_loss improved from 0.09912 to 0.09912, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0713 - accuracy: 0.9776 - val_loss: 0.0991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0768 - accuracy: 0.9752\n",
      "Epoch 969: val_loss improved from 0.09912 to 0.09910, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0713 - accuracy: 0.9776 - val_loss: 0.0991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 970/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0820 - accuracy: 0.9722\n",
      "Epoch 970: val_loss improved from 0.09910 to 0.09909, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0712 - accuracy: 0.9776 - val_loss: 0.0991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 971/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0722 - accuracy: 0.9771\n",
      "Epoch 971: val_loss improved from 0.09909 to 0.09906, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0712 - accuracy: 0.9776 - val_loss: 0.0991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0740 - accuracy: 0.9787\n",
      "Epoch 972: val_loss improved from 0.09906 to 0.09905, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0711 - accuracy: 0.9776 - val_loss: 0.0990 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9776\n",
      "Epoch 973: val_loss improved from 0.09905 to 0.09903, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0710 - accuracy: 0.9776 - val_loss: 0.0990 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0720 - accuracy: 0.9771\n",
      "Epoch 974: val_loss improved from 0.09903 to 0.09902, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 20ms/step - loss: 0.0710 - accuracy: 0.9776 - val_loss: 0.0990 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 975/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0757 - accuracy: 0.9752\n",
      "Epoch 975: val_loss improved from 0.09902 to 0.09901, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0709 - accuracy: 0.9776 - val_loss: 0.0990 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0815 - accuracy: 0.9722\n",
      "Epoch 976: val_loss improved from 0.09901 to 0.09900, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0708 - accuracy: 0.9776 - val_loss: 0.0990 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 977/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0702 - accuracy: 0.9776\n",
      "Epoch 977: val_loss improved from 0.09900 to 0.09890, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0708 - accuracy: 0.9776 - val_loss: 0.0989 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 978/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0764 - accuracy: 0.9767\n",
      "Epoch 978: val_loss improved from 0.09890 to 0.09889, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0707 - accuracy: 0.9776 - val_loss: 0.0989 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 979/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0718 - accuracy: 0.9767\n",
      "Epoch 979: val_loss improved from 0.09889 to 0.09888, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0707 - accuracy: 0.9776 - val_loss: 0.0989 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "45/53 [========================>.....] - ETA: 0s - loss: 0.0777 - accuracy: 0.9741\n",
      "Epoch 980: val_loss improved from 0.09888 to 0.09887, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0706 - accuracy: 0.9776 - val_loss: 0.0989 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0643 - accuracy: 0.9806\n",
      "Epoch 981: val_loss improved from 0.09887 to 0.09885, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 1s 11ms/step - loss: 0.0706 - accuracy: 0.9776 - val_loss: 0.0989 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 982/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0677 - accuracy: 0.9804\n",
      "Epoch 982: val_loss improved from 0.09885 to 0.09878, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0705 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 983/1000\n",
      "43/53 [=======================>......] - ETA: 0s - loss: 0.0722 - accuracy: 0.9767\n",
      "Epoch 983: val_loss improved from 0.09878 to 0.09878, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0704 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0711 - accuracy: 0.9771\n",
      "Epoch 984: val_loss did not improve from 0.09878\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0704 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 985/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0726 - accuracy: 0.9787\n",
      "Epoch 985: val_loss did not improve from 0.09878\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0703 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0717 - accuracy: 0.9792\n",
      "Epoch 986: val_loss did not improve from 0.09878\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0702 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 987/1000\n",
      "48/53 [==========================>...] - ETA: 0s - loss: 0.0748 - accuracy: 0.9757\n",
      "Epoch 987: val_loss did not improve from 0.09878\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0702 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9776\n",
      "Epoch 988: val_loss did not improve from 0.09878\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0701 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "52/53 [============================>.] - ETA: 0s - loss: 0.0703 - accuracy: 0.9776\n",
      "Epoch 989: val_loss did not improve from 0.09878\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0700 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 990/1000\n",
      "42/53 [======================>.......] - ETA: 0s - loss: 0.0560 - accuracy: 0.9802\n",
      "Epoch 990: val_loss did not improve from 0.09878\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0700 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0708 - accuracy: 0.9771\n",
      "Epoch 991: val_loss improved from 0.09878 to 0.09877, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0699 - accuracy: 0.9776 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "44/53 [=======================>......] - ETA: 0s - loss: 0.0745 - accuracy: 0.9811\n",
      "Epoch 992: val_loss improved from 0.09877 to 0.09876, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0699 - accuracy: 0.9808 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 993/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0618 - accuracy: 0.9855\n",
      "Epoch 993: val_loss improved from 0.09876 to 0.09875, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0698 - accuracy: 0.9808 - val_loss: 0.0988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "49/53 [==========================>...] - ETA: 0s - loss: 0.0733 - accuracy: 0.9796\n",
      "Epoch 994: val_loss improved from 0.09875 to 0.09874, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0697 - accuracy: 0.9808 - val_loss: 0.0987 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 995/1000\n",
      "47/53 [=========================>....] - ETA: 0s - loss: 0.0738 - accuracy: 0.9787\n",
      "Epoch 995: val_loss improved from 0.09874 to 0.09871, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0697 - accuracy: 0.9808 - val_loss: 0.0987 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "51/53 [===========================>..] - ETA: 0s - loss: 0.0630 - accuracy: 0.9837\n",
      "Epoch 996: val_loss improved from 0.09871 to 0.09866, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0696 - accuracy: 0.9808 - val_loss: 0.0987 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 997/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0690 - accuracy: 0.9800\n",
      "Epoch 997: val_loss improved from 0.09866 to 0.09865, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9808 - val_loss: 0.0987 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 998/1000\n",
      "50/53 [===========================>..] - ETA: 0s - loss: 0.0677 - accuracy: 0.9833\n",
      "Epoch 998: val_loss improved from 0.09865 to 0.09864, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0695 - accuracy: 0.9808 - val_loss: 0.0986 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "46/53 [=========================>....] - ETA: 0s - loss: 0.0749 - accuracy: 0.9783\n",
      "Epoch 999: val_loss improved from 0.09864 to 0.09863, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0695 - accuracy: 0.9808 - val_loss: 0.0986 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "53/53 [==============================] - ETA: 0s - loss: 0.0694 - accuracy: 0.9808\n",
      "Epoch 1000: val_loss improved from 0.09863 to 0.09863, saving model to model_checkpoint\\LSTM + GRU_SGD.h5\n",
      "53/53 [==============================] - 0s 8ms/step - loss: 0.0694 - accuracy: 0.9808 - val_loss: 0.0986 - val_accuracy: 0.9643 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 64)          38656     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                9408      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,097\n",
      "Trainable params: 48,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 1s 3ms/step - loss: 0.0986 - accuracy: 0.9643\n",
      "Test Loss: 0.09862752258777618\n",
      "Test Accuracy: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'LSTM + GRU_SGD.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True,input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.GRU(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=6, epochs=1000, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule),callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM + GRU K FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing GRU with K-fold\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.67037, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.67037 to 0.66491, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.66491 to 0.65949, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.65949 to 0.65410, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.65410 to 0.64870, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.64870 to 0.64327, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.64327 to 0.63779, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.63779 to 0.63225, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.63225 to 0.62670, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.62670 to 0.62115, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.62115 to 0.61563, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.61563 to 0.61012, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.61012 to 0.60463, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.60463 to 0.59916, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.59916 to 0.59369, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.59369 to 0.58822, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.58822 to 0.58275, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.58275 to 0.57728, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.57728 to 0.57180, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.57180 to 0.56631, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.56631 to 0.56081, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.56081 to 0.55531, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.55531 to 0.54980, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.54980 to 0.54430, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.54430 to 0.53880, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.53880 to 0.53332, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.53332 to 0.52787, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.52787 to 0.52244, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.52244 to 0.51705, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.51705 to 0.51170, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.51170 to 0.50641, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.50641 to 0.50118, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.50118 to 0.49601, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.49601 to 0.49093, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.49093 to 0.48594, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.48594 to 0.48103, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.48103 to 0.47622, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.47622 to 0.47151, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.47151 to 0.46688, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.46688 to 0.46235, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.46235 to 0.45791, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.45791 to 0.45358, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.45358 to 0.44934, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.44934 to 0.44522, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.44522 to 0.44120, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.44120 to 0.43728, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.43728 to 0.43345, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.43345 to 0.42974, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.42974 to 0.42612, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.42612 to 0.42260, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.42260 to 0.41918, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.41918 to 0.41585, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.41585 to 0.41263, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.41263 to 0.40949, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.40949 to 0.40645, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.40645 to 0.40349, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.40349 to 0.40062, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.40062 to 0.39784, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.39784 to 0.39512, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.39512 to 0.39249, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.39249 to 0.38992, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.38992 to 0.38744, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.38744 to 0.38502, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.38502 to 0.38267, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.38267 to 0.38038, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.38038 to 0.37815, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.37815 to 0.37598, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.37598 to 0.37386, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.37386 to 0.37181, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.37181 to 0.36982, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.36982 to 0.36786, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.36786 to 0.36597, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.36597 to 0.36412, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.36412 to 0.36231, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.36231 to 0.36055, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.36055 to 0.35883, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.35883 to 0.35714, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.35714 to 0.35548, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.35548 to 0.35387, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.35387 to 0.35231, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.35231 to 0.35077, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.35077 to 0.34924, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.34924 to 0.34775, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.34775 to 0.34630, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.34630 to 0.34487, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.34487 to 0.34347, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.34347 to 0.34210, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.34210 to 0.34074, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.34074 to 0.33941, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.33941 to 0.33810, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.33810 to 0.33682, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.33682 to 0.33556, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.33556 to 0.33430, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.33430 to 0.33308, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.33308 to 0.33187, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.33187 to 0.33068, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.33068 to 0.32950, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.32950 to 0.32835, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.32835 to 0.32721, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.32721 to 0.32607, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.32607 to 0.32496, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.32496 to 0.32386, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.32386 to 0.32277, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.32277 to 0.32168, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.32168 to 0.32063, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.32063 to 0.31957, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.31957 to 0.31852, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.31852 to 0.31750, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.31750 to 0.31649, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.31649 to 0.31548, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.31548 to 0.31449, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.31449 to 0.31350, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.31350 to 0.31253, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.31253 to 0.31157, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.31157 to 0.31061, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.31061 to 0.30967, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.30967 to 0.30872, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.30872 to 0.30779, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.30779 to 0.30687, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.30687 to 0.30595, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.30595 to 0.30502, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.30502 to 0.30413, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.30413 to 0.30324, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.30324 to 0.30236, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.30236 to 0.30149, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.30149 to 0.30062, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.30062 to 0.29977, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.29977 to 0.29891, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.29891 to 0.29807, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.29807 to 0.29724, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.29724 to 0.29640, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.29640 to 0.29559, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.29559 to 0.29478, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.29478 to 0.29398, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.29398 to 0.29318, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.29318 to 0.29240, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.29240 to 0.29161, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.29161 to 0.29082, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.29082 to 0.29006, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.29006 to 0.28929, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.28929 to 0.28852, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.28852 to 0.28777, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.28777 to 0.28703, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.28703 to 0.28628, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.28628 to 0.28556, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.28556 to 0.28484, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.28484 to 0.28411, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.28411 to 0.28339, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.28339 to 0.28267, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.28267 to 0.28197, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.28197 to 0.28127, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.28127 to 0.28057, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.28057 to 0.27986, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.27986 to 0.27915, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.27915 to 0.27846, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.27846 to 0.27779, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.27779 to 0.27712, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.27712 to 0.27645, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.27645 to 0.27577, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.27577 to 0.27509, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.27509 to 0.27441, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.27441 to 0.27375, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.27375 to 0.27310, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.27310 to 0.27246, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.27246 to 0.27183, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.27183 to 0.27119, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.27119 to 0.27056, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.27056 to 0.26992, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.26992 to 0.26928, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.26928 to 0.26866, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.26866 to 0.26805, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.26805 to 0.26742, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.26742 to 0.26681, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.26681 to 0.26622, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.26622 to 0.26562, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.26562 to 0.26501, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.26501 to 0.26440, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.26440 to 0.26383, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.26383 to 0.26325, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.26325 to 0.26267, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.26267 to 0.26210, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.26210 to 0.26154, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.26154 to 0.26100, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.26100 to 0.26043, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.26043 to 0.25987, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.25987 to 0.25933, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.25933 to 0.25878, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.25878 to 0.25823, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.25823 to 0.25769, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.25769 to 0.25716, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.25716 to 0.25662, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.25662 to 0.25606, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.25606 to 0.25553, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.25553 to 0.25501, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.25501 to 0.25448, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.25448 to 0.25396, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.25396 to 0.25344, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.25344 to 0.25293, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.25293 to 0.25243, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.25243 to 0.25192, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.25192 to 0.25141, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.25141 to 0.25091, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.25091 to 0.25040, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.25040 to 0.24990, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.24990 to 0.24941, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.24941 to 0.24892, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.24892 to 0.24843, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.24843 to 0.24793, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.24793 to 0.24743, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.24743 to 0.24696, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.24696 to 0.24649, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.24649 to 0.24601, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.24601 to 0.24555, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.24555 to 0.24509, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.24509 to 0.24462, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.24462 to 0.24416, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.24416 to 0.24370, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.24370 to 0.24325, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.24325 to 0.24279, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.24279 to 0.24231, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.24231 to 0.24187, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.24187 to 0.24144, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.24144 to 0.24099, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.24099 to 0.24056, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.24056 to 0.24012, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.24012 to 0.23968, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.23968 to 0.23925, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.23925 to 0.23882, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.23882 to 0.23839, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.23839 to 0.23798, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.23798 to 0.23757, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.23757 to 0.23715, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.23715 to 0.23674, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.23674 to 0.23633, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.23633 to 0.23594, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.23594 to 0.23553, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.23553 to 0.23512, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.23512 to 0.23470, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.23470 to 0.23428, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.23428 to 0.23383, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.23383 to 0.23343, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.23343 to 0.23301, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.23301 to 0.23262, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.23262 to 0.23223, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.23223 to 0.23184, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.23184 to 0.23145, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.23145 to 0.23106, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.23106 to 0.23067, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.23067 to 0.23030, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.23030 to 0.22992, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.22992 to 0.22954, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.22954 to 0.22917, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.22917 to 0.22880, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.22880 to 0.22844, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.22844 to 0.22807, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.22807 to 0.22770, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.22770 to 0.22734, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.22734 to 0.22697, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.22697 to 0.22660, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.22660 to 0.22624, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.22624 to 0.22588, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.22588 to 0.22550, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.22550 to 0.22515, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.22515 to 0.22476, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.22476 to 0.22440, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.22440 to 0.22405, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.22405 to 0.22368, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.22368 to 0.22330, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.22330 to 0.22296, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.22296 to 0.22262, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.22262 to 0.22227, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.22227 to 0.22193, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.22193 to 0.22159, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.22159 to 0.22126, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.22126 to 0.22093, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.22093 to 0.22060, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.22060 to 0.22026, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.22026 to 0.21993, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.21993 to 0.21960, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.21960 to 0.21927, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.21927 to 0.21894, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.21894 to 0.21857, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.21857 to 0.21825, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.21825 to 0.21792, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.21792 to 0.21759, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.21759 to 0.21728, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.21728 to 0.21696, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.21696 to 0.21665, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.21665 to 0.21634, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.21634 to 0.21602, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.21602 to 0.21571, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.21571 to 0.21540, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.21540 to 0.21509, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.21509 to 0.21479, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.21479 to 0.21447, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.21447 to 0.21417, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.21417 to 0.21386, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.21386 to 0.21356, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.21356 to 0.21325, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.21325 to 0.21291, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 301: val_loss improved from 0.21291 to 0.21260, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 302: val_loss improved from 0.21260 to 0.21230, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 303: val_loss improved from 0.21230 to 0.21197, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 304: val_loss improved from 0.21197 to 0.21167, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 305: val_loss improved from 0.21167 to 0.21138, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 306: val_loss improved from 0.21138 to 0.21108, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 307: val_loss improved from 0.21108 to 0.21078, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 308: val_loss improved from 0.21078 to 0.21052, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 309: val_loss improved from 0.21052 to 0.21023, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 310: val_loss improved from 0.21023 to 0.20993, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 311: val_loss improved from 0.20993 to 0.20959, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 312: val_loss improved from 0.20959 to 0.20930, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 313: val_loss improved from 0.20930 to 0.20900, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 314: val_loss improved from 0.20900 to 0.20871, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 315: val_loss improved from 0.20871 to 0.20844, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 316: val_loss improved from 0.20844 to 0.20815, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 317: val_loss improved from 0.20815 to 0.20788, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 318: val_loss improved from 0.20788 to 0.20759, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 319: val_loss improved from 0.20759 to 0.20732, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 320: val_loss improved from 0.20732 to 0.20704, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 321: val_loss improved from 0.20704 to 0.20676, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 322: val_loss improved from 0.20676 to 0.20647, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 323: val_loss improved from 0.20647 to 0.20620, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 324: val_loss improved from 0.20620 to 0.20593, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 325: val_loss improved from 0.20593 to 0.20564, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 326: val_loss improved from 0.20564 to 0.20537, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 327: val_loss improved from 0.20537 to 0.20511, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 328: val_loss improved from 0.20511 to 0.20485, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 329: val_loss improved from 0.20485 to 0.20458, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 330: val_loss improved from 0.20458 to 0.20431, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 331: val_loss improved from 0.20431 to 0.20404, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 332: val_loss improved from 0.20404 to 0.20378, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 333: val_loss improved from 0.20378 to 0.20350, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 334: val_loss improved from 0.20350 to 0.20323, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 335: val_loss improved from 0.20323 to 0.20294, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 336: val_loss improved from 0.20294 to 0.20267, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 337: val_loss improved from 0.20267 to 0.20241, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 338: val_loss improved from 0.20241 to 0.20214, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 339: val_loss improved from 0.20214 to 0.20188, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 340: val_loss improved from 0.20188 to 0.20162, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 341: val_loss improved from 0.20162 to 0.20136, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 342: val_loss improved from 0.20136 to 0.20109, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 343: val_loss improved from 0.20109 to 0.20083, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 344: val_loss improved from 0.20083 to 0.20057, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 345: val_loss improved from 0.20057 to 0.20031, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 346: val_loss improved from 0.20031 to 0.20005, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 347: val_loss improved from 0.20005 to 0.19979, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 348: val_loss improved from 0.19979 to 0.19954, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 349: val_loss improved from 0.19954 to 0.19930, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 350: val_loss improved from 0.19930 to 0.19904, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 351: val_loss improved from 0.19904 to 0.19879, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 352: val_loss improved from 0.19879 to 0.19853, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 353: val_loss improved from 0.19853 to 0.19825, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 354: val_loss improved from 0.19825 to 0.19801, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 355: val_loss improved from 0.19801 to 0.19773, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 356: val_loss improved from 0.19773 to 0.19748, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 357: val_loss improved from 0.19748 to 0.19724, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 358: val_loss improved from 0.19724 to 0.19697, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 359: val_loss improved from 0.19697 to 0.19672, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 360: val_loss improved from 0.19672 to 0.19648, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 361: val_loss improved from 0.19648 to 0.19624, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 362: val_loss improved from 0.19624 to 0.19598, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 363: val_loss improved from 0.19598 to 0.19574, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 364: val_loss improved from 0.19574 to 0.19549, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 365: val_loss improved from 0.19549 to 0.19521, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 366: val_loss improved from 0.19521 to 0.19498, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 367: val_loss improved from 0.19498 to 0.19474, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 368: val_loss improved from 0.19474 to 0.19451, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 369: val_loss improved from 0.19451 to 0.19427, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 370: val_loss improved from 0.19427 to 0.19404, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 371: val_loss improved from 0.19404 to 0.19379, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 372: val_loss improved from 0.19379 to 0.19357, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 373: val_loss improved from 0.19357 to 0.19333, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 374: val_loss improved from 0.19333 to 0.19309, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 375: val_loss improved from 0.19309 to 0.19286, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 376: val_loss improved from 0.19286 to 0.19262, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 377: val_loss improved from 0.19262 to 0.19238, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 378: val_loss improved from 0.19238 to 0.19213, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 379: val_loss improved from 0.19213 to 0.19191, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 380: val_loss improved from 0.19191 to 0.19162, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 381: val_loss improved from 0.19162 to 0.19138, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 382: val_loss improved from 0.19138 to 0.19115, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 383: val_loss improved from 0.19115 to 0.19092, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 384: val_loss improved from 0.19092 to 0.19068, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 385: val_loss improved from 0.19068 to 0.19045, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 386: val_loss improved from 0.19045 to 0.19021, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 387: val_loss improved from 0.19021 to 0.18999, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 388: val_loss improved from 0.18999 to 0.18976, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 389: val_loss improved from 0.18976 to 0.18953, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 390: val_loss improved from 0.18953 to 0.18931, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 391: val_loss improved from 0.18931 to 0.18908, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 392: val_loss improved from 0.18908 to 0.18885, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 393: val_loss improved from 0.18885 to 0.18863, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 394: val_loss improved from 0.18863 to 0.18841, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 395: val_loss improved from 0.18841 to 0.18814, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 396: val_loss improved from 0.18814 to 0.18792, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 397: val_loss improved from 0.18792 to 0.18769, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 398: val_loss improved from 0.18769 to 0.18746, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 399: val_loss improved from 0.18746 to 0.18724, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 400: val_loss improved from 0.18724 to 0.18703, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 401: val_loss improved from 0.18703 to 0.18681, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 402: val_loss improved from 0.18681 to 0.18658, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 403: val_loss improved from 0.18658 to 0.18637, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 404: val_loss improved from 0.18637 to 0.18615, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 405: val_loss improved from 0.18615 to 0.18594, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 406: val_loss improved from 0.18594 to 0.18572, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 407: val_loss improved from 0.18572 to 0.18550, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 408: val_loss improved from 0.18550 to 0.18528, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 409: val_loss improved from 0.18528 to 0.18506, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 410: val_loss improved from 0.18506 to 0.18476, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 411: val_loss improved from 0.18476 to 0.18454, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 412: val_loss improved from 0.18454 to 0.18434, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 413: val_loss improved from 0.18434 to 0.18412, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 414: val_loss improved from 0.18412 to 0.18391, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 415: val_loss improved from 0.18391 to 0.18369, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 416: val_loss improved from 0.18369 to 0.18348, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 417: val_loss improved from 0.18348 to 0.18327, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 418: val_loss improved from 0.18327 to 0.18306, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 419: val_loss improved from 0.18306 to 0.18286, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 420: val_loss improved from 0.18286 to 0.18263, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 421: val_loss improved from 0.18263 to 0.18239, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 422: val_loss improved from 0.18239 to 0.18218, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 423: val_loss improved from 0.18218 to 0.18198, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 424: val_loss improved from 0.18198 to 0.18175, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 425: val_loss improved from 0.18175 to 0.18155, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 426: val_loss improved from 0.18155 to 0.18133, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 427: val_loss improved from 0.18133 to 0.18112, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 428: val_loss improved from 0.18112 to 0.18092, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 429: val_loss improved from 0.18092 to 0.18070, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 430: val_loss improved from 0.18070 to 0.18048, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 431: val_loss improved from 0.18048 to 0.18028, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 432: val_loss improved from 0.18028 to 0.18007, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 433: val_loss improved from 0.18007 to 0.17986, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 434: val_loss improved from 0.17986 to 0.17964, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 435: val_loss improved from 0.17964 to 0.17943, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 436: val_loss improved from 0.17943 to 0.17922, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 437: val_loss improved from 0.17922 to 0.17901, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 438: val_loss improved from 0.17901 to 0.17880, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 439: val_loss improved from 0.17880 to 0.17860, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 440: val_loss improved from 0.17860 to 0.17840, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 441: val_loss improved from 0.17840 to 0.17818, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 442: val_loss improved from 0.17818 to 0.17797, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 443: val_loss improved from 0.17797 to 0.17776, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 444: val_loss improved from 0.17776 to 0.17755, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 445: val_loss improved from 0.17755 to 0.17735, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 446: val_loss improved from 0.17735 to 0.17713, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 447: val_loss improved from 0.17713 to 0.17691, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 448: val_loss improved from 0.17691 to 0.17671, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 449: val_loss improved from 0.17671 to 0.17651, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 450: val_loss improved from 0.17651 to 0.17630, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 451: val_loss improved from 0.17630 to 0.17609, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 452: val_loss improved from 0.17609 to 0.17587, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 453: val_loss improved from 0.17587 to 0.17567, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 454: val_loss improved from 0.17567 to 0.17546, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 455: val_loss improved from 0.17546 to 0.17525, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 456: val_loss improved from 0.17525 to 0.17504, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 457: val_loss improved from 0.17504 to 0.17483, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 458: val_loss improved from 0.17483 to 0.17462, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 459: val_loss improved from 0.17462 to 0.17442, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 460: val_loss improved from 0.17442 to 0.17421, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 461: val_loss improved from 0.17421 to 0.17399, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 462: val_loss improved from 0.17399 to 0.17379, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 463: val_loss improved from 0.17379 to 0.17359, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 464: val_loss improved from 0.17359 to 0.17333, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 465: val_loss improved from 0.17333 to 0.17314, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 466: val_loss improved from 0.17314 to 0.17292, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 467: val_loss improved from 0.17292 to 0.17272, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 468: val_loss improved from 0.17272 to 0.17252, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 469: val_loss improved from 0.17252 to 0.17233, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 470: val_loss improved from 0.17233 to 0.17212, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 471: val_loss improved from 0.17212 to 0.17189, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 472: val_loss improved from 0.17189 to 0.17170, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 473: val_loss improved from 0.17170 to 0.17149, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 474: val_loss improved from 0.17149 to 0.17128, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 475: val_loss improved from 0.17128 to 0.17104, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 476: val_loss improved from 0.17104 to 0.17083, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 477: val_loss improved from 0.17083 to 0.17063, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 478: val_loss improved from 0.17063 to 0.17044, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 479: val_loss improved from 0.17044 to 0.17023, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 480: val_loss improved from 0.17023 to 0.17004, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 481: val_loss improved from 0.17004 to 0.16984, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 482: val_loss improved from 0.16984 to 0.16965, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 483: val_loss improved from 0.16965 to 0.16944, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 484: val_loss improved from 0.16944 to 0.16924, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 485: val_loss improved from 0.16924 to 0.16903, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 486: val_loss improved from 0.16903 to 0.16882, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 487: val_loss improved from 0.16882 to 0.16863, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 488: val_loss improved from 0.16863 to 0.16841, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 489: val_loss improved from 0.16841 to 0.16821, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 490: val_loss improved from 0.16821 to 0.16801, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 491: val_loss improved from 0.16801 to 0.16781, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 492: val_loss improved from 0.16781 to 0.16761, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 493: val_loss improved from 0.16761 to 0.16740, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 494: val_loss improved from 0.16740 to 0.16719, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 495: val_loss improved from 0.16719 to 0.16696, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 496: val_loss improved from 0.16696 to 0.16677, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 497: val_loss improved from 0.16677 to 0.16658, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 498: val_loss improved from 0.16658 to 0.16639, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 499: val_loss improved from 0.16639 to 0.16620, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 500: val_loss improved from 0.16620 to 0.16601, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 501: val_loss improved from 0.16601 to 0.16584, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 502: val_loss improved from 0.16584 to 0.16565, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 503: val_loss improved from 0.16565 to 0.16546, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 504: val_loss improved from 0.16546 to 0.16527, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 505: val_loss improved from 0.16527 to 0.16507, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 506: val_loss improved from 0.16507 to 0.16488, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 507: val_loss improved from 0.16488 to 0.16466, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 508: val_loss improved from 0.16466 to 0.16443, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 509: val_loss improved from 0.16443 to 0.16421, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 510: val_loss improved from 0.16421 to 0.16402, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 511: val_loss improved from 0.16402 to 0.16383, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 512: val_loss improved from 0.16383 to 0.16363, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 513: val_loss improved from 0.16363 to 0.16342, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 514: val_loss improved from 0.16342 to 0.16319, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 515: val_loss improved from 0.16319 to 0.16299, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 516: val_loss improved from 0.16299 to 0.16280, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 517: val_loss improved from 0.16280 to 0.16262, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 518: val_loss improved from 0.16262 to 0.16243, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 519: val_loss improved from 0.16243 to 0.16224, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 520: val_loss improved from 0.16224 to 0.16207, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 521: val_loss improved from 0.16207 to 0.16188, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 522: val_loss improved from 0.16188 to 0.16168, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 523: val_loss improved from 0.16168 to 0.16150, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 524: val_loss improved from 0.16150 to 0.16132, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 525: val_loss improved from 0.16132 to 0.16114, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 526: val_loss improved from 0.16114 to 0.16096, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 527: val_loss improved from 0.16096 to 0.16077, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 528: val_loss improved from 0.16077 to 0.16058, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 529: val_loss improved from 0.16058 to 0.16040, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 530: val_loss improved from 0.16040 to 0.16019, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 531: val_loss improved from 0.16019 to 0.16000, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 532: val_loss improved from 0.16000 to 0.15980, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 533: val_loss improved from 0.15980 to 0.15955, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 534: val_loss improved from 0.15955 to 0.15936, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 535: val_loss improved from 0.15936 to 0.15916, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 536: val_loss improved from 0.15916 to 0.15898, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 537: val_loss improved from 0.15898 to 0.15879, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 538: val_loss improved from 0.15879 to 0.15861, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 539: val_loss improved from 0.15861 to 0.15842, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 540: val_loss improved from 0.15842 to 0.15821, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 541: val_loss improved from 0.15821 to 0.15804, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 542: val_loss improved from 0.15804 to 0.15779, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 543: val_loss improved from 0.15779 to 0.15760, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 544: val_loss improved from 0.15760 to 0.15742, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 545: val_loss improved from 0.15742 to 0.15723, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 546: val_loss improved from 0.15723 to 0.15705, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 547: val_loss improved from 0.15705 to 0.15685, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 548: val_loss improved from 0.15685 to 0.15668, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 549: val_loss improved from 0.15668 to 0.15650, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 550: val_loss improved from 0.15650 to 0.15632, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 551: val_loss improved from 0.15632 to 0.15613, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 552: val_loss improved from 0.15613 to 0.15595, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 553: val_loss improved from 0.15595 to 0.15578, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 554: val_loss improved from 0.15578 to 0.15559, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 555: val_loss improved from 0.15559 to 0.15541, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 556: val_loss improved from 0.15541 to 0.15522, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 557: val_loss improved from 0.15522 to 0.15506, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 558: val_loss improved from 0.15506 to 0.15485, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 559: val_loss improved from 0.15485 to 0.15467, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 560: val_loss improved from 0.15467 to 0.15450, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 561: val_loss improved from 0.15450 to 0.15432, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 562: val_loss improved from 0.15432 to 0.15416, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 563: val_loss improved from 0.15416 to 0.15400, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 564: val_loss improved from 0.15400 to 0.15382, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 565: val_loss improved from 0.15382 to 0.15364, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 566: val_loss improved from 0.15364 to 0.15346, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 567: val_loss improved from 0.15346 to 0.15331, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 568: val_loss improved from 0.15331 to 0.15313, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 569: val_loss improved from 0.15313 to 0.15298, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 570: val_loss improved from 0.15298 to 0.15280, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 571: val_loss improved from 0.15280 to 0.15260, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 572: val_loss improved from 0.15260 to 0.15243, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 573: val_loss improved from 0.15243 to 0.15227, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 574: val_loss improved from 0.15227 to 0.15211, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 575: val_loss improved from 0.15211 to 0.15193, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 576: val_loss improved from 0.15193 to 0.15175, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 577: val_loss improved from 0.15175 to 0.15159, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 578: val_loss improved from 0.15159 to 0.15143, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 579: val_loss improved from 0.15143 to 0.15126, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 580: val_loss improved from 0.15126 to 0.15109, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 581: val_loss improved from 0.15109 to 0.15093, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 582: val_loss improved from 0.15093 to 0.15076, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 583: val_loss improved from 0.15076 to 0.15060, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 584: val_loss improved from 0.15060 to 0.15042, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 585: val_loss improved from 0.15042 to 0.15025, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 586: val_loss improved from 0.15025 to 0.15007, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 587: val_loss improved from 0.15007 to 0.14993, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 588: val_loss improved from 0.14993 to 0.14976, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 589: val_loss improved from 0.14976 to 0.14956, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 590: val_loss improved from 0.14956 to 0.14939, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 591: val_loss improved from 0.14939 to 0.14923, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 592: val_loss improved from 0.14923 to 0.14904, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 593: val_loss improved from 0.14904 to 0.14888, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 594: val_loss improved from 0.14888 to 0.14871, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 595: val_loss improved from 0.14871 to 0.14855, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 596: val_loss improved from 0.14855 to 0.14839, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 597: val_loss improved from 0.14839 to 0.14821, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 598: val_loss improved from 0.14821 to 0.14804, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 599: val_loss improved from 0.14804 to 0.14789, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 600: val_loss improved from 0.14789 to 0.14772, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 601: val_loss improved from 0.14772 to 0.14755, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 602: val_loss improved from 0.14755 to 0.14740, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 603: val_loss improved from 0.14740 to 0.14723, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 604: val_loss improved from 0.14723 to 0.14709, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 605: val_loss improved from 0.14709 to 0.14691, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 606: val_loss improved from 0.14691 to 0.14674, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 607: val_loss improved from 0.14674 to 0.14655, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 608: val_loss improved from 0.14655 to 0.14637, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 609: val_loss improved from 0.14637 to 0.14622, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 610: val_loss improved from 0.14622 to 0.14607, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 611: val_loss improved from 0.14607 to 0.14591, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 612: val_loss improved from 0.14591 to 0.14575, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 613: val_loss improved from 0.14575 to 0.14559, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 614: val_loss improved from 0.14559 to 0.14544, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 615: val_loss improved from 0.14544 to 0.14529, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 616: val_loss improved from 0.14529 to 0.14514, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 617: val_loss improved from 0.14514 to 0.14498, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 618: val_loss improved from 0.14498 to 0.14480, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 619: val_loss improved from 0.14480 to 0.14466, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 620: val_loss improved from 0.14466 to 0.14451, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 621: val_loss improved from 0.14451 to 0.14436, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 622: val_loss improved from 0.14436 to 0.14419, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 623: val_loss improved from 0.14419 to 0.14405, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 624: val_loss improved from 0.14405 to 0.14389, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 625: val_loss improved from 0.14389 to 0.14375, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 626: val_loss improved from 0.14375 to 0.14361, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 627: val_loss improved from 0.14361 to 0.14347, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 628: val_loss improved from 0.14347 to 0.14331, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 629: val_loss improved from 0.14331 to 0.14316, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 630: val_loss improved from 0.14316 to 0.14298, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 631: val_loss improved from 0.14298 to 0.14283, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 632: val_loss improved from 0.14283 to 0.14268, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 633: val_loss improved from 0.14268 to 0.14252, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 634: val_loss improved from 0.14252 to 0.14238, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 635: val_loss improved from 0.14238 to 0.14223, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 636: val_loss improved from 0.14223 to 0.14200, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 637: val_loss improved from 0.14200 to 0.14187, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 638: val_loss improved from 0.14187 to 0.14172, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 639: val_loss improved from 0.14172 to 0.14150, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 640: val_loss improved from 0.14150 to 0.14136, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 641: val_loss improved from 0.14136 to 0.14122, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 642: val_loss improved from 0.14122 to 0.14108, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 643: val_loss improved from 0.14108 to 0.14094, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 644: val_loss improved from 0.14094 to 0.14075, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 645: val_loss improved from 0.14075 to 0.14062, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 646: val_loss improved from 0.14062 to 0.14049, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 647: val_loss improved from 0.14049 to 0.14036, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 648: val_loss improved from 0.14036 to 0.14019, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 649: val_loss improved from 0.14019 to 0.14005, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 650: val_loss improved from 0.14005 to 0.13992, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 651: val_loss improved from 0.13992 to 0.13978, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 652: val_loss improved from 0.13978 to 0.13965, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 653: val_loss improved from 0.13965 to 0.13952, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 654: val_loss improved from 0.13952 to 0.13937, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 655: val_loss improved from 0.13937 to 0.13928, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 656: val_loss improved from 0.13928 to 0.13914, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 657: val_loss improved from 0.13914 to 0.13900, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 658: val_loss improved from 0.13900 to 0.13887, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 659: val_loss improved from 0.13887 to 0.13875, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 660: val_loss improved from 0.13875 to 0.13861, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 661: val_loss improved from 0.13861 to 0.13847, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 662: val_loss improved from 0.13847 to 0.13834, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 663: val_loss improved from 0.13834 to 0.13822, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 664: val_loss improved from 0.13822 to 0.13805, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 665: val_loss improved from 0.13805 to 0.13793, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 666: val_loss improved from 0.13793 to 0.13780, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 667: val_loss improved from 0.13780 to 0.13769, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 668: val_loss improved from 0.13769 to 0.13757, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 669: val_loss improved from 0.13757 to 0.13743, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 670: val_loss improved from 0.13743 to 0.13728, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 671: val_loss improved from 0.13728 to 0.13714, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 672: val_loss improved from 0.13714 to 0.13700, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 673: val_loss improved from 0.13700 to 0.13688, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 674: val_loss improved from 0.13688 to 0.13675, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 675: val_loss improved from 0.13675 to 0.13660, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 676: val_loss improved from 0.13660 to 0.13648, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 677: val_loss improved from 0.13648 to 0.13637, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 678: val_loss improved from 0.13637 to 0.13623, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 679: val_loss improved from 0.13623 to 0.13610, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 680: val_loss improved from 0.13610 to 0.13599, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 681: val_loss improved from 0.13599 to 0.13588, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 682: val_loss improved from 0.13588 to 0.13575, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 683: val_loss improved from 0.13575 to 0.13562, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 684: val_loss improved from 0.13562 to 0.13549, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 685: val_loss improved from 0.13549 to 0.13537, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 686: val_loss improved from 0.13537 to 0.13525, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 687: val_loss improved from 0.13525 to 0.13512, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 688: val_loss improved from 0.13512 to 0.13499, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 689: val_loss improved from 0.13499 to 0.13482, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 690: val_loss improved from 0.13482 to 0.13470, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 691: val_loss improved from 0.13470 to 0.13457, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 692: val_loss improved from 0.13457 to 0.13445, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 693: val_loss improved from 0.13445 to 0.13432, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 694: val_loss improved from 0.13432 to 0.13419, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 695: val_loss improved from 0.13419 to 0.13407, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 696: val_loss improved from 0.13407 to 0.13396, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 697: val_loss improved from 0.13396 to 0.13378, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 698: val_loss improved from 0.13378 to 0.13366, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 699: val_loss improved from 0.13366 to 0.13354, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 700: val_loss improved from 0.13354 to 0.13342, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "Loss: 0.1334, Accuracy: 94.64%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.13342\n",
      "\n",
      "Epoch 207: val_loss improved from 0.13342 to 0.13324, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.13324 to 0.13286, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.13286 to 0.13247, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.13247 to 0.13209, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.13209 to 0.13171, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.13171 to 0.13134, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.13134 to 0.13098, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.13098 to 0.13061, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.13061 to 0.13025, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.13025 to 0.12989, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.12989 to 0.12953, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.12953 to 0.12917, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.12917 to 0.12883, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.12883 to 0.12847, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.12847 to 0.12812, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.12812 to 0.12778, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.12778 to 0.12745, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.12745 to 0.12712, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.12712 to 0.12677, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.12677 to 0.12644, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.12644 to 0.12610, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.12610 to 0.12577, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.12577 to 0.12545, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.12545 to 0.12512, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.12512 to 0.12480, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.12480 to 0.12447, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.12447 to 0.12415, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.12415 to 0.12383, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.12383 to 0.12352, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.12352 to 0.12321, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.12321 to 0.12289, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.12289 to 0.12258, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.12258 to 0.12227, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.12227 to 0.12196, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.12196 to 0.12167, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.12167 to 0.12136, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.12136 to 0.12106, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.12106 to 0.12076, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.12076 to 0.12047, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.12047 to 0.12018, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.12018 to 0.11987, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.11987 to 0.11958, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.11958 to 0.11930, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.11930 to 0.11902, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.11902 to 0.11873, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.11873 to 0.11845, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.11845 to 0.11817, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.11817 to 0.11790, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.11790 to 0.11762, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.11762 to 0.11735, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.11735 to 0.11707, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.11707 to 0.11680, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.11680 to 0.11653, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.11653 to 0.11626, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.11626 to 0.11600, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.11600 to 0.11573, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.11573 to 0.11547, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.11547 to 0.11521, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.11521 to 0.11495, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.11495 to 0.11470, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.11470 to 0.11445, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.11445 to 0.11419, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.11419 to 0.11394, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.11394 to 0.11368, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.11368 to 0.11344, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.11344 to 0.11319, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.11319 to 0.11296, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.11296 to 0.11270, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.11270 to 0.11245, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.11245 to 0.11220, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.11220 to 0.11196, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.11196 to 0.11172, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.11172 to 0.11148, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.11148 to 0.11124, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.11124 to 0.11100, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.11100 to 0.11076, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.11076 to 0.11053, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.11053 to 0.11030, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.11030 to 0.11007, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.11007 to 0.10984, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.10984 to 0.10961, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.10961 to 0.10937, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.10937 to 0.10914, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.10914 to 0.10891, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.10891 to 0.10868, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.10868 to 0.10846, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.10846 to 0.10824, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.10824 to 0.10802, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.10802 to 0.10780, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.10780 to 0.10757, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.10757 to 0.10735, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.10735 to 0.10713, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.10713 to 0.10691, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.10691 to 0.10671, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 301: val_loss improved from 0.10671 to 0.10649, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 302: val_loss improved from 0.10649 to 0.10628, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 303: val_loss improved from 0.10628 to 0.10605, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 304: val_loss improved from 0.10605 to 0.10584, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 305: val_loss improved from 0.10584 to 0.10563, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 306: val_loss improved from 0.10563 to 0.10542, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 307: val_loss improved from 0.10542 to 0.10522, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 308: val_loss improved from 0.10522 to 0.10501, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 309: val_loss improved from 0.10501 to 0.10481, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 310: val_loss improved from 0.10481 to 0.10460, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 311: val_loss improved from 0.10460 to 0.10440, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 312: val_loss improved from 0.10440 to 0.10420, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 313: val_loss improved from 0.10420 to 0.10400, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 314: val_loss improved from 0.10400 to 0.10379, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 315: val_loss improved from 0.10379 to 0.10360, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 316: val_loss improved from 0.10360 to 0.10340, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 317: val_loss improved from 0.10340 to 0.10321, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 318: val_loss improved from 0.10321 to 0.10301, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 319: val_loss improved from 0.10301 to 0.10282, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 320: val_loss improved from 0.10282 to 0.10263, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 321: val_loss improved from 0.10263 to 0.10243, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 322: val_loss improved from 0.10243 to 0.10224, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 323: val_loss improved from 0.10224 to 0.10206, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 324: val_loss improved from 0.10206 to 0.10187, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 325: val_loss improved from 0.10187 to 0.10168, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 326: val_loss improved from 0.10168 to 0.10148, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 327: val_loss improved from 0.10148 to 0.10129, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 328: val_loss improved from 0.10129 to 0.10110, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 329: val_loss improved from 0.10110 to 0.10092, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 330: val_loss improved from 0.10092 to 0.10073, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 331: val_loss improved from 0.10073 to 0.10055, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 332: val_loss improved from 0.10055 to 0.10037, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 333: val_loss improved from 0.10037 to 0.10018, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 334: val_loss improved from 0.10018 to 0.10000, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 335: val_loss improved from 0.10000 to 0.09982, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 336: val_loss improved from 0.09982 to 0.09964, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 337: val_loss improved from 0.09964 to 0.09947, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 338: val_loss improved from 0.09947 to 0.09929, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 339: val_loss improved from 0.09929 to 0.09911, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 340: val_loss improved from 0.09911 to 0.09894, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 341: val_loss improved from 0.09894 to 0.09877, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 342: val_loss improved from 0.09877 to 0.09859, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 343: val_loss improved from 0.09859 to 0.09842, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 344: val_loss improved from 0.09842 to 0.09824, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 345: val_loss improved from 0.09824 to 0.09807, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 346: val_loss improved from 0.09807 to 0.09790, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 347: val_loss improved from 0.09790 to 0.09773, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 348: val_loss improved from 0.09773 to 0.09756, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 349: val_loss improved from 0.09756 to 0.09739, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 350: val_loss improved from 0.09739 to 0.09722, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 351: val_loss improved from 0.09722 to 0.09705, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 352: val_loss improved from 0.09705 to 0.09688, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 353: val_loss improved from 0.09688 to 0.09671, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 354: val_loss improved from 0.09671 to 0.09655, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 355: val_loss improved from 0.09655 to 0.09640, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 356: val_loss improved from 0.09640 to 0.09624, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 357: val_loss improved from 0.09624 to 0.09608, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 358: val_loss improved from 0.09608 to 0.09592, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 359: val_loss improved from 0.09592 to 0.09576, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 360: val_loss improved from 0.09576 to 0.09559, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 361: val_loss improved from 0.09559 to 0.09543, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 362: val_loss improved from 0.09543 to 0.09527, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 363: val_loss improved from 0.09527 to 0.09512, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 364: val_loss improved from 0.09512 to 0.09496, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 365: val_loss improved from 0.09496 to 0.09480, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 366: val_loss improved from 0.09480 to 0.09464, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 367: val_loss improved from 0.09464 to 0.09449, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 368: val_loss improved from 0.09449 to 0.09433, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 369: val_loss improved from 0.09433 to 0.09418, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 370: val_loss improved from 0.09418 to 0.09402, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 371: val_loss improved from 0.09402 to 0.09386, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 372: val_loss improved from 0.09386 to 0.09371, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 373: val_loss improved from 0.09371 to 0.09355, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 374: val_loss improved from 0.09355 to 0.09339, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 375: val_loss improved from 0.09339 to 0.09324, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 376: val_loss improved from 0.09324 to 0.09309, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 377: val_loss improved from 0.09309 to 0.09294, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 378: val_loss improved from 0.09294 to 0.09280, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 379: val_loss improved from 0.09280 to 0.09265, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 380: val_loss improved from 0.09265 to 0.09250, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 381: val_loss improved from 0.09250 to 0.09236, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 382: val_loss improved from 0.09236 to 0.09222, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 383: val_loss improved from 0.09222 to 0.09207, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 384: val_loss improved from 0.09207 to 0.09193, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 385: val_loss improved from 0.09193 to 0.09179, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 386: val_loss improved from 0.09179 to 0.09164, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 387: val_loss improved from 0.09164 to 0.09149, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 388: val_loss improved from 0.09149 to 0.09135, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 389: val_loss improved from 0.09135 to 0.09121, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 390: val_loss improved from 0.09121 to 0.09106, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 391: val_loss improved from 0.09106 to 0.09092, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 392: val_loss improved from 0.09092 to 0.09078, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 393: val_loss improved from 0.09078 to 0.09064, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 394: val_loss improved from 0.09064 to 0.09050, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 395: val_loss improved from 0.09050 to 0.09035, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 396: val_loss improved from 0.09035 to 0.09021, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 397: val_loss improved from 0.09021 to 0.09007, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 398: val_loss improved from 0.09007 to 0.08994, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 399: val_loss improved from 0.08994 to 0.08980, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 400: val_loss improved from 0.08980 to 0.08966, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 401: val_loss improved from 0.08966 to 0.08953, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 402: val_loss improved from 0.08953 to 0.08939, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 403: val_loss improved from 0.08939 to 0.08927, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 404: val_loss improved from 0.08927 to 0.08914, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 405: val_loss improved from 0.08914 to 0.08901, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 406: val_loss improved from 0.08901 to 0.08888, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 407: val_loss improved from 0.08888 to 0.08874, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 408: val_loss improved from 0.08874 to 0.08861, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 409: val_loss improved from 0.08861 to 0.08848, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 410: val_loss improved from 0.08848 to 0.08836, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 411: val_loss improved from 0.08836 to 0.08823, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 412: val_loss improved from 0.08823 to 0.08810, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 413: val_loss improved from 0.08810 to 0.08796, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 414: val_loss improved from 0.08796 to 0.08783, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 415: val_loss improved from 0.08783 to 0.08769, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 416: val_loss improved from 0.08769 to 0.08756, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 417: val_loss improved from 0.08756 to 0.08742, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 418: val_loss improved from 0.08742 to 0.08729, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 419: val_loss improved from 0.08729 to 0.08717, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 420: val_loss improved from 0.08717 to 0.08704, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 421: val_loss improved from 0.08704 to 0.08692, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 422: val_loss improved from 0.08692 to 0.08679, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 423: val_loss improved from 0.08679 to 0.08666, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 424: val_loss improved from 0.08666 to 0.08653, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 425: val_loss improved from 0.08653 to 0.08641, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 426: val_loss improved from 0.08641 to 0.08628, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 427: val_loss improved from 0.08628 to 0.08616, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 428: val_loss improved from 0.08616 to 0.08603, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 429: val_loss improved from 0.08603 to 0.08590, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 430: val_loss improved from 0.08590 to 0.08579, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 431: val_loss improved from 0.08579 to 0.08568, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 432: val_loss improved from 0.08568 to 0.08555, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 433: val_loss improved from 0.08555 to 0.08543, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 434: val_loss improved from 0.08543 to 0.08531, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 435: val_loss improved from 0.08531 to 0.08519, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 436: val_loss improved from 0.08519 to 0.08506, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 437: val_loss improved from 0.08506 to 0.08494, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 438: val_loss improved from 0.08494 to 0.08482, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 439: val_loss improved from 0.08482 to 0.08470, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 440: val_loss improved from 0.08470 to 0.08457, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 441: val_loss improved from 0.08457 to 0.08445, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 442: val_loss improved from 0.08445 to 0.08433, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 443: val_loss improved from 0.08433 to 0.08422, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 444: val_loss improved from 0.08422 to 0.08410, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 445: val_loss improved from 0.08410 to 0.08399, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 446: val_loss improved from 0.08399 to 0.08388, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 447: val_loss improved from 0.08388 to 0.08375, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 448: val_loss improved from 0.08375 to 0.08362, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 449: val_loss improved from 0.08362 to 0.08350, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 450: val_loss improved from 0.08350 to 0.08339, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 451: val_loss improved from 0.08339 to 0.08327, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 452: val_loss improved from 0.08327 to 0.08316, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 453: val_loss improved from 0.08316 to 0.08304, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 454: val_loss improved from 0.08304 to 0.08293, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 455: val_loss improved from 0.08293 to 0.08280, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 456: val_loss improved from 0.08280 to 0.08269, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 457: val_loss improved from 0.08269 to 0.08259, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 458: val_loss improved from 0.08259 to 0.08248, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 459: val_loss improved from 0.08248 to 0.08236, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 460: val_loss improved from 0.08236 to 0.08225, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 461: val_loss improved from 0.08225 to 0.08215, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 462: val_loss improved from 0.08215 to 0.08204, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 463: val_loss improved from 0.08204 to 0.08192, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 464: val_loss improved from 0.08192 to 0.08180, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 465: val_loss improved from 0.08180 to 0.08169, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 466: val_loss improved from 0.08169 to 0.08159, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 467: val_loss improved from 0.08159 to 0.08148, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 468: val_loss improved from 0.08148 to 0.08136, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 469: val_loss improved from 0.08136 to 0.08124, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 470: val_loss improved from 0.08124 to 0.08113, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 471: val_loss improved from 0.08113 to 0.08101, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 472: val_loss improved from 0.08101 to 0.08090, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 473: val_loss improved from 0.08090 to 0.08080, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 474: val_loss improved from 0.08080 to 0.08070, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 475: val_loss improved from 0.08070 to 0.08059, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 476: val_loss improved from 0.08059 to 0.08049, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 477: val_loss improved from 0.08049 to 0.08039, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 478: val_loss improved from 0.08039 to 0.08029, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 479: val_loss improved from 0.08029 to 0.08017, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 480: val_loss improved from 0.08017 to 0.08007, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 481: val_loss improved from 0.08007 to 0.07996, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 482: val_loss improved from 0.07996 to 0.07986, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 483: val_loss improved from 0.07986 to 0.07975, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 484: val_loss improved from 0.07975 to 0.07965, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 485: val_loss improved from 0.07965 to 0.07954, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 486: val_loss improved from 0.07954 to 0.07943, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 487: val_loss improved from 0.07943 to 0.07932, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 488: val_loss improved from 0.07932 to 0.07922, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 489: val_loss improved from 0.07922 to 0.07912, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 490: val_loss improved from 0.07912 to 0.07901, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 491: val_loss improved from 0.07901 to 0.07890, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 492: val_loss improved from 0.07890 to 0.07879, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 493: val_loss improved from 0.07879 to 0.07870, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 494: val_loss improved from 0.07870 to 0.07860, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 495: val_loss improved from 0.07860 to 0.07849, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 496: val_loss improved from 0.07849 to 0.07839, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 497: val_loss improved from 0.07839 to 0.07831, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 498: val_loss improved from 0.07831 to 0.07820, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 499: val_loss improved from 0.07820 to 0.07809, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 500: val_loss improved from 0.07809 to 0.07799, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 501: val_loss improved from 0.07799 to 0.07789, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 502: val_loss improved from 0.07789 to 0.07780, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 503: val_loss improved from 0.07780 to 0.07768, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 504: val_loss improved from 0.07768 to 0.07757, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 505: val_loss improved from 0.07757 to 0.07746, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 506: val_loss improved from 0.07746 to 0.07736, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 507: val_loss improved from 0.07736 to 0.07727, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 508: val_loss improved from 0.07727 to 0.07718, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 509: val_loss improved from 0.07718 to 0.07708, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 510: val_loss improved from 0.07708 to 0.07698, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 511: val_loss improved from 0.07698 to 0.07688, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 512: val_loss improved from 0.07688 to 0.07678, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 513: val_loss improved from 0.07678 to 0.07668, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 514: val_loss improved from 0.07668 to 0.07658, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 515: val_loss improved from 0.07658 to 0.07649, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 516: val_loss improved from 0.07649 to 0.07639, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 517: val_loss improved from 0.07639 to 0.07629, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 518: val_loss improved from 0.07629 to 0.07620, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 519: val_loss improved from 0.07620 to 0.07611, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 520: val_loss improved from 0.07611 to 0.07601, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 521: val_loss improved from 0.07601 to 0.07592, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 522: val_loss improved from 0.07592 to 0.07584, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 523: val_loss improved from 0.07584 to 0.07575, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 524: val_loss improved from 0.07575 to 0.07566, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 525: val_loss improved from 0.07566 to 0.07556, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 526: val_loss improved from 0.07556 to 0.07547, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 527: val_loss improved from 0.07547 to 0.07537, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 528: val_loss improved from 0.07537 to 0.07528, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 529: val_loss improved from 0.07528 to 0.07518, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 530: val_loss improved from 0.07518 to 0.07509, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 531: val_loss improved from 0.07509 to 0.07499, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 532: val_loss improved from 0.07499 to 0.07490, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 533: val_loss improved from 0.07490 to 0.07480, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 534: val_loss improved from 0.07480 to 0.07470, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 535: val_loss improved from 0.07470 to 0.07461, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 536: val_loss improved from 0.07461 to 0.07453, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 537: val_loss improved from 0.07453 to 0.07444, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 538: val_loss improved from 0.07444 to 0.07434, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 539: val_loss improved from 0.07434 to 0.07425, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 540: val_loss improved from 0.07425 to 0.07416, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 541: val_loss improved from 0.07416 to 0.07405, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 542: val_loss improved from 0.07405 to 0.07396, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 543: val_loss improved from 0.07396 to 0.07387, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 544: val_loss improved from 0.07387 to 0.07378, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 545: val_loss improved from 0.07378 to 0.07370, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 546: val_loss improved from 0.07370 to 0.07361, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 547: val_loss improved from 0.07361 to 0.07351, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 548: val_loss improved from 0.07351 to 0.07343, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 549: val_loss improved from 0.07343 to 0.07334, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 550: val_loss improved from 0.07334 to 0.07327, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 551: val_loss improved from 0.07327 to 0.07318, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 552: val_loss improved from 0.07318 to 0.07309, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 553: val_loss improved from 0.07309 to 0.07300, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 554: val_loss improved from 0.07300 to 0.07291, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 555: val_loss improved from 0.07291 to 0.07281, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 556: val_loss improved from 0.07281 to 0.07273, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 557: val_loss improved from 0.07273 to 0.07264, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 558: val_loss improved from 0.07264 to 0.07255, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 559: val_loss improved from 0.07255 to 0.07247, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 560: val_loss improved from 0.07247 to 0.07238, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 561: val_loss improved from 0.07238 to 0.07230, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 562: val_loss improved from 0.07230 to 0.07221, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 563: val_loss improved from 0.07221 to 0.07211, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 564: val_loss improved from 0.07211 to 0.07202, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 565: val_loss improved from 0.07202 to 0.07193, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 566: val_loss improved from 0.07193 to 0.07185, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 567: val_loss improved from 0.07185 to 0.07176, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 568: val_loss improved from 0.07176 to 0.07167, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 569: val_loss improved from 0.07167 to 0.07158, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 570: val_loss improved from 0.07158 to 0.07149, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 571: val_loss improved from 0.07149 to 0.07141, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 572: val_loss improved from 0.07141 to 0.07132, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 573: val_loss improved from 0.07132 to 0.07121, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 574: val_loss improved from 0.07121 to 0.07113, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 575: val_loss improved from 0.07113 to 0.07104, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 576: val_loss improved from 0.07104 to 0.07096, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 577: val_loss improved from 0.07096 to 0.07089, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 578: val_loss improved from 0.07089 to 0.07081, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 579: val_loss improved from 0.07081 to 0.07071, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 580: val_loss improved from 0.07071 to 0.07063, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 581: val_loss improved from 0.07063 to 0.07054, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 582: val_loss improved from 0.07054 to 0.07046, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 583: val_loss improved from 0.07046 to 0.07037, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 584: val_loss improved from 0.07037 to 0.07028, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 585: val_loss improved from 0.07028 to 0.07020, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 586: val_loss improved from 0.07020 to 0.07012, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 587: val_loss improved from 0.07012 to 0.07003, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 588: val_loss improved from 0.07003 to 0.06994, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 589: val_loss improved from 0.06994 to 0.06985, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 590: val_loss improved from 0.06985 to 0.06978, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 591: val_loss improved from 0.06978 to 0.06968, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 592: val_loss improved from 0.06968 to 0.06960, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 593: val_loss improved from 0.06960 to 0.06951, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 594: val_loss improved from 0.06951 to 0.06943, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 595: val_loss improved from 0.06943 to 0.06934, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 596: val_loss improved from 0.06934 to 0.06926, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 597: val_loss improved from 0.06926 to 0.06919, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 598: val_loss improved from 0.06919 to 0.06911, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 599: val_loss improved from 0.06911 to 0.06904, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 600: val_loss improved from 0.06904 to 0.06895, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 601: val_loss improved from 0.06895 to 0.06886, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 602: val_loss improved from 0.06886 to 0.06878, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 603: val_loss improved from 0.06878 to 0.06869, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 604: val_loss improved from 0.06869 to 0.06861, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 605: val_loss improved from 0.06861 to 0.06853, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 606: val_loss improved from 0.06853 to 0.06844, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 607: val_loss improved from 0.06844 to 0.06836, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 608: val_loss improved from 0.06836 to 0.06828, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 609: val_loss improved from 0.06828 to 0.06819, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 610: val_loss improved from 0.06819 to 0.06811, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 611: val_loss improved from 0.06811 to 0.06802, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 612: val_loss improved from 0.06802 to 0.06795, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 613: val_loss improved from 0.06795 to 0.06787, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 614: val_loss improved from 0.06787 to 0.06778, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 615: val_loss improved from 0.06778 to 0.06770, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 616: val_loss improved from 0.06770 to 0.06759, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 617: val_loss improved from 0.06759 to 0.06751, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 618: val_loss improved from 0.06751 to 0.06744, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 619: val_loss improved from 0.06744 to 0.06736, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 620: val_loss improved from 0.06736 to 0.06728, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 621: val_loss improved from 0.06728 to 0.06721, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 622: val_loss improved from 0.06721 to 0.06714, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 623: val_loss improved from 0.06714 to 0.06706, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 624: val_loss improved from 0.06706 to 0.06697, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 625: val_loss improved from 0.06697 to 0.06689, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 626: val_loss improved from 0.06689 to 0.06682, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 627: val_loss improved from 0.06682 to 0.06674, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 628: val_loss improved from 0.06674 to 0.06666, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 629: val_loss improved from 0.06666 to 0.06659, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 630: val_loss improved from 0.06659 to 0.06652, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 631: val_loss improved from 0.06652 to 0.06644, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 632: val_loss improved from 0.06644 to 0.06636, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 633: val_loss improved from 0.06636 to 0.06629, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 634: val_loss improved from 0.06629 to 0.06620, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 635: val_loss improved from 0.06620 to 0.06613, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 636: val_loss improved from 0.06613 to 0.06605, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 637: val_loss improved from 0.06605 to 0.06597, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 638: val_loss improved from 0.06597 to 0.06590, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 639: val_loss improved from 0.06590 to 0.06583, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 640: val_loss improved from 0.06583 to 0.06574, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 641: val_loss improved from 0.06574 to 0.06567, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 642: val_loss improved from 0.06567 to 0.06559, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 643: val_loss improved from 0.06559 to 0.06551, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 644: val_loss improved from 0.06551 to 0.06543, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 645: val_loss improved from 0.06543 to 0.06536, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 646: val_loss improved from 0.06536 to 0.06529, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 647: val_loss improved from 0.06529 to 0.06518, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 648: val_loss improved from 0.06518 to 0.06511, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 649: val_loss improved from 0.06511 to 0.06504, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 650: val_loss improved from 0.06504 to 0.06496, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 651: val_loss improved from 0.06496 to 0.06489, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 652: val_loss improved from 0.06489 to 0.06484, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 653: val_loss improved from 0.06484 to 0.06476, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 654: val_loss improved from 0.06476 to 0.06469, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 655: val_loss improved from 0.06469 to 0.06462, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 656: val_loss improved from 0.06462 to 0.06454, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 657: val_loss improved from 0.06454 to 0.06447, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 658: val_loss improved from 0.06447 to 0.06440, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 659: val_loss improved from 0.06440 to 0.06432, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 660: val_loss improved from 0.06432 to 0.06422, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 661: val_loss improved from 0.06422 to 0.06415, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 662: val_loss improved from 0.06415 to 0.06408, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 663: val_loss improved from 0.06408 to 0.06401, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 664: val_loss improved from 0.06401 to 0.06394, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 665: val_loss improved from 0.06394 to 0.06386, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 666: val_loss improved from 0.06386 to 0.06376, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 667: val_loss improved from 0.06376 to 0.06370, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 668: val_loss improved from 0.06370 to 0.06364, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 669: val_loss improved from 0.06364 to 0.06357, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 670: val_loss improved from 0.06357 to 0.06350, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 671: val_loss improved from 0.06350 to 0.06342, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 672: val_loss improved from 0.06342 to 0.06335, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 673: val_loss improved from 0.06335 to 0.06328, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 674: val_loss improved from 0.06328 to 0.06320, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 675: val_loss improved from 0.06320 to 0.06313, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 676: val_loss improved from 0.06313 to 0.06307, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 677: val_loss improved from 0.06307 to 0.06299, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 678: val_loss improved from 0.06299 to 0.06292, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 679: val_loss improved from 0.06292 to 0.06285, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 680: val_loss improved from 0.06285 to 0.06278, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 681: val_loss improved from 0.06278 to 0.06271, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 682: val_loss improved from 0.06271 to 0.06264, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 683: val_loss improved from 0.06264 to 0.06257, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 684: val_loss improved from 0.06257 to 0.06251, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 685: val_loss improved from 0.06251 to 0.06243, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 686: val_loss improved from 0.06243 to 0.06239, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 687: val_loss improved from 0.06239 to 0.06232, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 688: val_loss improved from 0.06232 to 0.06225, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 689: val_loss improved from 0.06225 to 0.06219, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 690: val_loss improved from 0.06219 to 0.06212, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 691: val_loss improved from 0.06212 to 0.06206, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 692: val_loss improved from 0.06206 to 0.06199, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 693: val_loss improved from 0.06199 to 0.06192, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 694: val_loss improved from 0.06192 to 0.06184, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 695: val_loss improved from 0.06184 to 0.06177, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 696: val_loss improved from 0.06177 to 0.06170, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 697: val_loss improved from 0.06170 to 0.06163, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 698: val_loss improved from 0.06163 to 0.06156, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 699: val_loss improved from 0.06156 to 0.06149, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 700: val_loss improved from 0.06149 to 0.06142, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "Loss: 0.0614, Accuracy: 98.21%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.06142\n",
      "Loss: 0.1367, Accuracy: 96.43%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.06142\n",
      "Loss: 0.0937, Accuracy: 94.64%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.06142\n",
      "Loss: 0.1185, Accuracy: 96.43%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.06142\n",
      "\n",
      "Epoch 628: val_loss improved from 0.06142 to 0.06136, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 629: val_loss improved from 0.06136 to 0.06129, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 630: val_loss improved from 0.06129 to 0.06120, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 631: val_loss improved from 0.06120 to 0.06109, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 632: val_loss improved from 0.06109 to 0.06100, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 633: val_loss improved from 0.06100 to 0.06093, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 634: val_loss improved from 0.06093 to 0.06085, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 635: val_loss improved from 0.06085 to 0.06076, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 636: val_loss improved from 0.06076 to 0.06069, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 637: val_loss improved from 0.06069 to 0.06062, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 638: val_loss improved from 0.06062 to 0.06054, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 639: val_loss improved from 0.06054 to 0.06046, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 640: val_loss improved from 0.06046 to 0.06038, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 641: val_loss improved from 0.06038 to 0.06028, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 642: val_loss improved from 0.06028 to 0.06021, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 643: val_loss improved from 0.06021 to 0.06014, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 644: val_loss improved from 0.06014 to 0.06004, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 645: val_loss improved from 0.06004 to 0.05996, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 646: val_loss improved from 0.05996 to 0.05989, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 647: val_loss improved from 0.05989 to 0.05982, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 648: val_loss improved from 0.05982 to 0.05975, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 649: val_loss improved from 0.05975 to 0.05967, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 650: val_loss improved from 0.05967 to 0.05956, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 651: val_loss improved from 0.05956 to 0.05950, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 652: val_loss improved from 0.05950 to 0.05945, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 653: val_loss improved from 0.05945 to 0.05937, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 654: val_loss improved from 0.05937 to 0.05929, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 655: val_loss improved from 0.05929 to 0.05920, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 656: val_loss improved from 0.05920 to 0.05908, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 657: val_loss improved from 0.05908 to 0.05900, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 658: val_loss improved from 0.05900 to 0.05892, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 659: val_loss improved from 0.05892 to 0.05884, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 660: val_loss improved from 0.05884 to 0.05875, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 661: val_loss improved from 0.05875 to 0.05866, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 662: val_loss improved from 0.05866 to 0.05857, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 663: val_loss improved from 0.05857 to 0.05851, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 664: val_loss improved from 0.05851 to 0.05845, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 665: val_loss improved from 0.05845 to 0.05839, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 666: val_loss improved from 0.05839 to 0.05832, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 667: val_loss improved from 0.05832 to 0.05825, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 668: val_loss improved from 0.05825 to 0.05818, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 669: val_loss improved from 0.05818 to 0.05812, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 670: val_loss improved from 0.05812 to 0.05804, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 671: val_loss improved from 0.05804 to 0.05797, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 672: val_loss improved from 0.05797 to 0.05788, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 673: val_loss improved from 0.05788 to 0.05781, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 674: val_loss improved from 0.05781 to 0.05774, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 675: val_loss improved from 0.05774 to 0.05767, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 676: val_loss improved from 0.05767 to 0.05761, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 677: val_loss improved from 0.05761 to 0.05755, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 678: val_loss improved from 0.05755 to 0.05745, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 679: val_loss improved from 0.05745 to 0.05737, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 680: val_loss improved from 0.05737 to 0.05731, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 681: val_loss improved from 0.05731 to 0.05724, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 682: val_loss improved from 0.05724 to 0.05717, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 683: val_loss improved from 0.05717 to 0.05710, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 684: val_loss improved from 0.05710 to 0.05703, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 685: val_loss improved from 0.05703 to 0.05697, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 686: val_loss improved from 0.05697 to 0.05690, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 687: val_loss improved from 0.05690 to 0.05682, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 688: val_loss improved from 0.05682 to 0.05675, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 689: val_loss improved from 0.05675 to 0.05668, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 690: val_loss improved from 0.05668 to 0.05661, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 691: val_loss improved from 0.05661 to 0.05653, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 692: val_loss improved from 0.05653 to 0.05644, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 693: val_loss improved from 0.05644 to 0.05636, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 694: val_loss improved from 0.05636 to 0.05628, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 695: val_loss improved from 0.05628 to 0.05619, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 696: val_loss improved from 0.05619 to 0.05613, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 697: val_loss improved from 0.05613 to 0.05607, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 698: val_loss improved from 0.05607 to 0.05599, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 699: val_loss improved from 0.05599 to 0.05589, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 700: val_loss improved from 0.05589 to 0.05579, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "Loss: 0.0558, Accuracy: 100.00%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.05579\n",
      "Loss: 0.0948, Accuracy: 96.43%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.05579\n",
      "Loss: 0.1216, Accuracy: 96.43%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.05579\n",
      "Loss: 0.0889, Accuracy: 96.43%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.05579\n",
      "\n",
      "Epoch 529: val_loss improved from 0.05579 to 0.05579, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 530: val_loss improved from 0.05579 to 0.05569, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 531: val_loss improved from 0.05569 to 0.05563, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 532: val_loss improved from 0.05563 to 0.05552, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 533: val_loss improved from 0.05552 to 0.05542, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 534: val_loss improved from 0.05542 to 0.05533, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 535: val_loss improved from 0.05533 to 0.05525, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 536: val_loss improved from 0.05525 to 0.05515, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 537: val_loss improved from 0.05515 to 0.05504, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 538: val_loss improved from 0.05504 to 0.05492, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 539: val_loss improved from 0.05492 to 0.05483, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 540: val_loss improved from 0.05483 to 0.05475, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 541: val_loss improved from 0.05475 to 0.05465, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 542: val_loss improved from 0.05465 to 0.05454, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 543: val_loss improved from 0.05454 to 0.05444, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 544: val_loss improved from 0.05444 to 0.05432, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 545: val_loss improved from 0.05432 to 0.05423, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 546: val_loss improved from 0.05423 to 0.05414, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 547: val_loss improved from 0.05414 to 0.05405, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 548: val_loss improved from 0.05405 to 0.05396, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 549: val_loss improved from 0.05396 to 0.05387, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 550: val_loss improved from 0.05387 to 0.05378, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 551: val_loss improved from 0.05378 to 0.05369, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 552: val_loss improved from 0.05369 to 0.05362, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 553: val_loss improved from 0.05362 to 0.05350, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 554: val_loss improved from 0.05350 to 0.05342, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 555: val_loss improved from 0.05342 to 0.05333, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 556: val_loss improved from 0.05333 to 0.05322, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 557: val_loss improved from 0.05322 to 0.05313, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 558: val_loss improved from 0.05313 to 0.05304, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 559: val_loss improved from 0.05304 to 0.05295, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 560: val_loss improved from 0.05295 to 0.05286, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 561: val_loss improved from 0.05286 to 0.05278, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 562: val_loss improved from 0.05278 to 0.05269, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 563: val_loss improved from 0.05269 to 0.05259, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 564: val_loss improved from 0.05259 to 0.05250, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 565: val_loss improved from 0.05250 to 0.05240, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 566: val_loss improved from 0.05240 to 0.05231, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 567: val_loss improved from 0.05231 to 0.05224, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 568: val_loss improved from 0.05224 to 0.05217, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 569: val_loss improved from 0.05217 to 0.05209, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 570: val_loss improved from 0.05209 to 0.05202, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 571: val_loss improved from 0.05202 to 0.05192, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 572: val_loss improved from 0.05192 to 0.05183, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 573: val_loss improved from 0.05183 to 0.05172, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 574: val_loss improved from 0.05172 to 0.05162, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 575: val_loss improved from 0.05162 to 0.05152, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 576: val_loss improved from 0.05152 to 0.05144, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 577: val_loss improved from 0.05144 to 0.05136, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 578: val_loss improved from 0.05136 to 0.05126, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 579: val_loss improved from 0.05126 to 0.05118, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 580: val_loss improved from 0.05118 to 0.05109, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 581: val_loss improved from 0.05109 to 0.05102, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 582: val_loss improved from 0.05102 to 0.05094, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 583: val_loss improved from 0.05094 to 0.05085, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 584: val_loss improved from 0.05085 to 0.05077, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 585: val_loss improved from 0.05077 to 0.05069, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 586: val_loss improved from 0.05069 to 0.05061, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 587: val_loss improved from 0.05061 to 0.05052, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 588: val_loss improved from 0.05052 to 0.05044, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 589: val_loss improved from 0.05044 to 0.05039, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 590: val_loss improved from 0.05039 to 0.05030, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 591: val_loss improved from 0.05030 to 0.05019, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 592: val_loss improved from 0.05019 to 0.05011, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 593: val_loss improved from 0.05011 to 0.05002, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 594: val_loss improved from 0.05002 to 0.04993, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 595: val_loss improved from 0.04993 to 0.04985, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 596: val_loss improved from 0.04985 to 0.04979, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 597: val_loss improved from 0.04979 to 0.04971, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 598: val_loss improved from 0.04971 to 0.04962, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 599: val_loss improved from 0.04962 to 0.04953, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 600: val_loss improved from 0.04953 to 0.04945, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 601: val_loss improved from 0.04945 to 0.04937, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 602: val_loss improved from 0.04937 to 0.04928, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 603: val_loss improved from 0.04928 to 0.04919, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 604: val_loss improved from 0.04919 to 0.04911, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 605: val_loss improved from 0.04911 to 0.04903, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 606: val_loss improved from 0.04903 to 0.04895, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 607: val_loss improved from 0.04895 to 0.04885, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 608: val_loss improved from 0.04885 to 0.04879, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 609: val_loss improved from 0.04879 to 0.04869, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 610: val_loss improved from 0.04869 to 0.04861, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 611: val_loss improved from 0.04861 to 0.04855, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 612: val_loss improved from 0.04855 to 0.04847, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 613: val_loss improved from 0.04847 to 0.04839, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 614: val_loss improved from 0.04839 to 0.04831, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 615: val_loss improved from 0.04831 to 0.04822, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 616: val_loss improved from 0.04822 to 0.04813, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 617: val_loss improved from 0.04813 to 0.04805, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 618: val_loss improved from 0.04805 to 0.04797, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 619: val_loss improved from 0.04797 to 0.04789, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 620: val_loss improved from 0.04789 to 0.04782, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 621: val_loss improved from 0.04782 to 0.04774, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 622: val_loss improved from 0.04774 to 0.04765, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 623: val_loss improved from 0.04765 to 0.04756, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 624: val_loss improved from 0.04756 to 0.04749, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 625: val_loss improved from 0.04749 to 0.04740, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 626: val_loss improved from 0.04740 to 0.04731, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 627: val_loss improved from 0.04731 to 0.04723, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 628: val_loss improved from 0.04723 to 0.04712, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 629: val_loss improved from 0.04712 to 0.04705, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 630: val_loss improved from 0.04705 to 0.04698, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 631: val_loss improved from 0.04698 to 0.04691, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 632: val_loss improved from 0.04691 to 0.04684, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 633: val_loss improved from 0.04684 to 0.04679, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 634: val_loss improved from 0.04679 to 0.04672, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 635: val_loss improved from 0.04672 to 0.04663, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 636: val_loss improved from 0.04663 to 0.04659, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 637: val_loss improved from 0.04659 to 0.04651, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 638: val_loss improved from 0.04651 to 0.04645, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 639: val_loss improved from 0.04645 to 0.04636, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 640: val_loss improved from 0.04636 to 0.04629, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 641: val_loss improved from 0.04629 to 0.04620, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 642: val_loss improved from 0.04620 to 0.04611, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 643: val_loss improved from 0.04611 to 0.04604, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 644: val_loss improved from 0.04604 to 0.04595, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 645: val_loss improved from 0.04595 to 0.04588, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 646: val_loss improved from 0.04588 to 0.04581, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 647: val_loss improved from 0.04581 to 0.04573, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 648: val_loss improved from 0.04573 to 0.04562, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 649: val_loss improved from 0.04562 to 0.04556, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 650: val_loss improved from 0.04556 to 0.04548, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 651: val_loss improved from 0.04548 to 0.04545, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 652: val_loss improved from 0.04545 to 0.04536, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 653: val_loss improved from 0.04536 to 0.04529, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 654: val_loss improved from 0.04529 to 0.04520, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 655: val_loss improved from 0.04520 to 0.04512, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 656: val_loss improved from 0.04512 to 0.04504, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 657: val_loss improved from 0.04504 to 0.04496, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 658: val_loss improved from 0.04496 to 0.04488, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 659: val_loss improved from 0.04488 to 0.04478, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 660: val_loss improved from 0.04478 to 0.04471, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 661: val_loss improved from 0.04471 to 0.04465, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 662: val_loss improved from 0.04465 to 0.04457, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 663: val_loss improved from 0.04457 to 0.04450, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 664: val_loss improved from 0.04450 to 0.04442, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 665: val_loss improved from 0.04442 to 0.04434, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 666: val_loss improved from 0.04434 to 0.04427, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 667: val_loss improved from 0.04427 to 0.04421, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 668: val_loss improved from 0.04421 to 0.04415, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 669: val_loss improved from 0.04415 to 0.04407, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 670: val_loss improved from 0.04407 to 0.04400, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 671: val_loss improved from 0.04400 to 0.04393, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 672: val_loss improved from 0.04393 to 0.04386, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 673: val_loss improved from 0.04386 to 0.04378, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 674: val_loss improved from 0.04378 to 0.04369, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 675: val_loss improved from 0.04369 to 0.04361, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 676: val_loss improved from 0.04361 to 0.04354, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 677: val_loss improved from 0.04354 to 0.04348, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 678: val_loss improved from 0.04348 to 0.04342, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 679: val_loss improved from 0.04342 to 0.04334, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 680: val_loss improved from 0.04334 to 0.04328, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 681: val_loss improved from 0.04328 to 0.04321, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 682: val_loss improved from 0.04321 to 0.04313, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 683: val_loss improved from 0.04313 to 0.04306, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 684: val_loss improved from 0.04306 to 0.04298, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 685: val_loss improved from 0.04298 to 0.04289, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 686: val_loss improved from 0.04289 to 0.04284, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 687: val_loss improved from 0.04284 to 0.04277, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 688: val_loss improved from 0.04277 to 0.04268, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 689: val_loss improved from 0.04268 to 0.04262, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 690: val_loss improved from 0.04262 to 0.04257, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 691: val_loss improved from 0.04257 to 0.04248, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 692: val_loss improved from 0.04248 to 0.04242, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 693: val_loss improved from 0.04242 to 0.04236, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 694: val_loss improved from 0.04236 to 0.04231, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 695: val_loss improved from 0.04231 to 0.04223, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 696: val_loss improved from 0.04223 to 0.04215, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 697: val_loss improved from 0.04215 to 0.04206, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 698: val_loss improved from 0.04206 to 0.04197, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 699: val_loss improved from 0.04197 to 0.04190, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "\n",
      "Epoch 700: val_loss improved from 0.04190 to 0.04184, saving model to model_checkpoint\\LSTM + GRU_10-fold.h5\n",
      "Loss: 0.0418, Accuracy: 98.21%\n",
      "Vanilla_RNN finished in 2742.13 sec\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\Main-Mari_3 copy.ipynb Cell 71\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y130sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mVanilla_RNN finished in\u001b[39m\u001b[39m'\u001b[39m, t,\u001b[39m'\u001b[39m\u001b[39msec\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y130sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Plot of the average learning curves\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y130sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m plot_1(train_loss, train_acc, val_loss, val_acc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y130sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Calculate average performance\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/Main-Mari_3%20copy.ipynb#Y130sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m avg_accuracy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(test_acc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_1' is not defined"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'LSTM + GRU_10-fold.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "k_fold = 10 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing GRU with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True,input_shape=(None, x_train.shape[-1])))\n",
    "    model.add(layers.GRU(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 5, epochs = 700, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('Vanilla_RNN finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_1(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
