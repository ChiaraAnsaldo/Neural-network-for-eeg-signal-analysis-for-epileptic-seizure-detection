{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Model, Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.DatasetManage import read_and_store_data\n",
    "from ipynb.fs.full.FeatureExtraction import feature_extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = ['FP1-F7', 'F7-T7','T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'seizure']\n",
    "\n",
    "dataset = 'CHB_MIT'\n",
    "csvImportFile = 'CHB.csv'\n",
    "csvExportFile = 'CHB.csv'\n",
    "sample_rate = 256\n",
    "time_window = 2\n",
    "step = time_window * sample_rate\n",
    "\n",
    "test_ratio = 0.3 # ratio to split dataset into training and testing sets\n",
    "val_ratio = 0.2 # ratio to split dataset into validation and training sets\n",
    "\n",
    "pca_tolerance = 0.9 # desired percentage of variation in the data preserved\n",
    "\n",
    "undersampling_rate = 0.2 # undersampling rate for Cluster Centroids\n",
    "\n",
    "oversampling_neighbors = 11 # size of neighbourhood for K-nearest neighbours method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a method to split the dataset into training and testing sets, also returning the train/test\n",
    "# indexes for splitting the dataset into K folds for the K-fold cross validation\n",
    "\n",
    "def trainTestData_1 (features, test_ratio, k_fold):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size = test_ratio, shuffle = True, random_state=42)\n",
    "    kf = KFold(n_splits = k_fold, shuffle = True)\n",
    "    x_train = np.reshape(x_tr.values, (x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "    y_train = y_tr.values.astype(int)\n",
    "    x_test = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_test = y_ts.values.astype(int)\n",
    "    return x_train, x_test, y_train, y_test, kf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a method to split the dataset into training, validationa and testing sets\n",
    "\n",
    "def trainTestData_2 (features, test_ratio, val_ratio):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_1, x_ts, y_1, y_ts = train_test_split(x, y, test_size = test_ratio, random_state=42)\n",
    "    x_tr, x_v, y_tr, y_v = train_test_split(x_1, y_1, test_size = val_ratio, random_state=42)\n",
    "    x_train = np.reshape(x_tr.values, (x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "    y_train = y_tr.values.astype(int)\n",
    "    x_val = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_val = y_ts.values.astype(int)\n",
    "    x_test = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_test = y_ts.values.astype(int)\n",
    "    return x_train, x_test, y_train, y_test, x_val, y_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and store data from the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from CHB.csv\n"
     ]
    }
   ],
   "source": [
    "print('Reading data from', csvImportFile)\n",
    "df = pd.read_csv(csvImportFile, delimiter = ',', header = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the features and save them in the Feature.csv file\n",
    "\n",
    "ft = feature_extraction(df, sample_rate, step, pca_tolerance, undersampling_rate, oversampling_neighbors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = pd.read_csv(\"Features.csv\", delimiter = ',', header = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset splitting without validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 5\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset splitting with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, x_val, y_val = trainTestData_2 (ft, test_ratio, val_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_SGD (initial_learning_rate, decay_steps, decay_rate):\n",
    "    # We define the optimizer with an initial learning rate\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # We define the larning rate schedule with an exponential decay, specifying the number of decay steps and the decay rate\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "    \n",
    "    return optimizer, lr_schedule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2 (history):\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(train_loss))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss, label='Training loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNN: model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.9435 - accuracy: 0.3388 \n",
      "Epoch 1: val_loss improved from inf to 0.85379, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 1s 8ms/step - loss: 0.9274 - accuracy: 0.3520 - val_loss: 0.8538 - val_accuracy: 0.3988 - lr: 0.0010\n",
      "Epoch 2/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.7767 - accuracy: 0.4970\n",
      "Epoch 2: val_loss improved from 0.85379 to 0.71965, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.7556 - accuracy: 0.5306 - val_loss: 0.7197 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 3/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.6470 - accuracy: 0.6719\n",
      "Epoch 3: val_loss improved from 0.71965 to 0.62997, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.6432 - accuracy: 0.6735 - val_loss: 0.6300 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 4/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.5688 - accuracy: 0.7292\n",
      "Epoch 4: val_loss improved from 0.62997 to 0.56688, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.5679 - accuracy: 0.7321 - val_loss: 0.5669 - val_accuracy: 0.7262 - lr: 0.0010\n",
      "Epoch 5/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.5172 - accuracy: 0.7639\n",
      "Epoch 5: val_loss improved from 0.56688 to 0.52083, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.5153 - accuracy: 0.7653 - val_loss: 0.5208 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 6/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.4811 - accuracy: 0.7988\n",
      "Epoch 6: val_loss improved from 0.52083 to 0.48551, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.4766 - accuracy: 0.7959 - val_loss: 0.4855 - val_accuracy: 0.7976 - lr: 0.0010\n",
      "Epoch 7/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.4280 - accuracy: 0.8257\n",
      "Epoch 7: val_loss improved from 0.48551 to 0.45799, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.4471 - accuracy: 0.8061 - val_loss: 0.4580 - val_accuracy: 0.8274 - lr: 0.0010\n",
      "Epoch 8/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.4466 - accuracy: 0.8125\n",
      "Epoch 8: val_loss improved from 0.45799 to 0.43575, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.4240 - accuracy: 0.8163 - val_loss: 0.4357 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 9/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.4173 - accuracy: 0.8239\n",
      "Epoch 9: val_loss improved from 0.43575 to 0.41763, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.4053 - accuracy: 0.8291 - val_loss: 0.4176 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 10/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.3952 - accuracy: 0.8342\n",
      "Epoch 10: val_loss improved from 0.41763 to 0.40252, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3901 - accuracy: 0.8367 - val_loss: 0.4025 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 11/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.3756 - accuracy: 0.8684\n",
      "Epoch 11: val_loss improved from 0.40252 to 0.38969, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.8571 - val_loss: 0.3897 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 12/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.3822 - accuracy: 0.8629\n",
      "Epoch 12: val_loss improved from 0.38969 to 0.37872, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3664 - accuracy: 0.8648 - val_loss: 0.3787 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 13/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.4067 - accuracy: 0.8462\n",
      "Epoch 13: val_loss improved from 0.37872 to 0.36922, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 0.8699 - val_loss: 0.3692 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 14/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.3474 - accuracy: 0.8713\n",
      "Epoch 14: val_loss improved from 0.36922 to 0.36094, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3493 - accuracy: 0.8699 - val_loss: 0.3609 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 15/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.3510 - accuracy: 0.8750\n",
      "Epoch 15: val_loss improved from 0.36094 to 0.35363, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3423 - accuracy: 0.8750 - val_loss: 0.3536 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 16/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.3353 - accuracy: 0.8837\n",
      "Epoch 16: val_loss improved from 0.35363 to 0.34712, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3360 - accuracy: 0.8827 - val_loss: 0.3471 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 17/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.3254 - accuracy: 0.8824\n",
      "Epoch 17: val_loss improved from 0.34712 to 0.34128, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3305 - accuracy: 0.8852 - val_loss: 0.3413 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 18/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.3307 - accuracy: 0.8885\n",
      "Epoch 18: val_loss improved from 0.34128 to 0.33601, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3255 - accuracy: 0.8903 - val_loss: 0.3360 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 19/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.3184 - accuracy: 0.8980\n",
      "Epoch 19: val_loss improved from 0.33601 to 0.33122, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3210 - accuracy: 0.8980 - val_loss: 0.3312 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 20/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.3247 - accuracy: 0.8879\n",
      "Epoch 20: val_loss improved from 0.33122 to 0.32685, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.9005 - val_loss: 0.3268 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 21/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.3003 - accuracy: 0.9085\n",
      "Epoch 21: val_loss improved from 0.32685 to 0.32282, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3129 - accuracy: 0.9005 - val_loss: 0.3228 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 22/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.3172 - accuracy: 0.8967\n",
      "Epoch 22: val_loss improved from 0.32282 to 0.31910, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3094 - accuracy: 0.9005 - val_loss: 0.3191 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 23/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.3027 - accuracy: 0.9000\n",
      "Epoch 23: val_loss improved from 0.31910 to 0.31565, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3060 - accuracy: 0.9005 - val_loss: 0.3156 - val_accuracy: 0.8810 - lr: 0.0010\n",
      "Epoch 24/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.3350 - accuracy: 0.8902\n",
      "Epoch 24: val_loss improved from 0.31565 to 0.31243, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.3029 - accuracy: 0.9031 - val_loss: 0.3124 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 25/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.3343 - accuracy: 0.8849\n",
      "Epoch 25: val_loss improved from 0.31243 to 0.30942, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2999 - accuracy: 0.9031 - val_loss: 0.3094 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 26/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.2731 - accuracy: 0.9137\n",
      "Epoch 26: val_loss improved from 0.30942 to 0.30661, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2972 - accuracy: 0.9031 - val_loss: 0.3066 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 27/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.9036\n",
      "Epoch 27: val_loss improved from 0.30661 to 0.30397, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2946 - accuracy: 0.9031 - val_loss: 0.3040 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 28/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.2709 - accuracy: 0.9079\n",
      "Epoch 28: val_loss improved from 0.30397 to 0.30149, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2921 - accuracy: 0.9005 - val_loss: 0.3015 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 29/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.3086 - accuracy: 0.8966\n",
      "Epoch 29: val_loss improved from 0.30149 to 0.29913, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2898 - accuracy: 0.9005 - val_loss: 0.2991 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 30/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.2840 - accuracy: 0.8980\n",
      "Epoch 30: val_loss improved from 0.29913 to 0.29690, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2876 - accuracy: 0.9005 - val_loss: 0.2969 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 31/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.3094 - accuracy: 0.8902\n",
      "Epoch 31: val_loss improved from 0.29690 to 0.29477, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2855 - accuracy: 0.9005 - val_loss: 0.2948 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 32/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.2880 - accuracy: 0.9016\n",
      "Epoch 32: val_loss improved from 0.29477 to 0.29274, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2834 - accuracy: 0.9031 - val_loss: 0.2927 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 33/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.2948 - accuracy: 0.8934\n",
      "Epoch 33: val_loss improved from 0.29274 to 0.29080, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2815 - accuracy: 0.9056 - val_loss: 0.2908 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 34/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.2782 - accuracy: 0.9055\n",
      "Epoch 34: val_loss improved from 0.29080 to 0.28894, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2796 - accuracy: 0.9056 - val_loss: 0.2889 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 35/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.2669 - accuracy: 0.9062\n",
      "Epoch 35: val_loss improved from 0.28894 to 0.28717, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2778 - accuracy: 0.9056 - val_loss: 0.2872 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 36/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.2678 - accuracy: 0.9062\n",
      "Epoch 36: val_loss improved from 0.28717 to 0.28546, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2760 - accuracy: 0.9082 - val_loss: 0.2855 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 37/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.2814 - accuracy: 0.9130\n",
      "Epoch 37: val_loss improved from 0.28546 to 0.28380, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2743 - accuracy: 0.9133 - val_loss: 0.2838 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 38/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.2981 - accuracy: 0.9009\n",
      "Epoch 38: val_loss improved from 0.28380 to 0.28220, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2727 - accuracy: 0.9133 - val_loss: 0.2822 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 39/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.2779 - accuracy: 0.9096\n",
      "Epoch 39: val_loss improved from 0.28220 to 0.28066, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.2711 - accuracy: 0.9133 - val_loss: 0.2807 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 40/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.2893 - accuracy: 0.9015\n",
      "Epoch 40: val_loss improved from 0.28066 to 0.27917, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2695 - accuracy: 0.9133 - val_loss: 0.2792 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 41/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.2831 - accuracy: 0.9062\n",
      "Epoch 41: val_loss improved from 0.27917 to 0.27773, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2680 - accuracy: 0.9133 - val_loss: 0.2777 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 42/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.2684 - accuracy: 0.9167\n",
      "Epoch 42: val_loss improved from 0.27773 to 0.27632, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2666 - accuracy: 0.9133 - val_loss: 0.2763 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 43/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.2902 - accuracy: 0.9032\n",
      "Epoch 43: val_loss improved from 0.27632 to 0.27496, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2651 - accuracy: 0.9133 - val_loss: 0.2750 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 44/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.2506 - accuracy: 0.9187\n",
      "Epoch 44: val_loss improved from 0.27496 to 0.27363, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2637 - accuracy: 0.9133 - val_loss: 0.2736 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 45/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.2671 - accuracy: 0.9097\n",
      "Epoch 45: val_loss improved from 0.27363 to 0.27234, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2623 - accuracy: 0.9133 - val_loss: 0.2723 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 46/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.2790 - accuracy: 0.9046\n",
      "Epoch 46: val_loss improved from 0.27234 to 0.27108, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2610 - accuracy: 0.9133 - val_loss: 0.2711 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 47/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.2557 - accuracy: 0.9107\n",
      "Epoch 47: val_loss improved from 0.27108 to 0.26986, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2596 - accuracy: 0.9133 - val_loss: 0.2699 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 48/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.2461 - accuracy: 0.9242\n",
      "Epoch 48: val_loss improved from 0.26986 to 0.26867, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2583 - accuracy: 0.9133 - val_loss: 0.2687 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 49/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.2612 - accuracy: 0.9091\n",
      "Epoch 49: val_loss improved from 0.26867 to 0.26750, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2571 - accuracy: 0.9133 - val_loss: 0.2675 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 50/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.2569 - accuracy: 0.9115\n",
      "Epoch 50: val_loss improved from 0.26750 to 0.26636, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2558 - accuracy: 0.9133 - val_loss: 0.2664 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 51/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.2771 - accuracy: 0.9038\n",
      "Epoch 51: val_loss improved from 0.26636 to 0.26523, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2546 - accuracy: 0.9133 - val_loss: 0.2652 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 52/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.2427 - accuracy: 0.9205\n",
      "Epoch 52: val_loss improved from 0.26523 to 0.26414, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2534 - accuracy: 0.9133 - val_loss: 0.2641 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 53/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.2619 - accuracy: 0.9097\n",
      "Epoch 53: val_loss improved from 0.26414 to 0.26306, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2521 - accuracy: 0.9158 - val_loss: 0.2631 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 54/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.2340 - accuracy: 0.9231\n",
      "Epoch 54: val_loss improved from 0.26306 to 0.26201, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2510 - accuracy: 0.9158 - val_loss: 0.2620 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 55/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.2467 - accuracy: 0.9185\n",
      "Epoch 55: val_loss improved from 0.26201 to 0.26097, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2498 - accuracy: 0.9158 - val_loss: 0.2610 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 56/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.2552 - accuracy: 0.9038\n",
      "Epoch 56: val_loss improved from 0.26097 to 0.25995, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2487 - accuracy: 0.9158 - val_loss: 0.2600 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 57/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.2174 - accuracy: 0.9340\n",
      "Epoch 57: val_loss improved from 0.25995 to 0.25896, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2476 - accuracy: 0.9184 - val_loss: 0.2590 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 58/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.2765 - accuracy: 0.9071\n",
      "Epoch 58: val_loss improved from 0.25896 to 0.25797, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2464 - accuracy: 0.9184 - val_loss: 0.2580 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 59/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.2397 - accuracy: 0.9185\n",
      "Epoch 59: val_loss improved from 0.25797 to 0.25700, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2453 - accuracy: 0.9184 - val_loss: 0.2570 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 60/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.9184\n",
      "Epoch 60: val_loss improved from 0.25700 to 0.25605, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2443 - accuracy: 0.9184 - val_loss: 0.2561 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 61/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.2422 - accuracy: 0.9193\n",
      "Epoch 61: val_loss improved from 0.25605 to 0.25511, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2432 - accuracy: 0.9184 - val_loss: 0.2551 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 62/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.2458 - accuracy: 0.9155\n",
      "Epoch 62: val_loss improved from 0.25511 to 0.25419, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2422 - accuracy: 0.9184 - val_loss: 0.2542 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 63/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.2265 - accuracy: 0.9250\n",
      "Epoch 63: val_loss improved from 0.25419 to 0.25328, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2412 - accuracy: 0.9184 - val_loss: 0.2533 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 64/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.2394 - accuracy: 0.9179\n",
      "Epoch 64: val_loss improved from 0.25328 to 0.25238, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9184 - val_loss: 0.2524 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 65/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.2051 - accuracy: 0.9352\n",
      "Epoch 65: val_loss improved from 0.25238 to 0.25150, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2391 - accuracy: 0.9209 - val_loss: 0.2515 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 66/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.2345 - accuracy: 0.9229\n",
      "Epoch 66: val_loss improved from 0.25150 to 0.25064, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2382 - accuracy: 0.9209 - val_loss: 0.2506 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 67/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.2369 - accuracy: 0.9189\n",
      "Epoch 67: val_loss improved from 0.25064 to 0.24978, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2372 - accuracy: 0.9209 - val_loss: 0.2498 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 68/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.2323 - accuracy: 0.9228\n",
      "Epoch 68: val_loss improved from 0.24978 to 0.24893, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2363 - accuracy: 0.9209 - val_loss: 0.2489 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 69/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.2078 - accuracy: 0.9318\n",
      "Epoch 69: val_loss improved from 0.24893 to 0.24810, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2353 - accuracy: 0.9209 - val_loss: 0.2481 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 70/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.2396 - accuracy: 0.9167\n",
      "Epoch 70: val_loss improved from 0.24810 to 0.24727, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2344 - accuracy: 0.9209 - val_loss: 0.2473 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 71/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.2397 - accuracy: 0.9176\n",
      "Epoch 71: val_loss improved from 0.24727 to 0.24645, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2334 - accuracy: 0.9209 - val_loss: 0.2465 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 72/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.2185 - accuracy: 0.9219\n",
      "Epoch 72: val_loss improved from 0.24645 to 0.24564, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2325 - accuracy: 0.9209 - val_loss: 0.2456 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 73/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.2295 - accuracy: 0.9238\n",
      "Epoch 73: val_loss improved from 0.24564 to 0.24485, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2316 - accuracy: 0.9209 - val_loss: 0.2448 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 74/350\n",
      "24/49 [=============>................] - ETA: 0s - loss: 0.1979 - accuracy: 0.9323\n",
      "Epoch 74: val_loss improved from 0.24485 to 0.24406, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2307 - accuracy: 0.9209 - val_loss: 0.2441 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 75/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.2319 - accuracy: 0.9179\n",
      "Epoch 75: val_loss improved from 0.24406 to 0.24328, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2299 - accuracy: 0.9209 - val_loss: 0.2433 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 76/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.2295 - accuracy: 0.9185\n",
      "Epoch 76: val_loss improved from 0.24328 to 0.24251, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2290 - accuracy: 0.9209 - val_loss: 0.2425 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 77/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.2287 - accuracy: 0.9250\n",
      "Epoch 77: val_loss improved from 0.24251 to 0.24175, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2282 - accuracy: 0.9209 - val_loss: 0.2417 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 78/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.2400 - accuracy: 0.9157\n",
      "Epoch 78: val_loss improved from 0.24175 to 0.24099, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2273 - accuracy: 0.9209 - val_loss: 0.2410 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 79/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.2324 - accuracy: 0.9183\n",
      "Epoch 79: val_loss improved from 0.24099 to 0.24024, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2265 - accuracy: 0.9209 - val_loss: 0.2402 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 80/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.2118 - accuracy: 0.9327\n",
      "Epoch 80: val_loss improved from 0.24024 to 0.23951, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2257 - accuracy: 0.9209 - val_loss: 0.2395 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 81/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.1963 - accuracy: 0.9321\n",
      "Epoch 81: val_loss improved from 0.23951 to 0.23878, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2249 - accuracy: 0.9209 - val_loss: 0.2388 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 82/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.2261 - accuracy: 0.9193\n",
      "Epoch 82: val_loss improved from 0.23878 to 0.23806, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2240 - accuracy: 0.9209 - val_loss: 0.2381 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 83/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.2126 - accuracy: 0.9255\n",
      "Epoch 83: val_loss improved from 0.23806 to 0.23734, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2233 - accuracy: 0.9209 - val_loss: 0.2373 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 84/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.2041 - accuracy: 0.9338\n",
      "Epoch 84: val_loss improved from 0.23734 to 0.23663, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2225 - accuracy: 0.9209 - val_loss: 0.2366 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 85/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.2244 - accuracy: 0.9229\n",
      "Epoch 85: val_loss improved from 0.23663 to 0.23593, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2217 - accuracy: 0.9209 - val_loss: 0.2359 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 86/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.2074 - accuracy: 0.9266\n",
      "Epoch 86: val_loss improved from 0.23593 to 0.23524, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2209 - accuracy: 0.9209 - val_loss: 0.2352 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 87/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1934 - accuracy: 0.9327\n",
      "Epoch 87: val_loss improved from 0.23524 to 0.23455, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2202 - accuracy: 0.9209 - val_loss: 0.2345 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 88/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9209\n",
      "Epoch 88: val_loss improved from 0.23455 to 0.23387, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2194 - accuracy: 0.9209 - val_loss: 0.2339 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 89/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.2160 - accuracy: 0.9212\n",
      "Epoch 89: val_loss improved from 0.23387 to 0.23319, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2187 - accuracy: 0.9209 - val_loss: 0.2332 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 90/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1989 - accuracy: 0.9324\n",
      "Epoch 90: val_loss improved from 0.23319 to 0.23252, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2179 - accuracy: 0.9209 - val_loss: 0.2325 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 91/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.2037 - accuracy: 0.9309\n",
      "Epoch 91: val_loss improved from 0.23252 to 0.23186, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2172 - accuracy: 0.9209 - val_loss: 0.2319 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 92/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9209\n",
      "Epoch 92: val_loss improved from 0.23186 to 0.23120, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.9209 - val_loss: 0.2312 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 93/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2158 - accuracy: 0.9209\n",
      "Epoch 93: val_loss improved from 0.23120 to 0.23055, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2158 - accuracy: 0.9209 - val_loss: 0.2305 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 94/350\n",
      "21/49 [===========>..................] - ETA: 0s - loss: 0.2245 - accuracy: 0.9226\n",
      "Epoch 94: val_loss improved from 0.23055 to 0.22990, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2151 - accuracy: 0.9209 - val_loss: 0.2299 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 95/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.2243 - accuracy: 0.9167\n",
      "Epoch 95: val_loss improved from 0.22990 to 0.22925, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2143 - accuracy: 0.9209 - val_loss: 0.2293 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 96/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.2059 - accuracy: 0.9293\n",
      "Epoch 96: val_loss improved from 0.22925 to 0.22862, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2136 - accuracy: 0.9209 - val_loss: 0.2286 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 97/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.2293 - accuracy: 0.9167\n",
      "Epoch 97: val_loss improved from 0.22862 to 0.22799, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9209 - val_loss: 0.2280 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 98/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.2284 - accuracy: 0.9107\n",
      "Epoch 98: val_loss improved from 0.22799 to 0.22736, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2123 - accuracy: 0.9235 - val_loss: 0.2274 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 99/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1969 - accuracy: 0.9280\n",
      "Epoch 99: val_loss improved from 0.22736 to 0.22675, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2116 - accuracy: 0.9260 - val_loss: 0.2267 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 100/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9260\n",
      "Epoch 100: val_loss improved from 0.22675 to 0.22613, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2109 - accuracy: 0.9260 - val_loss: 0.2261 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 101/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.2204 - accuracy: 0.9215\n",
      "Epoch 101: val_loss improved from 0.22613 to 0.22552, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2103 - accuracy: 0.9260 - val_loss: 0.2255 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 102/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.2223 - accuracy: 0.9152\n",
      "Epoch 102: val_loss improved from 0.22552 to 0.22492, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 0.9260 - val_loss: 0.2249 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 103/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1986 - accuracy: 0.9276\n",
      "Epoch 103: val_loss improved from 0.22492 to 0.22433, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2090 - accuracy: 0.9260 - val_loss: 0.2243 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 104/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9245\n",
      "Epoch 104: val_loss improved from 0.22433 to 0.22374, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2083 - accuracy: 0.9260 - val_loss: 0.2237 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 105/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.2079 - accuracy: 0.9238\n",
      "Epoch 105: val_loss improved from 0.22374 to 0.22315, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2077 - accuracy: 0.9260 - val_loss: 0.2232 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 106/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.1960 - accuracy: 0.9327\n",
      "Epoch 106: val_loss improved from 0.22315 to 0.22257, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2071 - accuracy: 0.9260 - val_loss: 0.2226 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 107/350\n",
      "25/49 [==============>...............] - ETA: 0s - loss: 0.2079 - accuracy: 0.9300\n",
      "Epoch 107: val_loss improved from 0.22257 to 0.22200, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2064 - accuracy: 0.9260 - val_loss: 0.2220 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 108/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1954 - accuracy: 0.9299\n",
      "Epoch 108: val_loss improved from 0.22200 to 0.22143, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2058 - accuracy: 0.9260 - val_loss: 0.2214 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 109/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.1945 - accuracy: 0.9286\n",
      "Epoch 109: val_loss improved from 0.22143 to 0.22087, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2053 - accuracy: 0.9260 - val_loss: 0.2209 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 110/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.2103 - accuracy: 0.9352\n",
      "Epoch 110: val_loss improved from 0.22087 to 0.22031, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2046 - accuracy: 0.9260 - val_loss: 0.2203 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 111/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.2085 - accuracy: 0.9183\n",
      "Epoch 111: val_loss improved from 0.22031 to 0.21975, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9260 - val_loss: 0.2197 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 112/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.2051 - accuracy: 0.9268\n",
      "Epoch 112: val_loss improved from 0.21975 to 0.21920, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2034 - accuracy: 0.9260 - val_loss: 0.2192 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 113/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.2064 - accuracy: 0.9244\n",
      "Epoch 113: val_loss improved from 0.21920 to 0.21865, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2028 - accuracy: 0.9260 - val_loss: 0.2187 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 114/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.2149 - accuracy: 0.9259\n",
      "Epoch 114: val_loss improved from 0.21865 to 0.21811, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9286 - val_loss: 0.2181 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 115/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.2227 - accuracy: 0.9205\n",
      "Epoch 115: val_loss improved from 0.21811 to 0.21756, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.2016 - accuracy: 0.9286 - val_loss: 0.2176 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 116/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1703 - accuracy: 0.9470\n",
      "Epoch 116: val_loss improved from 0.21756 to 0.21703, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2011 - accuracy: 0.9286 - val_loss: 0.2170 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 117/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.1915 - accuracy: 0.9315\n",
      "Epoch 117: val_loss improved from 0.21703 to 0.21650, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.9311 - val_loss: 0.2165 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 118/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.2388 - accuracy: 0.9107\n",
      "Epoch 118: val_loss improved from 0.21650 to 0.21597, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1999 - accuracy: 0.9311 - val_loss: 0.2160 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 119/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.2063 - accuracy: 0.9263\n",
      "Epoch 119: val_loss improved from 0.21597 to 0.21545, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1994 - accuracy: 0.9311 - val_loss: 0.2154 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 120/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1828 - accuracy: 0.9375\n",
      "Epoch 120: val_loss improved from 0.21545 to 0.21493, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1988 - accuracy: 0.9311 - val_loss: 0.2149 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 121/350\n",
      "24/49 [=============>................] - ETA: 0s - loss: 0.1907 - accuracy: 0.9323\n",
      "Epoch 121: val_loss improved from 0.21493 to 0.21441, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1983 - accuracy: 0.9311 - val_loss: 0.2144 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 122/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1955 - accuracy: 0.9347\n",
      "Epoch 122: val_loss improved from 0.21441 to 0.21390, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1977 - accuracy: 0.9337 - val_loss: 0.2139 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 123/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1980 - accuracy: 0.9309\n",
      "Epoch 123: val_loss improved from 0.21390 to 0.21339, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9311 - val_loss: 0.2134 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 124/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1946 - accuracy: 0.9349\n",
      "Epoch 124: val_loss improved from 0.21339 to 0.21289, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1966 - accuracy: 0.9337 - val_loss: 0.2129 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 125/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1897 - accuracy: 0.9375\n",
      "Epoch 125: val_loss improved from 0.21289 to 0.21239, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1960 - accuracy: 0.9362 - val_loss: 0.2124 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 126/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9362\n",
      "Epoch 126: val_loss improved from 0.21239 to 0.21190, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1955 - accuracy: 0.9362 - val_loss: 0.2119 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 127/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.1839 - accuracy: 0.9453\n",
      "Epoch 127: val_loss improved from 0.21190 to 0.21141, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1950 - accuracy: 0.9362 - val_loss: 0.2114 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 128/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.1894 - accuracy: 0.9375\n",
      "Epoch 128: val_loss improved from 0.21141 to 0.21092, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1944 - accuracy: 0.9362 - val_loss: 0.2109 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 129/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9388\n",
      "Epoch 129: val_loss improved from 0.21092 to 0.21044, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1939 - accuracy: 0.9388 - val_loss: 0.2104 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 130/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.1919 - accuracy: 0.9375\n",
      "Epoch 130: val_loss improved from 0.21044 to 0.20996, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1934 - accuracy: 0.9388 - val_loss: 0.2100 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 131/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1945 - accuracy: 0.9375\n",
      "Epoch 131: val_loss improved from 0.20996 to 0.20948, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1929 - accuracy: 0.9388 - val_loss: 0.2095 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 132/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1898 - accuracy: 0.9415\n",
      "Epoch 132: val_loss improved from 0.20948 to 0.20901, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1923 - accuracy: 0.9388 - val_loss: 0.2090 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 133/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1784 - accuracy: 0.9441\n",
      "Epoch 133: val_loss improved from 0.20901 to 0.20854, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1919 - accuracy: 0.9388 - val_loss: 0.2085 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 134/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1923 - accuracy: 0.9388\n",
      "Epoch 134: val_loss improved from 0.20854 to 0.20807, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1913 - accuracy: 0.9388 - val_loss: 0.2081 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 135/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9388\n",
      "Epoch 135: val_loss improved from 0.20807 to 0.20761, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1908 - accuracy: 0.9388 - val_loss: 0.2076 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 136/350\n",
      "25/49 [==============>...............] - ETA: 0s - loss: 0.1881 - accuracy: 0.9450\n",
      "Epoch 136: val_loss improved from 0.20761 to 0.20714, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1903 - accuracy: 0.9388 - val_loss: 0.2071 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 137/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1949 - accuracy: 0.9391\n",
      "Epoch 137: val_loss improved from 0.20714 to 0.20669, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1898 - accuracy: 0.9388 - val_loss: 0.2067 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 138/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1947 - accuracy: 0.9348\n",
      "Epoch 138: val_loss improved from 0.20669 to 0.20624, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1894 - accuracy: 0.9388 - val_loss: 0.2062 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 139/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1885 - accuracy: 0.9451\n",
      "Epoch 139: val_loss improved from 0.20624 to 0.20579, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1888 - accuracy: 0.9388 - val_loss: 0.2058 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 140/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1893 - accuracy: 0.9360\n",
      "Epoch 140: val_loss improved from 0.20579 to 0.20534, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.9388 - val_loss: 0.2053 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 141/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1941 - accuracy: 0.9375\n",
      "Epoch 141: val_loss improved from 0.20534 to 0.20490, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9388 - val_loss: 0.2049 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 142/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9388\n",
      "Epoch 142: val_loss improved from 0.20490 to 0.20446, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1874 - accuracy: 0.9388 - val_loss: 0.2045 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 143/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1931 - accuracy: 0.9361\n",
      "Epoch 143: val_loss improved from 0.20446 to 0.20402, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1869 - accuracy: 0.9388 - val_loss: 0.2040 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 144/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1980 - accuracy: 0.9375\n",
      "Epoch 144: val_loss improved from 0.20402 to 0.20359, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1865 - accuracy: 0.9388 - val_loss: 0.2036 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 145/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1840 - accuracy: 0.9402\n",
      "Epoch 145: val_loss improved from 0.20359 to 0.20315, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9388 - val_loss: 0.2032 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 146/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1979 - accuracy: 0.9318\n",
      "Epoch 146: val_loss improved from 0.20315 to 0.20272, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1855 - accuracy: 0.9388 - val_loss: 0.2027 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 147/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9388\n",
      "Epoch 147: val_loss improved from 0.20272 to 0.20230, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1851 - accuracy: 0.9388 - val_loss: 0.2023 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 148/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1855 - accuracy: 0.9362\n",
      "Epoch 148: val_loss improved from 0.20230 to 0.20187, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1846 - accuracy: 0.9388 - val_loss: 0.2019 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 149/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1927 - accuracy: 0.9375\n",
      "Epoch 149: val_loss improved from 0.20187 to 0.20145, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1842 - accuracy: 0.9388 - val_loss: 0.2015 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 150/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1927 - accuracy: 0.9375\n",
      "Epoch 150: val_loss improved from 0.20145 to 0.20104, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9388 - val_loss: 0.2010 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 151/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1873 - accuracy: 0.9360\n",
      "Epoch 151: val_loss improved from 0.20104 to 0.20062, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1833 - accuracy: 0.9388 - val_loss: 0.2006 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 152/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1856 - accuracy: 0.9389\n",
      "Epoch 152: val_loss improved from 0.20062 to 0.20021, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1828 - accuracy: 0.9388 - val_loss: 0.2002 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 153/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1930 - accuracy: 0.9315\n",
      "Epoch 153: val_loss improved from 0.20021 to 0.19980, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1824 - accuracy: 0.9388 - val_loss: 0.1998 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 154/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1837 - accuracy: 0.9375\n",
      "Epoch 154: val_loss improved from 0.19980 to 0.19939, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1820 - accuracy: 0.9388 - val_loss: 0.1994 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 155/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1783 - accuracy: 0.9423\n",
      "Epoch 155: val_loss improved from 0.19939 to 0.19898, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1815 - accuracy: 0.9388 - val_loss: 0.1990 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 156/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1832 - accuracy: 0.9402\n",
      "Epoch 156: val_loss improved from 0.19898 to 0.19858, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1811 - accuracy: 0.9388 - val_loss: 0.1986 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 157/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1799 - accuracy: 0.9375\n",
      "Epoch 157: val_loss improved from 0.19858 to 0.19817, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1807 - accuracy: 0.9388 - val_loss: 0.1982 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 158/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.1804 - accuracy: 0.9398\n",
      "Epoch 158: val_loss improved from 0.19817 to 0.19778, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1803 - accuracy: 0.9388 - val_loss: 0.1978 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 159/350\n",
      "24/49 [=============>................] - ETA: 0s - loss: 0.1877 - accuracy: 0.9323\n",
      "Epoch 159: val_loss improved from 0.19778 to 0.19737, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1798 - accuracy: 0.9388 - val_loss: 0.1974 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 160/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1870 - accuracy: 0.9358\n",
      "Epoch 160: val_loss improved from 0.19737 to 0.19698, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1794 - accuracy: 0.9388 - val_loss: 0.1970 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 161/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1794 - accuracy: 0.9375\n",
      "Epoch 161: val_loss improved from 0.19698 to 0.19659, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 0.9388 - val_loss: 0.1966 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 162/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1829 - accuracy: 0.9375\n",
      "Epoch 162: val_loss improved from 0.19659 to 0.19620, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1786 - accuracy: 0.9388 - val_loss: 0.1962 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 163/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1779 - accuracy: 0.9402\n",
      "Epoch 163: val_loss improved from 0.19620 to 0.19582, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1782 - accuracy: 0.9388 - val_loss: 0.1958 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 164/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.1700 - accuracy: 0.9453\n",
      "Epoch 164: val_loss improved from 0.19582 to 0.19543, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9388 - val_loss: 0.1954 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 165/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1768 - accuracy: 0.9405\n",
      "Epoch 165: val_loss improved from 0.19543 to 0.19505, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9388 - val_loss: 0.1951 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 166/350\n",
      "23/49 [=============>................] - ETA: 0s - loss: 0.1875 - accuracy: 0.9293\n",
      "Epoch 166: val_loss improved from 0.19505 to 0.19467, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1769 - accuracy: 0.9388 - val_loss: 0.1947 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 167/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1759 - accuracy: 0.9356\n",
      "Epoch 167: val_loss improved from 0.19467 to 0.19429, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9388 - val_loss: 0.1943 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 168/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.1482 - accuracy: 0.9526\n",
      "Epoch 168: val_loss improved from 0.19429 to 0.19392, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1762 - accuracy: 0.9388 - val_loss: 0.1939 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 169/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1799 - accuracy: 0.9362\n",
      "Epoch 169: val_loss improved from 0.19392 to 0.19355, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9388 - val_loss: 0.1935 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 170/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.1711 - accuracy: 0.9340\n",
      "Epoch 170: val_loss improved from 0.19355 to 0.19318, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1754 - accuracy: 0.9388 - val_loss: 0.1932 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 171/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1790 - accuracy: 0.9360\n",
      "Epoch 171: val_loss improved from 0.19318 to 0.19281, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1750 - accuracy: 0.9388 - val_loss: 0.1928 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 172/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.1813 - accuracy: 0.9352\n",
      "Epoch 172: val_loss improved from 0.19281 to 0.19244, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1746 - accuracy: 0.9388 - val_loss: 0.1924 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 173/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1809 - accuracy: 0.9375\n",
      "Epoch 173: val_loss improved from 0.19244 to 0.19207, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9413 - val_loss: 0.1921 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 174/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1618 - accuracy: 0.9464\n",
      "Epoch 174: val_loss improved from 0.19207 to 0.19170, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1739 - accuracy: 0.9413 - val_loss: 0.1917 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 175/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1860 - accuracy: 0.9329\n",
      "Epoch 175: val_loss improved from 0.19170 to 0.19135, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1735 - accuracy: 0.9413 - val_loss: 0.1913 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 176/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1736 - accuracy: 0.9375\n",
      "Epoch 176: val_loss improved from 0.19135 to 0.19099, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.9413 - val_loss: 0.1910 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 177/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1783 - accuracy: 0.9389\n",
      "Epoch 177: val_loss improved from 0.19099 to 0.19063, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1727 - accuracy: 0.9413 - val_loss: 0.1906 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 178/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1722 - accuracy: 0.9489\n",
      "Epoch 178: val_loss improved from 0.19063 to 0.19028, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.9439 - val_loss: 0.1903 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 179/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1735 - accuracy: 0.9427\n",
      "Epoch 179: val_loss improved from 0.19028 to 0.18992, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1720 - accuracy: 0.9439 - val_loss: 0.1899 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 180/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1752 - accuracy: 0.9415\n",
      "Epoch 180: val_loss improved from 0.18992 to 0.18957, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1716 - accuracy: 0.9439 - val_loss: 0.1896 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 181/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1748 - accuracy: 0.9417\n",
      "Epoch 181: val_loss improved from 0.18957 to 0.18922, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1713 - accuracy: 0.9439 - val_loss: 0.1892 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 182/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1643 - accuracy: 0.9455\n",
      "Epoch 182: val_loss improved from 0.18922 to 0.18888, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1709 - accuracy: 0.9439 - val_loss: 0.1889 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 183/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1602 - accuracy: 0.9470\n",
      "Epoch 183: val_loss improved from 0.18888 to 0.18854, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9439 - val_loss: 0.1885 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 184/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.2219 - accuracy: 0.9167\n",
      "Epoch 184: val_loss improved from 0.18854 to 0.18819, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.9439 - val_loss: 0.1882 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 185/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1513 - accuracy: 0.9464\n",
      "Epoch 185: val_loss improved from 0.18819 to 0.18786, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1699 - accuracy: 0.9439 - val_loss: 0.1879 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 186/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.1504 - accuracy: 0.9514\n",
      "Epoch 186: val_loss improved from 0.18786 to 0.18753, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1695 - accuracy: 0.9439 - val_loss: 0.1875 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 187/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1684 - accuracy: 0.9417\n",
      "Epoch 187: val_loss improved from 0.18753 to 0.18720, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1692 - accuracy: 0.9439 - val_loss: 0.1872 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 188/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1810 - accuracy: 0.9375\n",
      "Epoch 188: val_loss improved from 0.18720 to 0.18687, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1689 - accuracy: 0.9439 - val_loss: 0.1869 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 189/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1777 - accuracy: 0.9419\n",
      "Epoch 189: val_loss improved from 0.18687 to 0.18654, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1685 - accuracy: 0.9439 - val_loss: 0.1865 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 190/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.2054 - accuracy: 0.9231\n",
      "Epoch 190: val_loss improved from 0.18654 to 0.18621, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1682 - accuracy: 0.9439 - val_loss: 0.1862 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 191/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9439\n",
      "Epoch 191: val_loss improved from 0.18621 to 0.18589, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.9439 - val_loss: 0.1859 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 192/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1821 - accuracy: 0.9330\n",
      "Epoch 192: val_loss improved from 0.18589 to 0.18557, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1675 - accuracy: 0.9439 - val_loss: 0.1856 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 193/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1614 - accuracy: 0.9453\n",
      "Epoch 193: val_loss improved from 0.18557 to 0.18525, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1672 - accuracy: 0.9439 - val_loss: 0.1853 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 194/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.1624 - accuracy: 0.9476\n",
      "Epoch 194: val_loss improved from 0.18525 to 0.18494, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1668 - accuracy: 0.9439 - val_loss: 0.1849 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 195/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1668 - accuracy: 0.9417\n",
      "Epoch 195: val_loss improved from 0.18494 to 0.18462, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9439 - val_loss: 0.1846 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 196/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1582 - accuracy: 0.9457\n",
      "Epoch 196: val_loss improved from 0.18462 to 0.18431, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.9439 - val_loss: 0.1843 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 197/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9439\n",
      "Epoch 197: val_loss improved from 0.18431 to 0.18401, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1659 - accuracy: 0.9439 - val_loss: 0.1840 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 198/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1594 - accuracy: 0.9420\n",
      "Epoch 198: val_loss improved from 0.18401 to 0.18369, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1656 - accuracy: 0.9439 - val_loss: 0.1837 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 199/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1767 - accuracy: 0.9392\n",
      "Epoch 199: val_loss improved from 0.18369 to 0.18339, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1653 - accuracy: 0.9439 - val_loss: 0.1834 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 200/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.1546 - accuracy: 0.9464\n",
      "Epoch 200: val_loss improved from 0.18339 to 0.18309, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1649 - accuracy: 0.9439 - val_loss: 0.1831 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 201/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1717 - accuracy: 0.9402\n",
      "Epoch 201: val_loss improved from 0.18309 to 0.18278, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1646 - accuracy: 0.9439 - val_loss: 0.1828 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 202/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1680 - accuracy: 0.9435\n",
      "Epoch 202: val_loss improved from 0.18278 to 0.18248, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1643 - accuracy: 0.9439 - val_loss: 0.1825 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 203/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1627 - accuracy: 0.9457\n",
      "Epoch 203: val_loss improved from 0.18248 to 0.18218, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1640 - accuracy: 0.9439 - val_loss: 0.1822 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 204/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1720 - accuracy: 0.9459\n",
      "Epoch 204: val_loss improved from 0.18218 to 0.18188, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1637 - accuracy: 0.9439 - val_loss: 0.1819 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 205/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1344 - accuracy: 0.9594\n",
      "Epoch 205: val_loss improved from 0.18188 to 0.18158, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1634 - accuracy: 0.9439 - val_loss: 0.1816 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 206/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1581 - accuracy: 0.9444\n",
      "Epoch 206: val_loss improved from 0.18158 to 0.18129, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1631 - accuracy: 0.9439 - val_loss: 0.1813 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 207/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1462 - accuracy: 0.9487\n",
      "Epoch 207: val_loss improved from 0.18129 to 0.18099, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9439 - val_loss: 0.1810 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 208/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9427\n",
      "Epoch 208: val_loss improved from 0.18099 to 0.18070, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1625 - accuracy: 0.9439 - val_loss: 0.1807 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 209/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1630 - accuracy: 0.9441\n",
      "Epoch 209: val_loss improved from 0.18070 to 0.18041, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9439 - val_loss: 0.1804 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 210/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1622 - accuracy: 0.9427\n",
      "Epoch 210: val_loss improved from 0.18041 to 0.18013, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1619 - accuracy: 0.9439 - val_loss: 0.1801 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 211/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1628 - accuracy: 0.9441\n",
      "Epoch 211: val_loss improved from 0.18013 to 0.17984, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1616 - accuracy: 0.9439 - val_loss: 0.1798 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 212/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1567 - accuracy: 0.9432\n",
      "Epoch 212: val_loss improved from 0.17984 to 0.17956, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1613 - accuracy: 0.9439 - val_loss: 0.1796 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 213/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1583 - accuracy: 0.9484\n",
      "Epoch 213: val_loss improved from 0.17956 to 0.17928, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9439 - val_loss: 0.1793 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 214/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1684 - accuracy: 0.9394\n",
      "Epoch 214: val_loss improved from 0.17928 to 0.17900, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1607 - accuracy: 0.9439 - val_loss: 0.1790 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 215/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1641 - accuracy: 0.9469\n",
      "Epoch 215: val_loss improved from 0.17900 to 0.17872, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.9439 - val_loss: 0.1787 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 216/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1513 - accuracy: 0.9438\n",
      "Epoch 216: val_loss improved from 0.17872 to 0.17845, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1601 - accuracy: 0.9439 - val_loss: 0.1785 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 217/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9439\n",
      "Epoch 217: val_loss improved from 0.17845 to 0.17817, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1599 - accuracy: 0.9439 - val_loss: 0.1782 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 218/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1563 - accuracy: 0.9477\n",
      "Epoch 218: val_loss improved from 0.17817 to 0.17789, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9439 - val_loss: 0.1779 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 219/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1498 - accuracy: 0.9512\n",
      "Epoch 219: val_loss improved from 0.17789 to 0.17762, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1593 - accuracy: 0.9439 - val_loss: 0.1776 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 220/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1546 - accuracy: 0.9495\n",
      "Epoch 220: val_loss improved from 0.17762 to 0.17735, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1590 - accuracy: 0.9439 - val_loss: 0.1773 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 221/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.1297 - accuracy: 0.9567\n",
      "Epoch 221: val_loss improved from 0.17735 to 0.17708, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9439 - val_loss: 0.1771 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 222/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.1578 - accuracy: 0.9375\n",
      "Epoch 222: val_loss improved from 0.17708 to 0.17681, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1585 - accuracy: 0.9439 - val_loss: 0.1768 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 223/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9427\n",
      "Epoch 223: val_loss improved from 0.17681 to 0.17654, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1582 - accuracy: 0.9439 - val_loss: 0.1765 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 224/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9439\n",
      "Epoch 224: val_loss improved from 0.17654 to 0.17628, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1579 - accuracy: 0.9439 - val_loss: 0.1763 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 225/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1506 - accuracy: 0.9521\n",
      "Epoch 225: val_loss improved from 0.17628 to 0.17601, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9439 - val_loss: 0.1760 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 226/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1525 - accuracy: 0.9459\n",
      "Epoch 226: val_loss improved from 0.17601 to 0.17576, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1574 - accuracy: 0.9439 - val_loss: 0.1758 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 227/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.1514 - accuracy: 0.9444\n",
      "Epoch 227: val_loss improved from 0.17576 to 0.17550, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9439 - val_loss: 0.1755 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 228/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1533 - accuracy: 0.9435\n",
      "Epoch 228: val_loss improved from 0.17550 to 0.17523, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.9439 - val_loss: 0.1752 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 229/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1557 - accuracy: 0.9453\n",
      "Epoch 229: val_loss improved from 0.17523 to 0.17497, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1565 - accuracy: 0.9464 - val_loss: 0.1750 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 230/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.1420 - accuracy: 0.9531\n",
      "Epoch 230: val_loss improved from 0.17497 to 0.17471, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1563 - accuracy: 0.9464 - val_loss: 0.1747 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 231/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.1561 - accuracy: 0.9429\n",
      "Epoch 231: val_loss improved from 0.17471 to 0.17446, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9464 - val_loss: 0.1745 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 232/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1581 - accuracy: 0.9453\n",
      "Epoch 232: val_loss improved from 0.17446 to 0.17420, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9464 - val_loss: 0.1742 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 233/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1295 - accuracy: 0.9527\n",
      "Epoch 233: val_loss improved from 0.17420 to 0.17395, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1555 - accuracy: 0.9464 - val_loss: 0.1740 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 234/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1615 - accuracy: 0.9429\n",
      "Epoch 234: val_loss improved from 0.17395 to 0.17370, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.9464 - val_loss: 0.1737 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 235/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.1770 - accuracy: 0.9375\n",
      "Epoch 235: val_loss improved from 0.17370 to 0.17344, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1550 - accuracy: 0.9464 - val_loss: 0.1734 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 236/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1416 - accuracy: 0.9512\n",
      "Epoch 236: val_loss improved from 0.17344 to 0.17320, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9464 - val_loss: 0.1732 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 237/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1537 - accuracy: 0.9448\n",
      "Epoch 237: val_loss improved from 0.17320 to 0.17295, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9464 - val_loss: 0.1730 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 238/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1545 - accuracy: 0.9468\n",
      "Epoch 238: val_loss improved from 0.17295 to 0.17271, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1542 - accuracy: 0.9464 - val_loss: 0.1727 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 239/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9453\n",
      "Epoch 239: val_loss improved from 0.17271 to 0.17246, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1540 - accuracy: 0.9464 - val_loss: 0.1725 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 240/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1556 - accuracy: 0.9468\n",
      "Epoch 240: val_loss improved from 0.17246 to 0.17222, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9464 - val_loss: 0.1722 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 241/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1477 - accuracy: 0.9469\n",
      "Epoch 241: val_loss improved from 0.17222 to 0.17197, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1534 - accuracy: 0.9464 - val_loss: 0.1720 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 242/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1599 - accuracy: 0.9435\n",
      "Epoch 242: val_loss improved from 0.17197 to 0.17173, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9464 - val_loss: 0.1717 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 243/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1349 - accuracy: 0.9595\n",
      "Epoch 243: val_loss improved from 0.17173 to 0.17150, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1529 - accuracy: 0.9464 - val_loss: 0.1715 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 244/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1614 - accuracy: 0.9438\n",
      "Epoch 244: val_loss improved from 0.17150 to 0.17125, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9464 - val_loss: 0.1713 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 245/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1432 - accuracy: 0.9528\n",
      "Epoch 245: val_loss improved from 0.17125 to 0.17102, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1524 - accuracy: 0.9464 - val_loss: 0.1710 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 246/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1381 - accuracy: 0.9531\n",
      "Epoch 246: val_loss improved from 0.17102 to 0.17078, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9464 - val_loss: 0.1708 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 247/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1541 - accuracy: 0.9435\n",
      "Epoch 247: val_loss improved from 0.17078 to 0.17055, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9464 - val_loss: 0.1706 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 248/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1400 - accuracy: 0.9545\n",
      "Epoch 248: val_loss improved from 0.17055 to 0.17033, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9464 - val_loss: 0.1703 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 249/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1489 - accuracy: 0.9464\n",
      "Epoch 249: val_loss improved from 0.17033 to 0.17010, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1514 - accuracy: 0.9464 - val_loss: 0.1701 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 250/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.1595 - accuracy: 0.9429\n",
      "Epoch 250: val_loss improved from 0.17010 to 0.16987, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.9464 - val_loss: 0.1699 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 251/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.1183 - accuracy: 0.9741\n",
      "Epoch 251: val_loss improved from 0.16987 to 0.16964, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1509 - accuracy: 0.9464 - val_loss: 0.1696 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 252/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1507 - accuracy: 0.9464\n",
      "Epoch 252: val_loss improved from 0.16964 to 0.16941, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1507 - accuracy: 0.9464 - val_loss: 0.1694 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 253/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1558 - accuracy: 0.9432\n",
      "Epoch 253: val_loss improved from 0.16941 to 0.16919, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9464 - val_loss: 0.1692 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 254/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1534 - accuracy: 0.9460\n",
      "Epoch 254: val_loss improved from 0.16919 to 0.16896, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9464 - val_loss: 0.1690 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 255/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1518 - accuracy: 0.9448\n",
      "Epoch 255: val_loss improved from 0.16896 to 0.16874, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9464 - val_loss: 0.1687 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 256/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1458 - accuracy: 0.9468\n",
      "Epoch 256: val_loss improved from 0.16874 to 0.16851, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1497 - accuracy: 0.9464 - val_loss: 0.1685 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 257/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1555 - accuracy: 0.9460\n",
      "Epoch 257: val_loss improved from 0.16851 to 0.16829, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9464 - val_loss: 0.1683 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 258/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1510 - accuracy: 0.9441\n",
      "Epoch 258: val_loss improved from 0.16829 to 0.16807, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1492 - accuracy: 0.9464 - val_loss: 0.1681 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 259/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1429 - accuracy: 0.9477\n",
      "Epoch 259: val_loss improved from 0.16807 to 0.16785, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1490 - accuracy: 0.9464 - val_loss: 0.1678 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 260/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1601 - accuracy: 0.9375\n",
      "Epoch 260: val_loss improved from 0.16785 to 0.16763, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9464 - val_loss: 0.1676 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 261/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1516 - accuracy: 0.9441\n",
      "Epoch 261: val_loss improved from 0.16763 to 0.16741, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1485 - accuracy: 0.9464 - val_loss: 0.1674 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 262/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.1553 - accuracy: 0.9485\n",
      "Epoch 262: val_loss improved from 0.16741 to 0.16720, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1483 - accuracy: 0.9464 - val_loss: 0.1672 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 263/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1550 - accuracy: 0.9493\n",
      "Epoch 263: val_loss improved from 0.16720 to 0.16697, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1481 - accuracy: 0.9464 - val_loss: 0.1670 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 264/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1123 - accuracy: 0.9671\n",
      "Epoch 264: val_loss improved from 0.16697 to 0.16676, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1478 - accuracy: 0.9464 - val_loss: 0.1668 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 265/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.1548 - accuracy: 0.9375\n",
      "Epoch 265: val_loss improved from 0.16676 to 0.16655, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1476 - accuracy: 0.9464 - val_loss: 0.1665 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 266/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1545 - accuracy: 0.9426\n",
      "Epoch 266: val_loss improved from 0.16655 to 0.16634, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1474 - accuracy: 0.9464 - val_loss: 0.1663 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 267/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1492 - accuracy: 0.9460\n",
      "Epoch 267: val_loss improved from 0.16634 to 0.16613, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.1471 - accuracy: 0.9464 - val_loss: 0.1661 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 268/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1456 - accuracy: 0.9495\n",
      "Epoch 268: val_loss improved from 0.16613 to 0.16592, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 9ms/step - loss: 0.1469 - accuracy: 0.9464 - val_loss: 0.1659 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 269/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1467 - accuracy: 0.9464\n",
      "Epoch 269: val_loss improved from 0.16592 to 0.16571, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 1s 13ms/step - loss: 0.1467 - accuracy: 0.9464 - val_loss: 0.1657 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 270/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 0.9464\n",
      "Epoch 270: val_loss improved from 0.16571 to 0.16550, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1465 - accuracy: 0.9464 - val_loss: 0.1655 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 271/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1493 - accuracy: 0.9441\n",
      "Epoch 271: val_loss improved from 0.16550 to 0.16528, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1462 - accuracy: 0.9464 - val_loss: 0.1653 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 272/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1511 - accuracy: 0.9429\n",
      "Epoch 272: val_loss improved from 0.16528 to 0.16508, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1460 - accuracy: 0.9464 - val_loss: 0.1651 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 273/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1458 - accuracy: 0.9464\n",
      "Epoch 273: val_loss improved from 0.16508 to 0.16487, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9464 - val_loss: 0.1649 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 274/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1476 - accuracy: 0.9453\n",
      "Epoch 274: val_loss improved from 0.16487 to 0.16466, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1456 - accuracy: 0.9464 - val_loss: 0.1647 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 275/350\n",
      "25/49 [==============>...............] - ETA: 0s - loss: 0.1396 - accuracy: 0.9500\n",
      "Epoch 275: val_loss improved from 0.16466 to 0.16446, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1454 - accuracy: 0.9464 - val_loss: 0.1645 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 276/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.1568 - accuracy: 0.9479\n",
      "Epoch 276: val_loss improved from 0.16446 to 0.16426, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1451 - accuracy: 0.9464 - val_loss: 0.1643 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 277/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1382 - accuracy: 0.9479\n",
      "Epoch 277: val_loss improved from 0.16426 to 0.16406, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1449 - accuracy: 0.9464 - val_loss: 0.1641 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 278/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.1499 - accuracy: 0.9417\n",
      "Epoch 278: val_loss improved from 0.16406 to 0.16385, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9464 - val_loss: 0.1639 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 279/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1189 - accuracy: 0.9643\n",
      "Epoch 279: val_loss improved from 0.16385 to 0.16366, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1445 - accuracy: 0.9464 - val_loss: 0.1637 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 280/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1313 - accuracy: 0.9509\n",
      "Epoch 280: val_loss improved from 0.16366 to 0.16346, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1442 - accuracy: 0.9464 - val_loss: 0.1635 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 281/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.9453\n",
      "Epoch 281: val_loss improved from 0.16346 to 0.16326, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9464 - val_loss: 0.1633 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 282/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1459 - accuracy: 0.9408\n",
      "Epoch 282: val_loss improved from 0.16326 to 0.16305, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1438 - accuracy: 0.9464 - val_loss: 0.1631 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 283/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1337 - accuracy: 0.9493\n",
      "Epoch 283: val_loss improved from 0.16305 to 0.16285, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9464 - val_loss: 0.1628 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 284/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1362 - accuracy: 0.9500\n",
      "Epoch 284: val_loss improved from 0.16285 to 0.16265, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1434 - accuracy: 0.9464 - val_loss: 0.1627 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 285/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1337 - accuracy: 0.9531\n",
      "Epoch 285: val_loss improved from 0.16265 to 0.16246, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1432 - accuracy: 0.9464 - val_loss: 0.1625 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 286/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1361 - accuracy: 0.9493\n",
      "Epoch 286: val_loss improved from 0.16246 to 0.16227, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1430 - accuracy: 0.9464 - val_loss: 0.1623 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 287/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1087 - accuracy: 0.9656\n",
      "Epoch 287: val_loss improved from 0.16227 to 0.16208, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1428 - accuracy: 0.9464 - val_loss: 0.1621 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 288/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1525 - accuracy: 0.9441\n",
      "Epoch 288: val_loss improved from 0.16208 to 0.16189, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1425 - accuracy: 0.9464 - val_loss: 0.1619 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 289/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1465 - accuracy: 0.9421\n",
      "Epoch 289: val_loss improved from 0.16189 to 0.16169, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1423 - accuracy: 0.9464 - val_loss: 0.1617 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 290/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1393 - accuracy: 0.9508\n",
      "Epoch 290: val_loss improved from 0.16169 to 0.16150, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1421 - accuracy: 0.9464 - val_loss: 0.1615 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 291/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1483 - accuracy: 0.9448\n",
      "Epoch 291: val_loss improved from 0.16150 to 0.16132, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1419 - accuracy: 0.9464 - val_loss: 0.1613 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 292/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.1394 - accuracy: 0.9522\n",
      "Epoch 292: val_loss improved from 0.16132 to 0.16113, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9464 - val_loss: 0.1611 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 293/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.1476 - accuracy: 0.9500\n",
      "Epoch 293: val_loss improved from 0.16113 to 0.16094, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1415 - accuracy: 0.9464 - val_loss: 0.1609 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 294/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1334 - accuracy: 0.9500\n",
      "Epoch 294: val_loss improved from 0.16094 to 0.16075, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1413 - accuracy: 0.9464 - val_loss: 0.1608 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 295/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1287 - accuracy: 0.9509\n",
      "Epoch 295: val_loss improved from 0.16075 to 0.16057, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1411 - accuracy: 0.9464 - val_loss: 0.1606 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 296/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1505 - accuracy: 0.9405\n",
      "Epoch 296: val_loss improved from 0.16057 to 0.16038, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1409 - accuracy: 0.9464 - val_loss: 0.1604 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 297/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.1448 - accuracy: 0.9435\n",
      "Epoch 297: val_loss improved from 0.16038 to 0.16020, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1407 - accuracy: 0.9464 - val_loss: 0.1602 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 298/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1435 - accuracy: 0.9441\n",
      "Epoch 298: val_loss improved from 0.16020 to 0.16002, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1405 - accuracy: 0.9464 - val_loss: 0.1600 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 299/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1501 - accuracy: 0.9403\n",
      "Epoch 299: val_loss improved from 0.16002 to 0.15983, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9464 - val_loss: 0.1598 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 300/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1355 - accuracy: 0.9531\n",
      "Epoch 300: val_loss improved from 0.15983 to 0.15965, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1401 - accuracy: 0.9464 - val_loss: 0.1597 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 301/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1055 - accuracy: 0.9643\n",
      "Epoch 301: val_loss improved from 0.15965 to 0.15947, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9464 - val_loss: 0.1595 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 302/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1324 - accuracy: 0.9482\n",
      "Epoch 302: val_loss improved from 0.15947 to 0.15930, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1396 - accuracy: 0.9464 - val_loss: 0.1593 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 303/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1401 - accuracy: 0.9435\n",
      "Epoch 303: val_loss improved from 0.15930 to 0.15911, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1394 - accuracy: 0.9464 - val_loss: 0.1591 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 304/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.1426 - accuracy: 0.9500\n",
      "Epoch 304: val_loss improved from 0.15911 to 0.15893, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1393 - accuracy: 0.9464 - val_loss: 0.1589 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 305/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1269 - accuracy: 0.9500\n",
      "Epoch 305: val_loss improved from 0.15893 to 0.15875, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1390 - accuracy: 0.9464 - val_loss: 0.1587 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 306/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.1618 - accuracy: 0.9353\n",
      "Epoch 306: val_loss improved from 0.15875 to 0.15857, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9464 - val_loss: 0.1586 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 307/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.1350 - accuracy: 0.9559\n",
      "Epoch 307: val_loss improved from 0.15857 to 0.15839, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1386 - accuracy: 0.9464 - val_loss: 0.1584 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 308/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1406 - accuracy: 0.9441\n",
      "Epoch 308: val_loss improved from 0.15839 to 0.15821, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9464 - val_loss: 0.1582 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 309/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.1349 - accuracy: 0.9507\n",
      "Epoch 309: val_loss improved from 0.15821 to 0.15804, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9490 - val_loss: 0.1580 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 310/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.9495\n",
      "Epoch 310: val_loss improved from 0.15804 to 0.15786, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9490 - val_loss: 0.1579 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 311/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.1235 - accuracy: 0.9583\n",
      "Epoch 311: val_loss improved from 0.15786 to 0.15770, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1379 - accuracy: 0.9490 - val_loss: 0.1577 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 312/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.9479\n",
      "Epoch 312: val_loss improved from 0.15770 to 0.15752, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1377 - accuracy: 0.9490 - val_loss: 0.1575 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 313/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.1633 - accuracy: 0.9397\n",
      "Epoch 313: val_loss improved from 0.15752 to 0.15734, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1375 - accuracy: 0.9490 - val_loss: 0.1573 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 314/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1261 - accuracy: 0.9500\n",
      "Epoch 314: val_loss improved from 0.15734 to 0.15717, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1373 - accuracy: 0.9490 - val_loss: 0.1572 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 315/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9505\n",
      "Epoch 315: val_loss improved from 0.15717 to 0.15700, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 0.1371 - accuracy: 0.9490 - val_loss: 0.1570 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 316/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9505\n",
      "Epoch 316: val_loss improved from 0.15700 to 0.15683, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 10ms/step - loss: 0.1369 - accuracy: 0.9490 - val_loss: 0.1568 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 317/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.1388 - accuracy: 0.9489\n",
      "Epoch 317: val_loss improved from 0.15683 to 0.15666, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1367 - accuracy: 0.9490 - val_loss: 0.1567 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 318/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.1372 - accuracy: 0.9516\n",
      "Epoch 318: val_loss improved from 0.15666 to 0.15649, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.9490 - val_loss: 0.1565 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 319/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.1374 - accuracy: 0.9432\n",
      "Epoch 319: val_loss improved from 0.15649 to 0.15632, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.9490 - val_loss: 0.1563 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 320/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1406 - accuracy: 0.9468\n",
      "Epoch 320: val_loss improved from 0.15632 to 0.15616, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1362 - accuracy: 0.9490 - val_loss: 0.1562 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 321/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1384 - accuracy: 0.9487\n",
      "Epoch 321: val_loss improved from 0.15616 to 0.15599, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9490 - val_loss: 0.1560 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 322/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9490\n",
      "Epoch 322: val_loss improved from 0.15599 to 0.15582, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.9490 - val_loss: 0.1558 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 323/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.1116 - accuracy: 0.9637\n",
      "Epoch 323: val_loss improved from 0.15582 to 0.15567, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1356 - accuracy: 0.9490 - val_loss: 0.1557 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 324/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.1204 - accuracy: 0.9542\n",
      "Epoch 324: val_loss improved from 0.15567 to 0.15550, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1354 - accuracy: 0.9490 - val_loss: 0.1555 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 325/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.1287 - accuracy: 0.9549\n",
      "Epoch 325: val_loss improved from 0.15550 to 0.15534, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1352 - accuracy: 0.9490 - val_loss: 0.1553 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 326/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.0941 - accuracy: 0.9704\n",
      "Epoch 326: val_loss improved from 0.15534 to 0.15519, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9490 - val_loss: 0.1552 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 327/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1274 - accuracy: 0.9564\n",
      "Epoch 327: val_loss improved from 0.15519 to 0.15503, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9490 - val_loss: 0.1550 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 328/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1451 - accuracy: 0.9438\n",
      "Epoch 328: val_loss improved from 0.15503 to 0.15487, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1346 - accuracy: 0.9490 - val_loss: 0.1549 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 329/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1397 - accuracy: 0.9464\n",
      "Epoch 329: val_loss improved from 0.15487 to 0.15470, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.9490 - val_loss: 0.1547 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 330/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.1375 - accuracy: 0.9448\n",
      "Epoch 330: val_loss improved from 0.15470 to 0.15454, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1343 - accuracy: 0.9490 - val_loss: 0.1545 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 331/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.1296 - accuracy: 0.9531\n",
      "Epoch 331: val_loss improved from 0.15454 to 0.15438, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9490 - val_loss: 0.1544 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 332/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.1483 - accuracy: 0.9417\n",
      "Epoch 332: val_loss improved from 0.15438 to 0.15421, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9490 - val_loss: 0.1542 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 333/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.1411 - accuracy: 0.9440\n",
      "Epoch 333: val_loss improved from 0.15421 to 0.15406, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9490 - val_loss: 0.1541 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 334/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1305 - accuracy: 0.9512\n",
      "Epoch 334: val_loss improved from 0.15406 to 0.15390, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1336 - accuracy: 0.9490 - val_loss: 0.1539 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 335/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1274 - accuracy: 0.9528\n",
      "Epoch 335: val_loss improved from 0.15390 to 0.15374, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9490 - val_loss: 0.1537 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 336/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1409 - accuracy: 0.9444\n",
      "Epoch 336: val_loss improved from 0.15374 to 0.15359, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1332 - accuracy: 0.9490 - val_loss: 0.1536 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 337/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.1336 - accuracy: 0.9469\n",
      "Epoch 337: val_loss improved from 0.15359 to 0.15343, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1330 - accuracy: 0.9490 - val_loss: 0.1534 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 338/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.1339 - accuracy: 0.9511\n",
      "Epoch 338: val_loss improved from 0.15343 to 0.15327, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.9515 - val_loss: 0.1533 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 339/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1156 - accuracy: 0.9688\n",
      "Epoch 339: val_loss improved from 0.15327 to 0.15311, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.9515 - val_loss: 0.1531 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 340/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1363 - accuracy: 0.9519\n",
      "Epoch 340: val_loss improved from 0.15311 to 0.15296, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9515 - val_loss: 0.1530 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 341/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1347 - accuracy: 0.9528\n",
      "Epoch 341: val_loss improved from 0.15296 to 0.15281, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1323 - accuracy: 0.9515 - val_loss: 0.1528 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 342/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1344 - accuracy: 0.9509\n",
      "Epoch 342: val_loss improved from 0.15281 to 0.15266, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1321 - accuracy: 0.9515 - val_loss: 0.1527 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 343/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.1347 - accuracy: 0.9464\n",
      "Epoch 343: val_loss improved from 0.15266 to 0.15251, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9515 - val_loss: 0.1525 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 344/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.1249 - accuracy: 0.9598\n",
      "Epoch 344: val_loss improved from 0.15251 to 0.15236, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9515 - val_loss: 0.1524 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 345/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1378 - accuracy: 0.9472\n",
      "Epoch 345: val_loss improved from 0.15236 to 0.15220, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1316 - accuracy: 0.9515 - val_loss: 0.1522 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 346/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.1296 - accuracy: 0.9528\n",
      "Epoch 346: val_loss improved from 0.15220 to 0.15206, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1314 - accuracy: 0.9515 - val_loss: 0.1521 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 347/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.1226 - accuracy: 0.9567\n",
      "Epoch 347: val_loss improved from 0.15206 to 0.15191, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1313 - accuracy: 0.9515 - val_loss: 0.1519 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 348/350\n",
      "23/49 [=============>................] - ETA: 0s - loss: 0.1505 - accuracy: 0.9402\n",
      "Epoch 348: val_loss improved from 0.15191 to 0.15176, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.9515 - val_loss: 0.1518 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 349/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.1365 - accuracy: 0.9482\n",
      "Epoch 349: val_loss improved from 0.15176 to 0.15161, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9515 - val_loss: 0.1516 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 350/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1342 - accuracy: 0.9495\n",
      "Epoch 350: val_loss improved from 0.15161 to 0.15146, saving model to model_checkpoint\\Vanilla_RNN.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9515 - val_loss: 0.1515 - val_accuracy: 0.9464 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_2 (SimpleRNN)    (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1515 - accuracy: 0.9464\n",
      "Test Loss: 0.15146104991436005\n",
      "Test Accuracy: 0.9464285969734192\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, epochs=350, batch_size=8, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "\"\"\"\n",
    "# optimizer \"adam\"\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNN with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.4740 - accuracy: 0.7963 \n",
      "Epoch 1: val_loss improved from inf to 0.30712, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 2s 10ms/step - loss: 0.4508 - accuracy: 0.8010 - val_loss: 0.3071 - val_accuracy: 0.8810\n",
      "Epoch 2/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.2982 - accuracy: 0.8971\n",
      "Epoch 2: val_loss improved from 0.30712 to 0.25451, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2900 - accuracy: 0.9005 - val_loss: 0.2545 - val_accuracy: 0.8929\n",
      "Epoch 3/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9115\n",
      "Epoch 3: val_loss improved from 0.25451 to 0.22853, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2385 - accuracy: 0.9107 - val_loss: 0.2285 - val_accuracy: 0.9048\n",
      "Epoch 4/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.2035 - accuracy: 0.9306\n",
      "Epoch 4: val_loss improved from 0.22853 to 0.21219, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2074 - accuracy: 0.9311 - val_loss: 0.2122 - val_accuracy: 0.9107\n",
      "Epoch 5/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1834 - accuracy: 0.9388\n",
      "Epoch 5: val_loss improved from 0.21219 to 0.19946, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1843 - accuracy: 0.9388 - val_loss: 0.1995 - val_accuracy: 0.9167\n",
      "Epoch 6/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.1643 - accuracy: 0.9479\n",
      "Epoch 6: val_loss improved from 0.19946 to 0.18917, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1672 - accuracy: 0.9439 - val_loss: 0.1892 - val_accuracy: 0.9286\n",
      "Epoch 7/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.1694 - accuracy: 0.9397\n",
      "Epoch 7: val_loss improved from 0.18917 to 0.18035, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1528 - accuracy: 0.9490 - val_loss: 0.1804 - val_accuracy: 0.9286\n",
      "Epoch 8/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.1477 - accuracy: 0.9500\n",
      "Epoch 8: val_loss improved from 0.18035 to 0.17247, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9541 - val_loss: 0.1725 - val_accuracy: 0.9345\n",
      "Epoch 9/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.1289 - accuracy: 0.9609\n",
      "Epoch 9: val_loss improved from 0.17247 to 0.16679, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1306 - accuracy: 0.9592 - val_loss: 0.1668 - val_accuracy: 0.9405\n",
      "Epoch 10/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.1210 - accuracy: 0.9617\n",
      "Epoch 10: val_loss improved from 0.16679 to 0.16102, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1210 - accuracy: 0.9617 - val_loss: 0.1610 - val_accuracy: 0.9464\n",
      "Epoch 11/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.1113 - accuracy: 0.9696\n",
      "Epoch 11: val_loss improved from 0.16102 to 0.15557, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.1134 - accuracy: 0.9668 - val_loss: 0.1556 - val_accuracy: 0.9464\n",
      "Epoch 12/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.1109 - accuracy: 0.9647\n",
      "Epoch 12: val_loss improved from 0.15557 to 0.15113, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9643 - val_loss: 0.1511 - val_accuracy: 0.9464\n",
      "Epoch 13/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.1015 - accuracy: 0.9707\n",
      "Epoch 13: val_loss improved from 0.15113 to 0.14741, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9719 - val_loss: 0.1474 - val_accuracy: 0.9464\n",
      "Epoch 14/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.0904 - accuracy: 0.9727\n",
      "Epoch 14: val_loss improved from 0.14741 to 0.14512, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0921 - accuracy: 0.9719 - val_loss: 0.1451 - val_accuracy: 0.9524\n",
      "Epoch 15/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0856 - accuracy: 0.9728\n",
      "Epoch 15: val_loss improved from 0.14512 to 0.14206, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0868 - accuracy: 0.9745 - val_loss: 0.1421 - val_accuracy: 0.9524\n",
      "Epoch 16/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.0665 - accuracy: 0.9792\n",
      "Epoch 16: val_loss improved from 0.14206 to 0.14129, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0814 - accuracy: 0.9719 - val_loss: 0.1413 - val_accuracy: 0.9524\n",
      "Epoch 17/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.0725 - accuracy: 0.9730\n",
      "Epoch 17: val_loss improved from 0.14129 to 0.13665, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9745 - val_loss: 0.1366 - val_accuracy: 0.9583\n",
      "Epoch 18/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0744 - accuracy: 0.9728\n",
      "Epoch 18: val_loss improved from 0.13665 to 0.13472, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9745 - val_loss: 0.1347 - val_accuracy: 0.9583\n",
      "Epoch 19/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0705 - accuracy: 0.9755\n",
      "Epoch 19: val_loss improved from 0.13472 to 0.13360, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9770 - val_loss: 0.1336 - val_accuracy: 0.9583\n",
      "Epoch 20/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0652 - accuracy: 0.9787\n",
      "Epoch 20: val_loss improved from 0.13360 to 0.13047, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9796 - val_loss: 0.1305 - val_accuracy: 0.9583\n",
      "Epoch 21/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.0575 - accuracy: 0.9816\n",
      "Epoch 21: val_loss improved from 0.13047 to 0.12875, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.9770 - val_loss: 0.1287 - val_accuracy: 0.9524\n",
      "Epoch 22/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 0.0604 - accuracy: 0.9812\n",
      "Epoch 22: val_loss did not improve from 0.12875\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9847 - val_loss: 0.1294 - val_accuracy: 0.9524\n",
      "Epoch 23/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0439 - accuracy: 0.9946\n",
      "Epoch 23: val_loss improved from 0.12875 to 0.12567, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9872 - val_loss: 0.1257 - val_accuracy: 0.9524\n",
      "Epoch 24/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0560 - accuracy: 0.9915\n",
      "Epoch 24: val_loss improved from 0.12567 to 0.12456, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0536 - accuracy: 0.9923 - val_loss: 0.1246 - val_accuracy: 0.9524\n",
      "Epoch 25/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0423 - accuracy: 0.9830\n",
      "Epoch 25: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0487 - accuracy: 0.9821 - val_loss: 0.1264 - val_accuracy: 0.9464\n",
      "Epoch 26/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0371 - accuracy: 0.9918\n",
      "Epoch 26: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0450 - accuracy: 0.9898 - val_loss: 0.1271 - val_accuracy: 0.9464\n",
      "Epoch 27/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9923\n",
      "Epoch 27: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.9923 - val_loss: 0.1282 - val_accuracy: 0.9524\n",
      "Epoch 28/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0362 - accuracy: 0.9940\n",
      "Epoch 28: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0404 - accuracy: 0.9923 - val_loss: 0.1272 - val_accuracy: 0.9464\n",
      "Epoch 29/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0412 - accuracy: 0.9911\n",
      "Epoch 29: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9923 - val_loss: 0.1261 - val_accuracy: 0.9524\n",
      "Epoch 30/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0380 - accuracy: 0.9918\n",
      "Epoch 30: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0364 - accuracy: 0.9923 - val_loss: 0.1250 - val_accuracy: 0.9524\n",
      "Epoch 31/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0380 - accuracy: 0.9904\n",
      "Epoch 31: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0354 - accuracy: 0.9923 - val_loss: 0.1266 - val_accuracy: 0.9524\n",
      "Epoch 32/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0342 - accuracy: 0.9911\n",
      "Epoch 32: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.9923 - val_loss: 0.1253 - val_accuracy: 0.9524\n",
      "Epoch 33/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9948\n",
      "Epoch 33: val_loss did not improve from 0.12456\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0311 - accuracy: 0.9949 - val_loss: 0.1253 - val_accuracy: 0.9524\n",
      "Epoch 34/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0294 - accuracy: 0.9936\n",
      "Epoch 34: val_loss improved from 0.12456 to 0.12449, saving model to model_checkpoint\\Vanilla_RNN_Adam.h5\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9949 - val_loss: 0.1245 - val_accuracy: 0.9524\n",
      "Epoch 35/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0266 - accuracy: 0.9960\n",
      "Epoch 35: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.9949 - val_loss: 0.1257 - val_accuracy: 0.9524\n",
      "Epoch 36/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.0312 - accuracy: 0.9922\n",
      "Epoch 36: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9949 - val_loss: 0.1257 - val_accuracy: 0.9524\n",
      "Epoch 37/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0294 - accuracy: 0.9914\n",
      "Epoch 37: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9949 - val_loss: 0.1249 - val_accuracy: 0.9524\n",
      "Epoch 38/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0248 - accuracy: 0.9946\n",
      "Epoch 38: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.9949 - val_loss: 0.1260 - val_accuracy: 0.9524\n",
      "Epoch 39/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9948\n",
      "Epoch 39: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.9949 - val_loss: 0.1261 - val_accuracy: 0.9524\n",
      "Epoch 40/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 40: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.9949 - val_loss: 0.1252 - val_accuracy: 0.9524\n",
      "Epoch 41/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.0211 - accuracy: 0.9942\n",
      "Epoch 41: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.9949 - val_loss: 0.1291 - val_accuracy: 0.9524\n",
      "Epoch 42/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0233 - accuracy: 0.9917\n",
      "Epoch 42: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 0.1274 - val_accuracy: 0.9524\n",
      "Epoch 43/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0192 - accuracy: 0.9946\n",
      "Epoch 43: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.9949 - val_loss: 0.1293 - val_accuracy: 0.9524\n",
      "Epoch 44/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.0173 - accuracy: 0.9942\n",
      "Epoch 44: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.9949 - val_loss: 0.1303 - val_accuracy: 0.9524\n",
      "Epoch 45/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0167 - accuracy: 0.9970\n",
      "Epoch 45: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 0.9974 - val_loss: 0.1306 - val_accuracy: 0.9524\n",
      "Epoch 46/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.0154 - accuracy: 0.9972\n",
      "Epoch 46: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0156 - accuracy: 0.9974 - val_loss: 0.1316 - val_accuracy: 0.9524\n",
      "Epoch 47/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0148 - accuracy: 0.9957\n",
      "Epoch 47: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.1320 - val_accuracy: 0.9524\n",
      "Epoch 48/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0151 - accuracy: 0.9972\n",
      "Epoch 48: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9974 - val_loss: 0.1322 - val_accuracy: 0.9524\n",
      "Epoch 49/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0150 - accuracy: 0.9958\n",
      "Epoch 49: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.9974 - val_loss: 0.1341 - val_accuracy: 0.9524\n",
      "Epoch 50/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.0150 - accuracy: 0.9963\n",
      "Epoch 50: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0128 - accuracy: 0.9974 - val_loss: 0.1356 - val_accuracy: 0.9524\n",
      "Epoch 51/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0128 - accuracy: 0.9973\n",
      "Epoch 51: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0123 - accuracy: 0.9974 - val_loss: 0.1359 - val_accuracy: 0.9524\n",
      "Epoch 52/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0145 - accuracy: 0.9960\n",
      "Epoch 52: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9974 - val_loss: 0.1371 - val_accuracy: 0.9524\n",
      "Epoch 53/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0112 - accuracy: 0.9973\n",
      "Epoch 53: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9974 - val_loss: 0.1385 - val_accuracy: 0.9524\n",
      "Epoch 54/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 54: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.9974 - val_loss: 0.1406 - val_accuracy: 0.9524\n",
      "Epoch 55/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 55: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9524\n",
      "Epoch 56/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0097 - accuracy: 0.9974\n",
      "Epoch 56: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.1422 - val_accuracy: 0.9524\n",
      "Epoch 57/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 57: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 0.9524\n",
      "Epoch 58/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 58: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.1449 - val_accuracy: 0.9524\n",
      "Epoch 59/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 59: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1463 - val_accuracy: 0.9524\n",
      "Epoch 60/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 60: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.9524\n",
      "Epoch 61/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 61: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1485 - val_accuracy: 0.9524\n",
      "Epoch 62/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 62: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.9524\n",
      "Epoch 63/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 63: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.9524\n",
      "Epoch 64/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 64: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9524\n",
      "Epoch 65/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 65: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9524\n",
      "Epoch 66/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 66: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9524\n",
      "Epoch 67/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 67: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9524\n",
      "Epoch 68/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 68: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.9524\n",
      "Epoch 69/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 0.0054 - accuracy: 1.0000  \n",
      "Epoch 69: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9524\n",
      "Epoch 70/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 70: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.1596 - val_accuracy: 0.9524\n",
      "Epoch 71/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 71: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.1606 - val_accuracy: 0.9524\n",
      "Epoch 72/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 72: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.1611 - val_accuracy: 0.9524\n",
      "Epoch 73/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 73: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.1638 - val_accuracy: 0.9524\n",
      "Epoch 74/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 74: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9524\n",
      "Epoch 75/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 75: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.1660 - val_accuracy: 0.9524\n",
      "Epoch 76/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 76: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.1669 - val_accuracy: 0.9524\n",
      "Epoch 77/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 77: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1678 - val_accuracy: 0.9524\n",
      "Epoch 78/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 78: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1694 - val_accuracy: 0.9524\n",
      "Epoch 79/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 79: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.1711 - val_accuracy: 0.9524\n",
      "Epoch 80/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 80: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9524\n",
      "Epoch 81/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 81: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.1731 - val_accuracy: 0.9524\n",
      "Epoch 82/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 82: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.1734 - val_accuracy: 0.9524\n",
      "Epoch 83/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000    \n",
      "Epoch 83: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1751 - val_accuracy: 0.9524\n",
      "Epoch 84/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.0030 - accuracy: 1.0000  \n",
      "Epoch 84: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.1765 - val_accuracy: 0.9524\n",
      "Epoch 85/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 85: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.1776 - val_accuracy: 0.9583\n",
      "Epoch 86/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0026 - accuracy: 1.0000  \n",
      "Epoch 86: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9583\n",
      "Epoch 87/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 87: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1796 - val_accuracy: 0.9583\n",
      "Epoch 88/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 88: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1817 - val_accuracy: 0.9583\n",
      "Epoch 89/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 89: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 7ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1818 - val_accuracy: 0.9583\n",
      "Epoch 90/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000    \n",
      "Epoch 90: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1839 - val_accuracy: 0.9583\n",
      "Epoch 91/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 91: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1845 - val_accuracy: 0.9583\n",
      "Epoch 92/350\n",
      "25/49 [==============>...............] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 92: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1864 - val_accuracy: 0.9583\n",
      "Epoch 93/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 93: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1874 - val_accuracy: 0.9583\n",
      "Epoch 94/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000  \n",
      "Epoch 94: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1881 - val_accuracy: 0.9583\n",
      "Epoch 95/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 95: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1892 - val_accuracy: 0.9583\n",
      "Epoch 96/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 96: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1899 - val_accuracy: 0.9583\n",
      "Epoch 97/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 97: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9583\n",
      "Epoch 98/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 98: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1920 - val_accuracy: 0.9524\n",
      "Epoch 99/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000  \n",
      "Epoch 99: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.9524\n",
      "Epoch 100/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000    \n",
      "Epoch 100: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1944 - val_accuracy: 0.9524\n",
      "Epoch 101/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 101: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1951 - val_accuracy: 0.9583\n",
      "Epoch 102/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 102: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9583\n",
      "Epoch 103/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 103: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9524\n",
      "Epoch 104/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000  \n",
      "Epoch 104: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1988 - val_accuracy: 0.9524\n",
      "Epoch 105/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000    \n",
      "Epoch 105: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2000 - val_accuracy: 0.9524\n",
      "Epoch 106/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000    \n",
      "Epoch 106: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.9524\n",
      "Epoch 107/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000  \n",
      "Epoch 107: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2021 - val_accuracy: 0.9524\n",
      "Epoch 108/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000    \n",
      "Epoch 108: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9524\n",
      "Epoch 109/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000  \n",
      "Epoch 109: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2043 - val_accuracy: 0.9524\n",
      "Epoch 110/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000  \n",
      "Epoch 110: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2052 - val_accuracy: 0.9524\n",
      "Epoch 111/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000    \n",
      "Epoch 111: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2067 - val_accuracy: 0.9524\n",
      "Epoch 112/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 9.3184e-04 - accuracy: 1.0000\n",
      "Epoch 112: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2074 - val_accuracy: 0.9524\n",
      "Epoch 113/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000    \n",
      "Epoch 113: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 9.9580e-04 - accuracy: 1.0000 - val_loss: 0.2094 - val_accuracy: 0.9524\n",
      "Epoch 114/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 6.3759e-04 - accuracy: 1.0000\n",
      "Epoch 114: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 9.7267e-04 - accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.9524\n",
      "Epoch 115/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 7.4733e-04 - accuracy: 1.0000\n",
      "Epoch 115: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 9.3038e-04 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9524\n",
      "Epoch 116/350\n",
      "46/49 [===========================>..] - ETA: 0s - loss: 9.3631e-04 - accuracy: 1.0000\n",
      "Epoch 116: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 9.1800e-04 - accuracy: 1.0000 - val_loss: 0.2118 - val_accuracy: 0.9524\n",
      "Epoch 117/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 117: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.7235e-04 - accuracy: 1.0000 - val_loss: 0.2145 - val_accuracy: 0.9524\n",
      "Epoch 118/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 7.4268e-04 - accuracy: 1.0000\n",
      "Epoch 118: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 8.4229e-04 - accuracy: 1.0000 - val_loss: 0.2143 - val_accuracy: 0.9524\n",
      "Epoch 119/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 8.2932e-04 - accuracy: 1.0000\n",
      "Epoch 119: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 8.2357e-04 - accuracy: 1.0000 - val_loss: 0.2157 - val_accuracy: 0.9524\n",
      "Epoch 120/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 8.1159e-04 - accuracy: 1.0000\n",
      "Epoch 120: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 7.8196e-04 - accuracy: 1.0000 - val_loss: 0.2172 - val_accuracy: 0.9524\n",
      "Epoch 121/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 7.9332e-04 - accuracy: 1.0000\n",
      "Epoch 121: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 7.7134e-04 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 0.9524\n",
      "Epoch 122/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 8.3540e-04 - accuracy: 1.0000\n",
      "Epoch 122: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 7.5591e-04 - accuracy: 1.0000 - val_loss: 0.2192 - val_accuracy: 0.9524\n",
      "Epoch 123/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 7.2348e-04 - accuracy: 1.0000\n",
      "Epoch 123: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 7.1589e-04 - accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9524\n",
      "Epoch 124/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 7.4346e-04 - accuracy: 1.0000\n",
      "Epoch 124: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 6.9400e-04 - accuracy: 1.0000 - val_loss: 0.2222 - val_accuracy: 0.9524\n",
      "Epoch 125/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 6.7692e-04 - accuracy: 1.0000\n",
      "Epoch 125: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 6.7786e-04 - accuracy: 1.0000 - val_loss: 0.2231 - val_accuracy: 0.9524\n",
      "Epoch 126/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 6.2259e-04 - accuracy: 1.0000\n",
      "Epoch 126: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 8ms/step - loss: 6.5565e-04 - accuracy: 1.0000 - val_loss: 0.2240 - val_accuracy: 0.9524\n",
      "Epoch 127/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 5.4319e-04 - accuracy: 1.0000\n",
      "Epoch 127: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 6.3759e-04 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9524\n",
      "Epoch 128/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.2339e-04 - accuracy: 1.0000\n",
      "Epoch 128: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 6.0976e-04 - accuracy: 1.0000 - val_loss: 0.2270 - val_accuracy: 0.9524\n",
      "Epoch 129/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 6.2693e-04 - accuracy: 1.0000\n",
      "Epoch 129: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 6.0519e-04 - accuracy: 1.0000 - val_loss: 0.2278 - val_accuracy: 0.9524\n",
      "Epoch 130/350\n",
      "21/49 [===========>..................] - ETA: 0s - loss: 3.9668e-04 - accuracy: 1.0000\n",
      "Epoch 130: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.7756e-04 - accuracy: 1.0000 - val_loss: 0.2293 - val_accuracy: 0.9524\n",
      "Epoch 131/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 6.1480e-04 - accuracy: 1.0000\n",
      "Epoch 131: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.6670e-04 - accuracy: 1.0000 - val_loss: 0.2303 - val_accuracy: 0.9524\n",
      "Epoch 132/350\n",
      "20/49 [===========>..................] - ETA: 0s - loss: 5.4247e-04 - accuracy: 1.0000\n",
      "Epoch 132: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 5.4540e-04 - accuracy: 1.0000 - val_loss: 0.2316 - val_accuracy: 0.9524\n",
      "Epoch 133/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 5.6293e-04 - accuracy: 1.0000\n",
      "Epoch 133: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.2322e-04 - accuracy: 1.0000 - val_loss: 0.2329 - val_accuracy: 0.9524\n",
      "Epoch 134/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 4.4216e-04 - accuracy: 1.0000\n",
      "Epoch 134: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.1304e-04 - accuracy: 1.0000 - val_loss: 0.2337 - val_accuracy: 0.9524\n",
      "Epoch 135/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 5.5773e-04 - accuracy: 1.0000\n",
      "Epoch 135: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.1270e-04 - accuracy: 1.0000 - val_loss: 0.2348 - val_accuracy: 0.9524\n",
      "Epoch 136/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 5.1112e-04 - accuracy: 1.0000\n",
      "Epoch 136: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.9175e-04 - accuracy: 1.0000 - val_loss: 0.2363 - val_accuracy: 0.9524\n",
      "Epoch 137/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 5.1341e-04 - accuracy: 1.0000\n",
      "Epoch 137: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.6155e-04 - accuracy: 1.0000 - val_loss: 0.2378 - val_accuracy: 0.9524\n",
      "Epoch 138/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 4.7625e-04 - accuracy: 1.0000\n",
      "Epoch 138: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.5782e-04 - accuracy: 1.0000 - val_loss: 0.2389 - val_accuracy: 0.9524\n",
      "Epoch 139/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 4.4201e-04 - accuracy: 1.0000\n",
      "Epoch 139: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.4299e-04 - accuracy: 1.0000 - val_loss: 0.2402 - val_accuracy: 0.9524\n",
      "Epoch 140/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 4.5456e-04 - accuracy: 1.0000\n",
      "Epoch 140: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.2682e-04 - accuracy: 1.0000 - val_loss: 0.2405 - val_accuracy: 0.9524\n",
      "Epoch 141/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 4.2117e-04 - accuracy: 1.0000\n",
      "Epoch 141: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 4.1391e-04 - accuracy: 1.0000 - val_loss: 0.2422 - val_accuracy: 0.9524\n",
      "Epoch 142/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 4.4905e-04 - accuracy: 1.0000\n",
      "Epoch 142: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.0095e-04 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9524\n",
      "Epoch 143/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 3.8842e-04 - accuracy: 1.0000\n",
      "Epoch 143: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.8760e-04 - accuracy: 1.0000 - val_loss: 0.2436 - val_accuracy: 0.9524\n",
      "Epoch 144/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 3.9979e-04 - accuracy: 1.0000\n",
      "Epoch 144: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.8249e-04 - accuracy: 1.0000 - val_loss: 0.2452 - val_accuracy: 0.9524\n",
      "Epoch 145/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 4.3224e-04 - accuracy: 1.0000\n",
      "Epoch 145: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.7110e-04 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 0.9524\n",
      "Epoch 146/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 4.1290e-04 - accuracy: 1.0000\n",
      "Epoch 146: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.5888e-04 - accuracy: 1.0000 - val_loss: 0.2475 - val_accuracy: 0.9524\n",
      "Epoch 147/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 2.5226e-04 - accuracy: 1.0000\n",
      "Epoch 147: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.5455e-04 - accuracy: 1.0000 - val_loss: 0.2478 - val_accuracy: 0.9524\n",
      "Epoch 148/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 3.2411e-04 - accuracy: 1.0000\n",
      "Epoch 148: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.3848e-04 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9524\n",
      "Epoch 149/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 3.5044e-04 - accuracy: 1.0000\n",
      "Epoch 149: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3516e-04 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.9524\n",
      "Epoch 150/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 3.0478e-04 - accuracy: 1.0000\n",
      "Epoch 150: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.2005e-04 - accuracy: 1.0000 - val_loss: 0.2515 - val_accuracy: 0.9524\n",
      "Epoch 151/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.4753e-04 - accuracy: 1.0000\n",
      "Epoch 151: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 3.1249e-04 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9524\n",
      "Epoch 152/350\n",
      "22/49 [============>.................] - ETA: 0s - loss: 2.9981e-04 - accuracy: 1.0000\n",
      "Epoch 152: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.0076e-04 - accuracy: 1.0000 - val_loss: 0.2526 - val_accuracy: 0.9524\n",
      "Epoch 153/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 2.5619e-04 - accuracy: 1.0000\n",
      "Epoch 153: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.9039e-04 - accuracy: 1.0000 - val_loss: 0.2537 - val_accuracy: 0.9524\n",
      "Epoch 154/350\n",
      "48/49 [============================>.] - ETA: 0s - loss: 2.8570e-04 - accuracy: 1.0000\n",
      "Epoch 154: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.8220e-04 - accuracy: 1.0000 - val_loss: 0.2551 - val_accuracy: 0.9524\n",
      "Epoch 155/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 3.0463e-04 - accuracy: 1.0000\n",
      "Epoch 155: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.7341e-04 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9524\n",
      "Epoch 156/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 3.0494e-04 - accuracy: 1.0000\n",
      "Epoch 156: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.6760e-04 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 0.9524\n",
      "Epoch 157/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 2.2556e-04 - accuracy: 1.0000\n",
      "Epoch 157: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.6041e-04 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 0.9524\n",
      "Epoch 158/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.6384e-04 - accuracy: 1.0000\n",
      "Epoch 158: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.5513e-04 - accuracy: 1.0000 - val_loss: 0.2588 - val_accuracy: 0.9524\n",
      "Epoch 159/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.4761e-04 - accuracy: 1.0000\n",
      "Epoch 159: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 2.4761e-04 - accuracy: 1.0000 - val_loss: 0.2602 - val_accuracy: 0.9524\n",
      "Epoch 160/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.6701e-04 - accuracy: 1.0000\n",
      "Epoch 160: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.3727e-04 - accuracy: 1.0000 - val_loss: 0.2602 - val_accuracy: 0.9524\n",
      "Epoch 161/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 2.1917e-04 - accuracy: 1.0000\n",
      "Epoch 161: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 2.3219e-04 - accuracy: 1.0000 - val_loss: 0.2607 - val_accuracy: 0.9524\n",
      "Epoch 162/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 1.9597e-04 - accuracy: 1.0000\n",
      "Epoch 162: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.2895e-04 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 0.9524\n",
      "Epoch 163/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 2.0744e-04 - accuracy: 1.0000\n",
      "Epoch 163: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.1679e-04 - accuracy: 1.0000 - val_loss: 0.2633 - val_accuracy: 0.9524\n",
      "Epoch 164/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 1.6711e-04 - accuracy: 1.0000\n",
      "Epoch 164: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.1422e-04 - accuracy: 1.0000 - val_loss: 0.2641 - val_accuracy: 0.9524\n",
      "Epoch 165/350\n",
      "26/49 [==============>...............] - ETA: 0s - loss: 1.7749e-04 - accuracy: 1.0000\n",
      "Epoch 165: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0519e-04 - accuracy: 1.0000 - val_loss: 0.2645 - val_accuracy: 0.9524\n",
      "Epoch 166/350\n",
      "47/49 [===========================>..] - ETA: 0s - loss: 2.0370e-04 - accuracy: 1.0000\n",
      "Epoch 166: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0004e-04 - accuracy: 1.0000 - val_loss: 0.2656 - val_accuracy: 0.9524\n",
      "Epoch 167/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 1.3114e-04 - accuracy: 1.0000\n",
      "Epoch 167: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0354e-04 - accuracy: 1.0000 - val_loss: 0.2665 - val_accuracy: 0.9524\n",
      "Epoch 168/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.8136e-04 - accuracy: 1.0000\n",
      "Epoch 168: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8928e-04 - accuracy: 1.0000 - val_loss: 0.2674 - val_accuracy: 0.9524\n",
      "Epoch 169/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.7480e-04 - accuracy: 1.0000\n",
      "Epoch 169: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8363e-04 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 0.9524\n",
      "Epoch 170/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.6984e-04 - accuracy: 1.0000\n",
      "Epoch 170: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7779e-04 - accuracy: 1.0000 - val_loss: 0.2690 - val_accuracy: 0.9524\n",
      "Epoch 171/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.4393e-04 - accuracy: 1.0000\n",
      "Epoch 171: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.7293e-04 - accuracy: 1.0000 - val_loss: 0.2696 - val_accuracy: 0.9524\n",
      "Epoch 172/350\n",
      "20/49 [===========>..................] - ETA: 0s - loss: 2.1455e-04 - accuracy: 1.0000\n",
      "Epoch 172: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.6958e-04 - accuracy: 1.0000 - val_loss: 0.2712 - val_accuracy: 0.9524\n",
      "Epoch 173/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.7808e-04 - accuracy: 1.0000\n",
      "Epoch 173: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6382e-04 - accuracy: 1.0000 - val_loss: 0.2720 - val_accuracy: 0.9524\n",
      "Epoch 174/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.4029e-04 - accuracy: 1.0000\n",
      "Epoch 174: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5948e-04 - accuracy: 1.0000 - val_loss: 0.2726 - val_accuracy: 0.9524\n",
      "Epoch 175/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.1734e-04 - accuracy: 1.0000\n",
      "Epoch 175: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5641e-04 - accuracy: 1.0000 - val_loss: 0.2729 - val_accuracy: 0.9524\n",
      "Epoch 176/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 1.4766e-04 - accuracy: 1.0000\n",
      "Epoch 176: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.5004e-04 - accuracy: 1.0000 - val_loss: 0.2738 - val_accuracy: 0.9524\n",
      "Epoch 177/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 1.2050e-04 - accuracy: 1.0000\n",
      "Epoch 177: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4582e-04 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9524\n",
      "Epoch 178/350\n",
      "22/49 [============>.................] - ETA: 0s - loss: 1.2560e-04 - accuracy: 1.0000\n",
      "Epoch 178: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.4194e-04 - accuracy: 1.0000 - val_loss: 0.2753 - val_accuracy: 0.9524\n",
      "Epoch 179/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 1.5557e-04 - accuracy: 1.0000\n",
      "Epoch 179: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.3655e-04 - accuracy: 1.0000 - val_loss: 0.2771 - val_accuracy: 0.9524\n",
      "Epoch 180/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 1.0750e-04 - accuracy: 1.0000\n",
      "Epoch 180: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3584e-04 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9524\n",
      "Epoch 181/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 1.0422e-04 - accuracy: 1.0000\n",
      "Epoch 181: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3129e-04 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.9524\n",
      "Epoch 182/350\n",
      "23/49 [=============>................] - ETA: 0s - loss: 1.5569e-04 - accuracy: 1.0000\n",
      "Epoch 182: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.2803e-04 - accuracy: 1.0000 - val_loss: 0.2787 - val_accuracy: 0.9524\n",
      "Epoch 183/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 1.1968e-04 - accuracy: 1.0000\n",
      "Epoch 183: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.2554e-04 - accuracy: 1.0000 - val_loss: 0.2795 - val_accuracy: 0.9524\n",
      "Epoch 184/350\n",
      "27/49 [===============>..............] - ETA: 0s - loss: 8.4760e-05 - accuracy: 1.0000\n",
      "Epoch 184: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.2071e-04 - accuracy: 1.0000 - val_loss: 0.2801 - val_accuracy: 0.9524\n",
      "Epoch 185/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 1.0200e-04 - accuracy: 1.0000\n",
      "Epoch 185: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.1686e-04 - accuracy: 1.0000 - val_loss: 0.2806 - val_accuracy: 0.9524\n",
      "Epoch 186/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 1.2289e-04 - accuracy: 1.0000\n",
      "Epoch 186: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.1303e-04 - accuracy: 1.0000 - val_loss: 0.2820 - val_accuracy: 0.9524\n",
      "Epoch 187/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 1.2227e-04 - accuracy: 1.0000\n",
      "Epoch 187: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.1004e-04 - accuracy: 1.0000 - val_loss: 0.2824 - val_accuracy: 0.9524\n",
      "Epoch 188/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 1.1854e-04 - accuracy: 1.0000\n",
      "Epoch 188: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.0908e-04 - accuracy: 1.0000 - val_loss: 0.2831 - val_accuracy: 0.9524\n",
      "Epoch 189/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.1906e-04 - accuracy: 1.0000\n",
      "Epoch 189: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0409e-04 - accuracy: 1.0000 - val_loss: 0.2841 - val_accuracy: 0.9524\n",
      "Epoch 190/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 1.0541e-04 - accuracy: 1.0000\n",
      "Epoch 190: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 1s 11ms/step - loss: 1.0200e-04 - accuracy: 1.0000 - val_loss: 0.2843 - val_accuracy: 0.9524\n",
      "Epoch 191/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.0386e-04 - accuracy: 1.0000\n",
      "Epoch 191: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 9.9969e-05 - accuracy: 1.0000 - val_loss: 0.2853 - val_accuracy: 0.9524\n",
      "Epoch 192/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.0587e-04 - accuracy: 1.0000\n",
      "Epoch 192: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 9.7921e-05 - accuracy: 1.0000 - val_loss: 0.2865 - val_accuracy: 0.9524\n",
      "Epoch 193/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 1.0194e-04 - accuracy: 1.0000\n",
      "Epoch 193: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 9.5459e-05 - accuracy: 1.0000 - val_loss: 0.2871 - val_accuracy: 0.9524\n",
      "Epoch 194/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 9.2509e-05 - accuracy: 1.0000\n",
      "Epoch 194: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 9.0881e-05 - accuracy: 1.0000 - val_loss: 0.2880 - val_accuracy: 0.9524\n",
      "Epoch 195/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 8.3901e-05 - accuracy: 1.0000\n",
      "Epoch 195: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.9977e-05 - accuracy: 1.0000 - val_loss: 0.2881 - val_accuracy: 0.9524\n",
      "Epoch 196/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 9.0546e-05 - accuracy: 1.0000\n",
      "Epoch 196: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.6203e-05 - accuracy: 1.0000 - val_loss: 0.2893 - val_accuracy: 0.9524\n",
      "Epoch 197/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 6.6564e-05 - accuracy: 1.0000\n",
      "Epoch 197: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 8.5379e-05 - accuracy: 1.0000 - val_loss: 0.2901 - val_accuracy: 0.9524\n",
      "Epoch 198/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 8.7326e-05 - accuracy: 1.0000\n",
      "Epoch 198: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.2391e-05 - accuracy: 1.0000 - val_loss: 0.2899 - val_accuracy: 0.9524\n",
      "Epoch 199/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 8.0629e-05 - accuracy: 1.0000\n",
      "Epoch 199: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 8.0629e-05 - accuracy: 1.0000 - val_loss: 0.2910 - val_accuracy: 0.9524\n",
      "Epoch 200/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 8.2217e-05 - accuracy: 1.0000\n",
      "Epoch 200: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 7.7410e-05 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9524\n",
      "Epoch 201/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 7.1906e-05 - accuracy: 1.0000\n",
      "Epoch 201: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.6005e-05 - accuracy: 1.0000 - val_loss: 0.2931 - val_accuracy: 0.9524\n",
      "Epoch 202/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 7.9222e-05 - accuracy: 1.0000\n",
      "Epoch 202: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.3750e-05 - accuracy: 1.0000 - val_loss: 0.2935 - val_accuracy: 0.9524\n",
      "Epoch 203/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 7.3031e-05 - accuracy: 1.0000\n",
      "Epoch 203: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.1589e-05 - accuracy: 1.0000 - val_loss: 0.2943 - val_accuracy: 0.9524\n",
      "Epoch 204/350\n",
      "28/49 [================>.............] - ETA: 0s - loss: 8.2848e-05 - accuracy: 1.0000\n",
      "Epoch 204: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 7.0328e-05 - accuracy: 1.0000 - val_loss: 0.2947 - val_accuracy: 0.9524\n",
      "Epoch 205/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 5.5959e-05 - accuracy: 1.0000\n",
      "Epoch 205: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.9047e-05 - accuracy: 1.0000 - val_loss: 0.2961 - val_accuracy: 0.9524\n",
      "Epoch 206/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 6.9355e-05 - accuracy: 1.0000\n",
      "Epoch 206: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.5881e-05 - accuracy: 1.0000 - val_loss: 0.2961 - val_accuracy: 0.9524\n",
      "Epoch 207/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.7760e-05 - accuracy: 1.0000\n",
      "Epoch 207: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 6.5989e-05 - accuracy: 1.0000 - val_loss: 0.2971 - val_accuracy: 0.9524\n",
      "Epoch 208/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 6.3869e-05 - accuracy: 1.0000\n",
      "Epoch 208: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.2865e-05 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 0.9524\n",
      "Epoch 209/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 6.7153e-05 - accuracy: 1.0000\n",
      "Epoch 209: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 6.1032e-05 - accuracy: 1.0000 - val_loss: 0.2986 - val_accuracy: 0.9524\n",
      "Epoch 210/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 6.1200e-05 - accuracy: 1.0000\n",
      "Epoch 210: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.9510e-05 - accuracy: 1.0000 - val_loss: 0.2993 - val_accuracy: 0.9524\n",
      "Epoch 211/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 4.9076e-05 - accuracy: 1.0000\n",
      "Epoch 211: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.8148e-05 - accuracy: 1.0000 - val_loss: 0.3001 - val_accuracy: 0.9524\n",
      "Epoch 212/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 5.6221e-05 - accuracy: 1.0000\n",
      "Epoch 212: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.6591e-05 - accuracy: 1.0000 - val_loss: 0.3004 - val_accuracy: 0.9524\n",
      "Epoch 213/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 5.6546e-05 - accuracy: 1.0000\n",
      "Epoch 213: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.5021e-05 - accuracy: 1.0000 - val_loss: 0.3016 - val_accuracy: 0.9524\n",
      "Epoch 214/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 4.8003e-05 - accuracy: 1.0000\n",
      "Epoch 214: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.3638e-05 - accuracy: 1.0000 - val_loss: 0.3019 - val_accuracy: 0.9524\n",
      "Epoch 215/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 4.6848e-05 - accuracy: 1.0000\n",
      "Epoch 215: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 5.2532e-05 - accuracy: 1.0000 - val_loss: 0.3028 - val_accuracy: 0.9524\n",
      "Epoch 216/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 5.4462e-05 - accuracy: 1.0000\n",
      "Epoch 216: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.0805e-05 - accuracy: 1.0000 - val_loss: 0.3035 - val_accuracy: 0.9524\n",
      "Epoch 217/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 5.2627e-05 - accuracy: 1.0000\n",
      "Epoch 217: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.9408e-05 - accuracy: 1.0000 - val_loss: 0.3041 - val_accuracy: 0.9524\n",
      "Epoch 218/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 5.4343e-05 - accuracy: 1.0000\n",
      "Epoch 218: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.8212e-05 - accuracy: 1.0000 - val_loss: 0.3052 - val_accuracy: 0.9524\n",
      "Epoch 219/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 4.8912e-05 - accuracy: 1.0000\n",
      "Epoch 219: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.7406e-05 - accuracy: 1.0000 - val_loss: 0.3057 - val_accuracy: 0.9524\n",
      "Epoch 220/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 4.6014e-05 - accuracy: 1.0000\n",
      "Epoch 220: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 4.5860e-05 - accuracy: 1.0000 - val_loss: 0.3065 - val_accuracy: 0.9524\n",
      "Epoch 221/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 5.0694e-05 - accuracy: 1.0000\n",
      "Epoch 221: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.4969e-05 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9524\n",
      "Epoch 222/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.4834e-05 - accuracy: 1.0000\n",
      "Epoch 222: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.3254e-05 - accuracy: 1.0000 - val_loss: 0.3074 - val_accuracy: 0.9524\n",
      "Epoch 223/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.5199e-05 - accuracy: 1.0000\n",
      "Epoch 223: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.2281e-05 - accuracy: 1.0000 - val_loss: 0.3085 - val_accuracy: 0.9524\n",
      "Epoch 224/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 4.3252e-05 - accuracy: 1.0000\n",
      "Epoch 224: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.1332e-05 - accuracy: 1.0000 - val_loss: 0.3093 - val_accuracy: 0.9524\n",
      "Epoch 225/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 4.3032e-05 - accuracy: 1.0000\n",
      "Epoch 225: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.0755e-05 - accuracy: 1.0000 - val_loss: 0.3099 - val_accuracy: 0.9524\n",
      "Epoch 226/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 4.2870e-05 - accuracy: 1.0000\n",
      "Epoch 226: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.9076e-05 - accuracy: 1.0000 - val_loss: 0.3109 - val_accuracy: 0.9524\n",
      "Epoch 227/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.7013e-05 - accuracy: 1.0000\n",
      "Epoch 227: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.9168e-05 - accuracy: 1.0000 - val_loss: 0.3122 - val_accuracy: 0.9524\n",
      "Epoch 228/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 2.9662e-05 - accuracy: 1.0000\n",
      "Epoch 228: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.6695e-05 - accuracy: 1.0000 - val_loss: 0.3114 - val_accuracy: 0.9524\n",
      "Epoch 229/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.5642e-05 - accuracy: 1.0000\n",
      "Epoch 229: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.5930e-05 - accuracy: 1.0000 - val_loss: 0.3126 - val_accuracy: 0.9524\n",
      "Epoch 230/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.8642e-05 - accuracy: 1.0000\n",
      "Epoch 230: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.4974e-05 - accuracy: 1.0000 - val_loss: 0.3134 - val_accuracy: 0.9524\n",
      "Epoch 231/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 3.6200e-05 - accuracy: 1.0000\n",
      "Epoch 231: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.4336e-05 - accuracy: 1.0000 - val_loss: 0.3140 - val_accuracy: 0.9524\n",
      "Epoch 232/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.5428e-05 - accuracy: 1.0000\n",
      "Epoch 232: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3805e-05 - accuracy: 1.0000 - val_loss: 0.3148 - val_accuracy: 0.9524\n",
      "Epoch 233/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.2266e-05 - accuracy: 1.0000\n",
      "Epoch 233: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.2499e-05 - accuracy: 1.0000 - val_loss: 0.3155 - val_accuracy: 0.9524\n",
      "Epoch 234/350\n",
      "23/49 [=============>................] - ETA: 0s - loss: 2.9751e-05 - accuracy: 1.0000\n",
      "Epoch 234: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.1412e-05 - accuracy: 1.0000 - val_loss: 0.3164 - val_accuracy: 0.9524\n",
      "Epoch 235/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 2.7613e-05 - accuracy: 1.0000\n",
      "Epoch 235: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.0914e-05 - accuracy: 1.0000 - val_loss: 0.3169 - val_accuracy: 0.9524\n",
      "Epoch 236/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.9012e-05 - accuracy: 1.0000\n",
      "Epoch 236: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.0048e-05 - accuracy: 1.0000 - val_loss: 0.3178 - val_accuracy: 0.9524\n",
      "Epoch 237/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 2.3099e-05 - accuracy: 1.0000\n",
      "Epoch 237: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.9646e-05 - accuracy: 1.0000 - val_loss: 0.3183 - val_accuracy: 0.9524\n",
      "Epoch 238/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 2.6509e-05 - accuracy: 1.0000\n",
      "Epoch 238: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.8982e-05 - accuracy: 1.0000 - val_loss: 0.3187 - val_accuracy: 0.9524\n",
      "Epoch 239/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 2.5498e-05 - accuracy: 1.0000\n",
      "Epoch 239: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.7608e-05 - accuracy: 1.0000 - val_loss: 0.3189 - val_accuracy: 0.9524\n",
      "Epoch 240/350\n",
      "44/49 [=========================>....] - ETA: 0s - loss: 2.8152e-05 - accuracy: 1.0000\n",
      "Epoch 240: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.6929e-05 - accuracy: 1.0000 - val_loss: 0.3201 - val_accuracy: 0.9524\n",
      "Epoch 241/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.8577e-05 - accuracy: 1.0000\n",
      "Epoch 241: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.6321e-05 - accuracy: 1.0000 - val_loss: 0.3202 - val_accuracy: 0.9524\n",
      "Epoch 242/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.5298e-05 - accuracy: 1.0000\n",
      "Epoch 242: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.5198e-05 - accuracy: 1.0000 - val_loss: 0.3213 - val_accuracy: 0.9524\n",
      "Epoch 243/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 2.4519e-05 - accuracy: 1.0000\n",
      "Epoch 243: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.4557e-05 - accuracy: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.9524\n",
      "Epoch 244/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 2.6171e-05 - accuracy: 1.0000\n",
      "Epoch 244: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.3924e-05 - accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 0.9524\n",
      "Epoch 245/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 2.1330e-05 - accuracy: 1.0000\n",
      "Epoch 245: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.3306e-05 - accuracy: 1.0000 - val_loss: 0.3236 - val_accuracy: 0.9524\n",
      "Epoch 246/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 2.2681e-05 - accuracy: 1.0000\n",
      "Epoch 246: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.2680e-05 - accuracy: 1.0000 - val_loss: 0.3241 - val_accuracy: 0.9524\n",
      "Epoch 247/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 1.8956e-05 - accuracy: 1.0000\n",
      "Epoch 247: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.2235e-05 - accuracy: 1.0000 - val_loss: 0.3244 - val_accuracy: 0.9524\n",
      "Epoch 248/350\n",
      "23/49 [=============>................] - ETA: 0s - loss: 1.6557e-05 - accuracy: 1.0000\n",
      "Epoch 248: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.1346e-05 - accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9524\n",
      "Epoch 249/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 1.7619e-05 - accuracy: 1.0000\n",
      "Epoch 249: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.1331e-05 - accuracy: 1.0000 - val_loss: 0.3261 - val_accuracy: 0.9524\n",
      "Epoch 250/350\n",
      "35/49 [====================>.........] - ETA: 0s - loss: 2.1096e-05 - accuracy: 1.0000\n",
      "Epoch 250: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0607e-05 - accuracy: 1.0000 - val_loss: 0.3269 - val_accuracy: 0.9524\n",
      "Epoch 251/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.8113e-05 - accuracy: 1.0000\n",
      "Epoch 251: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.0073e-05 - accuracy: 1.0000 - val_loss: 0.3271 - val_accuracy: 0.9524\n",
      "Epoch 252/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.7138e-05 - accuracy: 1.0000\n",
      "Epoch 252: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.9354e-05 - accuracy: 1.0000 - val_loss: 0.3281 - val_accuracy: 0.9524\n",
      "Epoch 253/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.8788e-05 - accuracy: 1.0000\n",
      "Epoch 253: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8778e-05 - accuracy: 1.0000 - val_loss: 0.3288 - val_accuracy: 0.9524\n",
      "Epoch 254/350\n",
      "29/49 [================>.............] - ETA: 0s - loss: 1.3672e-05 - accuracy: 1.0000\n",
      "Epoch 254: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.8530e-05 - accuracy: 1.0000 - val_loss: 0.3290 - val_accuracy: 0.9524\n",
      "Epoch 255/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.9255e-05 - accuracy: 1.0000\n",
      "Epoch 255: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8244e-05 - accuracy: 1.0000 - val_loss: 0.3305 - val_accuracy: 0.9524\n",
      "Epoch 256/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.8560e-05 - accuracy: 1.0000\n",
      "Epoch 256: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.7442e-05 - accuracy: 1.0000 - val_loss: 0.3308 - val_accuracy: 0.9524\n",
      "Epoch 257/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.9109e-05 - accuracy: 1.0000\n",
      "Epoch 257: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7195e-05 - accuracy: 1.0000 - val_loss: 0.3316 - val_accuracy: 0.9524\n",
      "Epoch 258/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 1.5583e-05 - accuracy: 1.0000\n",
      "Epoch 258: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6552e-05 - accuracy: 1.0000 - val_loss: 0.3318 - val_accuracy: 0.9524\n",
      "Epoch 259/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 1.6539e-05 - accuracy: 1.0000\n",
      "Epoch 259: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6193e-05 - accuracy: 1.0000 - val_loss: 0.3325 - val_accuracy: 0.9524\n",
      "Epoch 260/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.4213e-05 - accuracy: 1.0000\n",
      "Epoch 260: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5700e-05 - accuracy: 1.0000 - val_loss: 0.3333 - val_accuracy: 0.9524\n",
      "Epoch 261/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.6524e-05 - accuracy: 1.0000\n",
      "Epoch 261: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5273e-05 - accuracy: 1.0000 - val_loss: 0.3336 - val_accuracy: 0.9524\n",
      "Epoch 262/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.4988e-05 - accuracy: 1.0000\n",
      "Epoch 262: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4934e-05 - accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 0.9524\n",
      "Epoch 263/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.5764e-05 - accuracy: 1.0000\n",
      "Epoch 263: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.4547e-05 - accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.9524\n",
      "Epoch 264/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 1.3833e-05 - accuracy: 1.0000\n",
      "Epoch 264: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.4132e-05 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9524\n",
      "Epoch 265/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.5373e-05 - accuracy: 1.0000\n",
      "Epoch 265: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3790e-05 - accuracy: 1.0000 - val_loss: 0.3367 - val_accuracy: 0.9524\n",
      "Epoch 266/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 1.4118e-05 - accuracy: 1.0000\n",
      "Epoch 266: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.3378e-05 - accuracy: 1.0000 - val_loss: 0.3376 - val_accuracy: 0.9524\n",
      "Epoch 267/350\n",
      "31/49 [=================>............] - ETA: 0s - loss: 1.2972e-05 - accuracy: 1.0000\n",
      "Epoch 267: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.3104e-05 - accuracy: 1.0000 - val_loss: 0.3381 - val_accuracy: 0.9524\n",
      "Epoch 268/350\n",
      "25/49 [==============>...............] - ETA: 0s - loss: 1.5905e-05 - accuracy: 1.0000\n",
      "Epoch 268: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.2857e-05 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9524\n",
      "Epoch 269/350\n",
      "30/49 [=================>............] - ETA: 0s - loss: 1.2462e-05 - accuracy: 1.0000\n",
      "Epoch 269: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.2602e-05 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.9524\n",
      "Epoch 270/350\n",
      "45/49 [==========================>...] - ETA: 0s - loss: 1.2544e-05 - accuracy: 1.0000\n",
      "Epoch 270: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 1.2083e-05 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.9524\n",
      "Epoch 271/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 1.2732e-05 - accuracy: 1.0000\n",
      "Epoch 271: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1882e-05 - accuracy: 1.0000 - val_loss: 0.3410 - val_accuracy: 0.9524\n",
      "Epoch 272/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.2399e-05 - accuracy: 1.0000\n",
      "Epoch 272: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1578e-05 - accuracy: 1.0000 - val_loss: 0.3411 - val_accuracy: 0.9524\n",
      "Epoch 273/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.1236e-05 - accuracy: 1.0000\n",
      "Epoch 273: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1252e-05 - accuracy: 1.0000 - val_loss: 0.3417 - val_accuracy: 0.9524\n",
      "Epoch 274/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.1839e-05 - accuracy: 1.0000\n",
      "Epoch 274: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.1060e-05 - accuracy: 1.0000 - val_loss: 0.3427 - val_accuracy: 0.9524\n",
      "Epoch 275/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 1.0475e-05 - accuracy: 1.0000\n",
      "Epoch 275: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0656e-05 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9524\n",
      "Epoch 276/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.1046e-05 - accuracy: 1.0000\n",
      "Epoch 276: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.0528e-05 - accuracy: 1.0000 - val_loss: 0.3436 - val_accuracy: 0.9524\n",
      "Epoch 277/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 8.8946e-06 - accuracy: 1.0000\n",
      "Epoch 277: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.0239e-05 - accuracy: 1.0000 - val_loss: 0.3445 - val_accuracy: 0.9524\n",
      "Epoch 278/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.0739e-05 - accuracy: 1.0000\n",
      "Epoch 278: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.8802e-06 - accuracy: 1.0000 - val_loss: 0.3451 - val_accuracy: 0.9524\n",
      "Epoch 279/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.0216e-05 - accuracy: 1.0000\n",
      "Epoch 279: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.6744e-06 - accuracy: 1.0000 - val_loss: 0.3460 - val_accuracy: 0.9524\n",
      "Epoch 280/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 9.5692e-06 - accuracy: 1.0000\n",
      "Epoch 280: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.4026e-06 - accuracy: 1.0000 - val_loss: 0.3463 - val_accuracy: 0.9524\n",
      "Epoch 281/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 8.2910e-06 - accuracy: 1.0000\n",
      "Epoch 281: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 9.2064e-06 - accuracy: 1.0000 - val_loss: 0.3476 - val_accuracy: 0.9524\n",
      "Epoch 282/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 8.7688e-06 - accuracy: 1.0000\n",
      "Epoch 282: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.8696e-06 - accuracy: 1.0000 - val_loss: 0.3477 - val_accuracy: 0.9524\n",
      "Epoch 283/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 9.2036e-06 - accuracy: 1.0000\n",
      "Epoch 283: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.7260e-06 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.9524\n",
      "Epoch 284/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 7.6224e-06 - accuracy: 1.0000\n",
      "Epoch 284: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 8.5300e-06 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.9524\n",
      "Epoch 285/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 8.8643e-06 - accuracy: 1.0000\n",
      "Epoch 285: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.2009e-06 - accuracy: 1.0000 - val_loss: 0.3496 - val_accuracy: 0.9524\n",
      "Epoch 286/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 6.8122e-06 - accuracy: 1.0000\n",
      "Epoch 286: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.1327e-06 - accuracy: 1.0000 - val_loss: 0.3497 - val_accuracy: 0.9524\n",
      "Epoch 287/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 8.4111e-06 - accuracy: 1.0000\n",
      "Epoch 287: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 8.1430e-06 - accuracy: 1.0000 - val_loss: 0.3509 - val_accuracy: 0.9524\n",
      "Epoch 288/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 7.7178e-06 - accuracy: 1.0000\n",
      "Epoch 288: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.6597e-06 - accuracy: 1.0000 - val_loss: 0.3515 - val_accuracy: 0.9524\n",
      "Epoch 289/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 6.8747e-06 - accuracy: 1.0000\n",
      "Epoch 289: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.4056e-06 - accuracy: 1.0000 - val_loss: 0.3529 - val_accuracy: 0.9524\n",
      "Epoch 290/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 7.1802e-06 - accuracy: 1.0000\n",
      "Epoch 290: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 7.2665e-06 - accuracy: 1.0000 - val_loss: 0.3531 - val_accuracy: 0.9524\n",
      "Epoch 291/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 5.5728e-06 - accuracy: 1.0000\n",
      "Epoch 291: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 7.1374e-06 - accuracy: 1.0000 - val_loss: 0.3537 - val_accuracy: 0.9524\n",
      "Epoch 292/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.3575e-06 - accuracy: 1.0000\n",
      "Epoch 292: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.8277e-06 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.9524\n",
      "Epoch 293/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 7.2572e-06 - accuracy: 1.0000\n",
      "Epoch 293: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.7354e-06 - accuracy: 1.0000 - val_loss: 0.3548 - val_accuracy: 0.9524\n",
      "Epoch 294/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 5.6710e-06 - accuracy: 1.0000\n",
      "Epoch 294: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.6366e-06 - accuracy: 1.0000 - val_loss: 0.3554 - val_accuracy: 0.9524\n",
      "Epoch 295/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 6.2486e-06 - accuracy: 1.0000\n",
      "Epoch 295: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.5772e-06 - accuracy: 1.0000 - val_loss: 0.3561 - val_accuracy: 0.9524\n",
      "Epoch 296/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.0331e-06 - accuracy: 1.0000\n",
      "Epoch 296: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 6.2956e-06 - accuracy: 1.0000 - val_loss: 0.3566 - val_accuracy: 0.9524\n",
      "Epoch 297/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.3237e-06 - accuracy: 1.0000\n",
      "Epoch 297: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 6.0345e-06 - accuracy: 1.0000 - val_loss: 0.3575 - val_accuracy: 0.9524\n",
      "Epoch 298/350\n",
      "36/49 [=====================>........] - ETA: 0s - loss: 6.1259e-06 - accuracy: 1.0000\n",
      "Epoch 298: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.8883e-06 - accuracy: 1.0000 - val_loss: 0.3580 - val_accuracy: 0.9524\n",
      "Epoch 299/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 6.1277e-06 - accuracy: 1.0000\n",
      "Epoch 299: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.7553e-06 - accuracy: 1.0000 - val_loss: 0.3587 - val_accuracy: 0.9524\n",
      "Epoch 300/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 5.8315e-06 - accuracy: 1.0000\n",
      "Epoch 300: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.6060e-06 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.9524\n",
      "Epoch 301/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 5.6977e-06 - accuracy: 1.0000\n",
      "Epoch 301: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.5048e-06 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.9524\n",
      "Epoch 302/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 5.6239e-06 - accuracy: 1.0000\n",
      "Epoch 302: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.2713e-06 - accuracy: 1.0000 - val_loss: 0.3604 - val_accuracy: 0.9524\n",
      "Epoch 303/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 5.3054e-06 - accuracy: 1.0000\n",
      "Epoch 303: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.1996e-06 - accuracy: 1.0000 - val_loss: 0.3611 - val_accuracy: 0.9524\n",
      "Epoch 304/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 5.1259e-06 - accuracy: 1.0000\n",
      "Epoch 304: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 5.0706e-06 - accuracy: 1.0000 - val_loss: 0.3618 - val_accuracy: 0.9524\n",
      "Epoch 305/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 5.1515e-06 - accuracy: 1.0000\n",
      "Epoch 305: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.9181e-06 - accuracy: 1.0000 - val_loss: 0.3621 - val_accuracy: 0.9524\n",
      "Epoch 306/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.8552e-06 - accuracy: 1.0000\n",
      "Epoch 306: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.7994e-06 - accuracy: 1.0000 - val_loss: 0.3627 - val_accuracy: 0.9524\n",
      "Epoch 307/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 4.5810e-06 - accuracy: 1.0000\n",
      "Epoch 307: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.6997e-06 - accuracy: 1.0000 - val_loss: 0.3635 - val_accuracy: 0.9464\n",
      "Epoch 308/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 4.7000e-06 - accuracy: 1.0000\n",
      "Epoch 308: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.5639e-06 - accuracy: 1.0000 - val_loss: 0.3639 - val_accuracy: 0.9464\n",
      "Epoch 309/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 4.8591e-06 - accuracy: 1.0000\n",
      "Epoch 309: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 4.4842e-06 - accuracy: 1.0000 - val_loss: 0.3647 - val_accuracy: 0.9464\n",
      "Epoch 310/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 4.0064e-06 - accuracy: 1.0000\n",
      "Epoch 310: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.3473e-06 - accuracy: 1.0000 - val_loss: 0.3653 - val_accuracy: 0.9464\n",
      "Epoch 311/350\n",
      "32/49 [==================>...........] - ETA: 0s - loss: 4.0059e-06 - accuracy: 1.0000\n",
      "Epoch 311: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.2282e-06 - accuracy: 1.0000 - val_loss: 0.3659 - val_accuracy: 0.9464\n",
      "Epoch 312/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 4.5992e-06 - accuracy: 1.0000\n",
      "Epoch 312: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.0817e-06 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 0.9464\n",
      "Epoch 313/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.3696e-06 - accuracy: 1.0000\n",
      "Epoch 313: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 4.0772e-06 - accuracy: 1.0000 - val_loss: 0.3671 - val_accuracy: 0.9524\n",
      "Epoch 314/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.6570e-06 - accuracy: 1.0000\n",
      "Epoch 314: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.9245e-06 - accuracy: 1.0000 - val_loss: 0.3674 - val_accuracy: 0.9464\n",
      "Epoch 315/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.4546e-06 - accuracy: 1.0000\n",
      "Epoch 315: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.8216e-06 - accuracy: 1.0000 - val_loss: 0.3685 - val_accuracy: 0.9464\n",
      "Epoch 316/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 3.9638e-06 - accuracy: 1.0000\n",
      "Epoch 316: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.7542e-06 - accuracy: 1.0000 - val_loss: 0.3691 - val_accuracy: 0.9464\n",
      "Epoch 317/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.7180e-06 - accuracy: 1.0000\n",
      "Epoch 317: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.6577e-06 - accuracy: 1.0000 - val_loss: 0.3701 - val_accuracy: 0.9464\n",
      "Epoch 318/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.3925e-06 - accuracy: 1.0000\n",
      "Epoch 318: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.5525e-06 - accuracy: 1.0000 - val_loss: 0.3709 - val_accuracy: 0.9464\n",
      "Epoch 319/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.5394e-06 - accuracy: 1.0000\n",
      "Epoch 319: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.4502e-06 - accuracy: 1.0000 - val_loss: 0.3707 - val_accuracy: 0.9464\n",
      "Epoch 320/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 2.9789e-06 - accuracy: 1.0000\n",
      "Epoch 320: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3723e-06 - accuracy: 1.0000 - val_loss: 0.3714 - val_accuracy: 0.9464\n",
      "Epoch 321/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.4551e-06 - accuracy: 1.0000\n",
      "Epoch 321: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.3233e-06 - accuracy: 1.0000 - val_loss: 0.3722 - val_accuracy: 0.9464\n",
      "Epoch 322/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 3.3023e-06 - accuracy: 1.0000\n",
      "Epoch 322: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.2045e-06 - accuracy: 1.0000 - val_loss: 0.3730 - val_accuracy: 0.9464\n",
      "Epoch 323/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 3.1193e-06 - accuracy: 1.0000\n",
      "Epoch 323: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 3.1193e-06 - accuracy: 1.0000 - val_loss: 0.3732 - val_accuracy: 0.9464\n",
      "Epoch 324/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 2.7965e-06 - accuracy: 1.0000\n",
      "Epoch 324: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 3.0370e-06 - accuracy: 1.0000 - val_loss: 0.3739 - val_accuracy: 0.9464\n",
      "Epoch 325/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 3.2229e-06 - accuracy: 1.0000\n",
      "Epoch 325: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.9547e-06 - accuracy: 1.0000 - val_loss: 0.3748 - val_accuracy: 0.9464\n",
      "Epoch 326/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.0980e-06 - accuracy: 1.0000\n",
      "Epoch 326: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.8990e-06 - accuracy: 1.0000 - val_loss: 0.3754 - val_accuracy: 0.9464\n",
      "Epoch 327/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 3.1322e-06 - accuracy: 1.0000\n",
      "Epoch 327: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.8062e-06 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.9464\n",
      "Epoch 328/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.8040e-06 - accuracy: 1.0000\n",
      "Epoch 328: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.7760e-06 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.9464\n",
      "Epoch 329/350\n",
      "33/49 [===================>..........] - ETA: 0s - loss: 2.9983e-06 - accuracy: 1.0000\n",
      "Epoch 329: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.6784e-06 - accuracy: 1.0000 - val_loss: 0.3770 - val_accuracy: 0.9464\n",
      "Epoch 330/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.8975e-06 - accuracy: 1.0000\n",
      "Epoch 330: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.6132e-06 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9464\n",
      "Epoch 331/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 2.7116e-06 - accuracy: 1.0000\n",
      "Epoch 331: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.5368e-06 - accuracy: 1.0000 - val_loss: 0.3781 - val_accuracy: 0.9464\n",
      "Epoch 332/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 2.5718e-06 - accuracy: 1.0000\n",
      "Epoch 332: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.4704e-06 - accuracy: 1.0000 - val_loss: 0.3789 - val_accuracy: 0.9524\n",
      "Epoch 333/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 2.1020e-06 - accuracy: 1.0000\n",
      "Epoch 333: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 4ms/step - loss: 2.4261e-06 - accuracy: 1.0000 - val_loss: 0.3799 - val_accuracy: 0.9524\n",
      "Epoch 334/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 2.2278e-06 - accuracy: 1.0000\n",
      "Epoch 334: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.3477e-06 - accuracy: 1.0000 - val_loss: 0.3803 - val_accuracy: 0.9524\n",
      "Epoch 335/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 2.3773e-06 - accuracy: 1.0000\n",
      "Epoch 335: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.2979e-06 - accuracy: 1.0000 - val_loss: 0.3806 - val_accuracy: 0.9524\n",
      "Epoch 336/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 2.3264e-06 - accuracy: 1.0000\n",
      "Epoch 336: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.2369e-06 - accuracy: 1.0000 - val_loss: 0.3816 - val_accuracy: 0.9524\n",
      "Epoch 337/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 2.2962e-06 - accuracy: 1.0000\n",
      "Epoch 337: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.2035e-06 - accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 0.9524\n",
      "Epoch 338/350\n",
      "34/49 [===================>..........] - ETA: 0s - loss: 2.4008e-06 - accuracy: 1.0000\n",
      "Epoch 338: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.1333e-06 - accuracy: 1.0000 - val_loss: 0.3828 - val_accuracy: 0.9524\n",
      "Epoch 339/350\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0788e-06 - accuracy: 1.0000\n",
      "Epoch 339: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 2.0788e-06 - accuracy: 1.0000 - val_loss: 0.3837 - val_accuracy: 0.9524\n",
      "Epoch 340/350\n",
      "38/49 [======================>.......] - ETA: 0s - loss: 2.0718e-06 - accuracy: 1.0000\n",
      "Epoch 340: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 2.0356e-06 - accuracy: 1.0000 - val_loss: 0.3840 - val_accuracy: 0.9524\n",
      "Epoch 341/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.8599e-06 - accuracy: 1.0000\n",
      "Epoch 341: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.9799e-06 - accuracy: 1.0000 - val_loss: 0.3845 - val_accuracy: 0.9524\n",
      "Epoch 342/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.8692e-06 - accuracy: 1.0000\n",
      "Epoch 342: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.9158e-06 - accuracy: 1.0000 - val_loss: 0.3855 - val_accuracy: 0.9524\n",
      "Epoch 343/350\n",
      "37/49 [=====================>........] - ETA: 0s - loss: 1.9430e-06 - accuracy: 1.0000\n",
      "Epoch 343: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8829e-06 - accuracy: 1.0000 - val_loss: 0.3856 - val_accuracy: 0.9524\n",
      "Epoch 344/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.8979e-06 - accuracy: 1.0000\n",
      "Epoch 344: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.8490e-06 - accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9524\n",
      "Epoch 345/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.8822e-06 - accuracy: 1.0000\n",
      "Epoch 345: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7849e-06 - accuracy: 1.0000 - val_loss: 0.3874 - val_accuracy: 0.9524\n",
      "Epoch 346/350\n",
      "43/49 [=========================>....] - ETA: 0s - loss: 1.6634e-06 - accuracy: 1.0000\n",
      "Epoch 346: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7283e-06 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 0.9524\n",
      "Epoch 347/350\n",
      "40/49 [=======================>......] - ETA: 0s - loss: 1.6622e-06 - accuracy: 1.0000\n",
      "Epoch 347: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.7039e-06 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 0.9524\n",
      "Epoch 348/350\n",
      "42/49 [========================>.....] - ETA: 0s - loss: 1.7334e-06 - accuracy: 1.0000\n",
      "Epoch 348: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.6557e-06 - accuracy: 1.0000 - val_loss: 0.3886 - val_accuracy: 0.9524\n",
      "Epoch 349/350\n",
      "41/49 [========================>.....] - ETA: 0s - loss: 1.6179e-06 - accuracy: 1.0000\n",
      "Epoch 349: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 1.6118e-06 - accuracy: 1.0000 - val_loss: 0.3896 - val_accuracy: 0.9524\n",
      "Epoch 350/350\n",
      "39/49 [======================>.......] - ETA: 0s - loss: 1.5798e-06 - accuracy: 1.0000\n",
      "Epoch 350: val_loss did not improve from 0.12449\n",
      "49/49 [==============================] - 0s 2ms/step - loss: 1.5654e-06 - accuracy: 1.0000 - val_loss: 0.3899 - val_accuracy: 0.9524\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_4 (SimpleRNN)    (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.9524\n",
      "Test Loss: 0.3898967206478119\n",
      "Test Accuracy: 0.9523809552192688\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer \n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, epochs=350, batch_size=8, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot method for K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1 (train_loss, train_acc, val_loss, val_acc):\n",
    "    \n",
    "    avg_train_loss = np.mean(train_loss, axis=0)\n",
    "    avg_train_acc = np.mean(train_acc, axis=0)\n",
    "    avg_val_loss = np.mean(val_loss, axis=0)\n",
    "    avg_val_acc = np.mean(train_acc, axis=0)\n",
    "\n",
    "    # Plot delle curve di apprendimento mediate sulle K fold\n",
    "\n",
    "    epochs = range(1, len(train_loss[0]) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_loss, label='Training loss')\n",
    "    plt.plot(epochs, avg_val_loss, label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg_loss')\n",
    "    plt.title('Average train and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, avg_val_acc, label='Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg_accuracy')\n",
    "    plt.title('Average train and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation for the Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing vanilla RNN with K-fold\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.68943, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.68943 to 0.61284, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.61284 to 0.55715, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.55715 to 0.51534, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.51534 to 0.48297, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.48297 to 0.45693, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.45693 to 0.43575, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.43575 to 0.41810, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.41810 to 0.40304, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.40304 to 0.38989, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.38989 to 0.37841, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.37841 to 0.36837, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.36837 to 0.35952, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.35952 to 0.35156, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.35156 to 0.34423, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.34423 to 0.33752, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.33752 to 0.33135, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.33135 to 0.32568, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.32568 to 0.32059, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.32059 to 0.31577, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.31577 to 0.31129, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.31129 to 0.30715, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.30715 to 0.30343, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.30343 to 0.29983, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.29983 to 0.29653, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.29653 to 0.29341, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.29341 to 0.29048, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.29048 to 0.28775, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.28775 to 0.28519, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.28519 to 0.28276, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.28276 to 0.28042, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.28042 to 0.27824, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.27824 to 0.27619, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.27619 to 0.27422, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.27422 to 0.27234, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.27234 to 0.27057, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.27057 to 0.26882, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.26882 to 0.26717, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.26717 to 0.26561, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.26561 to 0.26413, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.26413 to 0.26266, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.26266 to 0.26121, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.26121 to 0.25979, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.25979 to 0.25847, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.25847 to 0.25718, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.25718 to 0.25592, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.25592 to 0.25465, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.25465 to 0.25346, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.25346 to 0.25231, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.25231 to 0.25117, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.25117 to 0.25011, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.25011 to 0.24902, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.24902 to 0.24800, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.24800 to 0.24700, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.24700 to 0.24604, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.24604 to 0.24510, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.24510 to 0.24417, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.24417 to 0.24323, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.24323 to 0.24239, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.24239 to 0.24151, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.24151 to 0.24066, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.24066 to 0.23983, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.23983 to 0.23901, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.23901 to 0.23822, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.23822 to 0.23744, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.23744 to 0.23665, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.23665 to 0.23590, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.23590 to 0.23518, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.23518 to 0.23447, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.23447 to 0.23374, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.23374 to 0.23304, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.23304 to 0.23234, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.23234 to 0.23166, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.23166 to 0.23098, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.23098 to 0.23037, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.23037 to 0.22975, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.22975 to 0.22913, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.22913 to 0.22849, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.22849 to 0.22783, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.22783 to 0.22720, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.22720 to 0.22660, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.22660 to 0.22602, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.22602 to 0.22549, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.22549 to 0.22488, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.22488 to 0.22432, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.22432 to 0.22378, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.22378 to 0.22320, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.22320 to 0.22263, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.22263 to 0.22210, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.22210 to 0.22154, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.22154 to 0.22101, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.22101 to 0.22049, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.22049 to 0.21999, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.21999 to 0.21947, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.21947 to 0.21896, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.21896 to 0.21846, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.21846 to 0.21797, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.21797 to 0.21747, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.21747 to 0.21699, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.21699 to 0.21652, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.21652 to 0.21609, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.21609 to 0.21562, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.21562 to 0.21516, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.21516 to 0.21475, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.21475 to 0.21430, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.21430 to 0.21382, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.21382 to 0.21337, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.21337 to 0.21292, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.21292 to 0.21249, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.21249 to 0.21203, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.21203 to 0.21163, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.21163 to 0.21117, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.21117 to 0.21076, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.21076 to 0.21034, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.21034 to 0.20993, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.20993 to 0.20953, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.20953 to 0.20918, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.20918 to 0.20878, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.20878 to 0.20835, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.20835 to 0.20795, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.20795 to 0.20757, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.20757 to 0.20719, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.20719 to 0.20680, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.20680 to 0.20644, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.20644 to 0.20607, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.20607 to 0.20573, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.20573 to 0.20537, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.20537 to 0.20500, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.20500 to 0.20465, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.20465 to 0.20428, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.20428 to 0.20394, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.20394 to 0.20358, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.20358 to 0.20325, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.20325 to 0.20290, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.20290 to 0.20255, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.20255 to 0.20221, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.20221 to 0.20187, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.20187 to 0.20152, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.20152 to 0.20117, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.20117 to 0.20082, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.20082 to 0.20047, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.20047 to 0.20016, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.20016 to 0.19983, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.19983 to 0.19949, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.19949 to 0.19915, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.19915 to 0.19884, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.19884 to 0.19852, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.19852 to 0.19822, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.19822 to 0.19795, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.19795 to 0.19763, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.19763 to 0.19732, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.19732 to 0.19704, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.19704 to 0.19675, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.19675 to 0.19644, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.19644 to 0.19615, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.19615 to 0.19586, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.19586 to 0.19557, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.19557 to 0.19530, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.19530 to 0.19500, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.19500 to 0.19475, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.19475 to 0.19445, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.19445 to 0.19417, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.19417 to 0.19389, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.19389 to 0.19360, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.19360 to 0.19329, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.19329 to 0.19300, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.19300 to 0.19273, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.19273 to 0.19244, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.19244 to 0.19216, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.19216 to 0.19193, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.19193 to 0.19166, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.19166 to 0.19139, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.19139 to 0.19112, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.19112 to 0.19084, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.19084 to 0.19057, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.19057 to 0.19032, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.19032 to 0.19005, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.19005 to 0.18983, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.18983 to 0.18957, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.18957 to 0.18931, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.18931 to 0.18906, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.18906 to 0.18882, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.18882 to 0.18856, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.18856 to 0.18829, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.18829 to 0.18804, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.18804 to 0.18778, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.18778 to 0.18753, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.18753 to 0.18728, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.18728 to 0.18704, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.18704 to 0.18681, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.18681 to 0.18654, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.18654 to 0.18632, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.18632 to 0.18616, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.18616 to 0.18593, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.18593 to 0.18571, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.18571 to 0.18545, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.18545 to 0.18520, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.18520 to 0.18497, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.18497 to 0.18475, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.18475 to 0.18454, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.18454 to 0.18433, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.18433 to 0.18411, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.18411 to 0.18389, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.18389 to 0.18370, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.18370 to 0.18350, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.18350 to 0.18327, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.18327 to 0.18306, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.18306 to 0.18285, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.18285 to 0.18266, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.18266 to 0.18244, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.18244 to 0.18224, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.18224 to 0.18205, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.18205 to 0.18187, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.18187 to 0.18166, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.18166 to 0.18145, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.18145 to 0.18124, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.18124 to 0.18102, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.18102 to 0.18084, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.18084 to 0.18067, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.18067 to 0.18048, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.18048 to 0.18030, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.18030 to 0.18012, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.18012 to 0.18000, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.18000 to 0.17979, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.17979 to 0.17962, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.17962 to 0.17945, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.17945 to 0.17928, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.17928 to 0.17912, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.17912 to 0.17894, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.17894 to 0.17877, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.17877 to 0.17858, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.17858 to 0.17841, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.17841 to 0.17823, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.17823 to 0.17804, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.17804 to 0.17787, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.17787 to 0.17771, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.17771 to 0.17753, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.17753 to 0.17737, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.17737 to 0.17725, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.17725 to 0.17709, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.17709 to 0.17691, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.17691 to 0.17674, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.17674 to 0.17656, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.17656 to 0.17640, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.17640 to 0.17624, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.17624 to 0.17608, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.17608 to 0.17595, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.17595 to 0.17579, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.17579 to 0.17563, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.17563 to 0.17547, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.17547 to 0.17532, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.17532 to 0.17517, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.17517 to 0.17503, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.17503 to 0.17489, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.17489 to 0.17474, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.17474 to 0.17459, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.17459 to 0.17449, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.17449 to 0.17435, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.17435 to 0.17419, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.17419 to 0.17403, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.17403 to 0.17390, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.17390 to 0.17371, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.17371 to 0.17358, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.17358 to 0.17343, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.17343 to 0.17330, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.17330 to 0.17313, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.17313 to 0.17301, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.17301 to 0.17294, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.17294 to 0.17279, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.17279 to 0.17273, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.17273 to 0.17264, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.17264 to 0.17250, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.17250 to 0.17238, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.17238 to 0.17224, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.17224 to 0.17211, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.17211 to 0.17197, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.17197 to 0.17184, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.17184 to 0.17170, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.17170 to 0.17156, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.17156 to 0.17144, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.17144 to 0.17130, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.17130 to 0.17116, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.17116 to 0.17105, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.17105 to 0.17091, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.17091 to 0.17078, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.17078 to 0.17065, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.17065 to 0.17057, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.17057 to 0.17043, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.17043 to 0.17034, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.17034 to 0.17021, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.17021 to 0.17007, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.17007 to 0.16996, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.16996 to 0.16986, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.16986 to 0.16974, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.16974 to 0.16963, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.16963 to 0.16948, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.16948 to 0.16935, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.16935 to 0.16926, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.16926 to 0.16915, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.16915 to 0.16903, saving model to model_checkpoint\\Vanilla_RNN_K-fold.h5\n",
      "Loss: 0.1690, Accuracy: 90.18%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16903\n",
      "Loss: 0.2156, Accuracy: 90.18%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16903\n",
      "Loss: 0.2371, Accuracy: 91.07%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16903\n",
      "Loss: 0.1905, Accuracy: 94.64%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16903\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16903\n",
      "Loss: 0.1874, Accuracy: 92.86%\n",
      "Vanilla_RNN finished in 189.71 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIh0lEQVR4nOzdd3xV9f3H8fe9N7n3Zu8FBMKSJUMDRFDBEYWKKNaBk+Fqq6gVbZXagqMV1w9xoFhbpVUruLegojgQRUEUkCkjjExC9rjJvef3x00uuSSERJLcm+T1fDzO49575ufmEPTNdxyTYRiGAAAAAABHZPZ1AQAAAADg7whOAAAAAHAUBCcAAAAAOAqCEwAAAAAcBcEJAAAAAI6C4AQAAAAAR0FwAgAAAICjIDgBAAAAwFEQnAAAAADgKAhOAIBWtWjRIplMJu3atcvXpTTb3XffLZPJ1ObXnTZtmlJSUrzWmUwm3X333Uc9tjVqXrFihUwmk1asWNGi522K0047TaeddlqbXxcADkdwAtBpPPXUUzKZTEpLS/N1KX7n/vvv11tvveXrMuBjTz31lBYtWuTrMgDALxGcAHQaL730klJSUrR69Wpt377d1+X4ldYMTldddZXKy8vVo0ePVjl/Z1FeXq6//vWvrXqNIwWnMWPGqLy8XGPGjGnV6wOAPyM4AegUdu7cqa+//lrz5s1TXFycXnrppTavweVyqaKios2v29JKS0ubtb/FYpHdbvdJl7eOxG63KyAgwCfXNpvNstvtMpv53wYAnRd/AwLoFF566SVFRUVpwoQJuuiii7yCU1VVlaKjozV9+vR6xxUVFclut+v222/3rKusrNScOXPUp08f2Ww2JScn689//rMqKyu9jjWZTJoxY4ZeeuklDRo0SDabTUuXLpUkPfLIIxo9erRiYmIUFBSk1NRUvfbaa/WuX15erptvvlmxsbEKCwvTeeedp3379jU43mXfvn26+uqrlZCQIJvNpkGDBum555476s/GZDKptLRU//nPf2QymWQymTRt2jRJh8bL/Pzzz7r88ssVFRWlU045RZL0008/adq0aerVq5fsdrsSExN19dVX68CBA17nb2iMU0pKis4991x99dVXGjlypOx2u3r16qX//ve/R623OT+/2nvw1ltv6fjjj/f8XGrvQ11fffWVRowYIbvdrt69e+uZZ55pUi0zZsxQaGioysrK6m277LLLlJiYKKfTKUl6++23NWHCBHXp0kU2m029e/fWfffd59nemIbueVNrfv7553XGGWcoPj5eNptNAwcO1NNPP+21T0pKijZu3KjPP//c8+egdmzRkcY4vfrqq0pNTVVQUJBiY2N15ZVXat++fV77TJs2TaGhodq3b58mTZqk0NBQxcXF6fbbb2/S925ITk6OrrnmGiUkJMhut2vo0KH6z3/+U2+/xYsXKzU1VWFhYQoPD9fgwYP12GOPebZXVVXpnnvuUd++fWW32xUTE6NTTjlFH3/88a+qC0DH5pt/ugKANvbSSy/pt7/9raxWqy677DI9/fTT+u677zRixAgFBgbqggsu0BtvvKFnnnlGVqvVc9xbb72lyspKXXrppZLcrUbnnXeevvrqK11//fUaMGCA1q9fr0cffVRbt26t193t008/1SuvvKIZM2YoNjbWM+D/scce03nnnacrrrhCDodDixcv1sUXX6z33ntPEyZM8Bw/bdo0vfLKK7rqqqt00kkn6fPPP/faXis7O1snnXSSJyjExcXpww8/1DXXXKOioiL98Y9/POLP5oUXXtC1116rkSNH6vrrr5ck9e7d22ufiy++WH379tX9998vwzAkSR9//LF27Nih6dOnKzExURs3btQ///lPbdy4Ud98881RW5i2b9+uiy66SNdcc42mTp2q5557TtOmTVNqaqoGDRrU6LFN/flJ7nDxxhtv6IYbblBYWJgef/xxXXjhhcrIyFBMTIwkaf369Tr77LMVFxenu+++W9XV1ZozZ44SEhIarUOSJk+erAULFuj999/XxRdf7FlfVlamd999V9OmTZPFYpHkDpGhoaGaOXOmQkND9emnn2r27NkqKirSww8/fNRr1dWcmp9++mkNGjRI5513ngICAvTuu+/qhhtukMvl0o033ihJmj9/vm666SaFhobqrrvukqRGv/+iRYs0ffp0jRgxQnPnzlV2drYee+wxrVy5Uj/88IMiIyM9+zqdTo0bN05paWl65JFH9Mknn+j//u//1Lt3b/3hD39o1vcuLy/Xaaedpu3bt2vGjBnq2bOnXn31VU2bNk0FBQW65ZZbJLn/fF522WU688wz9eCDD0qSNm3apJUrV3r2ufvuuzV37lzPn/+ioiJ9//33Wrt2rc4666xm1QWgEzAAoIP7/vvvDUnGxx9/bBiGYbhcLqNbt27GLbfc4tln2bJlhiTj3Xff9Tr2nHPOMXr16uX5/MILLxhms9n48ssvvfZbuHChIclYuXKlZ50kw2w2Gxs3bqxXU1lZmddnh8NhHH/88cYZZ5zhWbdmzRpDkvHHP/7Ra99p06YZkow5c+Z41l1zzTVGUlKSkZeX57XvpZdeakRERNS73uFCQkKMqVOn1ls/Z84cQ5Jx2WWXHfU7GIZhvPzyy4Yk44svvvCse/755w1Jxs6dOz3revToUW+/nJwcw2azGbfddlujtTZ07YZ+fobhvgdWq9XYvn27Z92PP/5oSDKeeOIJz7pJkyYZdrvd2L17t2fdzz//bFgsFuNo/6l0uVxG165djQsvvNBr/SuvvFLvOzb0M/vd735nBAcHGxUVFZ51U6dONXr06FHvu9S9582puaHrjhs3zuvPtmEYxqBBg4yxY8fW2/ezzz4zJBmfffaZYRjun3d8fLxx/PHHG+Xl5Z793nvvPUOSMXv2bK/vIsm49957vc55wgknGKmpqfWudbixY8d61TR//nxDkvHiiy961jkcDmPUqFFGaGioUVRUZBiGYdxyyy1GeHi4UV1dfcRzDx061JgwYcJRawAAwzAMuuoB6PBeeuklJSQk6PTTT5fk7vI0efJkLV682NNV6IwzzlBsbKyWLFniOe7gwYP6+OOPNXnyZM+6V199VQMGDFD//v2Vl5fnWc444wxJ0meffeZ17bFjx2rgwIH1agoKCvK6TmFhoU499VStXbvWs762O9kNN9zgdexNN93k9dkwDL3++uuaOHGiDMPwqmvcuHEqLCz0Ou+v8fvf/77R71BRUaG8vDyddNJJktSk6w0cOFCnnnqq53NcXJz69eunHTt2HPXYpvz8aqWnp3u1oA0ZMkTh4eGe6zidTi1btkyTJk1S9+7dPfsNGDBA48aNO2otJpNJF198sT744AOVlJR41i9ZskRdu3b1dG08vO7i4mLl5eXp1FNPVVlZmTZv3nzUa9Vqbs11r1tYWKi8vDyNHTtWO3bsUGFhYZOvW+v7779XTk6ObrjhBtntds/6CRMmqH///nr//ffrHXP4n6FTTz21Sff6cB988IESExN12WWXedYFBgbq5ptvVklJiT7//HNJUmRkpEpLSxvtdhcZGamNGzdq27Ztza4DQOdDcALQoTmdTi1evFinn366du7cqe3bt2v79u1KS0tTdna2li9fLkkKCAjQhRdeqLffftszVumNN95QVVWVV3Datm2bNm7cqLi4OK/luOOOk+Qee1FXz549G6zrvffe00knnSS73a7o6GjFxcXp6aef9vqf2N27d8tsNtc7R58+fbw+5+bmqqCgQP/85z/r1VU7buvwupqroe+Rn5+vW265RQkJCQoKClJcXJxnv6b8z3jd/+GvFRUVpYMHDx712Kb8/Jp6ndzcXJWXl6tv37719uvXr99Ra5Hc3fXKy8v1zjvvSJJKSkr0wQcf6OKLL/bqsrhx40ZdcMEFioiIUHh4uOLi4nTllVdKatrPrFZza165cqXS09MVEhKiyMhIxcXF6S9/+Uuzr1tr9+7dR7xW//79Pdtr2e12xcXFea1r6r1u6Np9+/atN1HFgAEDvGq74YYbdNxxx+k3v/mNunXrpquvvrre2LZ7771XBQUFOu644zR48GD96U9/0k8//dTsmgB0DoxxAtChffrpp8rMzNTixYu1ePHiettfeuklnX322ZKkSy+9VM8884w+/PBDTZo0Sa+88or69++voUOHevZ3uVwaPHiw5s2b1+D1kpOTvT7X/Zf+Wl9++aXOO+88jRkzRk899ZSSkpIUGBio559/Xv/73/+a/R1dLpck6corr9TUqVMb3GfIkCHNPm9dDX2PSy65RF9//bX+9Kc/adiwYQoNDZXL5dL48eM9NTWmdtzP4YyaMVRH0tyf36+9TnOcdNJJSklJ0SuvvKLLL79c7777rsrLy71Cd0FBgcaOHavw8HDde++96t27t+x2u9auXas77rijST+zX+OXX37RmWeeqf79+2vevHlKTk6W1WrVBx98oEcffbTVrlvXke5Ba4qPj9e6deu0bNkyffjhh/rwww/1/PPPa8qUKZ6JJMaMGaNffvlFb7/9tj766CP961//0qOPPqqFCxfq2muvbfOaAfg3ghOADu2ll15SfHy8FixYUG/bG2+8oTfffFMLFy5UUFCQxowZo6SkJC1ZskSnnHKKPv30U88g+Vq9e/fWjz/+qDPPPPNXT6/9+uuvy263a9myZbLZbJ71zz//vNd+PXr0kMvl0s6dO71aFg5/BlVcXJzCwsLkdDqVnp7+q2pq7nc5ePCgli9frnvuuUezZ8/2rG+LLk9N/fk1VVxcnIKCghqsfcuWLU0+zyWXXKLHHntMRUVFWrJkiVJSUjxdFyX3zHQHDhzQG2+84fU8pJ07d7Zqze+++64qKyv1zjvveLW+Hd6tVGr6n4PaZ3Jt2bLF00217vVb85ldPXr00E8//SSXy+XV6lTb1bHuta1WqyZOnKiJEyfK5XLphhtu0DPPPKO//e1vnpbb2hk1p0+frpKSEo0ZM0Z33303wQlAPXTVA9BhlZeX64033tC5556riy66qN4yY8YMFRcXe7pXmc1mXXTRRXr33Xf1wgsvqLq62qvFQHL/z/G+ffv07LPPNni9pjzjyGKxyGQyeU3FvGvXrnoz8tWOVXnqqae81j/xxBP1znfhhRfq9ddf14YNG+pdLzc396g1hYSEqKCg4Kj71b2mVL/VZv78+U0+x6/V1J9fc843btw4vfXWW8rIyPCs37Rpk5YtW9bk80yePFmVlZX6z3/+o6VLl+qSSy6pdx3J+2fmcDjq3d+Wrrmh6xYWFjYYNJv652D48OGKj4/XwoULvabh//DDD7Vp06YGZ35sKeecc46ysrK8xiNWV1friSeeUGhoqMaOHStJ9abFN5vNnpbX2poP3yc0NFR9+vSp92gBAJBocQLQgb3zzjsqLi7Weeed1+D2k046yfMw3NqANHnyZD3xxBOaM2eOBg8e7Bk3Ueuqq67SK6+8ot///vf67LPPdPLJJ8vpdGrz5s165ZVXtGzZMg0fPrzRuiZMmKB58+Zp/Pjxuvzyy5WTk6MFCxaoT58+XuMrUlNTdeGFF2r+/Pk6cOCAZzryrVu3SvJuHXjggQf02WefKS0tTdddd50GDhyo/Px8rV27Vp988ony8/MbrSk1NVWffPKJ5s2bpy5duqhnz55KS0s74v7h4eEaM2aMHnroIVVVValr16766KOPflXrSXM19efXHPfcc4+WLl2qU089VTfccIPnf8QHDRrU5HOeeOKJ6tOnj+666y5VVlbWC92jR49WVFSUpk6dqptvvlkmk0kvvPDCr+4y2NSazz77bE/Ly+9+9zuVlJTo2WefVXx8vDIzM73OmZqaqqefflp///vf1adPH8XHx9drUZLckzE8+OCDmj59usaOHavLLrvMMx15SkqKbr311l/1nZri+uuv1zPPPKNp06ZpzZo1SklJ0WuvvaaVK1dq/vz5CgsLkyRde+21ys/P1xlnnKFu3bpp9+7deuKJJzRs2DDP7/XAgQN12mmnKTU1VdHR0fr+++/12muvacaMGa1WP4B2zHcT+gFA65o4caJht9uN0tLSI+4zbdo0IzAw0DONt8vlMpKTkw1Jxt///vcGj3E4HMaDDz5oDBo0yLDZbEZUVJSRmppq3HPPPUZhYaFnP0nGjTfe2OA5/v3vfxt9+/Y1bDab0b9/f+P555/3TP1dV2lpqXHjjTca0dHRRmhoqDFp0iRjy5YthiTjgQce8No3OzvbuPHGG43k5GQjMDDQSExMNM4880zjn//851F/Vps3bzbGjBljBAUFGZI8U5PX1pSbm1vvmL179xoXXHCBERkZaURERBgXX3yxsX///nrTZh9pOvKGpoE+fOrpI2nqz+9I96BHjx71pl///PPPjdTUVMNqtRq9evUyFi5c2OA5G3PXXXcZkow+ffo0uH3lypXGSSedZAQFBRldunQx/vznP3umwq+d6tswmjYdeXNqfuedd4whQ4YYdrvdSElJMR588EHjueeeq3dfsrKyjAkTJhhhYWGGJM+9OHw68lpLliwxTjjhBMNmsxnR0dHGFVdcYezdu9drn6lTpxohISH1fhZN/dk29GciOzvbmD59uhEbG2tYrVZj8ODBxvPPP++1z2uvvWacffbZRnx8vGG1Wo3u3bsbv/vd74zMzEzPPn//+9+NkSNHGpGRkUZQUJDRv39/4x//+IfhcDiOWheAzsdkGC04OhYA0OrWrVunE044QS+++KKuuOIKX5cDAECnwBgnAPBj5eXl9dbNnz9fZrPZa4IBAADQuhjjBAB+7KGHHtKaNWt0+umnKyAgwDOt8vXXX19v6nMAANB66KoHAH7s448/1j333KOff/5ZJSUl6t69u6666irdddddCgjg374AAGgrBCcAAAAAOArGOAEAAADAUfhFcFqwYIFSUlJkt9uVlpam1atXN7r//Pnz1a9fPwUFBSk5OVm33nqrKioq2qhaAAAAAJ2NzzvIL1myRDNnztTChQuVlpam+fPna9y4cdqyZYvi4+Pr7f+///1Pd955p5577jmNHj1aW7du1bRp02QymTRv3ryjXs/lcmn//v0KCwvzengkAAAAgM7FMAwVFxerS5cuMpsbb1Py+RintLQ0jRgxQk8++aQkd7BJTk7WTTfdpDvvvLPe/jNmzNCmTZu0fPlyz7rbbrtN3377rb766qujXm/v3r3MRAUAAADAY8+ePerWrVuj+/i0xcnhcGjNmjWaNWuWZ53ZbFZ6erpWrVrV4DGjR4/Wiy++qNWrV2vkyJHasWOHPvjgA1111VUN7l9ZWanKykrP59qcuGfPHoWHh7fgtwEAAADQnhQVFSk5OVlhYWFH3denwSkvL09Op1MJCQle6xMSErR58+YGj7n88suVl5enU045RYZhqLq6Wr///e/1l7/8pcH9586dq3vuuafe+vDwcIITAAAAgCYN4fGLySGaY8WKFbr//vv11FNPae3atXrjjTf0/vvv67777mtw/1mzZqmwsNCz7Nmzp40rBgAAANDe+bTFKTY2VhaLRdnZ2V7rs7OzlZiY2OAxf/vb33TVVVfp2muvlSQNHjxYpaWluv7663XXXXfVG9Rls9lks9la5wsAAAAA6BR82uJktVqVmprqNdGDy+XS8uXLNWrUqAaPKSsrqxeOLBaLpEPjlwAAAACgJfl8OvKZM2dq6tSpGj58uEaOHKn58+ertLRU06dPlyRNmTJFXbt21dy5cyVJEydO1Lx583TCCScoLS1N27dv19/+9jdNnDjRE6AAAADQvjmdTlVVVfm6DHQAgYGBLZITfB6cJk+erNzcXM2ePVtZWVkaNmyYli5d6pkwIiMjw6uF6a9//atMJpP++te/at++fYqLi9PEiRP1j3/8w1dfAQAAAC2opKREe/fupTcRWoTJZFK3bt0UGhp6bOfx9XOc2lpRUZEiIiJUWFjIrHoAAAB+xul0atu2bQoODlZcXFyTZjsDjsQwDOXm5qqsrEx9+/at1/LUnGzg8xYnAAAAoFZVVZUMw1BcXJyCgoJ8XQ46gLi4OO3atUtVVVXH1GWv3U1HDgAAgI6Plia0lJb6s0RwAgAAAICjIDgBAAAAwFEQnAAAAAA/lJKSovnz5zd5/xUrVshkMqmgoKDVapKkRYsWKTIyslWv4Y8ITgAAAMAxMJlMjS533333rzrvd999p+uvv77J+48ePVqZmZmKiIj4VddD45hVDwAAADgGmZmZnvdLlizR7NmztWXLFs+6us8PMgxDTqdTAQFH/9/wuLi4ZtVhtVqVmJjYrGPQdLQ4AQAAwG8ZhqEyR7VPlqY+7jQxMdGzREREyGQyeT5v3rxZYWFh+vDDD5WamiqbzaavvvpKv/zyi84//3wlJCQoNDRUI0aM0CeffOJ13sO76plMJv3rX//SBRdcoODgYPXt21fvvPOOZ/vhXfVqu9QtW7ZMAwYMUGhoqMaPH+8V9Kqrq3XzzTcrMjJSMTExuuOOOzR16lRNmjSpWffp6aefVu/evWW1WtWvXz+98MILXvfw7rvvVvfu3WWz2dSlSxfdfPPNnu1PPfWU+vbtK7vdroSEBF100UXNunZbocUJAAAAfqu8yqmBs5f55No/3ztOwdaW+d/lO++8U4888oh69eqlqKgo7dmzR+ecc47+8Y9/yGaz6b///a8mTpyoLVu2qHv37kc8zz333KOHHnpIDz/8sJ544gldccUV2r17t6Kjoxvcv6ysTI888oheeOEFmc1mXXnllbr99tv10ksvSZIefPBBvfTSS3r++ec1YMAAPfbYY3rrrbd0+umnN/m7vfnmm7rllls0f/58paen67333tP06dPVrVs3nX766Xr99df16KOPavHixRo0aJCysrL0448/SpK+//573XzzzXrhhRc0evRo5efn68svv2zGT7btEJwAAACAVnbvvffqrLPO8nyOjo7W0KFDPZ/vu+8+vfnmm3rnnXc0Y8aMI55n2rRpuuyyyyRJ999/vx5//HGtXr1a48ePb3D/qqoqLVy4UL1795YkzZgxQ/fee69n+xNPPKFZs2bpggsukCQ9+eST+uCDD5r13R555BFNmzZNN9xwgyRp5syZ+uabb/TII4/o9NNPV0ZGhhITE5Wenq7AwEB1795dI0eOlCRlZGQoJCRE5557rsLCwtSjRw+dcMIJzbp+WyE4+dCe/DJt3F+kuDCrUns0/K8EAAAAnVlQoEU/3zvOZ9duKcOHD/f6XFJSorvvvlvvv/++MjMzVV1drfLycmVkZDR6niFDhnjeh4SEKDw8XDk5OUfcPzg42BOaJCkpKcmzf2FhobKzsz0hRpIsFotSU1Plcrma/N02bdpUbxKLk08+WY899pgk6eKLL9b8+fPVq1cvjR8/Xuecc44mTpyogIAAnXXWWerRo4dn2/jx4z1dEf0NY5x86Ittufr9i2v0zy92+LoUAAAAv2QymRRsDfDJYjKZWux7hISEeH2+/fbb9eabb+r+++/Xl19+qXXr1mnw4MFyOByNnicwMLDez6exkNPQ/k0du9VSkpOTtWXLFj311FMKCgrSDTfcoDFjxqiqqkphYWFau3atXn75ZSUlJWn27NkaOnRoq0+p/msQnHzIFuD+V4zK6qYnegAAALR/K1eu1LRp03TBBRdo8ODBSkxM1K5du9q0hoiICCUkJOi7777zrHM6nVq7dm2zzjNgwACtXLnSa93KlSs1cOBAz+egoCBNnDhRjz/+uFasWKFVq1Zp/fr1kqSAgAClp6froYce0k8//aRdu3bp008/PYZv1jroqudD1gB3bq2sIjgBAAB0Jn379tUbb7yhiRMnymQy6W9/+1uzuse1lJtuuklz585Vnz591L9/fz3xxBM6ePBgs1rb/vSnP+mSSy7RCSecoPT0dL377rt64403PLMELlq0SE6nU2lpaQoODtaLL76ooKAg9ejRQ++995527NihMWPGKCoqSh988IFcLpf69evXWl/5VyM4+ZCtNjhVO31cCQAAANrSvHnzdPXVV2v06NGKjY3VHXfcoaKiojav44477lBWVpamTJkii8Wi66+/XuPGjZPF0vTxXZMmTdJjjz2mRx55RLfccot69uyp559/XqeddpokKTIyUg888IBmzpwpp9OpwYMH691331VMTIwiIyP1xhtv6O6771ZFRYX69u2rl19+WYMGDWqlb/zrmYy27uToY0VFRYqIiFBhYaHCw8N9WsuKLTma9vx3GtQlXO/ffKpPawEAAPAHFRUV2rlzp3r27Cm73e7rcjodl8ulAQMG6JJLLtF9993n63JaRGN/ppqTDWhx8iHGOAEAAMCXdu/erY8++khjx45VZWWlnnzySe3cuVOXX365r0vzO0wO4UO2QLrqAQAAwHfMZrMWLVqkESNG6OSTT9b69ev1ySefaMCAAb4uze/Q4uRDtWOcHLQ4AQAAwAeSk5PrzYiHhtHi5EN01QMAAADaB4KTD9mYjhwAAABoFwhOPlR3OvJONrkhAAAA0K4QnHyotquey5CqXQQnAAAAwF8RnHyodlY9iXFOAAAAgD8jOPmQ1VInOFUxJTkAAADgrwhOPmQ2mzzhiRYnAACAzu20007TH//4R8/nlJQUzZ8/v9FjTCaT3nrrrWO+dkudpzF33323hg0b1qrXaE0EJx/jWU4AAADt28SJEzV+/PgGt3355ZcymUz66aefmn3e7777Ttdff/2xluflSOElMzNTv/nNb1r0Wh0NwcnHasc50eIEAADQPl1zzTX6+OOPtXfv3nrbnn/+eQ0fPlxDhgxp9nnj4uIUHBzcEiUeVWJiomw2W5tcq70iOPnYoYfgMsYJAACgHsOQHKW+WZr4uJhzzz1XcXFxWrRokdf6kpISvfrqq7rmmmt04MABXXbZZeratauCg4M1ePBgvfzyy42e9/Cuetu2bdOYMWNkt9s1cOBAffzxx/WOueOOO3TccccpODhYvXr10t/+9jdVVVVJkhYtWqR77rlHP/74o0wmk0wmk6fmw7vqrV+/XmeccYaCgoIUExOj66+/XiUlJZ7t06ZN06RJk/TII48oKSlJMTExuvHGGz3XagqXy6V7771X3bp1k81m07Bhw7R06VLPdofDoRkzZigpKUl2u109evTQ3LlzJUmGYejuu+9W9+7dZbPZ1KVLF918881NvvavEdCqZ8dRWQNocQIAADiiqjLp/i6+ufZf9kvWkKPuFhAQoClTpmjRokW66667ZDKZJEmvvvqqnE6nLrvsMpWUlCg1NVV33HGHwsPD9f777+uqq65S7969NXLkyKNew+Vy6be//a0SEhL07bffqrCw0Gs8VK2wsDAtWrRIXbp00fr163XdddcpLCxMf/7znzV58mRt2LBBS5cu1SeffCJJioiIqHeO0tJSjRs3TqNGjdJ3332nnJwcXXvttZoxY4ZXOPzss8+UlJSkzz77TNu3b9fkyZM1bNgwXXfddUf9PpL02GOP6f/+7//0zDPP6IQTTtBzzz2n8847Txs3blTfvn31+OOP65133tErr7yi7t27a8+ePdqzZ48k6fXXX9ejjz6qxYsXa9CgQcrKytKPP/7YpOv+WgQnH/M8BLeK4AQAANBeXX311Xr44Yf1+eef67TTTpPk7qZ34YUXKiIiQhEREbr99ts9+990001atmyZXnnllSYFp08++USbN2/WsmXL1KWLO0jef//99cYl/fWvf/W8T0lJ0e23367Fixfrz3/+s4KCghQaGqqAgAAlJiYe8Vr/+9//VFFRof/+978KCXEHxyeffFITJ07Ugw8+qISEBElSVFSUnnzySVksFvXv318TJkzQ8uXLmxycHnnkEd1xxx269NJLJUkPPvigPvvsM82fP18LFixQRkaG+vbtq1NOOUUmk0k9evTwHJuRkaHExESlp6crMDBQ3bt3b9LP8VgQnHzME5zoqgcAAFBfYLC75cdX126i/v37a/To0Xruued02mmnafv27fryyy917733SpKcTqfuv/9+vfLKK9q3b58cDocqKyubPIZp06ZNSk5O9oQmSRo1alS9/ZYsWaLHH39cv/zyi0pKSlRdXa3w8PAmf4/aaw0dOtQTmiTp5JNPlsvl0pYtWzzBadCgQbJYLJ59kpKStH79+iZdo6ioSPv379fJJ5/stf7kk0/2tBxNmzZNZ511lvr166fx48fr3HPP1dlnny1JuvjiizV//nz16tVL48eP1znnnKOJEycqIKD14g1jnHzs0BgnWpwAAADqMZnc3eV8sdR0uWuqa665Rq+//rqKi4v1/PPPq3fv3ho7dqwk6eGHH9Zjjz2mO+64Q5999pnWrVuncePGyeFwtNiPatWqVbriiit0zjnn6L333tMPP/ygu+66q0WvUVdgYKDXZ5PJJJer5f6f9sQTT9TOnTt13333qby8XJdccokuuugiSVJycrK2bNmip556SkFBQbrhhhs0ZsyYZo2xai6Ck48dmlWPFicAAID27JJLLpHZbNb//vc//fe//9XVV1/tGe+0cuVKnX/++bryyis1dOhQ9erVS1u3bm3yuQcMGKA9e/YoMzPTs+6bb77x2ufrr79Wjx49dNddd2n48OHq27evdu/e7bWP1WqV09n4/3cOGDBAP/74o0pLSz3rVq5cKbPZrH79+jW55saEh4erS5cuWrlypdf6lStXauDAgV77TZ48Wc8++6yWLFmi119/Xfn5+ZKkoKAgTZw4UY8//rhWrFihVatWNbnF69egq56P8RwnAACAjiE0NFSTJ0/WrFmzVFRUpGnTpnm29e3bV6+99pq+/vprRUVFad68ecrOzvYKCY1JT0/Xcccdp6lTp+rhhx9WUVGR7rrrLq99+vbtq4yMDC1evFgjRozQ+++/rzfffNNrn5SUFO3cuVPr1q1Tt27dFBYWVm8a8iuuuEJz5szR1KlTdffddys3N1c33XSTrrrqKk83vZbwpz/9SXPmzFHv3r01bNgwPf/881q3bp1eeuklSdK8efOUlJSkE044QWazWa+++qoSExMVGRmpRYsWyel0Ki0tTcHBwXrxxRcVFBTkNQ6qpdHi5GN01QMAAOg4rrnmGh08eFDjxo3zGo/017/+VSeeeKLGjRun0047TYmJiZo0aVKTz2s2m/Xmm2+qvLxcI0eO1LXXXqt//OMfXvucd955uvXWWzVjxgwNGzZMX3/9tf72t7957XPhhRdq/PjxOv300xUXF9fglOjBwcFatmyZ8vPzNWLECF100UU688wz9eSTTzbvh3EUN998s2bOnKnbbrtNgwcP1tKlS/XOO++ob9++ktwzBD700EMaPny4RowYoV27dumDDz6Q2WxWZGSknn32WZ188skaMmSIPvnkE7377ruKiYlp0RrrMhlGEyeo7yCKiooUERGhwsLCZg+Uaw0zl6zTGz/s013nDNB1Y3r5uhwAAACfqqio0M6dO9WzZ0/Z7XZfl4MOoLE/U83JBrQ4+ZiVWfUAAAAAv0dw8jEbD8AFAAAA/B7BycdsgYxxAgAAAPwdwcnHPC1OVXTVAwAAAPwVwcmXDEPBJoeCVUGLEwAAQB2dbP4ytKKW+rNEcPKl7/6lP6w8RQ8HLiQ4AQAASLJY3MMYHA6HjytBR1H7Z6n2z9avxQNwfSkwSJIUJAcPwAUAAJAUEBCg4OBg5ebmKjAwUGYz/86PX8/lcik3N1fBwcEKCDi26ENw8qU6wYnpyAEAACSTyaSkpCTt3LlTu3fv9nU56ADMZrO6d+8uk8l0TOchOPlSYLAkKchUSVc9AACAGlarVX379qW7HlqE1WptkZZLgpMv1bQ42eVQZRXBCQAAoJbZbJbdbvd1GYAHnUZ9KaBOcKKrHgAAAOC3CE6+VDvGia56AAAAgF8jOPlS7RgnOQhOAAAAgB8jOPmSZ4xTpSqr6KoHAAAA+CuCky/VBCerySlnNbPGAAAAAP6K4ORLNV31JMlUXeHDQgAAAAA0huDkSwE2GXI/iIvgBAAAAPgvvwhOCxYsUEpKiux2u9LS0rR69eoj7nvaaafJZDLVWyZMmNCGFbcQk8kzJbnFVSGXy/BxQQAAAAAa4vPgtGTJEs2cOVNz5szR2rVrNXToUI0bN045OTkN7v/GG28oMzPTs2zYsEEWi0UXX3xxG1feQmqnJJdDDicz6wEAAAD+yOfBad68ebruuus0ffp0DRw4UAsXLlRwcLCee+65BvePjo5WYmKiZ/n4448VHBzcfoOTtTY4VaqyiuAEAAAA+COfBieHw6E1a9YoPT3ds85sNis9PV2rVq1q0jn+/e9/69JLL1VISEiD2ysrK1VUVOS1+BWvZzkxJTkAAADgj3wanPLy8uR0OpWQkOC1PiEhQVlZWUc9fvXq1dqwYYOuvfbaI+4zd+5cRUREeJbk5ORjrrslmWq76pkqeQguAAAA4Kd83lXvWPz73//W4MGDNXLkyCPuM2vWLBUWFnqWPXv2tGGFTVAzOYRNDoITAAAA4KcCfHnx2NhYWSwWZWdne63Pzs5WYmJio8eWlpZq8eLFuvfeexvdz2azyWazHXOtrabO5BB01QMAAAD8k09bnKxWq1JTU7V8+XLPOpfLpeXLl2vUqFGNHvvqq6+qsrJSV155ZWuX2bpqxzjRVQ8AAADwWz5tcZKkmTNnaurUqRo+fLhGjhyp+fPnq7S0VNOnT5ckTZkyRV27dtXcuXO9jvv3v/+tSZMmKSYmxhdlt5y6LU7MqgcAAAD4JZ8Hp8mTJys3N1ezZ89WVlaWhg0bpqVLl3omjMjIyJDZ7N0wtmXLFn311Vf66KOPfFFyy6oJTnZV0lUPAAAA8FM+D06SNGPGDM2YMaPBbStWrKi3rl+/fjIMo5WraiOernpMDgEAAAD4q3Y9q16HEGiXJNmZVQ8AAADwWwQnX/M8ALdSlVV01QMAAAD8EcHJ1zwPwHXI4aTFCQAAAPBHBCdfq2lxsquSWfUAAAAAP0Vw8jWvB+ASnAAAAAB/RHDyNa/gxBgnAAAAwB8RnHwtoOY5TkxHDgAAAPgtgpOv1X0ALmOcAAAAAL9EcPI1z3TkdNUDAAAA/BXBydc805FX0lUPAAAA8FMEJ1/zanEiOAEAAAD+iODka3XGODnoqgcAAAD4JYKTr9UEJ6vJqaqqKh8XAwAAAKAhBCdfqwlOkmQ4yn1YCAAAAIAjITj5WoD90PvqMt/VAQAAAOCICE6+ZjLJaXG3OpmqaHECAAAA/BHByQ+4AmqCU3WFjysBAAAA0BCCkx+oDU5muuoBAAAAfong5A9qxjmZaXECAAAA/BLByQ8YNTPrWVwEJwAAAMAfEZz8gKkmOAXQ4gQAAAD4JYKTHzBZgyW5W5wMw/BxNQAAAAAOR3DyA6ZAd3Cyq1LVLoITAAAA4G8ITn7AbHMHpyA5VF7l9HE1AAAAAA5HcPID5poxTnY5VOEgOAEAAAD+huDkB2rHONlNlSolOAEAAAB+h+DkD2panILkUJmj2sfFAAAAADgcwckfBNaOcapUOS1OAAAAgN8hOPmD2hYnk0NlBCcAAADA7xCc/EGd6cgJTgAAAID/ITj5gwC7JMmuKpVXMcYJAAAA8DcEJ3/gmRyCFicAAADAHxGc/EHt5BAmB5NDAAAAAH6I4OQPPA/ArVRpJcEJAAAA8DcEJ3/gmY7coTLGOAEAAAB+h+DkDzzTkfMcJwAAAMAfEZz8gaerHs9xAgAAAPwRwckf1AlOtDgBAAAA/ofg5A9qxjjZTNWqqKz0cTEAAAAADkdw8gc1wUmSnJWlPiwEAAAAQEMITv4gwCaXKcD9vrLEt7UAAAAAqIfg5A9MJjkDQyVJZkeRj4sBAAAAcDiCk59wWd3ByVJFixMAAADgbwhO/sIWJkkKqGaMEwAAAOBvCE7+whOcaHECAAAA/A3ByU+Y7OGSJLurTE6X4eNqAAAAANRFcPITlprgFKZylTmqfVwNAAAAgLoITn7CXBOcQlWuMofTx9UAAAAAqIvg5CdMdvcYp1ATwQkAAADwNwQnf2GrbXEqo6seAAAA4GcITv6iZla9MFO5ymlxAgAAAPwKwclf1AQnxjgBAAAA/ofg5C9sjHECAAAA/BXByV94WpwqVF7FGCcAAADAn/hFcFqwYIFSUlJkt9uVlpam1atXN7p/QUGBbrzxRiUlJclms+m4447TBx980EbVthIb05EDAAAA/irA1wUsWbJEM2fO1MKFC5WWlqb58+dr3Lhx2rJli+Lj4+vt73A4dNZZZyk+Pl6vvfaaunbtqt27dysyMrLti29JdbvqVRKcAAAAAH/i8+A0b948XXfddZo+fbokaeHChXr//ff13HPP6c4776y3/3PPPaf8/Hx9/fXXCgwMlCSlpKS0Zcmto+7kEJV01QMAAAD8iU+76jkcDq1Zs0bp6emedWazWenp6Vq1alWDx7zzzjsaNWqUbrzxRiUkJOj444/X/fffL6ez4VaayspKFRUVeS1+qSY4BZqccjhKfVwMAAAAgLp8Gpzy8vLkdDqVkJDgtT4hIUFZWVkNHrNjxw699tprcjqd+uCDD/S3v/1N//d//6e///3vDe4/d+5cRUREeJbk5OQW/x4tIjBEhkySJFd5sY+LAQAAAFCXX0wO0Rwul0vx8fH65z//qdTUVE2ePFl33XWXFi5c2OD+s2bNUmFhoWfZs2dPG1fcRGazHJZgSZJR4aetYgAAAEAn5dMxTrGxsbJYLMrOzvZan52drcTExAaPSUpKUmBgoCwWi2fdgAEDlJWVJYfDIavV6rW/zWaTzWZr+eJbQXVAqGzOUqmSFicAAADAn/i0xclqtSo1NVXLly/3rHO5XFq+fLlGjRrV4DEnn3yytm/fLpfL5Vm3detWJSUl1QtN7U11YKgkyeQgOAEAAAD+xOdd9WbOnKlnn31W//nPf7Rp0yb94Q9/UGlpqWeWvSlTpmjWrFme/f/whz8oPz9ft9xyi7Zu3ar3339f999/v2688UZffYUW47LWBqcSH1cCAAAAoC6fT0c+efJk5ebmavbs2crKytKwYcO0dOlSz4QRGRkZMpsP5bvk5GQtW7ZMt956q4YMGaKuXbvqlltu0R133OGrr9BiXFb3zHoBVQQnAAAAwJ/4PDhJ0owZMzRjxowGt61YsaLeulGjRumbb75p5ap8wBOc6KoHAAAA+BOfd9VDHfaaZzlV8xwnAAAAwJ8QnPyI2R4uSbI6CU4AAACAPyE4+RFLTXCyu8p8XAkAAACAughOfiQgOEKSFGSUqdrpOsreAAAAANoKwcmPBAa7W5zCVK6yKqePqwEAAABQi+DkRwKC3C1OoSpXuYPgBAAAAPgLgpMfMdWMcQo1lamM4AQAAAD4DYKTP7G5pyMPVbnKHNU+LgYAAABALYKTP6kNTia66gEAAAD+hODkT2qCU5jK6aoHAAAA+BGCkz+pCU52U5XKK8p9XAwAAACAWgQnf2IN87ytLi/yYSEAAAAA6iI4+RNLgCpNdkmSo7TQx8UAAAAAqEVw8jOVlmBJkqOM4AQAAAD4C4KTn3FYQiVJ1QQnAAAAwG8QnPxMdWCIJMnJGCcAAADAbxCc/Iwr0D1BhKui2MeVAAAAAKhFcPIzLqu7q56pkq56AAAAgL8gOPkZIyhakhRQWeDbQgAAAAB4EJz8jDnEHZxsVbQ4AQAAAP6C4ORnLCExkqTg6gLfFgIAAADAg+DkZwLD4iRJIU5m1QMAAAD8BcHJz9jCYyVJ4UaxqpwuH1cDAAAAQCI4+R17TXCKUrGKK6p9XA0AAAAAieDkdwJC3V31okzFKiqv8nE1AAAAACSCk/8Jds+qF2EqU1FZuY+LAQAAACARnPyPPVIumSRJFUV5Pi4GAAAAgERw8j+WAJWaQiRJlYW5Pi4GAAAAgERw8ktllghJkqOEFicAAADAHxCc/FBZgDs4uUrzfVwJAAAAAIng5Jcc1khJklFKixMAAADgDwhOfqjKFiVJMpUf9HElAAAAACSCk19y2t3BKaCS4AQAAAD4A4KTHzKC3M9yCqws8G0hAAAAACQRnPySOSRGkmSrKvBtIQAAAAAkEZz8kqUmOAVXF/q4EgAAAAASwckvWcPcwSnUVeTjSgAAAABIBCe/ZI+IlySFGwQnAAAAwB8QnPxQUEScJCncKJGrutrH1QAAAAAgOPmh0Ch3i5PFZKi0+ICPqwEAAABAcPJDdnuQSowgSVLpwRwfVwMAAACA4OSnCk1hkqSKwjwfVwIAAACA4OSnis3hkqTKolwfVwIAAACA4OSnyiwRkqSqEsY4AQAAAL5GcPJT5YHu4GSU0lUPAAAA8DWCk59yWCMlSUZZvm8LAQAAAEBw8lfVtkhJkrnioG8LAQAAAEBw8ldOe7QkKaCCFicAAADA1whO/io4RpJkc9DiBAAAAPgawclfhSZIkkKqmFUPAAAA8DWCk58yhydKksKr8yTD8HE1AAAAQOdGcPJTtuhukiS7USlVFvm4GgAAAKBzIzj5qaiICBUawe4PRZm+LQYAAADo5AhOfiom1KZsI0qSZBQTnAAAAABf8ovgtGDBAqWkpMhutystLU2rV68+4r6LFi2SyWTyWux2extW2zZiQqye4FSRv8/H1QAAAACdm8+D05IlSzRz5kzNmTNHa9eu1dChQzVu3Djl5OQc8Zjw8HBlZmZ6lt27d7dhxW3DHmhRvtn9LKfy/L0+rgYAAADo3HwenObNm6frrrtO06dP18CBA7Vw4UIFBwfrueeeO+IxJpNJiYmJniUhIaENK247xYFxkqTqgv0+rgQAAADo3HwanBwOh9asWaP09HTPOrPZrPT0dK1ateqIx5WUlKhHjx5KTk7W+eefr40bNx5x38rKShUVFXkt7UW53R2cXEVZPq4EAAAA6Nx8Gpzy8vLkdDrrtRglJCQoK6vhsNCvXz8999xzevvtt/Xiiy/K5XJp9OjR2ru34e5sc+fOVUREhGdJTk5u8e/RWqqC4yVJllKCEwAAAOBLPu+q11yjRo3SlClTNGzYMI0dO1ZvvPGG4uLi9MwzzzS4/6xZs1RYWOhZ9uzZ08YV/3qu0CRJkq38yOO9AAAAALS+AF9ePDY2VhaLRdnZ2V7rs7OzlZiY2KRzBAYG6oQTTtD27dsb3G6z2WSz2Y65Vl8wR3SRJIU4ciXDkEwmH1cEAAAAdE4+bXGyWq1KTU3V8uXLPetcLpeWL1+uUaNGNekcTqdT69evV1JSUmuV6TP2SPd3shjVUlm+j6sBAAAAOi+ftjhJ0syZMzV16lQNHz5cI0eO1Pz581VaWqrp06dLkqZMmaKuXbtq7ty5kqR7771XJ510kvr06aOCggI9/PDD2r17t6699lpffo1WERUeojwjXLGmIqk4UwqJ8XVJAAAAQKfk8+A0efJk5ebmavbs2crKytKwYcO0dOlSz4QRGRkZMpsPNYwdPHhQ1113nbKyshQVFaXU1FR9/fXXGjhwoK++QquJDrEpx4g6FJwSj/d1SQAAAECnZDIMw/B1EW2pqKhIERERKiwsVHh4uK/LadSGfYXKXThRp1t+lM57Qjpxiq9LAgAAADqM5mSDZo9xWrp0qb766ivP5wULFmjYsGG6/PLLdfDgweZXiyOKCbUq24iSJLmKMn1cDQAAANB5NTs4/elPf/I8RHb9+vW67bbbdM4552jnzp2aOXNmixfYmUWHWJUtd3CqKtjv42oAAACAzqvZY5x27tzpGU/0+uuv69xzz9X999+vtWvX6pxzzmnxAjszW4BFhRb3hBDVBfvVPidVBwAAANq/Zrc4Wa1WlZWVSZI++eQTnX322ZKk6OhoT0sUWk65PV6SZBRn+bgSAAAAoPNqdovTKaecopkzZ+rkk0/W6tWrtWTJEknS1q1b1a1btxYvsLOrDkmUKqWAUoITAAAA4CvNbnF68sknFRAQoNdee01PP/20unbtKkn68MMPNX78+BYvsLMzQhMlSdaKPMnl9HE1AAAAQOfU7Ban7t2767333qu3/tFHH22RguDNGhGvasOsAJNLKsmWwrv4uiQAAACg02l2i9PatWu1fv16z+e3335bkyZN0l/+8hc5HI4WLQ5SVGiQMg33BBEqyPBtMQAAAEAn1ezg9Lvf/U5bt26VJO3YsUOXXnqpgoOD9eqrr+rPf/5zixfY2cWE2JRhuCeIUP5O3xYDAAAAdFLNDk5bt27VsGHDJEmvvvqqxowZo//9739atGiRXn/99Zaur9OLCbUeCk4Hd/m0FgAAAKCzanZwMgxDLpdLkns68tpnNyUnJysvL69lq0NNi1OC+wPBCQAAAPCJZgen4cOH6+9//7teeOEFff7555owYYIk94NxExISWrzAzi46hBYnAAAAwNeaHZzmz5+vtWvXasaMGbrrrrvUp08fSdJrr72m0aNHt3iBnV1sna56xkHGOAEAAAC+0OzpyIcMGeI1q16thx9+WBaLpUWKwiFRdVqcTCXZkqNMsgb7uCoAAACgc2l2cKq1Zs0abdq0SZI0cOBAnXjiiS1WFA4JtJhlCYlWYXWwIkxlUsFuKX6Ar8sCAAAAOpVmB6ecnBxNnjxZn3/+uSIjIyVJBQUFOv3007V48WLFxcW1dI2dXlKEXRl58Rps2uUe50RwAgAAANpUs8c43XTTTSopKdHGjRuVn5+v/Px8bdiwQUVFRbr55ptbo8ZOLynCzgQRAAAAgA81u8Vp6dKl+uSTTzRgwKFWj4EDB2rBggU6++yzW7Q4uCVFBB2akpyH4AIAAABtrtktTi6XS4GBgfXWBwYGep7vhJaVFEmLEwAAAOBLzQ5OZ5xxhm655Rbt37/fs27fvn269dZbdeaZZ7ZocXCjqx4AAADgW80OTk8++aSKioqUkpKi3r17q3fv3urZs6eKior0xBNPtEaNnZ67q15NcCrYLdGyBwAAALSpZo9xSk5O1tq1a/XJJ59o8+bNkqQBAwYoPT29xYuDW5eIIO03YlVtmBVQXSGVZEnhXXxdFgAAANBp/KrnOJlMJp111lk666yzWroeNCAhwianLNpnxKqHKcfdXY/gBAAAALSZJgWnxx9/vMknZErylmcLsCg21KqMynj1UE1w6jHa12UBAAAAnUaTgtOjjz7apJOZTCaCUytJigjS7uwEnaoNUt42X5cDAAAAdCpNCk47d/LsIF9LjLBra1Y394fczb4tBgAAAOhkmj2rXlOFh4drx44drXX6TqdLhF1bjWT3h5yffVsMAAAA0Mm0WnAyDKO1Tt0pJUUGaaurpsXp4C7JUerTegAAAIDOpNWCE1pWUoRd+QpXgTnKvSJ3i28LAgAAADoRglM7kRQRJEn6RTWtTjmbfFgNAAAA0LkQnNqJpAi7JGlDVVf3CsY5AQAAAG2m1YKTyWRqrVN3SgnhdplM0iYXLU4AAABAW2NyiHbCGmBWbKjt0AQRBCcAAACgzbRacPrwww/VtWvX1jp9p+SekrwmOBXvl8oP+rYgAAAAoJNo0gNw65o5c2aD600mk+x2u/r06aPzzz9fp5xyyjEXB29JEUH6cW+wSuxJCq3IlHI2Sz1G+bosAAAAoMNrdnD64YcftHbtWjmdTvXr10+StHXrVlksFvXv319PPfWUbrvtNn311VcaOHBgixfcmXWLcs+sl2lNUd+KTPcEEQQnAAAAoNU1u6ve+eefr/T0dO3fv19r1qzRmjVrtHfvXp111lm67LLLtG/fPo0ZM0a33npra9TbqaXEhkiStivZvSJ3sw+rAQAAADqPZgenhx9+WPfdd5/Cw8M96yIiInT33XfroYceUnBwsGbPnq01a9a0aKGQetYEp3WVSe4V2UxJDgAAALSFZgenwsJC5eTk1Fufm5uroqIiSVJkZKQcDsexVwcvtcFpZUlNcMr6SXK5fFgRAAAA0Dn8qq56V199td58803t3btXe/fu1ZtvvqlrrrlGkyZNkiStXr1axx13XEvX2uklhttlCzBrk7OrXAF2qbJIOrDd12UBAAAAHV6zg9MzzzyjM888U5deeql69OihHj166NJLL9WZZ56phQsXSpL69++vf/3rXy1ebGdnNpuUEhMipywqijrevXLf974tCgAAAOgEmj2rXmhoqJ599lk9+uij2rFjhySpV69eCg0N9ewzbNiwFisQ3nrGhmhLdrH2Bg1QpL6X9q2Rhl3u67IAAACADq3ZLU4vvviiysrKFBoaqiFDhmjIkCFeoQmtq3ZmvY3mvu4V+5iEAwAAAGhtzQ5Ot956q+Lj43X55Zfrgw8+kNPpbI26cAQ9Y4MlSd9UprhXZG2Qqip8VxAAAADQCTQ7OGVmZmrx4sUymUy65JJLlJSUpBtvvFFff/11a9SHw/SMdbfufXcwVAqOlVxVUtZ6H1cFAAAAdGzNDk4BAQE699xz9dJLLyknJ0ePPvqodu3apdNPP129e/dujRpRR0pNi9O+wgo5u6a6V9JdDwAAAGhVzZ4coq7g4GCNGzdOBw8e1O7du7Vp06aWqgtHEBdqU4jVolKHUwWRgxWjZQQnAAAAoJU1u8VJksrKyvTSSy/pnHPOUdeuXTV//nxdcMEF2rhxY0vXh8OYTCb1jHNPELHL3t+9kinJAQAAgFbV7OB06aWXKj4+Xrfeeqt69eqlFStWaPv27brvvvtUXV3dGjXiMCkx7uC0QTVdI/N3SGX5PqwIAAAA6NiaHZwsFoteeeUVZWZm6sknn9Txxx+vf/7zn0pLS9PQoUNbo0YcpmfNlOSbCwOkmJppyTNW+bAiAAAAoGNrdnCq7aK3cuVKTZ06VUlJSXrkkUd0+umn65tvvmmNGnGY2uC0I7dU6jnGvXLnFz6sCAAAAOjYmjU5RFZWlhYtWqR///vfKioq0iWXXKLKykq99dZbGjhwYGvViMP0jnNPSb49p0TGKWNk+v7f0o7PfVwVAAAA0HE1ucVp4sSJ6tevn3788UfNnz9f+/fv1xNPPNGateEIjksIk9kkHSh1KC92pHtl7iapJMe3hQEAAAAdVJOD04cffqhrrrlG9957ryZMmCCLxdKadaERQVaLp7vehoIAKXGwewPd9QAAAIBW0eTg9NVXX6m4uFipqalKS0vTk08+qby8vNasDY0Y2CVCkrQps0jqOda9cifd9QAAAIDW0OTgdNJJJ+nZZ59VZmamfve732nx4sXq0qWLXC6XPv74YxUXF//qIhYsWKCUlBTZ7XalpaVp9erVTTpu8eLFMplMmjRp0q++dns1IClMkrQps/hQcGKcEwAAANAqmj2rXkhIiK6++mp99dVXWr9+vW677TY98MADio+P13nnndfsApYsWaKZM2dqzpw5Wrt2rYYOHapx48YpJ6fx8Tq7du3S7bffrlNPPbXZ1+wIBiSFS5J+3l8o9RglmQOkgt3SwV2+LQwAAADogJodnOrq16+fHnroIe3du1cvv/zyrzrHvHnzdN1112n69OkaOHCgFi5cqODgYD333HNHPMbpdOqKK67QPffco169ejV6/srKShUVFXktHcGgmuC0M69UFeZgqWuqewPjnAAAAIAWd0zBqZbFYtGkSZP0zjvvNOs4h8OhNWvWKD09/VBBZrPS09O1atWRH+h67733Kj4+Xtdcc81RrzF37lxFRER4luTk5GbV6K/iwmyKCbHKZUhbsoqlXqe7N2z7yLeFAQAAAB1QiwSnXysvL09Op1MJCQle6xMSEpSVldXgMV999ZX+/e9/69lnn23SNWbNmqXCwkLPsmfPnmOu2x+YTCYN7FLTXS+zSOo33r1h+6dSVbkPKwMAAAA6Hp8Gp+YqLi7WVVddpWeffVaxsbFNOsZmsyk8PNxr6ShqxzltyiySkoZJ4V2lqlImiQAAAABaWIAvLx4bGyuLxaLs7Gyv9dnZ2UpMTKy3/y+//KJdu3Zp4sSJnnUul0uSFBAQoC1btqh3796tW7QfOTSzXpFkMkn9zpG+e1ba8v6hFigAAAAAx8ynLU5Wq1Wpqalavny5Z53L5dLy5cs1atSoevv3799f69ev17p16zzLeeedp9NPP13r1q3rMOOXmmpgUu2znIrlchlS/wnuDVs+lFxOH1YGAAAAdCw+bXGSpJkzZ2rq1KkaPny4Ro4cqfnz56u0tFTTp0+XJE2ZMkVdu3bV3LlzZbfbdfzxx3sdHxkZKUn11ncGveJCZLWYVVJZrT0Hy9Qj5RTJFiGV5kp7v5e6p/m6RAAAAKBD8Hlwmjx5snJzczV79mxlZWVp2LBhWrp0qWfCiIyMDJnN7WooVpsJtJg1sEu41u0p0A8ZBeoR01U67mxp/avu7noEJwAAAKBFmAzDMHxdRFsqKipSRESECgsLO8REEfe997P+/dVOXXVSD9036XhpwxvSa9Ol6F7STWvdY58AAAAA1NOcbEBTTjuX2iNKkrRm90H3ir5nS4HBUv4Oad9aH1YGAAAAdBwEp3auNjhtzipSSWW1ZAuV+p/r3vjjyz6sDAAAAOg4CE7tXEK4XV0jg+QypHUZBe6VQy91v254Xap2+Kw2AAAAoKMgOHUAw1MO667X6zQpNFEqz5e2f+y7wgAAAIAOguDUAXjGOWXUBCezRRpysfs93fUAAACAY0Zw6gBO7O4OTj/sPiinq2aSxKGXuV+3LpPK8n1UGQAAANAxEJw6gP6JYQq2WlRcWa1tOcXulQmDpMTBktMhrfufbwsEAAAA2jmCUwcQYDFrWHKkJOn7XQcPbRhxnft19T8ll7PtCwMAAAA6CIJTBzEiJVqS9M2OA4dWDr5YCoqSCnZL2z7yUWUAAABA+0dw6iBO7RsrSfpqe96hcU7WYOnEqe733y70UWUAAABA+0dw6iCGJkcqzBaggrIqbdhXeGjDiGskk1nasULK2eyz+gAAAID2jODUQQRazBrdJ0aS9MXW3EMbIrtL/Se436960geVAQAAAO0fwakDObVvnCTpy2153htG3eR+/fFlqSCjjasCAAAA2j+CUwcy9jh3cFqbcVDFFVWHNnRPk3qOlVzV0pfzfFQdAAAA0H4RnDqQ5OhgpcQEq9plaNUvB7w3nnan+/WHF6XCvW1fHAAAANCOEZw6mDE1rU5fbMv13tBjtJRyquSqkr561AeVAQAAAO0XwamDGVMzzmnFllwZhuG9cewd7tc1/5Hyd7RxZQAAAED7RXDqYEb3iZE90Ky9B8u1cX+R98aep0q9z3C3On1yt0/qAwAAANojglMHE2wN0GnHxUuSPtyQWX+Hs//ufq7Tz29Lu1e1cXUAAABA+0Rw6oB+MzhRkvTB+qz63fUSBkknTnG/X/YXyeVq4+oAAACA9ofg1AGd0T9e1gCzduaVakt2cf0dTr9LsoZK+9dKP/6v7QsEAAAA2hmCUwcUZg/0TBLx4fqs+juExh+aKOKjv0qlefX3AQAAAOBBcOqgzqnprtfgOCdJOukPUsJgqfygu8seAAAAgCMiOHVQZw5IUKDFpK3ZJdrWUHc9S6A08TFJJumnJdIvn7Z5jQAAAEB7QXDqoCKCAjW25mG4r63d2/BO3VKlkde7379zs1Re0DbFAQAAAO0MwakDu3h4siTp9TX7VOU8wux5Z/5NikqRCvdIH/yp7YoDAAAA2hGCUwd2Rv94xYZalVdSqRVbchveyRYm/fZZ97Od1r8ibXi9bYsEAAAA2gGCUwcWaDHrghO6SpJe+X7PkXdMHimNqWltevdWKX9nG1QHAAAAtB8Epw7ukpruep9uzlFOccWRdxzzJ6nbSKmyUHrlKqmqvI0qBAAAAPwfwamD65sQphO6R8rpMvTG2n1H3tESKF28SAqOlbLWS+/fJhlGm9UJAAAA+DOCUydw6Qh3q9MLq3ar+kiTREhSRFfpoufc453WvSR9+0wbVQgAAAD4N4JTJ3D+sK6KDrFqX0G5lm3MbnznXmOl9Lvd75fNkrZ82Or1AQAAAP6O4NQJ2AMtuvKkHpKkf3214+gHjL5ZOnGKZLik166W9v/QyhUCAAAA/o3g1ElcdVIPWS1m/ZBRoDW7Dza+s8kkTZgn9TpdqiqTXrxIyt3aNoUCAAAAfojg1EnEhdl0/rAukqR/N6XVyRIoXfIfKWmoVJYn/fc8pikHAABAp0Vw6kSuObWnJGnphixtzyk++gH2COnKN6W4AVJxpjs8HdzdylUCAAAA/ofg1In0TwzXWQMT5DKkx5Zvb9pBITHSlLek6F5SQYb0/G+kA7+0ap0AAACAvyE4dTJ/TO8rSXrvp/3amt2EVidJCkuUpn0gxR4nFe1zh6fsja1YJQAAAOBfCE6dzKAuEfrN8YkyDOmxT7Y1/cDwJHd4SjheKsmWnvuNtPPL1isUAAAA8CMEp07olppWp/fXZ2rj/sKmHxgaJ019V+o+SqoslF78rbT+tVaqEgAAAPAfBKdOqH9iuCYOdc+w9/f3NskwjKYfHBwtXfWWNOA8yemQXr9GWn6f5HK1TrEAAACAHyA4dVJ/HtdP1gCzVu04oI9+zm7ewYF26eJF7gflStKXj0iLL5fK8lu8TgAAAMAfEJw6qeToYF1XMz35/R9sUmW1s3knMFuks++TLvinZLFJWz+UFp4q7V7VCtUCAAAAvkVw6sRuOK2P4sJs2n2gTM+v3PXrTjJ0snTNMvd05UV7pUXnSCselFzNDGIAAACAHyM4dWIhtgDdMb6/JPcMe3vyy37dibqcIP3uC2nIpZLhklbcL/1nolS4twWrBQAAAHyH4NTJXXhiV6X1jFZ5lVN3vbWheRNF1GULk377jHTBM5I1VNq9UnpqlPTdv5g4AgAAAO0ewamTM5lMmvvbwbIGmPXF1ly98+P+Yzvh0EvdrU9dh0uVRdL7t0nPj5dyNrVMwQAAAIAPEJygXnGhuun0PpKke979WbnFlcd2wpje0jUfSb95yN36tOdb98QRn/5dqqpogYoBAACAtkVwgiTpd2N7q39imPJLHfrzaz/++i57tcwWKe130o3fSsf9RnJVSV88LD05wv3Q3GM9PwAAANCGCE6QJFkDzHr8shNkDTDrsy25evGb3S1z4ohu0mUvSxf/RwrrIhVmuB+a+68zpYxvWuYaAAAAQCsjOMHjuIQwzfqNe5a9v7+/SVuzi1vmxCaTNGiSdNMa6fS/SoEh0r410nPjpCVXSTmbW+Y6AAAAQCshOMHLtNEpGntcnCqrXfr9i2tUXFHVcie3Bktj/yTdvFY6cYpkMkub3pGeOkl6dZqU/XPLXQsAAABoQQQneDGZTJp3yVAlRdi1I7dUf37tp2Mf73S4sETpvCek338lDZgoyZA2vik9PUp6ZYqUtb5lrwcAAAAcI4IT6okJtempK05UoMWkDzdk6Z9f7GidCyUMkia/KP1+pTTgPPe6n9+WFp4i/XeStP0TJpEAAACAX/CL4LRgwQKlpKTIbrcrLS1Nq1evPuK+b7zxhoYPH67IyEiFhIRo2LBheuGFF9qw2s7hhO5Rmn3uQEnSA0s36+Ofs1vvYonHS5NfkP7wtTToAncXvh2fSS9e6H6I7toXmMYcAAAAPuXz4LRkyRLNnDlTc+bM0dq1azV06FCNGzdOOTk5De4fHR2tu+66S6tWrdJPP/2k6dOna/r06Vq2bFkbV97xXXlSD12e1l2GId388g/asK+wdS+YMEi6eJF08w9S2h/ck0jkbpLemSHNGyAtu0vK3dq6NQAAAAANMBktPoCledLS0jRixAg9+eSTkiSXy6Xk5GTddNNNuvPOO5t0jhNPPFETJkzQfffdd9R9i4qKFBERocLCQoWHhx9T7Z1BldOlqxd9py+35Skh3KbX/zBa3aKC2+bi5QXSmkXS6n9KRfsOre9xspQ6zd29L9DeNrUAAACgw2lONvBpi5PD4dCaNWuUnp7uWWc2m5Wenq5Vq1Yd9XjDMLR8+XJt2bJFY8aMaXCfyspKFRUVeS1oukCLWQuuOFHHJYQqu6hSV/17tfJKKtvm4kGR0il/lG75SbpsiftBuiaztHul9MZ10rz+0tJZUtYGxkIBAACgVfk0OOXl5cnpdCohIcFrfUJCgrKyso54XGFhoUJDQ2W1WjVhwgQ98cQTOuussxrcd+7cuYqIiPAsycnJLfodOoNwe6D+e3WaukYGaWdeqaY+t1pFLTlN+dFYAqR+46XLF0t/3CCd9hcpvJtUflD65ilp4cnusVBfPCId3NV2dQEAAKDT8PkYp18jLCxM69at03fffad//OMfmjlzplasWNHgvrNmzVJhYaFn2bNnT9sW20EkRtj1wjUjFRNi1cb9RZr63OqWfcZTU0V0lU67Q/rjT9Llr0r9z5UsVvdYqE/vkx4bKv3rLOnbZ6SShsfJAQAAAM3l0zFODodDwcHBeu211zRp0iTP+qlTp6qgoEBvv/12k85z7bXXas+ePU2aIIIxTsdm4/5CXf7styosr9IJ3SP136tHKswe6NuiygukTe9K61+Vdn0pGS73epNZSjlF6j9R6j/BHboAAACAGu1mjJPValVqaqqWL1/uWedyubR8+XKNGjWqyedxuVyqrGyjcTed3KAuEXrp2jRFBAXqh4wCXfmvb3WgrcY8HUlQpHTiVdLUd6SZm6TxD0hdU90BaucX0od/kh4dKP3zdHd3vtwtvq0XAAAA7Y7PZ9VbsmSJpk6dqmeeeUYjR47U/Pnz9corr2jz5s1KSEjQlClT1LVrV82dO1eSe8zS8OHD1bt3b1VWVuqDDz7QnXfeqaefflrXXnvtUa9Hi1PL2LCvUFf9+1sdLKtSr9gQ/efqkUqObqPZ9poqf4e06T1p83vSntWS6vxRj+kr9fuNdNw4KTlNsvi41QwAAABtrjnZIKCNajqiyZMnKzc3V7Nnz1ZWVpaGDRumpUuXeiaMyMjIkNl8qGGstLRUN9xwg/bu3augoCD1799fL774oiZPnuyrr9ApHd81Qq/+frSmPrdaO/JKdeHTX+s/V4/UgCQ/CqPRvaSTb3YvxdnSlg/cIWrH59KBbdLX26SvH5ds4VLv06W+46Q+6VJYwtHPDQAAgE7F5y1ObY0Wp5aVVVihqc+t1pbsYoXZA/TslOE6qVeMr8tqXEWRtO0j97L9E6nsgPf2pGHuANXrNCl5pBRg80WVAAAAaGXNyQYEJxyzwrIqXfff77V6V76sAWY9fNEQnT+snUzE4HJK+384FKT2/+C9PSBI6jFK6jnWHaQSh0jmdjkZJQAAAA5DcGoEwal1VFQ5dfPLP+ijn7MlSdee0lN3/qa/AiztLGSU5EjbPpZ2rHAvpYdNaR4UJaWc6g5RvU5zdwc0mdq+TgAAABwzglMjCE6tx+ky9H8fbdFTK36RJI3uHaMnLjtBMaHttKubYUi5m2tC1OfSrq8kR7H3PqEJUvdRUo/RUveTpITjJbPFJ+UCAACgeQhOjSA4tb4P12fqtld/VJnDqa6RQVp4ZaoGd4vwdVnHzlkt7V/rDlE7Vkh7V0tOh/c+tnD3uKjaMNXlRCnQ7pNyAQAA0DiCUyMITm1ja3axfvfCGu3MK5U1wKw7x/fXtNEpMps7ULe2qgpp3xop42sp4xsp49v6LVIWqzs89RjlDlNdh0shfj55BgAAQCdBcGoEwantFJZX6bZX1umTTe5xQmOOi9MjFw1RfHgHbYFxOaXsDdLuVe4wtXtV/TFSkhSV4g5Q3Ya7XxMH0yoFAADgAwSnRhCc2pZhGHrhm936x/ubVFntUlRwoB64cIjGDUr0dWmtzzDcD+HNWOUOUXtXS3lb6+9nDpQSj/cOUzG9mXQCAACglRGcGkFw8o1t2cW6ZfE6/ZxZJEmaPDxZf5kwQBFBgT6urI2VF7jHSe1dI+37Xtr7vVSWV38/e6TU5QQpaaiUNMT9bKmonkyFDgAA0IIITo0gOPlOZbVT8z7eqn9+sUOGIcWH2XTv+cdr/PGdoPXpSAxDKtjtDlD71rhfM3+UnJX197WGubv1ecLUUCm2n2QJaPu6AQAAOgCCUyMITr63eme+7nz9J+3IK5UkjR+UqHvOH6SEjjr2qbmqHVLORmn/OinrJ3eQyt4oVVfU39dikxIGHQpSSUOl+EGMmQIAAGgCglMjCE7+oaLKqSc/3a6Fn/+iapehMHuA/jyuny4b2b39PTS3LTir3eOjMn88FKay1kuVRfX3NVmkuP7uMJUwSIof6H4NTWDcFAAAQB0Ep0YQnPzLz/uLdOcbP+mnvYWSpP6JYZozcZBG9WbK7qNyuaSDOw8FqcwfpcyfGh4zJUnBMYdCVPxA98N64/tL1pC2rRsAAMBPEJwaQXDyP06XoZe+3a3/+2irCsurJEnnDE7UrN8MUHJ0sI+ra2cMQyraXxOmfnJ3+cv+Wcr/RTJcDRxgck+PXrdlKmGQFN1LMlvaunoAAIA2RXBqBMHJfx0sdWjex1v10re75TIkW4BZ00an6A+n9VZksNXX5bVvVeVS7mZ3iMr52T1mKntjw8+ZkqQAuxTXzx2mYo9zv4/rL0X2YDIKAADQYRCcGkFw8n+bMot077s/a9WOA5KkcHuAfn9ab00f3VNBVlpBWlRpnjtA5fzsfnhv9s/ugFVV1vD+FqsU06dOmOrnntkvpg8TUgAAgHaH4NQIglP7YBiGPtuSo4eWbtHmrGJJ7unLbz6zry4ZnixrABNItJrasVPZG6XcLVLelprXbVJ1ecPHmMzu1qi4fjWhqv+h93Z+zwAAgH8iODWC4NS+OF2G3l63T//30VbtK3D/T3uXCLv+cHofXTK8m2wBtEC1GZdLKsyQcrfWCVNb3S1UFYVHPi40wd0iFdNbiulb02LV1x20AuiCCQAAfIfg1AiCU/tUWe3U/77N0NMrflFOsfvhsEkRdv3htN66ZHiy7IEEKJ8xDKkkp36Yyt0qlWQd+TiTRYrqcShMxfR2B6qYPlJYElOnAwCAVkdwagTBqX2rqHJq8eoMPf35L8oucgeo2FCbpo3uoStP6sEkEv6molA68It0YPuhJW+be11V6ZGPCwypaaGqCVTRvWqW3lJILKEKAAC0CIJTIwhOHUNFlVOvfr9HT6/4RfsLKyRJQYEWTR6RrGtO6ck05v7OMKTirJowte1QuMrbJh3cJRnOIx9rDZOie9YJU3WWsERCFQAAaDKCUyMITh1LldOl93/K1DNf7NCmzCJJktkk/WZwkq4/tZeGJkf6tkA0n7NKOrj7UKjK3+EOVvk7pcI9khr5KyswWIrqWT9YRaVI4V2ZSh0AAHghODWC4NQxGYahldsP6JkvftGX2/I860ekROmqUSkaPyiRmfg6gupKd6jK31F/KchovKXKHCBFdHOHqMge7teoFPc4q6ieUlAUrVUAAHQyBKdGEJw6vk2ZRXr2ix1658f9qna5/3jHhtp02chkXTayu7pEBvm4QrSKaoe7RepIocrpaPx4W3hNoKobqmqWiGSeUwUAQAdEcGoEwanzyC6q0MurM/S/bzM8M/GZTdJZAxN01UkpOrlPjEy0MHQOLpdUnOkeP1Ww2/16cJe79ergrsZn/6sVEi9FJrtDVEQ3KbK7+33tuqDIVv0KAACg5RGcGkFw6nyqnC59/HO2Xli1W6t2HPCsT4kJ1sXDk3Xhid2UGEFrQqdWVe5ulaobpuqGLEfJ0c9hC68TpLrVed/d/RoSL5npLgoAgD8hODWC4NS5bcsu1ovf7Nbra/eppLJakrsVauxxcZo8Illn9E9gLBS8GYZUftAdrAr3SIV7pYI97ocBF+xxrys7cPTzWKyHAlXdlqra1/CuPBAYAIA2RnBqBMEJklRaWa0P1mfq1e/3avWufM/66BCrLjihqy4e3k39E/nzgSZylLoDVeGeQ2Gq7mvxfslwHeUkJveDf726A9ZpsYpIlmyhbfJ1AADoLAhOjSA44XA7ckv02pq9em3NXs9YKEnqnxim84Z10XlDu6hbFM+FwjFwVrnHWHmFqjotVoV7peqKo58nKModqMK7SeFdapau7teIbu7gZeXPKgAATUVwagTBCUdS7XTpi225euW7vfp0c44czkMtBCNSonT+sK46Z3CSokPoToUWZhhSaZ53mPKEqpr3FQVNO1dQ1KEwFd6l4ZBFyxUAAJIITo0iOKEpCsuqtHRjpt76Yb++2XlAtb8lAWaTxh4Xp/OGddFZAxMUbOWBqmgjlcU1YWqvVLRPKtpfs+w79NqUSSwkyRYhRXT1DlRhie4Wq9rX4FgmswAAdHgEp0YQnNBcmYXleu/HTL21bp827i/yrA+2WnR6v3iNPz5Rp/ePV6iNEAUfMgypssg7TBXuOyxk7ZcqC5t2PnOAFJpQJ1Al1Q9XYYk8OBgA0K4RnBpBcMKx2J5TrLfX7dfb6/YrI7/Ms94aYNaYvrEaf3yS0gfEKzKY7nzwU5XFh7VU7Xe3YpVku8dhFWdJJTmSmvifBout4UB1+KstjIAFAPA7BKdGEJzQEgzD0E97C7V0Y5aWbsjSzrxSz7YAs0mjesdo3KBEnT0oQfFhPCMK7YyzWirNORSk6r4W1Xlfnn/0c9UKDDksUNV5H5pQs8RL9ggCFgCgzRCcGkFwQkszDENbs0v04YZMLd2Qpc1ZxZ5tJpM0vEeUxh+fpHGDEpidDx1LdWVNiMpqOGTVbmtq90DJ3YJVG6IOf/WErHj3A4UD+UcJAMCxITg1guCE1rYzr1TLNmbpww1Z+nFPgde2/olhSh+QoDMGxGtot0hZzPzLOjoBR2kjASvL3U2wJKd5AUtyt07Vba3yvCbW+ZwgBccw0QUAoEEEp0YQnNCW9heUe0LU97vy5arz2xYTYtXp/eN1Zv94nXpcHJNLAFXl7gBVklMTprLrvK/7miU5HU0/r8kihcR5h6nQ+JqWqzgpJLbmNU4KipYs/C4CQGdBcGoEwQm+crDUoRVbc7R8U44+35qr4opqz7ZAi0kn9YrRGf3jdXq/eKXEhviwUsDPGYb7uVb1QlUDQas0T02e6EKSZJKCow8FqeCYQ+/rBqzaz4zJAoB2jeDUCIIT/EGV06XvduXr0005Wr45x2tyCUnqHh2sMcfFakzfOI3uE0trFPBrOavc4alewKoJVaW5h17LDqh5IUuSObCBUBXbwOea94FBrfI1AQC/DsGpEQQn+KMduSVavilHn27O0fe781XlPPRrGWA26cQeURp7XJzG9I3ToC7hMjM2Cmh5LqdUll8TpnK9Q5VXwMpzv68sOvo5D2cNPRSkgmOlkBh3q1ZwbM1r7RLt3s8WTosWALQiglMjCE7wd6WV1Vr1ywF9sS1XX2zN1a4DZV7bY0KsOqWvuzXq1ONime4c8JWqipoQdXjAOkLgas64rFrmgMMCVQNLyGGfadUCgCYjODWC4IT2JuNAmT6vCVGrfjmgkspqr+3HJYRqVK8Yjeodq5N6RfPwXcAfGYa7hao0r06oynF3DyzLr3mtWUprXqtKj37ehgQGNz1kBccwIQaATo3g1AiCE9qzKqdLa3cfrGmNytP6fd7TN5tM0sCkcI3qFaPRfWI0IiVaYfZAH1UL4JhUldcPVQ0tpXXeu6p+3bXskfW7CgZFSUGR7mAVFOVeguu8t4bSjRBAu0dwagTBCR1JfqlD3+44oFU7DujrXw5oe06J13aL2aTBXSM0uneMRvWO0fAe0QqyWnxULYBWZRhSZfFhrVh5hwWtmvWlNevLD6rZE2LUMgc2EKiia8LWYSGrbviyhhC4APgNglMjCE7oyHKKKrRqxwGt+sUdpnYfNj4q0GLSCclROql3jE7qGa1h3SMVbKWLDtBpuZxSecFh4SrPHajKD7qDVvlB9z7l+YfWOSt//TUt1vqBKjiq4ZBVN3wFBhO4ALQ4glMjCE7oTPYVlGvVLwf09S95WvXLAWUWVnhtDzCbNKhrhEamRGl4SrRGpEQrOoQxUgCOwlFWE6jyGwhZtesL6q/7NRNk1LJY3V0K7RHuVq3mvLeFS2bzsX5rAB0QwakRBCd0VoZhaPeBMk+L1He78usFKUnqEx+qESnRGpESpREp0eoWFSQT/8oL4FgZhlRVdljIOjx4FTQcxn7t2K1aJrM7PDUpaEXWvI849N7CWFGgoyI4NYLgBLgZhqF9BeX6ble+Vu88qO935WvbYWOkJCkpwq7hKdEamRKlET2jdVx8GM+RAtB2DENylLoDVEWBVFHoDlgVBTWvhY2/r67/D0TNFhjccKA6/L0tXLKH13lf88qshYDfIjg1guAEHFl+qUPf78rX97sPavXOfG3YV6hql/dfEeH2AA1PiVZqjyid2D1KQ5MjGCcFwH9VVRw9XNUGMU8oq1n/ax5y3JDAEHegqg1WtaGq7jp75JG309UQaDUEp0YQnICmK3NUa11Ggb7bdVDf7crX2oyDKnM4vfaxmE3qnximE7pH6sTu7jDVIyaY7n0A2j9ntTs8NbV1q6KoZv+a16qyRk7eTLbDg1e4dyuXJ2w11PIVzvTxwBEQnBpBcAJ+vWqnSz9nFmn1TneI+iGjoMFxUtEhVp2QHOkJU0OTIxVio1UKQCfjrKoJUYV1QlWhd8CqKDxse+26mvfHMoNhXZ5xXuGSrW5rVljNElrzWmedNfSwfcKYTh4dDsGpEQQnoGVlFpZr7e4C/ZBxUGszDmrDviI5nC6vfcwm6biEMJ3YI0onJEdqaHKkeseFysJYKQBoXHVl0wKWJ5Q1sM5V3YIFmbyDlCdghTUxiNVZF8AsrvA9glMjCE5A66qsdurn/UVam1GgtRkHtS6jQPsKyuvtF2K1aFDXCA3tFqEh3SI1tFukkqOZwQ8AWpRhSFXlh4Wpgpr3xe7FUVLzvs66ymKp8rD1hvOol2sWi/WwEFbTpdAW6n61hrpbuGw1r9awOp8P3x7K7If4VQhOjSA4AW0vu6hCa3e7W6R+3FOo9fsKVV5V/z/AUcGBNSHKHaaGJEcoPszug4oBAF48Aaw2aB0eshpYjrRfS479qstiazxYWUMOrbc14TMPXe4UCE6NIDgBvud0GdqeU6If9xbop70F+nFPoTZnFanKWf+vo6QIu4bUaZUa3C1CEUH8qyIAtFvO6jqtXLUhq+a1osg9/byj1L3OUepu+XLULrWf62w/lgcrN8pUJ3A1FsiO8Nka4g5fdd8HBjNDop9pd8FpwYIFevjhh5WVlaWhQ4fqiSee0MiRIxvc99lnn9V///tfbdiwQZKUmpqq+++//4j7H47gBPinymqnNmUWe4LUT3sLtD23RA39DdUjJliDuoRrUJcIz2tcmK3tiwYA+F6141Co8oSr4jqfS+qErcM/Hx7QavZRK/7vcW2A8gpXwe7AdbT3gSGHvQbX2UYL2a/RroLTkiVLNGXKFC1cuFBpaWmaP3++Xn31VW3ZskXx8fH19r/iiit08skna/To0bLb7XrwwQf15ptvauPGjeratetRr0dwAtqPkspqbdjnDlE/7nW/7smvP15KkuLDbBrUJVzHdz0UprpFMWYKANBMLpdUXX6Elq4S75B2xEBW81pVJjnKpKrStqndK0g1FK7qrPO0ggV5B7h64SzI/b6DTubRroJTWlqaRowYoSeffFKS5HK5lJycrJtuukl33nnnUY93Op2KiorSk08+qSlTphx1f4IT0L4dLHVo4/4ibdxfqI37i7Rhf6F25pU22DIVbg/QwC7hOr5LhAZ1dYepXrEhCrDQTQIA0IZqw5ijzB2qagPVkd5X1baGNfS+rGa/Uve4s+qG/0GxxZkDDgteNeHqiMHrsNawul0Xa9eHJkqBvh3L3Jxs4NMHqzgcDq1Zs0azZs3yrDObzUpPT9eqVauadI6ysjJVVVUpOjq6we2VlZWqrDz0DISiohZ6CjgAn4gKseqUvrE6pW+sZ11pZbU2ZxW5A9U+d5jaml2soopqfbMjX9/syPfsaw80q39iuKdVakBSmPolhinYynOmAACtxGw+1DVPcS17bpfLHaQ8YapO+Koqr7/u8ODV0HF196mdTdFV7Z4Sv7Kw5Wqf+p7U89SWO18r8+n/KeTl5cnpdCohIcFrfUJCgjZv3tykc9xxxx3q0qWL0tPTG9w+d+5c3XPPPcdcKwD/FWILUGqPaKX2OPQPKI5ql7blFGvj/iL9vL9IG/YValNmkUodTq3bU6B1ewo8+5pMUo/oYPVPDFf/pDD1TwzXgKQwJUcFy8yzpgAA/sxsrnluVmjrnL/a0UDgKjv0+YjBq7yR42pCmzW4dWpuJe36n1gfeOABLV68WCtWrJDd3nAz36xZszRz5kzP56KiIiUnJ7dViQB8xBpgrpk8IsKzzuUytOtAaU1XP3d3vy1ZxcoprtSuA2XadaBMSzdmefYPtlrUL/FQkOqfGK5+iWHM6gcA6DwCrO4lKMrXlficT4NTbGysLBaLsrOzvdZnZ2crMTGx0WMfeeQRPfDAA/rkk080ZMiQI+5ns9lkszHbFgDJbDapV1yoesWFauLQLp71B0oqtSWrWJuyirU5s0ibs4q1JbtYZQ6nfsgo0A8ZBV7n6RoZpP6JYeqfFKbjEtxLz9gQ2QMtbfyNAABAW/FpcLJarUpNTdXy5cs1adIkSe7JIZYvX64ZM2Yc8biHHnpI//jHP7Rs2TINHz68jaoF0FHFhNo0uo9No/scGjdV7XRp14Eybc4q0ubMYm3OKtKmzGLtKyj3LMs353j2N5ukHjEh6hsfqr4JoeobH6Y+8aHqEx9KoAIAoAPweVe9mTNnaurUqRo+fLhGjhyp+fPnq7S0VNOnT5ckTZkyRV27dtXcuXMlSQ8++KBmz56t//3vf0pJSVFWlrtbTWhoqEJDW6lvJ4BOJ8Bi9gSfc+s0aheWV2lrtrtl6ufMYm3LLvZMRLEzr1Q780r10c+HWtFNJql7dHBNoApzv9aEqiArgQoAgPbC58Fp8uTJys3N1ezZs5WVlaVhw4Zp6dKlngkjMjIyZK7zhOWnn35aDodDF110kdd55syZo7vvvrstSwfQCUUEBWpESrRGpByaiMIwDOWWVGpbdom2ZRdrW06JtmWXaGtOsQrKqrT7QJl2HyjTJ5sOtVCZTFK3qCD1jQ/zClV94kMVYvP5X80AAOAwPn+OU1vjOU4A2ophGDpQ6tDW7GJtrw1TNe8PlDqOeFzXyKCa7n7u8Vi9YkPUKy5UsaFWHugLAEALalcPwG1rBCcA/uBASaW7ZSqnRNuzi7U12/0+r6TyiMeE2QM8IapnbIh6xYWoV6z7Pd3+AABoPoJTIwhOAPzZwVKHtue6W6a2ZZdoR16pduaVaO/BcjX2t3WXCLt61gSpXnEh6hkbot5xoeoSGSQLz6ICAKBBBKdGEJwAtEcVVU7tPlCmnXkl+iW3VDty3YFqR16pCsqqjnicNcCslJhgr0DVKy5UveNCFBlsbcNvAACA/2lONmAEMgC0A/ZA98N4+yWG1dt2sNShHTWBamdeqXbklmhHbql2HyiTo9qlrdkl2ppdUu+4yOBA9YgJUY/oYKXEBKt7TEjNa7DiQm2MpwIAoA5anACgg3K6DO07WK5f8kq0M7dUO/JKalqqSpVZWNHoscFWi7pHByslJkQ9YoLdASsmWD1igpUUQfc/AEDHQFe9RhCcAEAqrazW7gNlysgv1a6a6dJ3H3C3Uu0vbHw8ldViVrfoIPWIPhSoUmJC1D0mWN2igmQLYKIKAED7QFc9AECjQmwBGtglXAO71P+PRGW1U3sPlivjQJl21YSp3QdKtTu/THvyy+RwurSjZpyVlOt1rMkkdYkI8m6lig5WcrQ7VEUEBdIFEADQLtHiBABoMqfLUGZhueehvrWtVLsOlCojv0xlDmejx4fZAtQtOljJUUFKrnntFuUOVsnRQQq28u95AIC2Q1e9RhCcAKB1GIah3JLKmpaqMmUccHcDzMgv096D5Y0+o6pWTIhV3Wpap5Kj3GEquSZYdYm00w0QANCiCE6NIDgBgG+UO5zae7BMew66g9Se/DLtyS/XnoPuLoBFFdWNHm8ySQlhdk+Yqm256hoVpK6RQUqMIFgBAJqH4NQIghMA+KfC8ip3sMovr3mtCVg168qrGu8GKElxYTZ1iQxS10i7ukYGqUvNUvs+KpgxVgCAQ5gcAgDQ7kQEBSoiKEKDukTU22YYhg6UOuqFqb0Hy7S/oFz7CspVUeVSbnGlcosr9eOehq9hDzR7glTdYNWlJmjRagUAOBKCEwDA75lMJsWG2hQbatMJ3aPqbTcMQwfLqjwhan/Nsq+gXPsKKrS/oFy5xZWqqKo7I2BD15HiQm11WqnsXq1WXSODFEmrFQB0SgQnAEC7ZzKZFB1iVXSIVcd3rd9iJbmnWc8qrKgJVhXad7AmYBUeClsVVS7lFFcqp7hS6/YUNHieoECLJ1A11GqVEG6XPZBWKwDoaAhOAIBOwRZgqXm2VEiD2+u2Wu09eKjVyh2sDrValVc59UtuqX45QquVJEUFByoxIkhJEXYlhNuVFGFXYkTNa7j7fZg9sLW+KgCgFRCcAABQ01qtKqrcrVaHugRWHApXB92vFVUuHSyr0sGyKm3KLDri9UJtAUqsE6Rqw9Whz0xmAQD+hOAEAEAT2QMtSokNUUrskVutisqrlVlUrszCCmUXViizsEJZhRXKLKr9XK6iimqVVFZre06JtueUHPF61gCzd7AKPxSu4sPtSgi3KS7MxoQWANAGCE4AALQQk8mkiOBARQQHqn/ikae1La2sVlZRnWBV5A5UWTXvsworlFfikKPapYx890OEGxMVHKiE2jAVZqt5b1N8mDtcJYTbFRdmU6DF3NJfGQA6DYITAABtLMQWoN5xoeodF3rEfSqrncopqqwJVRXKKiz3tF5lF1Uou8g99brDeahr4Oas4kavGxtqVVxtmKp5dbdc2RVfE7hiQ60KIGABQD0EJwAA/JAtwKLk6GAlRwcfcR/DMFRQVqXsYneQyi6qUG5xZU2wcq/LKapQTnGlql2G8kocyitxaFPmka9rMkmxoTZ3qAqr7Q7oDlZxtUuo+5XZAwF0JgQnAADaKZPJpKgQq6JCrOqfeOT9XC5DB8sc7nBVXKGc2lBVfChcZRdVKrekUk6X4XmQsHTkyS0kKcwWoLgwm2IPC1R1A1Z8mE3RIbRiAWj/CE4AAHRwZrNJMaE2xYTaNFBHHnvldBnKL3Uou6jCE6qya1qs8ordwSq35jlXjmqXiiurVVxZrR15R56aXXK3YsWEWBXbQLCqfR8fZlNcqF3hQQHMJAjALxGcAACAJMliNnmCjNTwlOySu4tgcWW1p2XKs5TU/3ygpFIuQ55ugkcbh2W1mA+1YoW6w1ZsqE0xoVbFhNoUG2JVbJhNMSFWRQZbZTETsgC0DYITAABoFpPJpHB7oMLtgY1OcCEdasU6UrDKLa7wfC6qqJbD6dK+mudkHY3ZJEWH2BQbanUHq5BDISu29nNNyIoNtSnIypgsAL8ewQkAALQa71asxlVUOZVXJ1zlFFfqQIlDB0rdr7UtWAdKHSooq6ppyapUXkllk2oJsVpquiy6Q1VcmPs1JtT94OOYEPd4rJhQq6KCrbIGMC4LwCEEJwAA4BfsgRZ1iwpWt6gjzyRYq8rpUn6pQ3klh8JVXrFDeTUh60BJpfLqvDqcLpU6nCptwnOxaoXZAxRTM/lGTIg7XEWH2A69D6273qpgK/9bBXRk/IYDAIB2J9BiVkLNM6iOxjAMlVRWewWp2qB1oLTSE74OljmUX+peXIZUXFGt4opq7TrQtKBlDzR7Wq2i64YtT8CyKToksObVqnA7E2EA7QnBCQAAdGgmk0lh9kCF2QPVMzbkqPu7XIYKy6t0oLQ2SFUqv7RK+aWVddY5dKDk0HuH06WKqqaPz5KkQItJUcHuboFRIYE1r1ZFBQfWX1+zjbAF+A7BCQAAoA6z+dDzsZrCMAyVOpzKr2nJyi91eALWwTrvD9SGsBKHSh1OVTkN5dSM5Woqi9mkyKDAegErMiRQ0XUCVlRwoCKD3S1eEUGBzD4ItACCEwAAwDEwmUwKtQUo1Bag7jFHH58luSfCOFjmbrUqKKtSfplDBWUOHSyt0sGyQ90GC8qqal7dYcvpMnSgJoQ1vT4pIqg2ZLlf3aHqULiqDVrubYGKDA6ULYBZCIG6CE4AAABtzB5oUVJEkJIigpp8TGW1UwVlVQ2GqvzSKnfwKnMov6yqZp1DxRXVMgypoKxKBWVV2tmMGoMCLTUhyqrIoEBPoPL+XPuewIWOj+AEAADQDtgCLEoItzRpQoxaVU5XTWhy6GDdoFXmHbwOllXpYKlDBeXufV2GVF7lVHmhU5mFFc2qszmBKyI4UBFB7iUo0ML4Lfg1ghMAAEAHFWgxN/k5WrVcLkPFldUqLKtSQbk7VBWUOVRYXuVp8XJv835/rIEr0GJSRFCgwoPcD1euDVSHL+FBgQoPCvBaF2pj0gy0PoITAAAAPMxmkyeQdFfTxmxJ9QOXJ2Q1ELgKalq8CsvdS7XLUJXTUF6JQ3klTR+/VctiNincHuAJVnVf6wWvw0JZmD1AZibPQBMQnAAAAHDMfm3gMgxDZQ6nJ0TVXYpqlga3VVSrsLxKjmqXnC7D3d2wrKrZdZtMUpgtwNNt8PBgdXgAC/cKYQEKsJibfU20TwQnAAAA+IzJZFKILUAhtgB1iWz6ZBm1KqoOC11ltcGqfgg79N4dusqrnDIMqaiiWkUV1dqjpj2Dq65Q26GWrtpWr8NbtNzPETv0Gu75TPBqTwhOAAAAaLfsgRbZA5s3aUatymqnJ0TVDVdFFYcC2OFLcU1LV0lltSSppLJaJZXVTX7w8eGCrZYjhKua97YAwpefIDgBAACgU7IFWBQXZmnW5Bm1qp0uT3fBhlq2iuoEraIK92ux57Va5VVOSVKZw6kyh1PZRU1/EPLhCF9tg+AEAAAANFOAxazoEPcDhH+NKqfLK0wV1QlVxQ0ErcPDV1FFlSqqXJIIX22F4AQAAAC0scBjDF7SrwtfRYdta8nwFRRoUXhQ08PX8JToY/r+bY3gBAAAALRDbRm+isqrVVxZu0/D4au8yqnyqqaHr8XXn6STesX86trbGsEJAAAA6KRaInw5ql0qqTxy+CoqrxO06oSv2ND209okEZwAAAAAHANrgFnRAccWvtqDjj2CCwAAAABaAMEJAAAAAI6C4AQAAAAAR0FwAgAAAICjIDgBAAAAwFEQnAAAAADgKAhOAAAAAHAUBCcAAAAAOAqCEwAAAAAcBcEJAAAAAI6C4AQAAAAAR+EXwWnBggVKSUmR3W5XWlqaVq9efcR9N27cqAsvvFApKSkymUyaP39+2xUKAAAAoFPyeXBasmSJZs6cqTlz5mjt2rUaOnSoxo0bp5ycnAb3LysrU69evfTAAw8oMTGxjasFAAAA0Bn5PDjNmzdP1113naZPn66BAwdq4cKFCg4O1nPPPdfg/iNGjNDDDz+sSy+9VDabrY2rBQAAANAZ+TQ4ORwOrVmzRunp6Z51ZrNZ6enpWrVqVYtco7KyUkVFRV4LAAAAADSHT4NTXl6enE6nEhISvNYnJCQoKyurRa4xd+5cRUREeJbk5OQWOS8AAACAzsPnXfVa26xZs1RYWOhZ9uzZ4+uSAAAAALQzAb68eGxsrCwWi7Kzs73WZ2dnt9jEDzabjbFQAAAAAI6JT4OT1WpVamqqli9frkmTJkmSXC6Xli9frhkzZrTKNQ3DkCTGOgEAAACdXG0mqM0IjfFpcJKkmTNnaurUqRo+fLhGjhyp+fPnq7S0VNOnT5ckTZkyRV27dtXcuXMluSeU+Pnnnz3v9+3bp3Xr1ik0NFR9+vQ56vWKi4slibFOAAAAACS5M0JERESj+5iMpsSrVvbkk0/q4YcfVlZWloYNG6bHH39caWlpkqTTTjtNKSkpWrRokSRp165d6tmzZ71zjB07VitWrDjqtVwul/bv36+wsDCZTKaW/BpNVlRUpOTkZO3Zs0fh4eE+qQEtj/vaMXFfOybua8fEfe14uKcdkz/dV8MwVFxcrC5dushsbnz6B78ITp1NUVGRIiIiVFhY6PM/LGg53NeOifvaMXFfOybua8fDPe2Y2ut97fCz6gEAAADAsSI4AQAAAMBREJx8wGazac6cOUyT3sFwXzsm7mvHxH3tmLivHQ/3tGNqr/eVMU4AAAAAcBS0OAEAAADAURCcAAAAAOAoCE4AAAAAcBQEJwAAAAA4CoKTDyxYsEApKSmy2+1KS0vT6tWrfV0Smujuu++WyWTyWvr37+/ZXlFRoRtvvFExMTEKDQ3VhRdeqOzsbB9WjIZ88cUXmjhxorp06SKTyaS33nrLa7thGJo9e7aSkpIUFBSk9PR0bdu2zWuf/Px8XXHFFQoPD1dkZKSuueYalZSUtOG3wOGOdl+nTZtW7/d3/PjxXvtwX/3L3LlzNWLECIWFhSk+Pl6TJk3Sli1bvPZpyt+7GRkZmjBhgoKDgxUfH68//elPqq6ubsuvgjqacl9PO+20er+vv//977324b76l6efflpDhgxReHi4wsPDNWrUKH344Yee7R3hd5Xg1MaWLFmimTNnas6cOVq7dq2GDh2qcePGKScnx9eloYkGDRqkzMxMz/LVV195tt16661699139eqrr+rzzz/X/v379dvf/taH1aIhpaWlGjp0qBYsWNDg9oceekiPP/64Fi5cqG+//VYhISEaN26cKioqPPtcccUV2rhxoz7++GO99957+uKLL3T99de31VdAA452XyVp/PjxXr+/L7/8std27qt/+fzzz3XjjTfqm2++0ccff6yqqiqdffbZKi0t9exztL93nU6nJkyYIIfDoa+//lr/+c9/tGjRIs2ePdsXXwlq2n2VpOuuu87r9/Whhx7ybOO++p9u3brpgQce0Jo1a/T999/rjDPO0Pnnn6+NGzdK6iC/qwba1MiRI40bb7zR89npdBpdunQx5s6d68Oq0FRz5swxhg4d2uC2goICIzAw0Hj11Vc96zZt2mRIMlatWtVGFaK5JBlvvvmm57PL5TISExONhx9+2LOuoKDAsNlsxssvv2wYhmH8/PPPhiTju+++8+zz4YcfGiaTydi3b1+b1Y4jO/y+GoZhTJ061Tj//POPeAz31f/l5OQYkozPP//cMIym/b37wQcfGGaz2cjKyvLs8/TTTxvh4eFGZWVl234BNOjw+2oYhjF27FjjlltuOeIx3Nf2ISoqyvjXv/7VYX5XaXFqQw6HQ2vWrFF6erpnndlsVnp6ulatWuXDytAc27ZtU5cuXdSrVy9dccUVysjIkCStWbNGVVVVXve3f//+6t69O/e3Hdm5c6eysrK87mNERITS0tI893HVqlWKjIzU8OHDPfukp6fLbDbr22+/bfOa0XQrVqxQfHy8+vXrpz/84Q86cOCAZxv31f8VFhZKkqKjoyU17e/dVatWafDgwUpISPDsM27cOBUVFXn+JRy+dfh9rfXSSy8pNjZWxx9/vGbNmqWysjLPNu6rf3M6nVq8eLFKS0s1atSoDvO7GuDrAjqTvLw8OZ1Orz8QkpSQkKDNmzf7qCo0R1pamhYtWqR+/fopMzNT99xzj0499VRt2LBBWVlZslqtioyM9DomISFBWVlZvikYzVZ7rxr6Pa3dlpWVpfj4eK/tAQEBio6O5l77sfHjx+u3v/2tevbsqV9++UV/+ctf9Jvf/EarVq2SxWLhvvo5l8ulP/7xjzr55JN1/PHHS1KT/t7Nyspq8Pe5dht8q6H7KkmXX365evTooS5duuinn37SHXfcoS1btuiNN96QxH31V+vXr9eoUaNUUVGh0NBQvfnmmxo4cKDWrVvXIX5XCU5AM/zmN7/xvB8yZIjS0tLUo0cPvfLKKwoKCvJhZQCO5tJLL/W8Hzx4sIYMGaLevXtrxYoVOvPMM31YGZrixhtv1IYNG7zGlaL9O9J9rTu2cPDgwUpKStKZZ56pX375Rb17927rMtFE/fr107p161RYWKjXXntNU6dO1eeff+7rsloMXfXaUGxsrCwWS70ZRLKzs5WYmOijqnAsIiMjddxxx2n79u1KTEyUw+FQQUGB1z7c3/al9l419nuamJhYb0KX6upq5efnc6/bkV69eik2Nlbbt2+XxH31ZzNmzNB7772nzz77TN26dfOsb8rfu4mJiQ3+Ptdug+8c6b42JC0tTZK8fl+5r/7HarWqT58+Sk1N1dy5czV06FA99thjHeZ3leDUhqxWq1JTU7V8+XLPOpfLpeXLl2vUqFE+rAy/VklJiX755RclJSUpNTVVgYGBXvd3y5YtysjI4P62Iz179lRiYqLXfSwqKtK3337ruY+jRo1SQUGB1qxZ49nn008/lcvl8vzHHf5v7969OnDggJKSkiRxX/2RYRiaMWOG3nzzTX366afq2bOn1/am/L07atQorV+/3isUf/zxxwoPD9fAgQPb5ovAy9Hua0PWrVsnSV6/r9xX/+dyuVRZWdlxfld9PTtFZ7N48WLDZrMZixYtMn7++Wfj+uuvNyIjI71mEIH/uu2224wVK1YYO3fuNFauXGmkp6cbsbGxRk5OjmEYhvH73//e6N69u/Hpp58a33//vTFq1Chj1KhRPq4ahysuLjZ++OEH44cffjAkGfPmzTN++OEHY/fu3YZhGMYDDzxgREZGGm+//bbx008/Geeff77Rs2dPo7y83HOO8ePHGyeccILx7bffGl999ZXRt29f47LLLvPVV4LR+H0tLi42br/9dmPVqlXGzp07jU8++cQ48cQTjb59+xoVFRWec3Bf/csf/vAHIyIiwlixYoWRmZnpWcrKyjz7HO3v3erqauP44483zj77bGPdunXG0qVLjbi4OGPWrFm++Eowjn5ft2/fbtx7773G999/b+zcudN4++23jV69ehljxozxnIP76n/uvPNO4/PPPzd27txp/PTTT8add95pmEwm46OPPjIMo2P8rhKcfOCJJ54wunfvblit/9/O/YU01cdxHP+cyK1tJmhba3hhhCImFPSP7B/UwLagUBZZjJgGiWnSRUUkWUZdRnXVoEhvjASDQkKLiq4EKYhMaAldWIFFfwmzksDfcxENhtGRpx43e94vOHDO+Z2d8z3nxwafnfM7DrNixQrT19eX7pIwSVVVVSYQCBiHw2Hy8/NNVVWVefr0abL9y5cvpr6+3uTm5hq3220qKyvNy5cv01gxfubu3btG0oQpFosZY76/kry5udn4/X7jdDpNMBg0g4ODKft49+6d2bFjh8nOzjY5OTmmpqbGjIyMpOFs8MOv+vXz58+mvLzc+Hw+k5WVZQoKCszu3bsn/GlFv2aWn/WnJNPW1pbcZjK/u0NDQyYcDhuXy2W8Xq/Zv3+/+fbt2xSfDX6w69fnz5+bdevWmby8PON0Ok1hYaE5ePCg+fjxY8p+6NfMsmvXLlNQUGAcDofx+XwmGAwmQ5Mxf8d31TLGmKm7vwUAAAAA0w9jnAAAAADABsEJAAAAAGwQnAAAAADABsEJAAAAAGwQnAAAAADABsEJAAAAAGwQnAAAAADABsEJAAAAAGwQnAAA+AXLsnTt2rV0lwEASDOCEwAgY1VXV8uyrAlTKBRKd2kAgP+ZmekuAACAXwmFQmpra0tZ53Q601QNAOD/ijtOAICM5nQ6NW/evJQpNzdX0vfH6OLxuMLhsFwulxYsWKArV66kfH5gYEAbNmyQy+XSnDlzVFtbq0+fPqVs09raqtLSUjmdTgUCAe3duzel/e3bt6qsrJTb7VZRUZG6urqSbR8+fFA0GpXP55PL5VJRUdGEoAcAmP4ITgCAaa25uVmRSET9/f2KRqPavn27EomEJGl0dFQbN25Ubm6u7t+/r87OTt2+fTslGMXjcTU0NKi2tlYDAwPq6upSYWFhyjGOHz+ubdu26dGjR9q0aZOi0ajev3+fPP7jx4/V09OjRCKheDwur9c7dRcAADAlLGOMSXcRAAD8THV1tdrb2zVr1qyU9U1NTWpqapJlWaqrq1M8Hk+2rVy5UkuWLNG5c+d04cIFHTp0SC9evJDH45EkdXd3a/PmzRoeHpbf71d+fr5qamp08uTJn9ZgWZaOHDmiEydOSPoexrKzs9XT06NQKKQtW7bI6/WqtbX1P7oKAIBMwBgnAEBGW79+fUowkqS8vLzkfFlZWUpbWVmZHj58KElKJBJavHhxMjRJ0urVqzU+Pq7BwUFZlqXh4WEFg8Ff1rBo0aLkvMfjUU5Ojl6/fi1J2rNnjyKRiB48eKDy8nJVVFRo1apV/+pcAQCZi+AEAMhoHo9nwqNzf4rL5ZrUdllZWSnLlmVpfHxckhQOh/Xs2TN1d3fr1q1bCgaDamho0KlTp/54vQCA9GGMEwBgWuvr65uwXFJSIkkqKSlRf3+/RkdHk+29vb2aMWOGiouLNXv2bM2fP1937tz5rRp8Pp9isZja29t19uxZnT9//rf2BwDIPNxxAgBktLGxMb169Spl3cyZM5MvYOjs7NSyZcu0Zs0aXbp0Sffu3dPFixclSdFoVMeOHVMsFlNLS4vevHmjxsZG7dy5U36/X5LU0tKiuro6zZ07V+FwWCMjI+rt7VVjY+Ok6jt69KiWLl2q0tJSjY2N6fr168ngBgD4exCcAAAZ7caNGwoEAinriouL9eTJE0nf33jX0dGh+vp6BQIBXb58WQsXLpQkud1u3bx5U/v27dPy5cvldrsViUR0+vTp5L5isZi+fv2qM2fO6MCBA/J6vdq6deuk63M4HDp8+LCGhobkcrm0du1adXR0/IEzBwBkEt6qBwCYtizL0tWrV1VRUZHuUgAAfznGOAEAAACADYITAAAAANhgjBMAYNriaXMAwFThjhMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAIANghMAAAAA2CA4AQAAAICNfwCfSVVMtQl7XAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_10 (SimpleRNN)   (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Average accuracy: 0.9179\n",
      "Average loss: 0.1999\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN_K-fold.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "k_fold = 5 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing vanilla RNN with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 300, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('Vanilla_RNN finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_1(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prova 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing vanilla RNN with K-fold\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 7ms/step - loss: 1.2282 - accuracy: 0.2701 - val_loss: 1.0591 - val_accuracy: 0.3393 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.9764 - accuracy: 0.3906 - val_loss: 0.8644 - val_accuracy: 0.4464 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.8002 - accuracy: 0.5558 - val_loss: 0.7287 - val_accuracy: 0.5804 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.6783 - accuracy: 0.6696 - val_loss: 0.6348 - val_accuracy: 0.6696 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.5928 - accuracy: 0.7299 - val_loss: 0.5690 - val_accuracy: 0.7232 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.5310 - accuracy: 0.7612 - val_loss: 0.5222 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4851 - accuracy: 0.7924 - val_loss: 0.4879 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.4501 - accuracy: 0.8080 - val_loss: 0.4621 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.8214 - val_loss: 0.4423 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4014 - accuracy: 0.8348 - val_loss: 0.4266 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3839 - accuracy: 0.8415 - val_loss: 0.4138 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3693 - accuracy: 0.8460 - val_loss: 0.4033 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3571 - accuracy: 0.8594 - val_loss: 0.3945 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8594 - val_loss: 0.3871 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3373 - accuracy: 0.8616 - val_loss: 0.3807 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8683 - val_loss: 0.3752 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.8705 - val_loss: 0.3704 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8728 - val_loss: 0.3661 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3095 - accuracy: 0.8728 - val_loss: 0.3623 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3040 - accuracy: 0.8728 - val_loss: 0.3589 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8772 - val_loss: 0.3559 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2944 - accuracy: 0.8817 - val_loss: 0.3531 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8817 - val_loss: 0.3505 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8884 - val_loss: 0.3482 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2823 - accuracy: 0.8884 - val_loss: 0.3460 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2787 - accuracy: 0.8884 - val_loss: 0.3440 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2755 - accuracy: 0.8884 - val_loss: 0.3422 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.8906 - val_loss: 0.3404 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2695 - accuracy: 0.8906 - val_loss: 0.3388 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.8906 - val_loss: 0.3373 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2641 - accuracy: 0.8951 - val_loss: 0.3358 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2616 - accuracy: 0.8973 - val_loss: 0.3344 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2592 - accuracy: 0.8973 - val_loss: 0.3331 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2570 - accuracy: 0.9018 - val_loss: 0.3319 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2548 - accuracy: 0.9040 - val_loss: 0.3307 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.9040 - val_loss: 0.3295 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2508 - accuracy: 0.9062 - val_loss: 0.3284 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.9062 - val_loss: 0.3274 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2470 - accuracy: 0.9062 - val_loss: 0.3263 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2452 - accuracy: 0.9062 - val_loss: 0.3253 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2435 - accuracy: 0.9085 - val_loss: 0.3244 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.9085 - val_loss: 0.3234 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2402 - accuracy: 0.9085 - val_loss: 0.3225 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2387 - accuracy: 0.9107 - val_loss: 0.3216 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2371 - accuracy: 0.9107 - val_loss: 0.3208 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2357 - accuracy: 0.9129 - val_loss: 0.3199 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2342 - accuracy: 0.9152 - val_loss: 0.3191 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.9174 - val_loss: 0.3183 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.9196 - val_loss: 0.3174 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9196 - val_loss: 0.3167 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.9196 - val_loss: 0.3159 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9196 - val_loss: 0.3151 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.9219 - val_loss: 0.3144 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2251 - accuracy: 0.9219 - val_loss: 0.3136 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.9241 - val_loss: 0.3129 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2228 - accuracy: 0.9241 - val_loss: 0.3122 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2216 - accuracy: 0.9241 - val_loss: 0.3114 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9263 - val_loss: 0.3107 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2194 - accuracy: 0.9263 - val_loss: 0.3100 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9263 - val_loss: 0.3093 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2173 - accuracy: 0.9263 - val_loss: 0.3086 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2162 - accuracy: 0.9263 - val_loss: 0.3079 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2152 - accuracy: 0.9286 - val_loss: 0.3072 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9286 - val_loss: 0.3065 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9308 - val_loss: 0.3059 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9308 - val_loss: 0.3052 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9308 - val_loss: 0.3045 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2104 - accuracy: 0.9308 - val_loss: 0.3039 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.9308 - val_loss: 0.3032 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9308 - val_loss: 0.3026 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.9308 - val_loss: 0.3020 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2068 - accuracy: 0.9308 - val_loss: 0.3013 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9308 - val_loss: 0.3007 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2051 - accuracy: 0.9308 - val_loss: 0.3001 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9308 - val_loss: 0.2995 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2034 - accuracy: 0.9308 - val_loss: 0.2988 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9330 - val_loss: 0.2982 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2017 - accuracy: 0.9353 - val_loss: 0.2976 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9353 - val_loss: 0.2970 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.9353 - val_loss: 0.2964 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.9353 - val_loss: 0.2958 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9353 - val_loss: 0.2953 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1978 - accuracy: 0.9375 - val_loss: 0.2947 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1971 - accuracy: 0.9375 - val_loss: 0.2941 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1963 - accuracy: 0.9375 - val_loss: 0.2935 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9375 - val_loss: 0.2929 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1948 - accuracy: 0.9375 - val_loss: 0.2923 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1941 - accuracy: 0.9397 - val_loss: 0.2918 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1934 - accuracy: 0.9420 - val_loss: 0.2912 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1927 - accuracy: 0.9420 - val_loss: 0.2906 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1920 - accuracy: 0.9420 - val_loss: 0.2900 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9420 - val_loss: 0.2895 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9420 - val_loss: 0.2889 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1900 - accuracy: 0.9420 - val_loss: 0.2883 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1893 - accuracy: 0.9420 - val_loss: 0.2878 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9420 - val_loss: 0.2872 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9420 - val_loss: 0.2867 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9420 - val_loss: 0.2861 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9420 - val_loss: 0.2856 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.9464 - val_loss: 0.2851 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1854 - accuracy: 0.9464 - val_loss: 0.2846 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1848 - accuracy: 0.9442 - val_loss: 0.2840 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.9464 - val_loss: 0.2835 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9464 - val_loss: 0.2830 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1829 - accuracy: 0.9464 - val_loss: 0.2824 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1823 - accuracy: 0.9487 - val_loss: 0.2819 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.9464 - val_loss: 0.2814 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9487 - val_loss: 0.2809 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9487 - val_loss: 0.2803 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1799 - accuracy: 0.9487 - val_loss: 0.2798 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.9487 - val_loss: 0.2793 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.9487 - val_loss: 0.2788 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1782 - accuracy: 0.9509 - val_loss: 0.2783 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9509 - val_loss: 0.2778 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9509 - val_loss: 0.2773 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.9509 - val_loss: 0.2768 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.9509 - val_loss: 0.2763 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1754 - accuracy: 0.9509 - val_loss: 0.2759 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1748 - accuracy: 0.9509 - val_loss: 0.2754 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9509 - val_loss: 0.2749 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1737 - accuracy: 0.9509 - val_loss: 0.2744 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1732 - accuracy: 0.9509 - val_loss: 0.2739 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1727 - accuracy: 0.9509 - val_loss: 0.2734 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1722 - accuracy: 0.9531 - val_loss: 0.2730 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1717 - accuracy: 0.9531 - val_loss: 0.2725 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9531 - val_loss: 0.2720 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.9531 - val_loss: 0.2716 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9531 - val_loss: 0.2711 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9531 - val_loss: 0.2706 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1692 - accuracy: 0.9531 - val_loss: 0.2702 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9531 - val_loss: 0.2697 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9531 - val_loss: 0.2693 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.9531 - val_loss: 0.2688 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1673 - accuracy: 0.9531 - val_loss: 0.2684 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9531 - val_loss: 0.2679 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9531 - val_loss: 0.2675 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9531 - val_loss: 0.2670 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1654 - accuracy: 0.9531 - val_loss: 0.2666 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1650 - accuracy: 0.9531 - val_loss: 0.2661 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1646 - accuracy: 0.9531 - val_loss: 0.2657 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9531 - val_loss: 0.2653 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9531 - val_loss: 0.2648 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1633 - accuracy: 0.9531 - val_loss: 0.2644 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9531 - val_loss: 0.2640 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1624 - accuracy: 0.9531 - val_loss: 0.2635 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1620 - accuracy: 0.9531 - val_loss: 0.2631 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9531 - val_loss: 0.2627 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9531 - val_loss: 0.2622 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9531 - val_loss: 0.2618 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1603 - accuracy: 0.9531 - val_loss: 0.2614 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9531 - val_loss: 0.2610 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1595 - accuracy: 0.9531 - val_loss: 0.2606 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9531 - val_loss: 0.2602 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9531 - val_loss: 0.2598 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.9531 - val_loss: 0.2593 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.9531 - val_loss: 0.2589 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9531 - val_loss: 0.2585 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9531 - val_loss: 0.2581 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1567 - accuracy: 0.9531 - val_loss: 0.2577 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9531 - val_loss: 0.2573 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1560 - accuracy: 0.9531 - val_loss: 0.2569 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1556 - accuracy: 0.9531 - val_loss: 0.2565 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1552 - accuracy: 0.9531 - val_loss: 0.2561 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1549 - accuracy: 0.9531 - val_loss: 0.2557 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1545 - accuracy: 0.9531 - val_loss: 0.2554 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1541 - accuracy: 0.9531 - val_loss: 0.2550 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1538 - accuracy: 0.9531 - val_loss: 0.2546 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1534 - accuracy: 0.9531 - val_loss: 0.2542 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.9531 - val_loss: 0.2538 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1527 - accuracy: 0.9531 - val_loss: 0.2534 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1523 - accuracy: 0.9531 - val_loss: 0.2531 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1520 - accuracy: 0.9531 - val_loss: 0.2527 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1517 - accuracy: 0.9531 - val_loss: 0.2523 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9531 - val_loss: 0.2520 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.9531 - val_loss: 0.2516 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1506 - accuracy: 0.9531 - val_loss: 0.2513 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1503 - accuracy: 0.9531 - val_loss: 0.2509 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1499 - accuracy: 0.9531 - val_loss: 0.2505 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.9531 - val_loss: 0.2502 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1493 - accuracy: 0.9531 - val_loss: 0.2498 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1489 - accuracy: 0.9531 - val_loss: 0.2495 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1486 - accuracy: 0.9531 - val_loss: 0.2491 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.9531 - val_loss: 0.2488 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1480 - accuracy: 0.9531 - val_loss: 0.2484 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1477 - accuracy: 0.9531 - val_loss: 0.2481 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1474 - accuracy: 0.9531 - val_loss: 0.2477 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1471 - accuracy: 0.9531 - val_loss: 0.2474 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.9531 - val_loss: 0.2470 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.9531 - val_loss: 0.2467 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9531 - val_loss: 0.2464 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9531 - val_loss: 0.2460 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9531 - val_loss: 0.2457 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1452 - accuracy: 0.9531 - val_loss: 0.2454 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1449 - accuracy: 0.9531 - val_loss: 0.2450 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1446 - accuracy: 0.9554 - val_loss: 0.2447 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1444 - accuracy: 0.9554 - val_loss: 0.2444 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.9576 - val_loss: 0.2440 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1437 - accuracy: 0.9576 - val_loss: 0.2437 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1435 - accuracy: 0.9598 - val_loss: 0.2434 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9598 - val_loss: 0.2430 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9598 - val_loss: 0.2427 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1426 - accuracy: 0.9598 - val_loss: 0.2424 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1423 - accuracy: 0.9598 - val_loss: 0.2421 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9598 - val_loss: 0.2418 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9598 - val_loss: 0.2414 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.9598 - val_loss: 0.2411 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9598 - val_loss: 0.2408 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1410 - accuracy: 0.9598 - val_loss: 0.2405 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9598 - val_loss: 0.2402 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1404 - accuracy: 0.9598 - val_loss: 0.2399 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.9598 - val_loss: 0.2396 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9598 - val_loss: 0.2392 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1396 - accuracy: 0.9598 - val_loss: 0.2389 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9598 - val_loss: 0.2386 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1391 - accuracy: 0.9598 - val_loss: 0.2383 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1389 - accuracy: 0.9598 - val_loss: 0.2380 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9598 - val_loss: 0.2377 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9598 - val_loss: 0.2374 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9598 - val_loss: 0.2371 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1378 - accuracy: 0.9598 - val_loss: 0.2368 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9598 - val_loss: 0.2365 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1373 - accuracy: 0.9598 - val_loss: 0.2362 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1371 - accuracy: 0.9598 - val_loss: 0.2359 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1368 - accuracy: 0.9598 - val_loss: 0.2356 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1366 - accuracy: 0.9598 - val_loss: 0.2353 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1364 - accuracy: 0.9598 - val_loss: 0.2350 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1361 - accuracy: 0.9598 - val_loss: 0.2347 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1359 - accuracy: 0.9598 - val_loss: 0.2344 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9598 - val_loss: 0.2341 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1354 - accuracy: 0.9598 - val_loss: 0.2338 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1351 - accuracy: 0.9598 - val_loss: 0.2335 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1349 - accuracy: 0.9598 - val_loss: 0.2333 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1347 - accuracy: 0.9598 - val_loss: 0.2330 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1344 - accuracy: 0.9598 - val_loss: 0.2327 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1342 - accuracy: 0.9598 - val_loss: 0.2324 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1340 - accuracy: 0.9598 - val_loss: 0.2321 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9598 - val_loss: 0.2318 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.9598 - val_loss: 0.2316 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9598 - val_loss: 0.2313 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1331 - accuracy: 0.9598 - val_loss: 0.2310 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.9598 - val_loss: 0.2307 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9598 - val_loss: 0.2304 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1324 - accuracy: 0.9598 - val_loss: 0.2302 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9598 - val_loss: 0.2299 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1319 - accuracy: 0.9598 - val_loss: 0.2296 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1317 - accuracy: 0.9598 - val_loss: 0.2293 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9598 - val_loss: 0.2291 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1313 - accuracy: 0.9598 - val_loss: 0.2288 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.9598 - val_loss: 0.2285 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9621 - val_loss: 0.2282 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9621 - val_loss: 0.2280 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1304 - accuracy: 0.9621 - val_loss: 0.2277 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1302 - accuracy: 0.9621 - val_loss: 0.2274 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1300 - accuracy: 0.9621 - val_loss: 0.2271 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9621 - val_loss: 0.2269 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1296 - accuracy: 0.9621 - val_loss: 0.2266 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9621 - val_loss: 0.2264 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1292 - accuracy: 0.9621 - val_loss: 0.2261 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9621 - val_loss: 0.2258 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1287 - accuracy: 0.9621 - val_loss: 0.2255 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1285 - accuracy: 0.9621 - val_loss: 0.2253 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1283 - accuracy: 0.9621 - val_loss: 0.2250 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9621 - val_loss: 0.2248 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1279 - accuracy: 0.9621 - val_loss: 0.2245 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9621 - val_loss: 0.2243 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9621 - val_loss: 0.2240 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9621 - val_loss: 0.2237 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1271 - accuracy: 0.9621 - val_loss: 0.2235 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9621 - val_loss: 0.2232 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1267 - accuracy: 0.9621 - val_loss: 0.2230 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9621 - val_loss: 0.2227 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1263 - accuracy: 0.9621 - val_loss: 0.2225 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1261 - accuracy: 0.9621 - val_loss: 0.2222 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1259 - accuracy: 0.9621 - val_loss: 0.2220 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1257 - accuracy: 0.9621 - val_loss: 0.2217 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9621 - val_loss: 0.2214 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9621 - val_loss: 0.2212 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9621 - val_loss: 0.2209 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9621 - val_loss: 0.2207 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1247 - accuracy: 0.9621 - val_loss: 0.2205 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1245 - accuracy: 0.9621 - val_loss: 0.2202 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1243 - accuracy: 0.9621 - val_loss: 0.2200 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9621 - val_loss: 0.2197 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1239 - accuracy: 0.9621 - val_loss: 0.2195 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1238 - accuracy: 0.9621 - val_loss: 0.2192 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1236 - accuracy: 0.9621 - val_loss: 0.2190 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1234 - accuracy: 0.9621 - val_loss: 0.2188 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9621 - val_loss: 0.2185 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9621 - val_loss: 0.2183 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1228 - accuracy: 0.9621 - val_loss: 0.2181 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1226 - accuracy: 0.9621 - val_loss: 0.2178 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1225 - accuracy: 0.9621 - val_loss: 0.2176 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1223 - accuracy: 0.9621 - val_loss: 0.2174 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.9621 - val_loss: 0.2171 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1219 - accuracy: 0.9621 - val_loss: 0.2169 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1217 - accuracy: 0.9621 - val_loss: 0.2167 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.9621 - val_loss: 0.2164 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9621 - val_loss: 0.2162 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9621 - val_loss: 0.2159 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.9621 - val_loss: 0.2157 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Loss: 0.2157, Accuracy: 89.29%\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 8ms/step - loss: 0.6391 - accuracy: 0.6496 - val_loss: 0.6775 - val_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5664 - accuracy: 0.6897 - val_loss: 0.6098 - val_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5150 - accuracy: 0.7388 - val_loss: 0.5583 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4772 - accuracy: 0.7723 - val_loss: 0.5182 - val_accuracy: 0.6786 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4478 - accuracy: 0.7969 - val_loss: 0.4862 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8125 - val_loss: 0.4604 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4055 - accuracy: 0.8192 - val_loss: 0.4389 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3897 - accuracy: 0.8393 - val_loss: 0.4209 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8482 - val_loss: 0.4053 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3648 - accuracy: 0.8527 - val_loss: 0.3918 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3547 - accuracy: 0.8549 - val_loss: 0.3799 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3458 - accuracy: 0.8571 - val_loss: 0.3694 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3380 - accuracy: 0.8638 - val_loss: 0.3601 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3310 - accuracy: 0.8638 - val_loss: 0.3518 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3246 - accuracy: 0.8661 - val_loss: 0.3444 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8683 - val_loss: 0.3377 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8750 - val_loss: 0.3315 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8750 - val_loss: 0.3258 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3044 - accuracy: 0.8772 - val_loss: 0.3207 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.8817 - val_loss: 0.3159 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2964 - accuracy: 0.8839 - val_loss: 0.3115 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2929 - accuracy: 0.8862 - val_loss: 0.3074 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2895 - accuracy: 0.8906 - val_loss: 0.3035 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8906 - val_loss: 0.2999 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2834 - accuracy: 0.8996 - val_loss: 0.2965 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.8996 - val_loss: 0.2934 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2778 - accuracy: 0.9018 - val_loss: 0.2904 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.9018 - val_loss: 0.2875 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2727 - accuracy: 0.9018 - val_loss: 0.2848 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2704 - accuracy: 0.9018 - val_loss: 0.2822 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2681 - accuracy: 0.9040 - val_loss: 0.2798 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2659 - accuracy: 0.9040 - val_loss: 0.2774 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2638 - accuracy: 0.9040 - val_loss: 0.2753 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2617 - accuracy: 0.9040 - val_loss: 0.2732 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2598 - accuracy: 0.9040 - val_loss: 0.2711 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2579 - accuracy: 0.9062 - val_loss: 0.2692 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2560 - accuracy: 0.9062 - val_loss: 0.2673 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2542 - accuracy: 0.9062 - val_loss: 0.2654 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2524 - accuracy: 0.9085 - val_loss: 0.2637 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2507 - accuracy: 0.9107 - val_loss: 0.2620 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.2489 - accuracy: 0.9107 - val_loss: 0.2604 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2473 - accuracy: 0.9107 - val_loss: 0.2588 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2457 - accuracy: 0.9129 - val_loss: 0.2573 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2441 - accuracy: 0.9129 - val_loss: 0.2558 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2425 - accuracy: 0.9152 - val_loss: 0.2544 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.9174 - val_loss: 0.2530 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.9196 - val_loss: 0.2516 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2381 - accuracy: 0.9219 - val_loss: 0.2503 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2366 - accuracy: 0.9241 - val_loss: 0.2491 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2352 - accuracy: 0.9241 - val_loss: 0.2478 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2339 - accuracy: 0.9241 - val_loss: 0.2466 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2325 - accuracy: 0.9241 - val_loss: 0.2455 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2312 - accuracy: 0.9263 - val_loss: 0.2443 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2299 - accuracy: 0.9263 - val_loss: 0.2432 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2286 - accuracy: 0.9263 - val_loss: 0.2421 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 0.9263 - val_loss: 0.2411 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2261 - accuracy: 0.9263 - val_loss: 0.2400 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.9263 - val_loss: 0.2390 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.9263 - val_loss: 0.2381 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2225 - accuracy: 0.9286 - val_loss: 0.2371 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2214 - accuracy: 0.9308 - val_loss: 0.2362 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2202 - accuracy: 0.9308 - val_loss: 0.2352 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2191 - accuracy: 0.9330 - val_loss: 0.2343 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2180 - accuracy: 0.9330 - val_loss: 0.2335 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2169 - accuracy: 0.9330 - val_loss: 0.2326 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2159 - accuracy: 0.9330 - val_loss: 0.2317 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9330 - val_loss: 0.2309 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9330 - val_loss: 0.2301 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2128 - accuracy: 0.9330 - val_loss: 0.2293 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2118 - accuracy: 0.9330 - val_loss: 0.2285 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2108 - accuracy: 0.9353 - val_loss: 0.2278 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2099 - accuracy: 0.9375 - val_loss: 0.2270 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9375 - val_loss: 0.2263 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2079 - accuracy: 0.9375 - val_loss: 0.2256 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9397 - val_loss: 0.2248 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9420 - val_loss: 0.2241 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9420 - val_loss: 0.2234 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9420 - val_loss: 0.2228 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2034 - accuracy: 0.9442 - val_loss: 0.2221 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2025 - accuracy: 0.9464 - val_loss: 0.2214 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9464 - val_loss: 0.2208 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2008 - accuracy: 0.9464 - val_loss: 0.2201 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1999 - accuracy: 0.9464 - val_loss: 0.2195 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.9464 - val_loss: 0.2189 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1983 - accuracy: 0.9464 - val_loss: 0.2183 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9464 - val_loss: 0.2177 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1966 - accuracy: 0.9464 - val_loss: 0.2171 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1958 - accuracy: 0.9464 - val_loss: 0.2165 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1951 - accuracy: 0.9464 - val_loss: 0.2159 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1943 - accuracy: 0.9464 - val_loss: 0.2154 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9464 - val_loss: 0.2148 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1928 - accuracy: 0.9464 - val_loss: 0.2143 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9487 - val_loss: 0.2137 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.9487 - val_loss: 0.2132 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1906 - accuracy: 0.9487 - val_loss: 0.2126 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1898 - accuracy: 0.9487 - val_loss: 0.2121 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1891 - accuracy: 0.9487 - val_loss: 0.2116 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1885 - accuracy: 0.9487 - val_loss: 0.2111 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1877 - accuracy: 0.9487 - val_loss: 0.2106 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1871 - accuracy: 0.9487 - val_loss: 0.2101 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1864 - accuracy: 0.9487 - val_loss: 0.2096 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.9487 - val_loss: 0.2091 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9487 - val_loss: 0.2086 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9487 - val_loss: 0.2082 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9487 - val_loss: 0.2077 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1831 - accuracy: 0.9487 - val_loss: 0.2073 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.9487 - val_loss: 0.2068 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1818 - accuracy: 0.9487 - val_loss: 0.2064 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9487 - val_loss: 0.2059 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1806 - accuracy: 0.9487 - val_loss: 0.2055 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1799 - accuracy: 0.9487 - val_loss: 0.2050 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1793 - accuracy: 0.9487 - val_loss: 0.2046 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.9487 - val_loss: 0.2042 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1781 - accuracy: 0.9487 - val_loss: 0.2037 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1776 - accuracy: 0.9487 - val_loss: 0.2033 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1770 - accuracy: 0.9487 - val_loss: 0.2029 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9487 - val_loss: 0.2025 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9487 - val_loss: 0.2021 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.9487 - val_loss: 0.2017 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9487 - val_loss: 0.2013 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1741 - accuracy: 0.9487 - val_loss: 0.2009 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.9487 - val_loss: 0.2005 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1730 - accuracy: 0.9487 - val_loss: 0.2001 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9487 - val_loss: 0.1997 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9487 - val_loss: 0.1993 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1713 - accuracy: 0.9487 - val_loss: 0.1990 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9487 - val_loss: 0.1986 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1703 - accuracy: 0.9487 - val_loss: 0.1982 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.9487 - val_loss: 0.1979 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1692 - accuracy: 0.9487 - val_loss: 0.1975 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9487 - val_loss: 0.1972 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9487 - val_loss: 0.1968 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.9487 - val_loss: 0.1965 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9487 - val_loss: 0.1961 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1667 - accuracy: 0.9509 - val_loss: 0.1958 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9509 - val_loss: 0.1954 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1657 - accuracy: 0.9509 - val_loss: 0.1951 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1652 - accuracy: 0.9509 - val_loss: 0.1948 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1647 - accuracy: 0.9509 - val_loss: 0.1944 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.9509 - val_loss: 0.1941 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9509 - val_loss: 0.1938 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1633 - accuracy: 0.9509 - val_loss: 0.1935 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9509 - val_loss: 0.1932 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1623 - accuracy: 0.9509 - val_loss: 0.1928 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1618 - accuracy: 0.9509 - val_loss: 0.1925 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.9509 - val_loss: 0.1922 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9509 - val_loss: 0.1919 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9509 - val_loss: 0.1916 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9509 - val_loss: 0.1913 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9509 - val_loss: 0.1910 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9509 - val_loss: 0.1907 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1587 - accuracy: 0.9509 - val_loss: 0.1904 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9509 - val_loss: 0.1901 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.9509 - val_loss: 0.1898 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.9509 - val_loss: 0.1895 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1569 - accuracy: 0.9509 - val_loss: 0.1893 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9509 - val_loss: 0.1890 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1561 - accuracy: 0.9509 - val_loss: 0.1887 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1557 - accuracy: 0.9509 - val_loss: 0.1884 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9509 - val_loss: 0.1882 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1548 - accuracy: 0.9509 - val_loss: 0.1879 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.9509 - val_loss: 0.1876 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9509 - val_loss: 0.1874 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1536 - accuracy: 0.9509 - val_loss: 0.1871 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.9509 - val_loss: 0.1868 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1528 - accuracy: 0.9509 - val_loss: 0.1866 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9509 - val_loss: 0.1863 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1520 - accuracy: 0.9509 - val_loss: 0.1860 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.9509 - val_loss: 0.1858 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1512 - accuracy: 0.9509 - val_loss: 0.1855 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9509 - val_loss: 0.1853 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1504 - accuracy: 0.9509 - val_loss: 0.1850 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1501 - accuracy: 0.9509 - val_loss: 0.1848 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1497 - accuracy: 0.9509 - val_loss: 0.1845 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1493 - accuracy: 0.9509 - val_loss: 0.1843 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1489 - accuracy: 0.9509 - val_loss: 0.1841 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1485 - accuracy: 0.9509 - val_loss: 0.1838 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1482 - accuracy: 0.9509 - val_loss: 0.1836 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1478 - accuracy: 0.9509 - val_loss: 0.1833 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1474 - accuracy: 0.9509 - val_loss: 0.1831 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1471 - accuracy: 0.9509 - val_loss: 0.1829 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1467 - accuracy: 0.9509 - val_loss: 0.1826 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1463 - accuracy: 0.9509 - val_loss: 0.1824 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9509 - val_loss: 0.1822 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1456 - accuracy: 0.9509 - val_loss: 0.1820 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.9509 - val_loss: 0.1817 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1449 - accuracy: 0.9509 - val_loss: 0.1815 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9509 - val_loss: 0.1813 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1442 - accuracy: 0.9509 - val_loss: 0.1811 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1439 - accuracy: 0.9509 - val_loss: 0.1809 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1436 - accuracy: 0.9509 - val_loss: 0.1807 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1432 - accuracy: 0.9509 - val_loss: 0.1804 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9509 - val_loss: 0.1802 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1425 - accuracy: 0.9509 - val_loss: 0.1800 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.9509 - val_loss: 0.1798 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9509 - val_loss: 0.1796 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1415 - accuracy: 0.9509 - val_loss: 0.1794 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1412 - accuracy: 0.9509 - val_loss: 0.1792 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1409 - accuracy: 0.9509 - val_loss: 0.1790 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1406 - accuracy: 0.9509 - val_loss: 0.1788 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1402 - accuracy: 0.9509 - val_loss: 0.1786 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9509 - val_loss: 0.1784 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9509 - val_loss: 0.1782 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9509 - val_loss: 0.1780 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9509 - val_loss: 0.1778 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1387 - accuracy: 0.9509 - val_loss: 0.1776 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1383 - accuracy: 0.9509 - val_loss: 0.1775 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1381 - accuracy: 0.9531 - val_loss: 0.1773 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9531 - val_loss: 0.1771 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1375 - accuracy: 0.9554 - val_loss: 0.1769 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1372 - accuracy: 0.9554 - val_loss: 0.1767 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1368 - accuracy: 0.9554 - val_loss: 0.1765 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1365 - accuracy: 0.9554 - val_loss: 0.1763 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1363 - accuracy: 0.9554 - val_loss: 0.1762 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1359 - accuracy: 0.9554 - val_loss: 0.1760 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9554 - val_loss: 0.1758 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1354 - accuracy: 0.9554 - val_loss: 0.1756 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9554 - val_loss: 0.1755 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1348 - accuracy: 0.9554 - val_loss: 0.1753 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1345 - accuracy: 0.9554 - val_loss: 0.1752 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1342 - accuracy: 0.9554 - val_loss: 0.1750 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1339 - accuracy: 0.9554 - val_loss: 0.1748 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1336 - accuracy: 0.9554 - val_loss: 0.1746 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9554 - val_loss: 0.1745 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9554 - val_loss: 0.1743 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9554 - val_loss: 0.1742 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9554 - val_loss: 0.1740 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1322 - accuracy: 0.9576 - val_loss: 0.1738 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9576 - val_loss: 0.1737 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1317 - accuracy: 0.9576 - val_loss: 0.1735 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9576 - val_loss: 0.1733 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9576 - val_loss: 0.1732 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1309 - accuracy: 0.9576 - val_loss: 0.1730 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9576 - val_loss: 0.1729 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1304 - accuracy: 0.9576 - val_loss: 0.1727 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1301 - accuracy: 0.9576 - val_loss: 0.1726 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9598 - val_loss: 0.1724 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9598 - val_loss: 0.1723 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9598 - val_loss: 0.1721 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1291 - accuracy: 0.9621 - val_loss: 0.1720 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1288 - accuracy: 0.9621 - val_loss: 0.1718 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1285 - accuracy: 0.9621 - val_loss: 0.1717 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1283 - accuracy: 0.9621 - val_loss: 0.1715 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1280 - accuracy: 0.9621 - val_loss: 0.1714 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9621 - val_loss: 0.1712 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9621 - val_loss: 0.1711 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1273 - accuracy: 0.9621 - val_loss: 0.1710 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1271 - accuracy: 0.9621 - val_loss: 0.1708 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1268 - accuracy: 0.9621 - val_loss: 0.1707 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9621 - val_loss: 0.1705 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1263 - accuracy: 0.9621 - val_loss: 0.1704 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.9621 - val_loss: 0.1703 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1258 - accuracy: 0.9621 - val_loss: 0.1702 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1256 - accuracy: 0.9621 - val_loss: 0.1700 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1253 - accuracy: 0.9621 - val_loss: 0.1699 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9621 - val_loss: 0.1698 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9621 - val_loss: 0.1696 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1246 - accuracy: 0.9621 - val_loss: 0.1695 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1244 - accuracy: 0.9621 - val_loss: 0.1694 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1242 - accuracy: 0.9621 - val_loss: 0.1693 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9621 - val_loss: 0.1691 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1237 - accuracy: 0.9621 - val_loss: 0.1690 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1235 - accuracy: 0.9621 - val_loss: 0.1689 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9621 - val_loss: 0.1688 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9621 - val_loss: 0.1686 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1228 - accuracy: 0.9621 - val_loss: 0.1685 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.9621 - val_loss: 0.1684 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1223 - accuracy: 0.9621 - val_loss: 0.1683 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.9621 - val_loss: 0.1682 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1219 - accuracy: 0.9621 - val_loss: 0.1680 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1217 - accuracy: 0.9621 - val_loss: 0.1679 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9621 - val_loss: 0.1678 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9621 - val_loss: 0.1677 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 0.9621 - val_loss: 0.1676 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1208 - accuracy: 0.9621 - val_loss: 0.1674 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.9621 - val_loss: 0.1673 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.9621 - val_loss: 0.1672 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1201 - accuracy: 0.9621 - val_loss: 0.1671 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.9621 - val_loss: 0.1670 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1197 - accuracy: 0.9621 - val_loss: 0.1669 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.9621 - val_loss: 0.1668 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1193 - accuracy: 0.9621 - val_loss: 0.1667 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9621 - val_loss: 0.1666 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1188 - accuracy: 0.9621 - val_loss: 0.1665 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1186 - accuracy: 0.9621 - val_loss: 0.1664 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1184 - accuracy: 0.9621 - val_loss: 0.1662 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1182 - accuracy: 0.9621 - val_loss: 0.1661 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1180 - accuracy: 0.9621 - val_loss: 0.1660 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9621 - val_loss: 0.1659 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.9621 - val_loss: 0.1658 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9621 - val_loss: 0.1657 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1172 - accuracy: 0.9621 - val_loss: 0.1656 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.9621 - val_loss: 0.1655 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1168 - accuracy: 0.9643 - val_loss: 0.1654 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1166 - accuracy: 0.9643 - val_loss: 0.1653 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1164 - accuracy: 0.9643 - val_loss: 0.1652 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9643 - val_loss: 0.1651 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1160 - accuracy: 0.9643 - val_loss: 0.1650 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1158 - accuracy: 0.9643 - val_loss: 0.1649 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.9643 - val_loss: 0.1648 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Loss: 0.1648, Accuracy: 94.64%\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 9ms/step - loss: 0.7485 - accuracy: 0.4821 - val_loss: 0.8229 - val_accuracy: 0.5357 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6268 - accuracy: 0.6719 - val_loss: 0.7260 - val_accuracy: 0.6607 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.5431 - accuracy: 0.7946 - val_loss: 0.6571 - val_accuracy: 0.7054 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4847 - accuracy: 0.8393 - val_loss: 0.6064 - val_accuracy: 0.7321 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4427 - accuracy: 0.8527 - val_loss: 0.5678 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4112 - accuracy: 0.8527 - val_loss: 0.5379 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8594 - val_loss: 0.5141 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8616 - val_loss: 0.4946 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3524 - accuracy: 0.8728 - val_loss: 0.4783 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3396 - accuracy: 0.8728 - val_loss: 0.4646 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3291 - accuracy: 0.8750 - val_loss: 0.4530 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8750 - val_loss: 0.4431 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3125 - accuracy: 0.8817 - val_loss: 0.4343 - val_accuracy: 0.7946 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.8839 - val_loss: 0.4266 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3001 - accuracy: 0.8862 - val_loss: 0.4198 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2950 - accuracy: 0.8839 - val_loss: 0.4137 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2905 - accuracy: 0.8884 - val_loss: 0.4082 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8951 - val_loss: 0.4031 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2827 - accuracy: 0.8951 - val_loss: 0.3985 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2793 - accuracy: 0.8996 - val_loss: 0.3944 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2762 - accuracy: 0.9018 - val_loss: 0.3906 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2733 - accuracy: 0.9018 - val_loss: 0.3870 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.9062 - val_loss: 0.3837 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2681 - accuracy: 0.9129 - val_loss: 0.3807 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2658 - accuracy: 0.9129 - val_loss: 0.3777 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.9129 - val_loss: 0.3750 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2615 - accuracy: 0.9129 - val_loss: 0.3725 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2595 - accuracy: 0.9129 - val_loss: 0.3701 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2576 - accuracy: 0.9129 - val_loss: 0.3678 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2558 - accuracy: 0.9152 - val_loss: 0.3655 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2541 - accuracy: 0.9152 - val_loss: 0.3634 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.9152 - val_loss: 0.3614 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2508 - accuracy: 0.9152 - val_loss: 0.3595 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2492 - accuracy: 0.9174 - val_loss: 0.3576 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.9174 - val_loss: 0.3558 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2462 - accuracy: 0.9174 - val_loss: 0.3541 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2448 - accuracy: 0.9196 - val_loss: 0.3525 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2434 - accuracy: 0.9196 - val_loss: 0.3509 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2421 - accuracy: 0.9241 - val_loss: 0.3493 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2408 - accuracy: 0.9241 - val_loss: 0.3479 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2396 - accuracy: 0.9241 - val_loss: 0.3465 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2383 - accuracy: 0.9241 - val_loss: 0.3452 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2372 - accuracy: 0.9241 - val_loss: 0.3438 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.9241 - val_loss: 0.3426 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2349 - accuracy: 0.9263 - val_loss: 0.3414 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9263 - val_loss: 0.3402 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2326 - accuracy: 0.9263 - val_loss: 0.3391 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2316 - accuracy: 0.9263 - val_loss: 0.3379 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9263 - val_loss: 0.3369 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2295 - accuracy: 0.9263 - val_loss: 0.3358 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2285 - accuracy: 0.9286 - val_loss: 0.3348 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2275 - accuracy: 0.9286 - val_loss: 0.3338 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2266 - accuracy: 0.9286 - val_loss: 0.3328 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2256 - accuracy: 0.9286 - val_loss: 0.3319 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2247 - accuracy: 0.9286 - val_loss: 0.3309 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2237 - accuracy: 0.9286 - val_loss: 0.3299 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2228 - accuracy: 0.9286 - val_loss: 0.3290 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9286 - val_loss: 0.3282 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9286 - val_loss: 0.3273 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2202 - accuracy: 0.9286 - val_loss: 0.3265 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2194 - accuracy: 0.9286 - val_loss: 0.3256 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2185 - accuracy: 0.9286 - val_loss: 0.3249 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.2177 - accuracy: 0.9286 - val_loss: 0.3240 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2169 - accuracy: 0.9308 - val_loss: 0.3233 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2161 - accuracy: 0.9308 - val_loss: 0.3225 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.9308 - val_loss: 0.3217 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2145 - accuracy: 0.9308 - val_loss: 0.3210 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2137 - accuracy: 0.9308 - val_loss: 0.3202 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 0.9308 - val_loss: 0.3195 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2122 - accuracy: 0.9308 - val_loss: 0.3187 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2114 - accuracy: 0.9308 - val_loss: 0.3180 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9308 - val_loss: 0.3173 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2099 - accuracy: 0.9308 - val_loss: 0.3166 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.9330 - val_loss: 0.3158 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9330 - val_loss: 0.3151 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2077 - accuracy: 0.9330 - val_loss: 0.3144 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2070 - accuracy: 0.9330 - val_loss: 0.3138 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9330 - val_loss: 0.3131 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2056 - accuracy: 0.9330 - val_loss: 0.3124 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2049 - accuracy: 0.9330 - val_loss: 0.3118 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9353 - val_loss: 0.3111 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2036 - accuracy: 0.9353 - val_loss: 0.3104 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9375 - val_loss: 0.3098 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2022 - accuracy: 0.9375 - val_loss: 0.3091 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2015 - accuracy: 0.9375 - val_loss: 0.3085 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9375 - val_loss: 0.3079 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2003 - accuracy: 0.9375 - val_loss: 0.3072 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1996 - accuracy: 0.9375 - val_loss: 0.3066 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1990 - accuracy: 0.9375 - val_loss: 0.3060 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9375 - val_loss: 0.3054 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1978 - accuracy: 0.9375 - val_loss: 0.3048 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1972 - accuracy: 0.9375 - val_loss: 0.3042 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1966 - accuracy: 0.9375 - val_loss: 0.3037 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9375 - val_loss: 0.3031 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1955 - accuracy: 0.9375 - val_loss: 0.3026 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1949 - accuracy: 0.9375 - val_loss: 0.3021 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1943 - accuracy: 0.9375 - val_loss: 0.3016 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1938 - accuracy: 0.9375 - val_loss: 0.3011 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1932 - accuracy: 0.9375 - val_loss: 0.3006 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1926 - accuracy: 0.9375 - val_loss: 0.3001 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1921 - accuracy: 0.9375 - val_loss: 0.2996 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1916 - accuracy: 0.9375 - val_loss: 0.2991 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1910 - accuracy: 0.9375 - val_loss: 0.2986 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1905 - accuracy: 0.9375 - val_loss: 0.2982 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1900 - accuracy: 0.9375 - val_loss: 0.2977 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1895 - accuracy: 0.9375 - val_loss: 0.2972 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9375 - val_loss: 0.2968 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1884 - accuracy: 0.9375 - val_loss: 0.2964 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9375 - val_loss: 0.2959 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1874 - accuracy: 0.9375 - val_loss: 0.2955 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1869 - accuracy: 0.9375 - val_loss: 0.2950 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9375 - val_loss: 0.2946 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9397 - val_loss: 0.2943 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1855 - accuracy: 0.9397 - val_loss: 0.2939 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1850 - accuracy: 0.9397 - val_loss: 0.2934 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1845 - accuracy: 0.9397 - val_loss: 0.2931 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1840 - accuracy: 0.9397 - val_loss: 0.2927 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9397 - val_loss: 0.2923 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1831 - accuracy: 0.9397 - val_loss: 0.2919 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1826 - accuracy: 0.9397 - val_loss: 0.2916 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9397 - val_loss: 0.2912 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.9397 - val_loss: 0.2908 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9397 - val_loss: 0.2905 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.9397 - val_loss: 0.2901 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1803 - accuracy: 0.9397 - val_loss: 0.2897 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1799 - accuracy: 0.9397 - val_loss: 0.2894 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1794 - accuracy: 0.9397 - val_loss: 0.2890 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.9397 - val_loss: 0.2887 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.9397 - val_loss: 0.2883 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1781 - accuracy: 0.9397 - val_loss: 0.2880 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1777 - accuracy: 0.9397 - val_loss: 0.2876 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1772 - accuracy: 0.9397 - val_loss: 0.2873 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1768 - accuracy: 0.9397 - val_loss: 0.2870 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1764 - accuracy: 0.9397 - val_loss: 0.2867 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.9397 - val_loss: 0.2863 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9420 - val_loss: 0.2860 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9420 - val_loss: 0.2857 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9420 - val_loss: 0.2854 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9420 - val_loss: 0.2850 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1738 - accuracy: 0.9420 - val_loss: 0.2847 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1734 - accuracy: 0.9420 - val_loss: 0.2844 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.9442 - val_loss: 0.2841 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1726 - accuracy: 0.9442 - val_loss: 0.2838 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1722 - accuracy: 0.9442 - val_loss: 0.2835 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1718 - accuracy: 0.9442 - val_loss: 0.2831 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1714 - accuracy: 0.9442 - val_loss: 0.2828 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9442 - val_loss: 0.2825 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1706 - accuracy: 0.9442 - val_loss: 0.2822 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9442 - val_loss: 0.2818 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9442 - val_loss: 0.2815 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9442 - val_loss: 0.2812 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9442 - val_loss: 0.2809 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9442 - val_loss: 0.2806 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9442 - val_loss: 0.2803 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9442 - val_loss: 0.2800 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1676 - accuracy: 0.9442 - val_loss: 0.2797 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1672 - accuracy: 0.9442 - val_loss: 0.2794 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1669 - accuracy: 0.9442 - val_loss: 0.2792 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9442 - val_loss: 0.2789 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1662 - accuracy: 0.9442 - val_loss: 0.2786 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1658 - accuracy: 0.9442 - val_loss: 0.2783 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.9442 - val_loss: 0.2780 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1651 - accuracy: 0.9442 - val_loss: 0.2776 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1648 - accuracy: 0.9442 - val_loss: 0.2774 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9464 - val_loss: 0.2771 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9464 - val_loss: 0.2768 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1638 - accuracy: 0.9464 - val_loss: 0.2765 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1634 - accuracy: 0.9464 - val_loss: 0.2762 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1631 - accuracy: 0.9464 - val_loss: 0.2759 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9464 - val_loss: 0.2756 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9464 - val_loss: 0.2754 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.9464 - val_loss: 0.2751 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1618 - accuracy: 0.9464 - val_loss: 0.2748 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9464 - val_loss: 0.2745 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9464 - val_loss: 0.2742 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1608 - accuracy: 0.9487 - val_loss: 0.2740 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9509 - val_loss: 0.2737 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9509 - val_loss: 0.2734 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1599 - accuracy: 0.9509 - val_loss: 0.2732 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9509 - val_loss: 0.2729 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1593 - accuracy: 0.9509 - val_loss: 0.2726 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1590 - accuracy: 0.9509 - val_loss: 0.2723 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1587 - accuracy: 0.9509 - val_loss: 0.2721 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1584 - accuracy: 0.9531 - val_loss: 0.2718 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.9531 - val_loss: 0.2716 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1578 - accuracy: 0.9531 - val_loss: 0.2713 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9531 - val_loss: 0.2711 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1572 - accuracy: 0.9531 - val_loss: 0.2708 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1569 - accuracy: 0.9531 - val_loss: 0.2705 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1566 - accuracy: 0.9531 - val_loss: 0.2702 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1563 - accuracy: 0.9531 - val_loss: 0.2700 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9531 - val_loss: 0.2697 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1557 - accuracy: 0.9531 - val_loss: 0.2695 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.9531 - val_loss: 0.2692 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.9531 - val_loss: 0.2689 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1549 - accuracy: 0.9531 - val_loss: 0.2687 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1546 - accuracy: 0.9531 - val_loss: 0.2684 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1543 - accuracy: 0.9531 - val_loss: 0.2682 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1540 - accuracy: 0.9531 - val_loss: 0.2679 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9531 - val_loss: 0.2677 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1535 - accuracy: 0.9531 - val_loss: 0.2675 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1532 - accuracy: 0.9531 - val_loss: 0.2672 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1529 - accuracy: 0.9531 - val_loss: 0.2670 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1526 - accuracy: 0.9531 - val_loss: 0.2667 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1524 - accuracy: 0.9531 - val_loss: 0.2665 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.9531 - val_loss: 0.2663 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9531 - val_loss: 0.2660 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1516 - accuracy: 0.9531 - val_loss: 0.2658 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1513 - accuracy: 0.9531 - val_loss: 0.2655 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1510 - accuracy: 0.9531 - val_loss: 0.2653 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9531 - val_loss: 0.2650 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1505 - accuracy: 0.9531 - val_loss: 0.2648 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9531 - val_loss: 0.2646 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1500 - accuracy: 0.9531 - val_loss: 0.2643 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1497 - accuracy: 0.9531 - val_loss: 0.2641 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9531 - val_loss: 0.2639 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9531 - val_loss: 0.2636 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9531 - val_loss: 0.2634 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1487 - accuracy: 0.9531 - val_loss: 0.2632 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9531 - val_loss: 0.2630 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9531 - val_loss: 0.2627 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1479 - accuracy: 0.9531 - val_loss: 0.2625 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.9531 - val_loss: 0.2623 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.9531 - val_loss: 0.2621 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1472 - accuracy: 0.9531 - val_loss: 0.2618 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1470 - accuracy: 0.9531 - val_loss: 0.2616 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.9531 - val_loss: 0.2614 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1465 - accuracy: 0.9531 - val_loss: 0.2611 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9531 - val_loss: 0.2609 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9531 - val_loss: 0.2607 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9531 - val_loss: 0.2605 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1455 - accuracy: 0.9531 - val_loss: 0.2603 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.9554 - val_loss: 0.2601 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1450 - accuracy: 0.9554 - val_loss: 0.2599 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1448 - accuracy: 0.9554 - val_loss: 0.2597 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9554 - val_loss: 0.2595 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.9554 - val_loss: 0.2593 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1441 - accuracy: 0.9554 - val_loss: 0.2591 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1439 - accuracy: 0.9554 - val_loss: 0.2589 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9554 - val_loss: 0.2587 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1434 - accuracy: 0.9554 - val_loss: 0.2584 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9554 - val_loss: 0.2582 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1429 - accuracy: 0.9554 - val_loss: 0.2580 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9554 - val_loss: 0.2578 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1425 - accuracy: 0.9554 - val_loss: 0.2576 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1422 - accuracy: 0.9554 - val_loss: 0.2574 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1420 - accuracy: 0.9554 - val_loss: 0.2573 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1418 - accuracy: 0.9554 - val_loss: 0.2571 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1416 - accuracy: 0.9554 - val_loss: 0.2569 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1414 - accuracy: 0.9554 - val_loss: 0.2567 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1411 - accuracy: 0.9554 - val_loss: 0.2565 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.9554 - val_loss: 0.2563 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1407 - accuracy: 0.9554 - val_loss: 0.2561 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1405 - accuracy: 0.9554 - val_loss: 0.2559 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.9554 - val_loss: 0.2557 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1400 - accuracy: 0.9554 - val_loss: 0.2555 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1398 - accuracy: 0.9554 - val_loss: 0.2553 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1396 - accuracy: 0.9554 - val_loss: 0.2551 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9554 - val_loss: 0.2549 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1392 - accuracy: 0.9554 - val_loss: 0.2547 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1390 - accuracy: 0.9554 - val_loss: 0.2545 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1387 - accuracy: 0.9554 - val_loss: 0.2544 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 0.9554 - val_loss: 0.2542 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1383 - accuracy: 0.9554 - val_loss: 0.2540 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9554 - val_loss: 0.2539 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1379 - accuracy: 0.9554 - val_loss: 0.2537 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1377 - accuracy: 0.9554 - val_loss: 0.2535 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1375 - accuracy: 0.9554 - val_loss: 0.2533 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1373 - accuracy: 0.9554 - val_loss: 0.2531 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1371 - accuracy: 0.9554 - val_loss: 0.2529 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9554 - val_loss: 0.2528 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1367 - accuracy: 0.9554 - val_loss: 0.2526 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1365 - accuracy: 0.9554 - val_loss: 0.2524 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.9554 - val_loss: 0.2522 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1360 - accuracy: 0.9554 - val_loss: 0.2520 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1358 - accuracy: 0.9554 - val_loss: 0.2519 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9554 - val_loss: 0.2517 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9554 - val_loss: 0.2515 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1352 - accuracy: 0.9554 - val_loss: 0.2513 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1350 - accuracy: 0.9554 - val_loss: 0.2512 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.9554 - val_loss: 0.2510 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1347 - accuracy: 0.9554 - val_loss: 0.2508 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1345 - accuracy: 0.9554 - val_loss: 0.2506 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9554 - val_loss: 0.2504 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1341 - accuracy: 0.9554 - val_loss: 0.2503 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1339 - accuracy: 0.9554 - val_loss: 0.2501 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9554 - val_loss: 0.2499 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1335 - accuracy: 0.9554 - val_loss: 0.2497 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9554 - val_loss: 0.2496 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.9554 - val_loss: 0.2494 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1329 - accuracy: 0.9554 - val_loss: 0.2493 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9554 - val_loss: 0.2491 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1325 - accuracy: 0.9554 - val_loss: 0.2489 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 0.9554 - val_loss: 0.2487 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1321 - accuracy: 0.9554 - val_loss: 0.2486 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1319 - accuracy: 0.9576 - val_loss: 0.2484 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9576 - val_loss: 0.2483 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1316 - accuracy: 0.9576 - val_loss: 0.2481 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9576 - val_loss: 0.2479 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9576 - val_loss: 0.2478 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Loss: 0.2478, Accuracy: 89.29%\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 9ms/step - loss: 0.6109 - accuracy: 0.7254 - val_loss: 0.5535 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5415 - accuracy: 0.7790 - val_loss: 0.5046 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4908 - accuracy: 0.8237 - val_loss: 0.4688 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4531 - accuracy: 0.8482 - val_loss: 0.4419 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.8594 - val_loss: 0.4212 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4018 - accuracy: 0.8638 - val_loss: 0.4050 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3837 - accuracy: 0.8728 - val_loss: 0.3918 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3686 - accuracy: 0.8795 - val_loss: 0.3810 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3559 - accuracy: 0.8839 - val_loss: 0.3719 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8884 - val_loss: 0.3642 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3354 - accuracy: 0.8951 - val_loss: 0.3575 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3269 - accuracy: 0.8996 - val_loss: 0.3518 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8996 - val_loss: 0.3468 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3126 - accuracy: 0.9062 - val_loss: 0.3424 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3065 - accuracy: 0.9085 - val_loss: 0.3385 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3011 - accuracy: 0.9085 - val_loss: 0.3350 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2960 - accuracy: 0.9085 - val_loss: 0.3319 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2914 - accuracy: 0.9085 - val_loss: 0.3291 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2871 - accuracy: 0.9085 - val_loss: 0.3265 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2831 - accuracy: 0.9107 - val_loss: 0.3242 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2794 - accuracy: 0.9107 - val_loss: 0.3221 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.9107 - val_loss: 0.3202 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2726 - accuracy: 0.9107 - val_loss: 0.3184 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2696 - accuracy: 0.9129 - val_loss: 0.3167 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.9129 - val_loss: 0.3152 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2640 - accuracy: 0.9152 - val_loss: 0.3138 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2615 - accuracy: 0.9174 - val_loss: 0.3125 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2590 - accuracy: 0.9174 - val_loss: 0.3112 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2568 - accuracy: 0.9174 - val_loss: 0.3100 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2545 - accuracy: 0.9174 - val_loss: 0.3089 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.9196 - val_loss: 0.3079 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.9196 - val_loss: 0.3069 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2485 - accuracy: 0.9219 - val_loss: 0.3060 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2466 - accuracy: 0.9219 - val_loss: 0.3050 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.9219 - val_loss: 0.3042 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2431 - accuracy: 0.9219 - val_loss: 0.3033 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2414 - accuracy: 0.9241 - val_loss: 0.3025 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2397 - accuracy: 0.9263 - val_loss: 0.3018 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2382 - accuracy: 0.9286 - val_loss: 0.3010 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2366 - accuracy: 0.9286 - val_loss: 0.3003 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2351 - accuracy: 0.9286 - val_loss: 0.2996 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2337 - accuracy: 0.9286 - val_loss: 0.2989 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.9308 - val_loss: 0.2983 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2309 - accuracy: 0.9308 - val_loss: 0.2976 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.9308 - val_loss: 0.2970 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2284 - accuracy: 0.9308 - val_loss: 0.2964 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2271 - accuracy: 0.9308 - val_loss: 0.2958 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2259 - accuracy: 0.9330 - val_loss: 0.2952 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.2248 - accuracy: 0.9330 - val_loss: 0.2946 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.9330 - val_loss: 0.2941 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2225 - accuracy: 0.9330 - val_loss: 0.2935 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2214 - accuracy: 0.9353 - val_loss: 0.2930 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2204 - accuracy: 0.9353 - val_loss: 0.2925 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2194 - accuracy: 0.9353 - val_loss: 0.2919 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9375 - val_loss: 0.2914 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2174 - accuracy: 0.9375 - val_loss: 0.2909 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2164 - accuracy: 0.9375 - val_loss: 0.2904 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2155 - accuracy: 0.9375 - val_loss: 0.2899 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2146 - accuracy: 0.9375 - val_loss: 0.2894 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2137 - accuracy: 0.9375 - val_loss: 0.2890 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2128 - accuracy: 0.9375 - val_loss: 0.2885 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2119 - accuracy: 0.9375 - val_loss: 0.2880 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9375 - val_loss: 0.2876 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2102 - accuracy: 0.9375 - val_loss: 0.2871 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2094 - accuracy: 0.9375 - val_loss: 0.2867 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2086 - accuracy: 0.9375 - val_loss: 0.2862 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2078 - accuracy: 0.9375 - val_loss: 0.2858 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2070 - accuracy: 0.9375 - val_loss: 0.2854 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2062 - accuracy: 0.9375 - val_loss: 0.2849 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2055 - accuracy: 0.9375 - val_loss: 0.2845 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9397 - val_loss: 0.2841 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9397 - val_loss: 0.2837 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2033 - accuracy: 0.9397 - val_loss: 0.2832 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2026 - accuracy: 0.9397 - val_loss: 0.2828 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9397 - val_loss: 0.2824 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2012 - accuracy: 0.9397 - val_loss: 0.2820 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2005 - accuracy: 0.9397 - val_loss: 0.2816 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9397 - val_loss: 0.2812 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1991 - accuracy: 0.9397 - val_loss: 0.2808 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1985 - accuracy: 0.9420 - val_loss: 0.2804 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1978 - accuracy: 0.9420 - val_loss: 0.2800 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1972 - accuracy: 0.9420 - val_loss: 0.2797 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9420 - val_loss: 0.2793 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1959 - accuracy: 0.9420 - val_loss: 0.2789 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1953 - accuracy: 0.9420 - val_loss: 0.2785 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1947 - accuracy: 0.9420 - val_loss: 0.2781 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1941 - accuracy: 0.9420 - val_loss: 0.2778 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9420 - val_loss: 0.2774 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.9420 - val_loss: 0.2770 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1923 - accuracy: 0.9420 - val_loss: 0.2767 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1917 - accuracy: 0.9420 - val_loss: 0.2763 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1911 - accuracy: 0.9420 - val_loss: 0.2759 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1906 - accuracy: 0.9420 - val_loss: 0.2756 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1900 - accuracy: 0.9420 - val_loss: 0.2752 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1895 - accuracy: 0.9420 - val_loss: 0.2749 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1889 - accuracy: 0.9420 - val_loss: 0.2745 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1884 - accuracy: 0.9420 - val_loss: 0.2742 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9420 - val_loss: 0.2738 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9420 - val_loss: 0.2735 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1868 - accuracy: 0.9420 - val_loss: 0.2731 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9420 - val_loss: 0.2728 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.9420 - val_loss: 0.2724 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1853 - accuracy: 0.9420 - val_loss: 0.2721 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9420 - val_loss: 0.2718 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1842 - accuracy: 0.9420 - val_loss: 0.2714 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.9420 - val_loss: 0.2711 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.9420 - val_loss: 0.2708 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1828 - accuracy: 0.9420 - val_loss: 0.2705 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1823 - accuracy: 0.9420 - val_loss: 0.2701 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.9420 - val_loss: 0.2698 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1813 - accuracy: 0.9420 - val_loss: 0.2695 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.9420 - val_loss: 0.2692 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1804 - accuracy: 0.9420 - val_loss: 0.2688 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1799 - accuracy: 0.9420 - val_loss: 0.2685 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1795 - accuracy: 0.9420 - val_loss: 0.2682 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1790 - accuracy: 0.9442 - val_loss: 0.2679 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1786 - accuracy: 0.9442 - val_loss: 0.2676 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 11ms/step - loss: 0.1781 - accuracy: 0.9442 - val_loss: 0.2673 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1777 - accuracy: 0.9442 - val_loss: 0.2670 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1773 - accuracy: 0.9442 - val_loss: 0.2667 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1768 - accuracy: 0.9442 - val_loss: 0.2664 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1764 - accuracy: 0.9442 - val_loss: 0.2661 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1760 - accuracy: 0.9442 - val_loss: 0.2658 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1755 - accuracy: 0.9442 - val_loss: 0.2655 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9442 - val_loss: 0.2652 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.9442 - val_loss: 0.2649 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9442 - val_loss: 0.2646 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9464 - val_loss: 0.2643 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.9464 - val_loss: 0.2640 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1731 - accuracy: 0.9464 - val_loss: 0.2637 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1727 - accuracy: 0.9464 - val_loss: 0.2634 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1723 - accuracy: 0.9464 - val_loss: 0.2632 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9464 - val_loss: 0.2629 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9464 - val_loss: 0.2626 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.9464 - val_loss: 0.2623 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1707 - accuracy: 0.9464 - val_loss: 0.2620 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9464 - val_loss: 0.2617 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1699 - accuracy: 0.9464 - val_loss: 0.2615 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.9464 - val_loss: 0.2612 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1692 - accuracy: 0.9487 - val_loss: 0.2609 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9487 - val_loss: 0.2606 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9487 - val_loss: 0.2604 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1680 - accuracy: 0.9487 - val_loss: 0.2601 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1677 - accuracy: 0.9487 - val_loss: 0.2598 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1673 - accuracy: 0.9509 - val_loss: 0.2595 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1670 - accuracy: 0.9509 - val_loss: 0.2593 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9509 - val_loss: 0.2590 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1662 - accuracy: 0.9509 - val_loss: 0.2587 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9509 - val_loss: 0.2585 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1655 - accuracy: 0.9509 - val_loss: 0.2582 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1652 - accuracy: 0.9509 - val_loss: 0.2580 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1648 - accuracy: 0.9509 - val_loss: 0.2577 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9509 - val_loss: 0.2574 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1641 - accuracy: 0.9509 - val_loss: 0.2572 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1638 - accuracy: 0.9509 - val_loss: 0.2569 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.9509 - val_loss: 0.2567 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.9509 - val_loss: 0.2564 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.9509 - val_loss: 0.2562 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1624 - accuracy: 0.9509 - val_loss: 0.2560 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1621 - accuracy: 0.9509 - val_loss: 0.2557 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1618 - accuracy: 0.9509 - val_loss: 0.2555 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9509 - val_loss: 0.2552 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1611 - accuracy: 0.9509 - val_loss: 0.2550 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1608 - accuracy: 0.9509 - val_loss: 0.2547 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9509 - val_loss: 0.2545 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1602 - accuracy: 0.9531 - val_loss: 0.2542 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1598 - accuracy: 0.9531 - val_loss: 0.2540 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.9531 - val_loss: 0.2538 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 0.9531 - val_loss: 0.2535 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1589 - accuracy: 0.9531 - val_loss: 0.2533 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1586 - accuracy: 0.9531 - val_loss: 0.2531 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1583 - accuracy: 0.9531 - val_loss: 0.2528 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1580 - accuracy: 0.9531 - val_loss: 0.2526 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1577 - accuracy: 0.9531 - val_loss: 0.2524 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.9531 - val_loss: 0.2521 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.9531 - val_loss: 0.2519 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9531 - val_loss: 0.2517 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1564 - accuracy: 0.9531 - val_loss: 0.2515 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9531 - val_loss: 0.2512 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9531 - val_loss: 0.2510 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1555 - accuracy: 0.9531 - val_loss: 0.2508 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1553 - accuracy: 0.9531 - val_loss: 0.2506 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1550 - accuracy: 0.9531 - val_loss: 0.2504 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9531 - val_loss: 0.2501 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9531 - val_loss: 0.2499 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9531 - val_loss: 0.2497 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1538 - accuracy: 0.9531 - val_loss: 0.2495 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.9531 - val_loss: 0.2493 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9531 - val_loss: 0.2490 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1529 - accuracy: 0.9531 - val_loss: 0.2488 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1527 - accuracy: 0.9531 - val_loss: 0.2486 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1524 - accuracy: 0.9531 - val_loss: 0.2484 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.9531 - val_loss: 0.2482 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1518 - accuracy: 0.9531 - val_loss: 0.2480 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1515 - accuracy: 0.9531 - val_loss: 0.2478 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9531 - val_loss: 0.2476 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1510 - accuracy: 0.9531 - val_loss: 0.2474 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1507 - accuracy: 0.9531 - val_loss: 0.2472 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1504 - accuracy: 0.9531 - val_loss: 0.2470 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9531 - val_loss: 0.2468 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1499 - accuracy: 0.9531 - val_loss: 0.2466 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1496 - accuracy: 0.9531 - val_loss: 0.2464 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1494 - accuracy: 0.9531 - val_loss: 0.2462 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1491 - accuracy: 0.9554 - val_loss: 0.2460 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1488 - accuracy: 0.9554 - val_loss: 0.2458 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9554 - val_loss: 0.2456 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9554 - val_loss: 0.2454 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1480 - accuracy: 0.9554 - val_loss: 0.2452 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9554 - val_loss: 0.2450 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9554 - val_loss: 0.2448 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1473 - accuracy: 0.9554 - val_loss: 0.2446 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1470 - accuracy: 0.9554 - val_loss: 0.2444 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1468 - accuracy: 0.9554 - val_loss: 0.2442 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1465 - accuracy: 0.9554 - val_loss: 0.2441 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1463 - accuracy: 0.9554 - val_loss: 0.2439 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9554 - val_loss: 0.2437 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9554 - val_loss: 0.2435 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1455 - accuracy: 0.9554 - val_loss: 0.2433 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1452 - accuracy: 0.9554 - val_loss: 0.2431 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9554 - val_loss: 0.2429 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9554 - val_loss: 0.2427 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1445 - accuracy: 0.9554 - val_loss: 0.2425 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1443 - accuracy: 0.9554 - val_loss: 0.2423 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1440 - accuracy: 0.9576 - val_loss: 0.2422 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1438 - accuracy: 0.9576 - val_loss: 0.2420 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1435 - accuracy: 0.9576 - val_loss: 0.2418 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1433 - accuracy: 0.9576 - val_loss: 0.2416 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1430 - accuracy: 0.9576 - val_loss: 0.2415 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1428 - accuracy: 0.9576 - val_loss: 0.2413 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1426 - accuracy: 0.9576 - val_loss: 0.2411 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1423 - accuracy: 0.9576 - val_loss: 0.2409 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1421 - accuracy: 0.9576 - val_loss: 0.2408 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1418 - accuracy: 0.9576 - val_loss: 0.2406 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1416 - accuracy: 0.9576 - val_loss: 0.2404 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1414 - accuracy: 0.9576 - val_loss: 0.2403 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1411 - accuracy: 0.9576 - val_loss: 0.2401 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.9576 - val_loss: 0.2399 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1407 - accuracy: 0.9576 - val_loss: 0.2397 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1404 - accuracy: 0.9576 - val_loss: 0.2396 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9576 - val_loss: 0.2394 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1400 - accuracy: 0.9576 - val_loss: 0.2392 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1397 - accuracy: 0.9576 - val_loss: 0.2391 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9576 - val_loss: 0.2389 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9576 - val_loss: 0.2388 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1391 - accuracy: 0.9576 - val_loss: 0.2386 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1388 - accuracy: 0.9576 - val_loss: 0.2384 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9576 - val_loss: 0.2383 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.9576 - val_loss: 0.2381 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1382 - accuracy: 0.9576 - val_loss: 0.2380 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1379 - accuracy: 0.9576 - val_loss: 0.2378 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9576 - val_loss: 0.2377 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9576 - val_loss: 0.2375 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1373 - accuracy: 0.9576 - val_loss: 0.2373 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1371 - accuracy: 0.9576 - val_loss: 0.2372 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9576 - val_loss: 0.2370 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1366 - accuracy: 0.9576 - val_loss: 0.2369 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1364 - accuracy: 0.9576 - val_loss: 0.2367 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1362 - accuracy: 0.9576 - val_loss: 0.2366 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9576 - val_loss: 0.2364 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.9576 - val_loss: 0.2363 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9576 - val_loss: 0.2361 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1353 - accuracy: 0.9576 - val_loss: 0.2360 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1351 - accuracy: 0.9598 - val_loss: 0.2358 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1349 - accuracy: 0.9576 - val_loss: 0.2357 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1347 - accuracy: 0.9598 - val_loss: 0.2355 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.9598 - val_loss: 0.2354 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9598 - val_loss: 0.2352 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1341 - accuracy: 0.9598 - val_loss: 0.2351 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.9598 - val_loss: 0.2350 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1337 - accuracy: 0.9598 - val_loss: 0.2348 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9598 - val_loss: 0.2346 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1333 - accuracy: 0.9598 - val_loss: 0.2345 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1330 - accuracy: 0.9598 - val_loss: 0.2344 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1329 - accuracy: 0.9598 - val_loss: 0.2342 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1327 - accuracy: 0.9598 - val_loss: 0.2341 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1324 - accuracy: 0.9598 - val_loss: 0.2339 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1322 - accuracy: 0.9598 - val_loss: 0.2338 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9598 - val_loss: 0.2337 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1318 - accuracy: 0.9598 - val_loss: 0.2335 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1316 - accuracy: 0.9621 - val_loss: 0.2334 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9621 - val_loss: 0.2333 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9621 - val_loss: 0.2331 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1310 - accuracy: 0.9621 - val_loss: 0.2330 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1308 - accuracy: 0.9621 - val_loss: 0.2329 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9621 - val_loss: 0.2327 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9621 - val_loss: 0.2326 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1303 - accuracy: 0.9621 - val_loss: 0.2325 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1301 - accuracy: 0.9621 - val_loss: 0.2323 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1299 - accuracy: 0.9621 - val_loss: 0.2322 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9621 - val_loss: 0.2320 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.9621 - val_loss: 0.2319 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9621 - val_loss: 0.2318 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9621 - val_loss: 0.2316 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1289 - accuracy: 0.9621 - val_loss: 0.2315 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1287 - accuracy: 0.9621 - val_loss: 0.2314 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1285 - accuracy: 0.9621 - val_loss: 0.2313 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1283 - accuracy: 0.9621 - val_loss: 0.2311 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9621 - val_loss: 0.2310 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1279 - accuracy: 0.9621 - val_loss: 0.2309 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1278 - accuracy: 0.9621 - val_loss: 0.2308 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Loss: 0.2308, Accuracy: 91.96%\n",
      "Epoch 1/300\n",
      "45/45 [==============================] - 1s 8ms/step - loss: 0.7219 - accuracy: 0.6384 - val_loss: 0.6069 - val_accuracy: 0.6696 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.6257 - accuracy: 0.7098 - val_loss: 0.5230 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5572 - accuracy: 0.7500 - val_loss: 0.4648 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5065 - accuracy: 0.7768 - val_loss: 0.4231 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4678 - accuracy: 0.7991 - val_loss: 0.3923 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4376 - accuracy: 0.8080 - val_loss: 0.3689 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4134 - accuracy: 0.8192 - val_loss: 0.3506 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3936 - accuracy: 0.8326 - val_loss: 0.3360 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3772 - accuracy: 0.8482 - val_loss: 0.3240 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3634 - accuracy: 0.8504 - val_loss: 0.3141 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3518 - accuracy: 0.8661 - val_loss: 0.3058 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.8683 - val_loss: 0.2988 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3331 - accuracy: 0.8728 - val_loss: 0.2927 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8750 - val_loss: 0.2874 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3189 - accuracy: 0.8750 - val_loss: 0.2828 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3130 - accuracy: 0.8839 - val_loss: 0.2787 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3076 - accuracy: 0.8884 - val_loss: 0.2750 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8929 - val_loss: 0.2717 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2982 - accuracy: 0.8951 - val_loss: 0.2687 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.8951 - val_loss: 0.2660 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2901 - accuracy: 0.8996 - val_loss: 0.2636 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2866 - accuracy: 0.9018 - val_loss: 0.2613 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2832 - accuracy: 0.9018 - val_loss: 0.2592 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.9018 - val_loss: 0.2572 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2771 - accuracy: 0.9040 - val_loss: 0.2554 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2743 - accuracy: 0.9040 - val_loss: 0.2537 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2716 - accuracy: 0.9085 - val_loss: 0.2521 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2692 - accuracy: 0.9085 - val_loss: 0.2506 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.9129 - val_loss: 0.2492 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2645 - accuracy: 0.9152 - val_loss: 0.2478 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2624 - accuracy: 0.9152 - val_loss: 0.2465 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2603 - accuracy: 0.9152 - val_loss: 0.2453 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2583 - accuracy: 0.9174 - val_loss: 0.2441 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2564 - accuracy: 0.9174 - val_loss: 0.2430 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.9219 - val_loss: 0.2419 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2529 - accuracy: 0.9241 - val_loss: 0.2408 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.9241 - val_loss: 0.2397 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9286 - val_loss: 0.2387 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2480 - accuracy: 0.9286 - val_loss: 0.2378 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2464 - accuracy: 0.9286 - val_loss: 0.2368 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2450 - accuracy: 0.9308 - val_loss: 0.2359 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 42/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2436 - accuracy: 0.9308 - val_loss: 0.2350 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 43/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2422 - accuracy: 0.9308 - val_loss: 0.2341 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 44/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2408 - accuracy: 0.9308 - val_loss: 0.2332 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 45/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.9308 - val_loss: 0.2324 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 46/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.9330 - val_loss: 0.2316 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 47/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2370 - accuracy: 0.9330 - val_loss: 0.2307 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 48/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2357 - accuracy: 0.9330 - val_loss: 0.2299 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 49/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.9330 - val_loss: 0.2292 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 50/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2334 - accuracy: 0.9330 - val_loss: 0.2284 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 51/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2323 - accuracy: 0.9330 - val_loss: 0.2276 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 52/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2311 - accuracy: 0.9330 - val_loss: 0.2269 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 53/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2300 - accuracy: 0.9330 - val_loss: 0.2261 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 54/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2289 - accuracy: 0.9353 - val_loss: 0.2254 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 55/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2279 - accuracy: 0.9353 - val_loss: 0.2247 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 56/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2268 - accuracy: 0.9353 - val_loss: 0.2240 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 57/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2258 - accuracy: 0.9353 - val_loss: 0.2233 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 58/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.9353 - val_loss: 0.2226 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 59/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2238 - accuracy: 0.9353 - val_loss: 0.2219 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 60/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2229 - accuracy: 0.9353 - val_loss: 0.2213 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 61/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9353 - val_loss: 0.2206 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 62/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.9353 - val_loss: 0.2200 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 63/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.9353 - val_loss: 0.2193 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 64/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2191 - accuracy: 0.9353 - val_loss: 0.2187 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 65/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2183 - accuracy: 0.9353 - val_loss: 0.2180 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 66/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2174 - accuracy: 0.9353 - val_loss: 0.2174 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 67/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2165 - accuracy: 0.9375 - val_loss: 0.2168 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 68/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2156 - accuracy: 0.9375 - val_loss: 0.2162 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 69/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2148 - accuracy: 0.9375 - val_loss: 0.2156 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 70/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2139 - accuracy: 0.9375 - val_loss: 0.2150 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 71/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9375 - val_loss: 0.2144 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 72/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9397 - val_loss: 0.2138 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 73/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2115 - accuracy: 0.9397 - val_loss: 0.2132 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 74/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.9397 - val_loss: 0.2126 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 75/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9397 - val_loss: 0.2121 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 76/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2091 - accuracy: 0.9397 - val_loss: 0.2115 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 77/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2084 - accuracy: 0.9397 - val_loss: 0.2110 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 78/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2077 - accuracy: 0.9397 - val_loss: 0.2104 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 79/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.9397 - val_loss: 0.2099 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 80/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2062 - accuracy: 0.9397 - val_loss: 0.2093 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 81/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2054 - accuracy: 0.9397 - val_loss: 0.2088 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 82/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2047 - accuracy: 0.9397 - val_loss: 0.2082 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 83/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9397 - val_loss: 0.2077 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 84/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2033 - accuracy: 0.9397 - val_loss: 0.2072 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 85/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2026 - accuracy: 0.9397 - val_loss: 0.2067 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 86/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2019 - accuracy: 0.9397 - val_loss: 0.2062 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 87/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2012 - accuracy: 0.9397 - val_loss: 0.2056 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 88/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2005 - accuracy: 0.9397 - val_loss: 0.2051 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 89/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1999 - accuracy: 0.9397 - val_loss: 0.2046 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 90/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1993 - accuracy: 0.9420 - val_loss: 0.2041 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 91/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9420 - val_loss: 0.2036 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 92/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1979 - accuracy: 0.9420 - val_loss: 0.2031 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 93/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1973 - accuracy: 0.9420 - val_loss: 0.2026 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 94/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1966 - accuracy: 0.9420 - val_loss: 0.2022 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 95/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1960 - accuracy: 0.9420 - val_loss: 0.2017 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 96/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1954 - accuracy: 0.9420 - val_loss: 0.2012 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 97/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1948 - accuracy: 0.9420 - val_loss: 0.2007 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 98/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1942 - accuracy: 0.9420 - val_loss: 0.2003 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 99/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9420 - val_loss: 0.1998 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 100/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1930 - accuracy: 0.9420 - val_loss: 0.1993 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 101/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1924 - accuracy: 0.9420 - val_loss: 0.1989 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 102/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1918 - accuracy: 0.9420 - val_loss: 0.1984 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 103/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1913 - accuracy: 0.9442 - val_loss: 0.1980 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 104/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1907 - accuracy: 0.9442 - val_loss: 0.1975 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 105/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.9442 - val_loss: 0.1971 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 106/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1895 - accuracy: 0.9464 - val_loss: 0.1967 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 107/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1890 - accuracy: 0.9464 - val_loss: 0.1962 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 108/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1884 - accuracy: 0.9464 - val_loss: 0.1958 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 109/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9464 - val_loss: 0.1954 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 110/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1873 - accuracy: 0.9464 - val_loss: 0.1950 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 111/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1868 - accuracy: 0.9464 - val_loss: 0.1946 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 112/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1863 - accuracy: 0.9464 - val_loss: 0.1941 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 113/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1857 - accuracy: 0.9464 - val_loss: 0.1937 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 114/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1853 - accuracy: 0.9464 - val_loss: 0.1933 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 115/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9464 - val_loss: 0.1929 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 116/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1842 - accuracy: 0.9464 - val_loss: 0.1925 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 117/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1837 - accuracy: 0.9464 - val_loss: 0.1921 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 118/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.9464 - val_loss: 0.1917 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 119/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1827 - accuracy: 0.9464 - val_loss: 0.1913 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 120/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9464 - val_loss: 0.1910 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 121/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.9464 - val_loss: 0.1906 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 122/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1812 - accuracy: 0.9464 - val_loss: 0.1902 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 123/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1807 - accuracy: 0.9464 - val_loss: 0.1898 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 124/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1802 - accuracy: 0.9464 - val_loss: 0.1894 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 125/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9464 - val_loss: 0.1891 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 126/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9464 - val_loss: 0.1887 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 127/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1788 - accuracy: 0.9464 - val_loss: 0.1883 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 128/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1783 - accuracy: 0.9464 - val_loss: 0.1880 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 129/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9464 - val_loss: 0.1876 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 130/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1774 - accuracy: 0.9464 - val_loss: 0.1873 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 131/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9464 - val_loss: 0.1869 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 132/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1765 - accuracy: 0.9464 - val_loss: 0.1866 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 133/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9464 - val_loss: 0.1862 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 134/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1756 - accuracy: 0.9487 - val_loss: 0.1859 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 135/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9487 - val_loss: 0.1855 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 136/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9487 - val_loss: 0.1852 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 137/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1743 - accuracy: 0.9487 - val_loss: 0.1849 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 138/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1738 - accuracy: 0.9487 - val_loss: 0.1845 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 139/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1734 - accuracy: 0.9487 - val_loss: 0.1842 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 140/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1729 - accuracy: 0.9487 - val_loss: 0.1839 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 141/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1725 - accuracy: 0.9487 - val_loss: 0.1835 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 142/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1721 - accuracy: 0.9487 - val_loss: 0.1832 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 143/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1717 - accuracy: 0.9487 - val_loss: 0.1829 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 144/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1712 - accuracy: 0.9487 - val_loss: 0.1826 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 145/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9487 - val_loss: 0.1823 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 146/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.9487 - val_loss: 0.1819 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 147/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9487 - val_loss: 0.1816 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 148/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.9487 - val_loss: 0.1813 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 149/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1692 - accuracy: 0.9487 - val_loss: 0.1810 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 150/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1688 - accuracy: 0.9487 - val_loss: 0.1807 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 151/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9487 - val_loss: 0.1804 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 152/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9487 - val_loss: 0.1801 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 153/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.9487 - val_loss: 0.1798 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 154/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1672 - accuracy: 0.9487 - val_loss: 0.1795 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 155/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1668 - accuracy: 0.9487 - val_loss: 0.1792 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 156/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9487 - val_loss: 0.1789 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 157/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1660 - accuracy: 0.9487 - val_loss: 0.1786 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 158/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1657 - accuracy: 0.9487 - val_loss: 0.1783 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 159/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1653 - accuracy: 0.9487 - val_loss: 0.1780 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 160/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1649 - accuracy: 0.9487 - val_loss: 0.1778 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 161/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1645 - accuracy: 0.9487 - val_loss: 0.1775 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 162/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1642 - accuracy: 0.9487 - val_loss: 0.1772 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 163/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1638 - accuracy: 0.9487 - val_loss: 0.1769 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 164/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9487 - val_loss: 0.1766 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 165/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1631 - accuracy: 0.9487 - val_loss: 0.1764 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 166/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.9487 - val_loss: 0.1761 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 167/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9464 - val_loss: 0.1758 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 168/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9464 - val_loss: 0.1755 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 169/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1617 - accuracy: 0.9487 - val_loss: 0.1753 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 170/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1613 - accuracy: 0.9487 - val_loss: 0.1750 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 171/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1609 - accuracy: 0.9464 - val_loss: 0.1747 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 172/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.9487 - val_loss: 0.1745 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 173/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1603 - accuracy: 0.9464 - val_loss: 0.1742 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 174/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1599 - accuracy: 0.9464 - val_loss: 0.1739 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 175/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1596 - accuracy: 0.9464 - val_loss: 0.1737 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 176/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1592 - accuracy: 0.9464 - val_loss: 0.1734 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 177/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1589 - accuracy: 0.9464 - val_loss: 0.1732 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 178/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1586 - accuracy: 0.9487 - val_loss: 0.1729 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 179/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9487 - val_loss: 0.1727 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 180/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1579 - accuracy: 0.9487 - val_loss: 0.1724 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 181/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.9487 - val_loss: 0.1722 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 182/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.9487 - val_loss: 0.1719 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 183/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1569 - accuracy: 0.9487 - val_loss: 0.1717 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 184/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1566 - accuracy: 0.9487 - val_loss: 0.1714 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 185/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.9487 - val_loss: 0.1712 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 186/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1560 - accuracy: 0.9487 - val_loss: 0.1709 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 187/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1556 - accuracy: 0.9487 - val_loss: 0.1707 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 188/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9487 - val_loss: 0.1704 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 189/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9487 - val_loss: 0.1702 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 190/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1547 - accuracy: 0.9487 - val_loss: 0.1700 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9487 - val_loss: 0.1697 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1540 - accuracy: 0.9487 - val_loss: 0.1695 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.9487 - val_loss: 0.1693 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1534 - accuracy: 0.9487 - val_loss: 0.1690 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.9487 - val_loss: 0.1688 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 196/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1528 - accuracy: 0.9487 - val_loss: 0.1686 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 197/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.9487 - val_loss: 0.1684 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 198/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9487 - val_loss: 0.1681 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 199/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9487 - val_loss: 0.1679 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 200/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.9487 - val_loss: 0.1677 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 201/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1513 - accuracy: 0.9487 - val_loss: 0.1675 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 202/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.9487 - val_loss: 0.1672 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 203/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1507 - accuracy: 0.9487 - val_loss: 0.1670 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 204/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.9487 - val_loss: 0.1668 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 205/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.9487 - val_loss: 0.1666 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 206/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9487 - val_loss: 0.1664 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 207/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9487 - val_loss: 0.1661 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 208/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9487 - val_loss: 0.1659 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 209/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1489 - accuracy: 0.9487 - val_loss: 0.1657 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 210/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9487 - val_loss: 0.1655 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 211/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1483 - accuracy: 0.9487 - val_loss: 0.1653 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 212/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1480 - accuracy: 0.9487 - val_loss: 0.1651 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 213/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.9487 - val_loss: 0.1649 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 214/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1474 - accuracy: 0.9487 - val_loss: 0.1647 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 215/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1471 - accuracy: 0.9487 - val_loss: 0.1644 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 216/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1468 - accuracy: 0.9487 - val_loss: 0.1642 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 217/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1465 - accuracy: 0.9487 - val_loss: 0.1640 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 218/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1462 - accuracy: 0.9487 - val_loss: 0.1638 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 219/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1459 - accuracy: 0.9487 - val_loss: 0.1636 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 220/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1456 - accuracy: 0.9464 - val_loss: 0.1634 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 221/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1453 - accuracy: 0.9464 - val_loss: 0.1632 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 222/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1450 - accuracy: 0.9487 - val_loss: 0.1630 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 223/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1447 - accuracy: 0.9464 - val_loss: 0.1628 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 224/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9464 - val_loss: 0.1626 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 225/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1441 - accuracy: 0.9487 - val_loss: 0.1624 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 226/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1437 - accuracy: 0.9464 - val_loss: 0.1622 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 227/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1435 - accuracy: 0.9464 - val_loss: 0.1620 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 228/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1431 - accuracy: 0.9487 - val_loss: 0.1618 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 229/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1428 - accuracy: 0.9464 - val_loss: 0.1616 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 230/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1425 - accuracy: 0.9464 - val_loss: 0.1614 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 231/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1423 - accuracy: 0.9464 - val_loss: 0.1612 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 232/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.9464 - val_loss: 0.1610 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 233/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9464 - val_loss: 0.1608 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 234/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1414 - accuracy: 0.9464 - val_loss: 0.1607 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 235/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1411 - accuracy: 0.9464 - val_loss: 0.1605 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 236/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1409 - accuracy: 0.9464 - val_loss: 0.1603 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 237/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1406 - accuracy: 0.9464 - val_loss: 0.1601 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 238/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1404 - accuracy: 0.9464 - val_loss: 0.1599 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 239/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1401 - accuracy: 0.9464 - val_loss: 0.1597 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 240/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.9464 - val_loss: 0.1595 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 241/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9464 - val_loss: 0.1594 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 242/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1394 - accuracy: 0.9464 - val_loss: 0.1592 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 243/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1391 - accuracy: 0.9464 - val_loss: 0.1590 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 244/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1389 - accuracy: 0.9464 - val_loss: 0.1588 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 245/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1386 - accuracy: 0.9464 - val_loss: 0.1586 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 246/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9487 - val_loss: 0.1585 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 247/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9487 - val_loss: 0.1583 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 248/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1379 - accuracy: 0.9487 - val_loss: 0.1581 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 249/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1377 - accuracy: 0.9487 - val_loss: 0.1579 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 250/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.9487 - val_loss: 0.1578 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 251/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1372 - accuracy: 0.9487 - val_loss: 0.1576 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 252/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1369 - accuracy: 0.9487 - val_loss: 0.1574 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 253/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1367 - accuracy: 0.9487 - val_loss: 0.1572 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 254/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1365 - accuracy: 0.9487 - val_loss: 0.1571 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 255/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1363 - accuracy: 0.9487 - val_loss: 0.1569 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 256/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1360 - accuracy: 0.9487 - val_loss: 0.1567 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 257/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1358 - accuracy: 0.9487 - val_loss: 0.1566 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 258/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9487 - val_loss: 0.1564 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 259/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 0.9487 - val_loss: 0.1562 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 260/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1351 - accuracy: 0.9487 - val_loss: 0.1561 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 261/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9487 - val_loss: 0.1559 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 262/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1347 - accuracy: 0.9487 - val_loss: 0.1557 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 263/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9487 - val_loss: 0.1556 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 264/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9487 - val_loss: 0.1554 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 265/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1340 - accuracy: 0.9487 - val_loss: 0.1553 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 266/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1338 - accuracy: 0.9487 - val_loss: 0.1551 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 267/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9487 - val_loss: 0.1549 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 268/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9487 - val_loss: 0.1548 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 269/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1332 - accuracy: 0.9487 - val_loss: 0.1546 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 270/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.9487 - val_loss: 0.1545 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 271/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1327 - accuracy: 0.9487 - val_loss: 0.1543 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 272/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9487 - val_loss: 0.1541 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 273/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1323 - accuracy: 0.9509 - val_loss: 0.1540 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 274/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1321 - accuracy: 0.9509 - val_loss: 0.1538 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 275/300\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1319 - accuracy: 0.9509 - val_loss: 0.1537 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 276/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9509 - val_loss: 0.1535 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 277/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9509 - val_loss: 0.1534 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 278/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1312 - accuracy: 0.9509 - val_loss: 0.1532 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 279/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1310 - accuracy: 0.9509 - val_loss: 0.1531 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 280/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9509 - val_loss: 0.1529 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 281/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9509 - val_loss: 0.1528 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 282/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1304 - accuracy: 0.9509 - val_loss: 0.1526 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 283/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9509 - val_loss: 0.1525 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 284/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1300 - accuracy: 0.9509 - val_loss: 0.1523 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 285/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9509 - val_loss: 0.1522 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 286/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9509 - val_loss: 0.1520 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 287/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9509 - val_loss: 0.1519 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 288/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9509 - val_loss: 0.1517 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 289/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1290 - accuracy: 0.9509 - val_loss: 0.1516 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 290/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1288 - accuracy: 0.9531 - val_loss: 0.1514 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 291/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1286 - accuracy: 0.9531 - val_loss: 0.1513 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 292/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1284 - accuracy: 0.9531 - val_loss: 0.1512 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 293/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1282 - accuracy: 0.9531 - val_loss: 0.1510 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 294/300\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9531 - val_loss: 0.1509 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 295/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9531 - val_loss: 0.1507 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 296/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1277 - accuracy: 0.9531 - val_loss: 0.1506 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 297/300\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1274 - accuracy: 0.9531 - val_loss: 0.1504 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 298/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1273 - accuracy: 0.9531 - val_loss: 0.1503 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 299/300\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1271 - accuracy: 0.9531 - val_loss: 0.1502 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 300/300\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1269 - accuracy: 0.9531 - val_loss: 0.1500 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Loss: 0.1500, Accuracy: 94.64%\n",
      "Vanilla_RNN finished in 275.41 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_20 (SimpleRNN)   (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Average accuracy: 0.9196\n",
      "Average loss: 0.2018\n"
     ]
    }
   ],
   "source": [
    "k_fold = 5 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing vanilla RNN with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 300, verbose = 1, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('Vanilla_RNN finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_1(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing vanilla RNN with K-fold\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.46640, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.46640 to 0.43242, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.43242 to 0.40616, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.40616 to 0.38537, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.38537 to 0.36865, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.36865 to 0.35482, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.35482 to 0.34320, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.34320 to 0.33319, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.33319 to 0.32475, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.32475 to 0.31755, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.31755 to 0.31106, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.31106 to 0.30529, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.30529 to 0.30016, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.30016 to 0.29559, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.29559 to 0.29134, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.29134 to 0.28758, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.28758 to 0.28418, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.28418 to 0.28105, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.28105 to 0.27815, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.27815 to 0.27550, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.27550 to 0.27312, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.27312 to 0.27091, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.27091 to 0.26876, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.26876 to 0.26671, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.26671 to 0.26481, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.26481 to 0.26303, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.26303 to 0.26148, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.26148 to 0.25989, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.25989 to 0.25838, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.25838 to 0.25710, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.25710 to 0.25590, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.25590 to 0.25458, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.25458 to 0.25332, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.25332 to 0.25203, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.25203 to 0.25088, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.25088 to 0.24979, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.24979 to 0.24872, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.24872 to 0.24771, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.24771 to 0.24668, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.24668 to 0.24570, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.24570 to 0.24478, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.24478 to 0.24385, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.24385 to 0.24296, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.24296 to 0.24211, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.24211 to 0.24129, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.24129 to 0.24050, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.24050 to 0.23971, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.23971 to 0.23895, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.23895 to 0.23823, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.23823 to 0.23750, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.23750 to 0.23669, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.23669 to 0.23597, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.23597 to 0.23535, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.23535 to 0.23469, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.23469 to 0.23405, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.23405 to 0.23347, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.23347 to 0.23296, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.23296 to 0.23231, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.23231 to 0.23171, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.23171 to 0.23111, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.23111 to 0.23049, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.23049 to 0.22990, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.22990 to 0.22933, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.22933 to 0.22869, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.22869 to 0.22815, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.22815 to 0.22759, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.22759 to 0.22708, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.22708 to 0.22653, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.22653 to 0.22598, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.22598 to 0.22547, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.22547 to 0.22495, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.22495 to 0.22442, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.22442 to 0.22392, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.22392 to 0.22344, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.22344 to 0.22293, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.22293 to 0.22248, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.22248 to 0.22203, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.22203 to 0.22157, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.22157 to 0.22116, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.22116 to 0.22070, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.22070 to 0.22022, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.22022 to 0.21974, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.21974 to 0.21926, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.21926 to 0.21883, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.21883 to 0.21841, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.21841 to 0.21799, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.21799 to 0.21751, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.21751 to 0.21710, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.21710 to 0.21670, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.21670 to 0.21627, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.21627 to 0.21579, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.21579 to 0.21536, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.21536 to 0.21495, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.21495 to 0.21462, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.21462 to 0.21424, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.21424 to 0.21385, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.21385 to 0.21346, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.21346 to 0.21316, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.21316 to 0.21268, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.21268 to 0.21229, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.21229 to 0.21191, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.21191 to 0.21154, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.21154 to 0.21115, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.21115 to 0.21084, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.21084 to 0.21043, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.21043 to 0.21008, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.21008 to 0.20971, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.20971 to 0.20933, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.20933 to 0.20896, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.20896 to 0.20859, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.20859 to 0.20823, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.20823 to 0.20788, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.20788 to 0.20753, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.20753 to 0.20718, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.20718 to 0.20683, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.20683 to 0.20648, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.20648 to 0.20608, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.20608 to 0.20579, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.20579 to 0.20545, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.20545 to 0.20513, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.20513 to 0.20480, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.20480 to 0.20445, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.20445 to 0.20414, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.20414 to 0.20383, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.20383 to 0.20353, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.20353 to 0.20322, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.20322 to 0.20282, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.20282 to 0.20252, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.20252 to 0.20222, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.20222 to 0.20191, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.20191 to 0.20159, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.20159 to 0.20124, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.20124 to 0.20095, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.20095 to 0.20065, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.20065 to 0.20036, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.20036 to 0.20008, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.20008 to 0.19983, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.19983 to 0.19953, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.19953 to 0.19924, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.19924 to 0.19894, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.19894 to 0.19870, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.19870 to 0.19838, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.19838 to 0.19809, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.19809 to 0.19778, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.19778 to 0.19748, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.19748 to 0.19720, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.19720 to 0.19693, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.19693 to 0.19661, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.19661 to 0.19632, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.19632 to 0.19604, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.19604 to 0.19569, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.19569 to 0.19543, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.19543 to 0.19518, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.19518 to 0.19493, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.19493 to 0.19472, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.19472 to 0.19447, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.19447 to 0.19420, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.19420 to 0.19394, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.19394 to 0.19369, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.19369 to 0.19342, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.19342 to 0.19315, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.19315 to 0.19297, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.19297 to 0.19270, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.19270 to 0.19242, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.19242 to 0.19230, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.19230 to 0.19207, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.19207 to 0.19177, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.19177 to 0.19147, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.19147 to 0.19122, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.19122 to 0.19097, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.19097 to 0.19075, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.19075 to 0.19051, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.19051 to 0.19028, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.19028 to 0.19006, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.19006 to 0.18981, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.18981 to 0.18958, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.18958 to 0.18934, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.18934 to 0.18913, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.18913 to 0.18890, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.18890 to 0.18868, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.18868 to 0.18849, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.18849 to 0.18825, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.18825 to 0.18812, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.18812 to 0.18792, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.18792 to 0.18771, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.18771 to 0.18748, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.18748 to 0.18727, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.18727 to 0.18698, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.18698 to 0.18674, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.18674 to 0.18653, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.18653 to 0.18628, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.18628 to 0.18606, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.18606 to 0.18588, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.18588 to 0.18575, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.18575 to 0.18552, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.18552 to 0.18531, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.18531 to 0.18521, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.18521 to 0.18500, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.18500 to 0.18478, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.18478 to 0.18452, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.18452 to 0.18433, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.18433 to 0.18411, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.18411 to 0.18388, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.18388 to 0.18368, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.18368 to 0.18351, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.18351 to 0.18331, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.18331 to 0.18311, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.18311 to 0.18292, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.18292 to 0.18273, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.18273 to 0.18254, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.18254 to 0.18249, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.18249 to 0.18224, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.18224 to 0.18207, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.18207 to 0.18183, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.18183 to 0.18162, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.18162 to 0.18142, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.18142 to 0.18122, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.18122 to 0.18104, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.18104 to 0.18080, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.18080 to 0.18064, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.18064 to 0.18046, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.18046 to 0.18028, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.18028 to 0.18010, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.18010 to 0.17994, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.17994 to 0.17978, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.17978 to 0.17967, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.17967 to 0.17949, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.17949 to 0.17930, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.17930 to 0.17917, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.17917 to 0.17898, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.17898 to 0.17887, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.17887 to 0.17871, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.17871 to 0.17847, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.17847 to 0.17827, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.17827 to 0.17812, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.17812 to 0.17795, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.17795 to 0.17780, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.17780 to 0.17763, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.17763 to 0.17746, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.17746 to 0.17728, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.17728 to 0.17712, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.17712 to 0.17695, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.17695 to 0.17680, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.17680\n",
      "\n",
      "Epoch 245: val_loss improved from 0.17680 to 0.17665, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.17665 to 0.17649, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.17649 to 0.17632, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.17632 to 0.17616, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.17616 to 0.17601, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.17601 to 0.17585, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.17585 to 0.17570, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.17570 to 0.17555, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.17555 to 0.17540, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.17540 to 0.17525, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.17525 to 0.17509, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.17509 to 0.17494, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.17494 to 0.17476, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.17476 to 0.17461, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.17461 to 0.17448, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.17448 to 0.17435, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.17435 to 0.17415, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.17415 to 0.17404, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.17404 to 0.17391, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.17391 to 0.17378, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.17378 to 0.17365, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.17365 to 0.17351, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.17351 to 0.17334, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.17334 to 0.17330, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.17330 to 0.17317, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.17317 to 0.17304, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.17304 to 0.17290, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.17290 to 0.17276, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.17276 to 0.17265, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.17265 to 0.17247, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.17247 to 0.17244, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.17244 to 0.17229, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.17229 to 0.17214, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.17214 to 0.17201, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.17201 to 0.17185, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.17185 to 0.17173, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.17173 to 0.17159, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.17159 to 0.17147, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.17147 to 0.17132, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.17132 to 0.17120, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.17120 to 0.17106, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.17106 to 0.17092, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.17092 to 0.17080, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.17080 to 0.17065, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.17065 to 0.17052, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.17052 to 0.17037, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.17037 to 0.17024, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.17024 to 0.17013, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.17013 to 0.16999, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.16999 to 0.16987, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.16987 to 0.16974, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.16974 to 0.16963, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.16963 to 0.16951, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.16951 to 0.16939, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.16939 to 0.16932, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.16932 to 0.16919, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "Loss: 0.1692, Accuracy: 94.64%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.16919\n",
      "Loss: 0.1963, Accuracy: 91.07%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.16919\n",
      "\n",
      "Epoch 35: val_loss improved from 0.16919 to 0.16888, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.16888 to 0.16759, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.16759 to 0.16630, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.16630 to 0.16506, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.16506 to 0.16386, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.16386 to 0.16275, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.16275 to 0.16162, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.16162 to 0.16070, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.16070 to 0.15963, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.15963 to 0.15859, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.15859 to 0.15770, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.15770 to 0.15672, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.15672 to 0.15579, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.15579 to 0.15500, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.15500 to 0.15409, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.15409 to 0.15326, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.15326 to 0.15239, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.15239 to 0.15150, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.15150 to 0.15066, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.15066 to 0.14991, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.14991 to 0.14911, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.14911 to 0.14835, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.14835 to 0.14760, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.14760 to 0.14685, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.14685 to 0.14615, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.14615 to 0.14545, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.14545 to 0.14476, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.14476 to 0.14409, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.14409 to 0.14342, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.14342 to 0.14276, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.14276 to 0.14208, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.14208 to 0.14148, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.14148 to 0.14083, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.14083 to 0.14022, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.14022 to 0.13959, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.13959 to 0.13896, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.13896 to 0.13835, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.13835 to 0.13776, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.13776 to 0.13720, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.13720 to 0.13664, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.13664 to 0.13608, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.13608 to 0.13552, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.13552 to 0.13495, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.13495 to 0.13442, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.13442 to 0.13388, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.13388 to 0.13336, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.13336 to 0.13289, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.13289 to 0.13245, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.13245 to 0.13195, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.13195 to 0.13144, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.13144 to 0.13098, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.13098 to 0.13049, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.13049 to 0.13001, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.13001 to 0.12954, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.12954 to 0.12907, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.12907 to 0.12868, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.12868 to 0.12823, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.12823 to 0.12780, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.12780 to 0.12734, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.12734 to 0.12692, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.12692 to 0.12654, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.12654 to 0.12610, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.12610 to 0.12572, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.12572 to 0.12529, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.12529 to 0.12485, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.12485 to 0.12440, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.12440 to 0.12398, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.12398 to 0.12377, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.12377 to 0.12339, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.12339 to 0.12296, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.12296 to 0.12255, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.12255 to 0.12216, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.12216 to 0.12183, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.12183 to 0.12142, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.12142 to 0.12102, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.12102 to 0.12063, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.12063 to 0.12030, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.12030 to 0.11991, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.11991 to 0.11953, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.11953 to 0.11916, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.11916 to 0.11879, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.11879 to 0.11847, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.11847 to 0.11808, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.11808 to 0.11772, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.11772 to 0.11747, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.11747 to 0.11710, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.11710 to 0.11676, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.11676 to 0.11641, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.11641 to 0.11607, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.11607 to 0.11573, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.11573 to 0.11538, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.11538 to 0.11508, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.11508 to 0.11475, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.11475 to 0.11444, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.11444 to 0.11412, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.11412 to 0.11382, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.11382 to 0.11354, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.11354 to 0.11322, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.11322 to 0.11298, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.11298 to 0.11266, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.11266 to 0.11235, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.11235 to 0.11205, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.11205 to 0.11182, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.11182 to 0.11152, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.11152 to 0.11123, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.11123 to 0.11094, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.11094 to 0.11066, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.11066 to 0.11040, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.11040 to 0.11012, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.11012 to 0.10984, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.10984 to 0.10957, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.10957 to 0.10939, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.10939 to 0.10911, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.10911 to 0.10888, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.10888 to 0.10864, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.10864 to 0.10837, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.10837 to 0.10813, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.10813 to 0.10786, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.10786 to 0.10761, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.10761 to 0.10735, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.10735 to 0.10712, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.10712 to 0.10693, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.10693 to 0.10668, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.10668 to 0.10645, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.10645 to 0.10621, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.10621 to 0.10599, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.10599 to 0.10576, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.10576 to 0.10552, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.10552 to 0.10530, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.10530 to 0.10508, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.10508 to 0.10488, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.10488 to 0.10468, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.10468 to 0.10446, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.10446 to 0.10423, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.10423 to 0.10404, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.10404 to 0.10385, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.10385 to 0.10364, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.10364 to 0.10345, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.10345 to 0.10326, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.10326 to 0.10306, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.10306 to 0.10287, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.10287 to 0.10268, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.10268 to 0.10250, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.10250 to 0.10231, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.10231 to 0.10212, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.10212 to 0.10192, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.10192 to 0.10173, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.10173 to 0.10155, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.10155 to 0.10139, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.10139 to 0.10124, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.10124 to 0.10110, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.10110 to 0.10093, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.10093 to 0.10075, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.10075 to 0.10060, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.10060 to 0.10044, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.10044 to 0.10028, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.10028 to 0.10010, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.10010 to 0.09995, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.09995 to 0.09979, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.09979 to 0.09964, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.09964 to 0.09948, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.09948 to 0.09934, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.09934 to 0.09919, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.09919 to 0.09905, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.09905 to 0.09889, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.09889 to 0.09874, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.09874 to 0.09860, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.09860 to 0.09846, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.09846 to 0.09834, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.09834 to 0.09821, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.09821 to 0.09808, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.09808 to 0.09794, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.09794 to 0.09780, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.09780 to 0.09768, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.09768 to 0.09754, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.09754 to 0.09741, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.09741 to 0.09739, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.09739 to 0.09726, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.09726 to 0.09711, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.09711 to 0.09701, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.09701 to 0.09691, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.09691 to 0.09678, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.09678 to 0.09666, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.09666 to 0.09652, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.09652 to 0.09639, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.09639 to 0.09627, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.09627 to 0.09615, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.09615 to 0.09603, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.09603 to 0.09593, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.09593 to 0.09582, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.09582 to 0.09570, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.09570 to 0.09559, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.09559 to 0.09546, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.09546 to 0.09536, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.09536 to 0.09525, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.09525 to 0.09515, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.09515 to 0.09504, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.09504 to 0.09497, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.09497 to 0.09489, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.09489 to 0.09478, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.09478 to 0.09466, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.09466 to 0.09456, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.09456 to 0.09445, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.09445 to 0.09442, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.09442 to 0.09432, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.09432 to 0.09421, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.09421 to 0.09413, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.09413 to 0.09404, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.09404 to 0.09393, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.09393 to 0.09384, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.09384 to 0.09376, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.09376 to 0.09367, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.09367 to 0.09356, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.09356 to 0.09345, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.09345 to 0.09343, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.09343 to 0.09336, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.09336 to 0.09328, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.09328 to 0.09322, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.09322 to 0.09316, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.09316 to 0.09308, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.09308 to 0.09300, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.09300 to 0.09296, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.09296 to 0.09286, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.09286 to 0.09277, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.09277 to 0.09269, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.09269 to 0.09262, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.09262 to 0.09254, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.09254 to 0.09246, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.09246 to 0.09241, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.09241 to 0.09233, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.09233 to 0.09226, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.09226 to 0.09217, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.09217 to 0.09209, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.09209 to 0.09207, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09207\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09207\n",
      "\n",
      "Epoch 271: val_loss improved from 0.09207 to 0.09205, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.09205 to 0.09199, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.09199 to 0.09190, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09190\n",
      "\n",
      "Epoch 275: val_loss improved from 0.09190 to 0.09185, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.09185 to 0.09177, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.09177 to 0.09167, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.09167 to 0.09160, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.09160 to 0.09154, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.09154 to 0.09147, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.09147 to 0.09140, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.09140 to 0.09132, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.09132 to 0.09126, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.09126 to 0.09119, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.09119 to 0.09111, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.09111 to 0.09107, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.09107 to 0.09100, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.09100 to 0.09093, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.09093 to 0.09086, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.09086 to 0.09081, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.09081 to 0.09075, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09075\n",
      "\n",
      "Epoch 293: val_loss improved from 0.09075 to 0.09069, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.09069 to 0.09064, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.09064 to 0.09059, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.09059 to 0.09050, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.09050 to 0.09044, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.09044 to 0.09039, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.09039 to 0.09033, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.09033 to 0.09027, saving model to model_checkpoint\\Vanilla_RNN_10-fold.h5\n",
      "Loss: 0.0903, Accuracy: 98.21%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.1526, Accuracy: 98.21%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.1962, Accuracy: 92.86%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.2637, Accuracy: 91.07%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.2185, Accuracy: 91.07%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.3254, Accuracy: 89.29%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.1954, Accuracy: 92.86%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09027\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09027\n",
      "Loss: 0.1466, Accuracy: 94.64%\n",
      "Vanilla_RNN finished in 372.18 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_30 (SimpleRNN)   (None, 32)                3808      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Average accuracy: 0.9339\n",
      "Average loss: 0.1954\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN_10-fold.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "k_fold = 10 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing vanilla RNN with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 300, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('Vanilla_RNN finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_1(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average loss curve\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(train_loss[i])\n",
    "plt.title('Training Loss - All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(train_acc[i])\n",
    "plt.title('Training Accuracy - All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "40/40 [==============================] - 3s 22ms/step - loss: 0.7058 - accuracy: 0.4856 - val_loss: 0.7151 - val_accuracy: 0.4643 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.5304 - val_loss: 0.7023 - val_accuracy: 0.5476 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6808 - accuracy: 0.5847 - val_loss: 0.6900 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6688 - accuracy: 0.6006 - val_loss: 0.6782 - val_accuracy: 0.5952 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6572 - accuracy: 0.6581 - val_loss: 0.6671 - val_accuracy: 0.6369 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6463 - accuracy: 0.6901 - val_loss: 0.6562 - val_accuracy: 0.6726 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6356 - accuracy: 0.7284 - val_loss: 0.6462 - val_accuracy: 0.7024 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6259 - accuracy: 0.7508 - val_loss: 0.6362 - val_accuracy: 0.7262 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6160 - accuracy: 0.7636 - val_loss: 0.6264 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.6064 - accuracy: 0.7700 - val_loss: 0.6171 - val_accuracy: 0.7560 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5973 - accuracy: 0.7859 - val_loss: 0.6079 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5882 - accuracy: 0.8019 - val_loss: 0.5991 - val_accuracy: 0.7798 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5796 - accuracy: 0.8243 - val_loss: 0.5905 - val_accuracy: 0.7798 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5712 - accuracy: 0.8339 - val_loss: 0.5822 - val_accuracy: 0.7917 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5631 - accuracy: 0.8403 - val_loss: 0.5743 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5553 - accuracy: 0.8498 - val_loss: 0.5665 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5478 - accuracy: 0.8562 - val_loss: 0.5590 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.5405 - accuracy: 0.8722 - val_loss: 0.5517 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5333 - accuracy: 0.8818 - val_loss: 0.5448 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5266 - accuracy: 0.8850 - val_loss: 0.5380 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5199 - accuracy: 0.8946 - val_loss: 0.5315 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5135 - accuracy: 0.8946 - val_loss: 0.5251 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5072 - accuracy: 0.9010 - val_loss: 0.5188 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.5011 - accuracy: 0.9010 - val_loss: 0.5127 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4952 - accuracy: 0.9010 - val_loss: 0.5070 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4895 - accuracy: 0.9010 - val_loss: 0.5014 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4840 - accuracy: 0.9010 - val_loss: 0.4959 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4786 - accuracy: 0.9010 - val_loss: 0.4904 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.9010 - val_loss: 0.4850 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4682 - accuracy: 0.9010 - val_loss: 0.4801 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4634 - accuracy: 0.9010 - val_loss: 0.4751 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4585 - accuracy: 0.9010 - val_loss: 0.4702 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4537 - accuracy: 0.9042 - val_loss: 0.4654 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4491 - accuracy: 0.9042 - val_loss: 0.4607 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4446 - accuracy: 0.9073 - val_loss: 0.4561 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4402 - accuracy: 0.9105 - val_loss: 0.4517 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4359 - accuracy: 0.9105 - val_loss: 0.4474 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4317 - accuracy: 0.9105 - val_loss: 0.4432 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4276 - accuracy: 0.9105 - val_loss: 0.4390 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4236 - accuracy: 0.9105 - val_loss: 0.4350 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.9137 - val_loss: 0.4310 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.4159 - accuracy: 0.9137 - val_loss: 0.4271 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4122 - accuracy: 0.9137 - val_loss: 0.4233 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4086 - accuracy: 0.9137 - val_loss: 0.4196 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4050 - accuracy: 0.9137 - val_loss: 0.4160 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.4015 - accuracy: 0.9169 - val_loss: 0.4124 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3981 - accuracy: 0.9169 - val_loss: 0.4089 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3947 - accuracy: 0.9169 - val_loss: 0.4055 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3915 - accuracy: 0.9169 - val_loss: 0.4024 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3885 - accuracy: 0.9169 - val_loss: 0.3995 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3856 - accuracy: 0.9169 - val_loss: 0.3962 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3824 - accuracy: 0.9169 - val_loss: 0.3931 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3794 - accuracy: 0.9169 - val_loss: 0.3900 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3764 - accuracy: 0.9169 - val_loss: 0.3869 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3735 - accuracy: 0.9201 - val_loss: 0.3840 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3707 - accuracy: 0.9201 - val_loss: 0.3810 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3678 - accuracy: 0.9201 - val_loss: 0.3782 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.3651 - accuracy: 0.9201 - val_loss: 0.3754 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3624 - accuracy: 0.9201 - val_loss: 0.3726 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3598 - accuracy: 0.9233 - val_loss: 0.3699 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3572 - accuracy: 0.9233 - val_loss: 0.3674 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3548 - accuracy: 0.9233 - val_loss: 0.3648 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3523 - accuracy: 0.9233 - val_loss: 0.3622 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3499 - accuracy: 0.9233 - val_loss: 0.3598 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3475 - accuracy: 0.9233 - val_loss: 0.3573 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3452 - accuracy: 0.9233 - val_loss: 0.3552 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3431 - accuracy: 0.9233 - val_loss: 0.3529 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3409 - accuracy: 0.9233 - val_loss: 0.3506 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3387 - accuracy: 0.9233 - val_loss: 0.3483 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.9233 - val_loss: 0.3460 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.3344 - accuracy: 0.9233 - val_loss: 0.3439 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3323 - accuracy: 0.9233 - val_loss: 0.3417 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.3302 - accuracy: 0.9233 - val_loss: 0.3397 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3282 - accuracy: 0.9265 - val_loss: 0.3376 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3263 - accuracy: 0.9265 - val_loss: 0.3356 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3243 - accuracy: 0.9265 - val_loss: 0.3336 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3224 - accuracy: 0.9265 - val_loss: 0.3316 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3205 - accuracy: 0.9265 - val_loss: 0.3297 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3186 - accuracy: 0.9265 - val_loss: 0.3278 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.9297 - val_loss: 0.3259 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3150 - accuracy: 0.9297 - val_loss: 0.3240 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.9297 - val_loss: 0.3222 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3115 - accuracy: 0.9297 - val_loss: 0.3204 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3098 - accuracy: 0.9297 - val_loss: 0.3186 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3081 - accuracy: 0.9297 - val_loss: 0.3169 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3065 - accuracy: 0.9297 - val_loss: 0.3152 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.3048 - accuracy: 0.9297 - val_loss: 0.3135 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.3032 - accuracy: 0.9297 - val_loss: 0.3119 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.3016 - accuracy: 0.9297 - val_loss: 0.3103 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.3001 - accuracy: 0.9297 - val_loss: 0.3088 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2986 - accuracy: 0.9297 - val_loss: 0.3073 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2971 - accuracy: 0.9297 - val_loss: 0.3057 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2956 - accuracy: 0.9297 - val_loss: 0.3041 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2941 - accuracy: 0.9297 - val_loss: 0.3026 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2926 - accuracy: 0.9297 - val_loss: 0.3012 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2912 - accuracy: 0.9297 - val_loss: 0.2997 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2898 - accuracy: 0.9297 - val_loss: 0.2983 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2884 - accuracy: 0.9297 - val_loss: 0.2969 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2870 - accuracy: 0.9297 - val_loss: 0.2955 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2856 - accuracy: 0.9297 - val_loss: 0.2941 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2843 - accuracy: 0.9297 - val_loss: 0.2927 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2830 - accuracy: 0.9297 - val_loss: 0.2915 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2817 - accuracy: 0.9297 - val_loss: 0.2902 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2805 - accuracy: 0.9297 - val_loss: 0.2889 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2792 - accuracy: 0.9297 - val_loss: 0.2876 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2779 - accuracy: 0.9297 - val_loss: 0.2864 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2767 - accuracy: 0.9297 - val_loss: 0.2851 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2754 - accuracy: 0.9297 - val_loss: 0.2839 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2742 - accuracy: 0.9297 - val_loss: 0.2827 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2730 - accuracy: 0.9297 - val_loss: 0.2815 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2719 - accuracy: 0.9297 - val_loss: 0.2803 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2707 - accuracy: 0.9297 - val_loss: 0.2792 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2696 - accuracy: 0.9297 - val_loss: 0.2781 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2685 - accuracy: 0.9297 - val_loss: 0.2769 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2674 - accuracy: 0.9297 - val_loss: 0.2758 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2663 - accuracy: 0.9297 - val_loss: 0.2747 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2652 - accuracy: 0.9297 - val_loss: 0.2739 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2643 - accuracy: 0.9297 - val_loss: 0.2728 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2633 - accuracy: 0.9297 - val_loss: 0.2718 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2622 - accuracy: 0.9297 - val_loss: 0.2707 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2612 - accuracy: 0.9297 - val_loss: 0.2697 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.9265 - val_loss: 0.2687 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2592 - accuracy: 0.9265 - val_loss: 0.2678 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2582 - accuracy: 0.9265 - val_loss: 0.2668 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2573 - accuracy: 0.9265 - val_loss: 0.2658 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2563 - accuracy: 0.9265 - val_loss: 0.2649 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2553 - accuracy: 0.9265 - val_loss: 0.2639 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2544 - accuracy: 0.9265 - val_loss: 0.2630 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2534 - accuracy: 0.9265 - val_loss: 0.2621 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2525 - accuracy: 0.9265 - val_loss: 0.2611 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2516 - accuracy: 0.9265 - val_loss: 0.2602 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2507 - accuracy: 0.9265 - val_loss: 0.2594 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2498 - accuracy: 0.9265 - val_loss: 0.2585 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2489 - accuracy: 0.9265 - val_loss: 0.2576 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2481 - accuracy: 0.9265 - val_loss: 0.2568 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2472 - accuracy: 0.9265 - val_loss: 0.2559 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2464 - accuracy: 0.9265 - val_loss: 0.2551 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2455 - accuracy: 0.9265 - val_loss: 0.2542 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2447 - accuracy: 0.9265 - val_loss: 0.2534 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2439 - accuracy: 0.9265 - val_loss: 0.2526 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2431 - accuracy: 0.9265 - val_loss: 0.2518 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2423 - accuracy: 0.9265 - val_loss: 0.2510 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2415 - accuracy: 0.9265 - val_loss: 0.2502 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2407 - accuracy: 0.9265 - val_loss: 0.2494 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2399 - accuracy: 0.9265 - val_loss: 0.2487 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2391 - accuracy: 0.9265 - val_loss: 0.2479 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2384 - accuracy: 0.9265 - val_loss: 0.2472 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2376 - accuracy: 0.9265 - val_loss: 0.2464 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2369 - accuracy: 0.9265 - val_loss: 0.2457 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2362 - accuracy: 0.9265 - val_loss: 0.2450 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2354 - accuracy: 0.9265 - val_loss: 0.2443 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2347 - accuracy: 0.9265 - val_loss: 0.2436 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2340 - accuracy: 0.9265 - val_loss: 0.2429 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2333 - accuracy: 0.9265 - val_loss: 0.2422 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2326 - accuracy: 0.9265 - val_loss: 0.2416 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2320 - accuracy: 0.9265 - val_loss: 0.2409 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2313 - accuracy: 0.9265 - val_loss: 0.2402 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2306 - accuracy: 0.9265 - val_loss: 0.2397 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2301 - accuracy: 0.9265 - val_loss: 0.2391 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2294 - accuracy: 0.9265 - val_loss: 0.2384 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2287 - accuracy: 0.9265 - val_loss: 0.2378 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2281 - accuracy: 0.9265 - val_loss: 0.2372 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2275 - accuracy: 0.9297 - val_loss: 0.2365 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2268 - accuracy: 0.9297 - val_loss: 0.2359 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2262 - accuracy: 0.9297 - val_loss: 0.2353 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2256 - accuracy: 0.9297 - val_loss: 0.2347 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2250 - accuracy: 0.9297 - val_loss: 0.2341 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.2244 - accuracy: 0.9297 - val_loss: 0.2335 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2238 - accuracy: 0.9297 - val_loss: 0.2329 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2232 - accuracy: 0.9297 - val_loss: 0.2324 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2226 - accuracy: 0.9297 - val_loss: 0.2318 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2220 - accuracy: 0.9297 - val_loss: 0.2312 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2214 - accuracy: 0.9297 - val_loss: 0.2307 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2209 - accuracy: 0.9297 - val_loss: 0.2301 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2203 - accuracy: 0.9297 - val_loss: 0.2296 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2197 - accuracy: 0.9297 - val_loss: 0.2290 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2192 - accuracy: 0.9297 - val_loss: 0.2285 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2186 - accuracy: 0.9297 - val_loss: 0.2279 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2181 - accuracy: 0.9297 - val_loss: 0.2274 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2176 - accuracy: 0.9297 - val_loss: 0.2269 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2170 - accuracy: 0.9297 - val_loss: 0.2264 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2165 - accuracy: 0.9297 - val_loss: 0.2259 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2160 - accuracy: 0.9297 - val_loss: 0.2253 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2155 - accuracy: 0.9297 - val_loss: 0.2248 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2149 - accuracy: 0.9297 - val_loss: 0.2243 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2144 - accuracy: 0.9297 - val_loss: 0.2239 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2139 - accuracy: 0.9297 - val_loss: 0.2234 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2134 - accuracy: 0.9297 - val_loss: 0.2229 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2129 - accuracy: 0.9297 - val_loss: 0.2224 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2125 - accuracy: 0.9297 - val_loss: 0.2219 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9297 - val_loss: 0.2215 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.2115 - accuracy: 0.9297 - val_loss: 0.2210 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2110 - accuracy: 0.9297 - val_loss: 0.2205 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2105 - accuracy: 0.9297 - val_loss: 0.2201 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2101 - accuracy: 0.9297 - val_loss: 0.2196 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2096 - accuracy: 0.9297 - val_loss: 0.2192 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2091 - accuracy: 0.9297 - val_loss: 0.2187 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2087 - accuracy: 0.9297 - val_loss: 0.2183 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2082 - accuracy: 0.9297 - val_loss: 0.2178 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2078 - accuracy: 0.9297 - val_loss: 0.2174 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2073 - accuracy: 0.9297 - val_loss: 0.2170 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2069 - accuracy: 0.9297 - val_loss: 0.2166 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2065 - accuracy: 0.9297 - val_loss: 0.2162 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2060 - accuracy: 0.9297 - val_loss: 0.2157 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2056 - accuracy: 0.9297 - val_loss: 0.2153 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9297 - val_loss: 0.2149 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2047 - accuracy: 0.9297 - val_loss: 0.2145 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9297 - val_loss: 0.2141 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2039 - accuracy: 0.9297 - val_loss: 0.2137 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2034 - accuracy: 0.9297 - val_loss: 0.2133 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9297 - val_loss: 0.2129 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2026 - accuracy: 0.9297 - val_loss: 0.2125 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2022 - accuracy: 0.9297 - val_loss: 0.2121 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9297 - val_loss: 0.2117 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.2014 - accuracy: 0.9297 - val_loss: 0.2113 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9297 - val_loss: 0.2109 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2006 - accuracy: 0.9297 - val_loss: 0.2105 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.2002 - accuracy: 0.9297 - val_loss: 0.2101 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9297 - val_loss: 0.2098 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1994 - accuracy: 0.9297 - val_loss: 0.2094 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1991 - accuracy: 0.9297 - val_loss: 0.2090 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1987 - accuracy: 0.9297 - val_loss: 0.2086 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1983 - accuracy: 0.9297 - val_loss: 0.2083 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1979 - accuracy: 0.9297 - val_loss: 0.2079 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1975 - accuracy: 0.9297 - val_loss: 0.2076 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1971 - accuracy: 0.9297 - val_loss: 0.2072 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1968 - accuracy: 0.9297 - val_loss: 0.2069 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1964 - accuracy: 0.9297 - val_loss: 0.2065 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1960 - accuracy: 0.9297 - val_loss: 0.2062 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1957 - accuracy: 0.9297 - val_loss: 0.2058 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9297 - val_loss: 0.2055 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1949 - accuracy: 0.9297 - val_loss: 0.2051 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1946 - accuracy: 0.9297 - val_loss: 0.2048 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1943 - accuracy: 0.9297 - val_loss: 0.2045 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1939 - accuracy: 0.9297 - val_loss: 0.2041 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1935 - accuracy: 0.9297 - val_loss: 0.2038 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1932 - accuracy: 0.9297 - val_loss: 0.2034 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1928 - accuracy: 0.9297 - val_loss: 0.2031 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1925 - accuracy: 0.9297 - val_loss: 0.2028 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1922 - accuracy: 0.9297 - val_loss: 0.2025 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1918 - accuracy: 0.9297 - val_loss: 0.2022 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1915 - accuracy: 0.9297 - val_loss: 0.2018 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1912 - accuracy: 0.9297 - val_loss: 0.2015 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1908 - accuracy: 0.9297 - val_loss: 0.2012 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1905 - accuracy: 0.9329 - val_loss: 0.2009 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1902 - accuracy: 0.9329 - val_loss: 0.2006 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1898 - accuracy: 0.9329 - val_loss: 0.2003 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1895 - accuracy: 0.9329 - val_loss: 0.1999 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1892 - accuracy: 0.9329 - val_loss: 0.1996 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1889 - accuracy: 0.9329 - val_loss: 0.1993 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9329 - val_loss: 0.1990 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1883 - accuracy: 0.9329 - val_loss: 0.1988 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1880 - accuracy: 0.9329 - val_loss: 0.1985 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1876 - accuracy: 0.9329 - val_loss: 0.1982 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1873 - accuracy: 0.9329 - val_loss: 0.1979 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1870 - accuracy: 0.9329 - val_loss: 0.1976 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.9329 - val_loss: 0.1974 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1864 - accuracy: 0.9329 - val_loss: 0.1971 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9329 - val_loss: 0.1968 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1858 - accuracy: 0.9329 - val_loss: 0.1965 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1855 - accuracy: 0.9329 - val_loss: 0.1962 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1852 - accuracy: 0.9329 - val_loss: 0.1959 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1849 - accuracy: 0.9329 - val_loss: 0.1957 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1846 - accuracy: 0.9329 - val_loss: 0.1954 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.9329 - val_loss: 0.1951 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9329 - val_loss: 0.1948 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1838 - accuracy: 0.9329 - val_loss: 0.1946 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1835 - accuracy: 0.9329 - val_loss: 0.1943 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1832 - accuracy: 0.9329 - val_loss: 0.1940 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1829 - accuracy: 0.9329 - val_loss: 0.1938 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9329 - val_loss: 0.1935 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9361 - val_loss: 0.1933 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1821 - accuracy: 0.9361 - val_loss: 0.1930 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.9361 - val_loss: 0.1927 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9361 - val_loss: 0.1925 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1813 - accuracy: 0.9361 - val_loss: 0.1922 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1810 - accuracy: 0.9361 - val_loss: 0.1920 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1807 - accuracy: 0.9361 - val_loss: 0.1917 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9361 - val_loss: 0.1914 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9361 - val_loss: 0.1912 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1799 - accuracy: 0.9361 - val_loss: 0.1909 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1797 - accuracy: 0.9361 - val_loss: 0.1907 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1794 - accuracy: 0.9361 - val_loss: 0.1904 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1791 - accuracy: 0.9361 - val_loss: 0.1902 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9361 - val_loss: 0.1900 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1786 - accuracy: 0.9361 - val_loss: 0.1897 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1784 - accuracy: 0.9361 - val_loss: 0.1894 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1781 - accuracy: 0.9361 - val_loss: 0.1892 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.9361 - val_loss: 0.1890 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9361 - val_loss: 0.1887 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9361 - val_loss: 0.1885 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.9361 - val_loss: 0.1883 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9361 - val_loss: 0.1880 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9361 - val_loss: 0.1878 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1764 - accuracy: 0.9361 - val_loss: 0.1876 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.9361 - val_loss: 0.1873 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1759 - accuracy: 0.9361 - val_loss: 0.1871 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1757 - accuracy: 0.9361 - val_loss: 0.1869 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1754 - accuracy: 0.9361 - val_loss: 0.1867 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1752 - accuracy: 0.9361 - val_loss: 0.1865 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9361 - val_loss: 0.1863 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1747 - accuracy: 0.9361 - val_loss: 0.1860 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1744 - accuracy: 0.9361 - val_loss: 0.1858 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.9361 - val_loss: 0.1856 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9361 - val_loss: 0.1854 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 306/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1737 - accuracy: 0.9361 - val_loss: 0.1852 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1735 - accuracy: 0.9361 - val_loss: 0.1850 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1733 - accuracy: 0.9361 - val_loss: 0.1848 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1731 - accuracy: 0.9361 - val_loss: 0.1846 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1728 - accuracy: 0.9361 - val_loss: 0.1844 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1726 - accuracy: 0.9361 - val_loss: 0.1841 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.9361 - val_loss: 0.1839 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1722 - accuracy: 0.9361 - val_loss: 0.1837 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1719 - accuracy: 0.9361 - val_loss: 0.1835 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1717 - accuracy: 0.9361 - val_loss: 0.1833 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1715 - accuracy: 0.9361 - val_loss: 0.1831 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9361 - val_loss: 0.1829 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 318/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1711 - accuracy: 0.9361 - val_loss: 0.1827 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1709 - accuracy: 0.9361 - val_loss: 0.1825 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1706 - accuracy: 0.9361 - val_loss: 0.1823 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1704 - accuracy: 0.9361 - val_loss: 0.1820 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1702 - accuracy: 0.9361 - val_loss: 0.1818 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1700 - accuracy: 0.9361 - val_loss: 0.1816 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1698 - accuracy: 0.9361 - val_loss: 0.1814 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 325/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1696 - accuracy: 0.9361 - val_loss: 0.1812 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9361 - val_loss: 0.1810 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.9361 - val_loss: 0.1808 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.9361 - val_loss: 0.1806 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.9361 - val_loss: 0.1805 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1685 - accuracy: 0.9361 - val_loss: 0.1803 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1683 - accuracy: 0.9393 - val_loss: 0.1801 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9393 - val_loss: 0.1799 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1679 - accuracy: 0.9425 - val_loss: 0.1797 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1677 - accuracy: 0.9425 - val_loss: 0.1795 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1675 - accuracy: 0.9425 - val_loss: 0.1793 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.9425 - val_loss: 0.1791 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1670 - accuracy: 0.9425 - val_loss: 0.1789 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9425 - val_loss: 0.1787 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1666 - accuracy: 0.9425 - val_loss: 0.1786 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1664 - accuracy: 0.9425 - val_loss: 0.1784 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.9425 - val_loss: 0.1782 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 342/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1660 - accuracy: 0.9425 - val_loss: 0.1780 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1658 - accuracy: 0.9425 - val_loss: 0.1778 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 344/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1656 - accuracy: 0.9425 - val_loss: 0.1777 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9425 - val_loss: 0.1775 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1652 - accuracy: 0.9425 - val_loss: 0.1773 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1651 - accuracy: 0.9425 - val_loss: 0.1771 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1649 - accuracy: 0.9425 - val_loss: 0.1769 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.9425 - val_loss: 0.1768 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1645 - accuracy: 0.9425 - val_loss: 0.1766 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 351/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9425 - val_loss: 0.1764 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1641 - accuracy: 0.9425 - val_loss: 0.1762 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1639 - accuracy: 0.9425 - val_loss: 0.1760 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1637 - accuracy: 0.9425 - val_loss: 0.1759 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 355/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9425 - val_loss: 0.1757 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1633 - accuracy: 0.9425 - val_loss: 0.1755 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1632 - accuracy: 0.9425 - val_loss: 0.1753 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.9425 - val_loss: 0.1752 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1628 - accuracy: 0.9425 - val_loss: 0.1750 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1626 - accuracy: 0.9425 - val_loss: 0.1748 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 361/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1624 - accuracy: 0.9425 - val_loss: 0.1747 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 362/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.9425 - val_loss: 0.1745 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1621 - accuracy: 0.9425 - val_loss: 0.1743 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1619 - accuracy: 0.9425 - val_loss: 0.1742 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1617 - accuracy: 0.9425 - val_loss: 0.1740 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1615 - accuracy: 0.9425 - val_loss: 0.1738 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1613 - accuracy: 0.9425 - val_loss: 0.1737 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 368/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1612 - accuracy: 0.9425 - val_loss: 0.1735 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1610 - accuracy: 0.9425 - val_loss: 0.1734 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 370/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1608 - accuracy: 0.9425 - val_loss: 0.1732 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.9425 - val_loss: 0.1731 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9425 - val_loss: 0.1729 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1603 - accuracy: 0.9425 - val_loss: 0.1727 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1601 - accuracy: 0.9425 - val_loss: 0.1726 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1599 - accuracy: 0.9425 - val_loss: 0.1724 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1598 - accuracy: 0.9425 - val_loss: 0.1722 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9425 - val_loss: 0.1721 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1594 - accuracy: 0.9425 - val_loss: 0.1719 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.9425 - val_loss: 0.1718 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1591 - accuracy: 0.9425 - val_loss: 0.1716 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 381/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1589 - accuracy: 0.9425 - val_loss: 0.1715 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9425 - val_loss: 0.1713 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1586 - accuracy: 0.9425 - val_loss: 0.1711 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1584 - accuracy: 0.9425 - val_loss: 0.1710 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1582 - accuracy: 0.9425 - val_loss: 0.1708 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1581 - accuracy: 0.9425 - val_loss: 0.1707 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9425 - val_loss: 0.1705 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1577 - accuracy: 0.9425 - val_loss: 0.1704 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1576 - accuracy: 0.9425 - val_loss: 0.1702 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1574 - accuracy: 0.9425 - val_loss: 0.1700 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1572 - accuracy: 0.9425 - val_loss: 0.1699 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1571 - accuracy: 0.9425 - val_loss: 0.1697 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1569 - accuracy: 0.9425 - val_loss: 0.1696 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1568 - accuracy: 0.9425 - val_loss: 0.1694 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1566 - accuracy: 0.9425 - val_loss: 0.1693 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1564 - accuracy: 0.9457 - val_loss: 0.1692 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1563 - accuracy: 0.9457 - val_loss: 0.1690 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9457 - val_loss: 0.1689 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1560 - accuracy: 0.9457 - val_loss: 0.1687 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1558 - accuracy: 0.9457 - val_loss: 0.1686 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1557 - accuracy: 0.9457 - val_loss: 0.1684 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9457 - val_loss: 0.1683 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1554 - accuracy: 0.9457 - val_loss: 0.1682 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1552 - accuracy: 0.9457 - val_loss: 0.1680 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1550 - accuracy: 0.9457 - val_loss: 0.1679 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1549 - accuracy: 0.9489 - val_loss: 0.1677 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1547 - accuracy: 0.9489 - val_loss: 0.1676 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1546 - accuracy: 0.9489 - val_loss: 0.1674 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1544 - accuracy: 0.9489 - val_loss: 0.1673 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1543 - accuracy: 0.9489 - val_loss: 0.1671 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9489 - val_loss: 0.1670 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1540 - accuracy: 0.9489 - val_loss: 0.1669 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1538 - accuracy: 0.9489 - val_loss: 0.1667 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9489 - val_loss: 0.1666 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1536 - accuracy: 0.9489 - val_loss: 0.1664 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 416/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1534 - accuracy: 0.9489 - val_loss: 0.1663 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 417/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1532 - accuracy: 0.9489 - val_loss: 0.1662 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 418/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1531 - accuracy: 0.9489 - val_loss: 0.1660 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1529 - accuracy: 0.9521 - val_loss: 0.1659 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 420/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1528 - accuracy: 0.9521 - val_loss: 0.1657 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1527 - accuracy: 0.9521 - val_loss: 0.1656 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1525 - accuracy: 0.9521 - val_loss: 0.1655 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1524 - accuracy: 0.9521 - val_loss: 0.1653 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9521 - val_loss: 0.1652 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 425/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9521 - val_loss: 0.1651 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9521 - val_loss: 0.1649 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1518 - accuracy: 0.9521 - val_loss: 0.1648 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9521 - val_loss: 0.1647 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1515 - accuracy: 0.9521 - val_loss: 0.1645 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9521 - val_loss: 0.1644 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.9521 - val_loss: 0.1643 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1511 - accuracy: 0.9521 - val_loss: 0.1642 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1510 - accuracy: 0.9521 - val_loss: 0.1640 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1508 - accuracy: 0.9521 - val_loss: 0.1639 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1507 - accuracy: 0.9521 - val_loss: 0.1638 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1505 - accuracy: 0.9521 - val_loss: 0.1636 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9553 - val_loss: 0.1635 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1503 - accuracy: 0.9553 - val_loss: 0.1634 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1501 - accuracy: 0.9553 - val_loss: 0.1633 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1500 - accuracy: 0.9553 - val_loss: 0.1631 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1499 - accuracy: 0.9553 - val_loss: 0.1630 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1497 - accuracy: 0.9553 - val_loss: 0.1629 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1496 - accuracy: 0.9553 - val_loss: 0.1627 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1495 - accuracy: 0.9553 - val_loss: 0.1626 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1493 - accuracy: 0.9553 - val_loss: 0.1625 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 446/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1492 - accuracy: 0.9553 - val_loss: 0.1624 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 447/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1491 - accuracy: 0.9553 - val_loss: 0.1622 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 448/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9553 - val_loss: 0.1621 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 449/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1488 - accuracy: 0.9553 - val_loss: 0.1620 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 450/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.9553 - val_loss: 0.1619 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9553 - val_loss: 0.1618 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 452/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1484 - accuracy: 0.9553 - val_loss: 0.1616 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 453/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9553 - val_loss: 0.1615 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 454/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1481 - accuracy: 0.9553 - val_loss: 0.1614 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1480 - accuracy: 0.9553 - val_loss: 0.1612 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 456/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1479 - accuracy: 0.9553 - val_loss: 0.1611 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 457/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1477 - accuracy: 0.9553 - val_loss: 0.1610 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1476 - accuracy: 0.9553 - val_loss: 0.1609 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1475 - accuracy: 0.9553 - val_loss: 0.1607 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 460/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1473 - accuracy: 0.9553 - val_loss: 0.1606 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 461/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1472 - accuracy: 0.9553 - val_loss: 0.1605 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1471 - accuracy: 0.9553 - val_loss: 0.1604 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1470 - accuracy: 0.9553 - val_loss: 0.1603 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1468 - accuracy: 0.9553 - val_loss: 0.1602 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 465/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1467 - accuracy: 0.9553 - val_loss: 0.1600 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "40/40 [==============================] - 1s 14ms/step - loss: 0.1466 - accuracy: 0.9553 - val_loss: 0.1599 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 467/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1465 - accuracy: 0.9553 - val_loss: 0.1598 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1463 - accuracy: 0.9553 - val_loss: 0.1597 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 469/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1462 - accuracy: 0.9553 - val_loss: 0.1596 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9553 - val_loss: 0.1595 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1459 - accuracy: 0.9553 - val_loss: 0.1593 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 472/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9553 - val_loss: 0.1592 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1457 - accuracy: 0.9553 - val_loss: 0.1591 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1455 - accuracy: 0.9553 - val_loss: 0.1590 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1454 - accuracy: 0.9553 - val_loss: 0.1589 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 476/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1453 - accuracy: 0.9553 - val_loss: 0.1588 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 477/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1452 - accuracy: 0.9553 - val_loss: 0.1587 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1451 - accuracy: 0.9553 - val_loss: 0.1585 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1449 - accuracy: 0.9553 - val_loss: 0.1584 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 480/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1448 - accuracy: 0.9553 - val_loss: 0.1583 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 481/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9553 - val_loss: 0.1582 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 482/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1446 - accuracy: 0.9553 - val_loss: 0.1581 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 483/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1445 - accuracy: 0.9553 - val_loss: 0.1581 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9553 - val_loss: 0.1579 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9553 - val_loss: 0.1578 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 486/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1441 - accuracy: 0.9553 - val_loss: 0.1577 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1440 - accuracy: 0.9553 - val_loss: 0.1576 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1439 - accuracy: 0.9553 - val_loss: 0.1575 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 489/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1438 - accuracy: 0.9553 - val_loss: 0.1574 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 490/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9553 - val_loss: 0.1573 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9553 - val_loss: 0.1572 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1434 - accuracy: 0.9553 - val_loss: 0.1571 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 493/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1433 - accuracy: 0.9553 - val_loss: 0.1570 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 494/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1432 - accuracy: 0.9553 - val_loss: 0.1569 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1431 - accuracy: 0.9553 - val_loss: 0.1568 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 496/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1429 - accuracy: 0.9553 - val_loss: 0.1566 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9553 - val_loss: 0.1565 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1427 - accuracy: 0.9553 - val_loss: 0.1564 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1426 - accuracy: 0.9553 - val_loss: 0.1563 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1425 - accuracy: 0.9553 - val_loss: 0.1562 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 501/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1423 - accuracy: 0.9553 - val_loss: 0.1561 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 502/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1422 - accuracy: 0.9553 - val_loss: 0.1560 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 503/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1421 - accuracy: 0.9553 - val_loss: 0.1559 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 504/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1420 - accuracy: 0.9553 - val_loss: 0.1558 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1419 - accuracy: 0.9553 - val_loss: 0.1557 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 506/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1418 - accuracy: 0.9553 - val_loss: 0.1556 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1417 - accuracy: 0.9553 - val_loss: 0.1555 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 508/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1416 - accuracy: 0.9553 - val_loss: 0.1554 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 509/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1414 - accuracy: 0.9553 - val_loss: 0.1553 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 510/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1413 - accuracy: 0.9553 - val_loss: 0.1552 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1412 - accuracy: 0.9553 - val_loss: 0.1551 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 512/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1411 - accuracy: 0.9553 - val_loss: 0.1550 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 513/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1410 - accuracy: 0.9553 - val_loss: 0.1549 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 514/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.9553 - val_loss: 0.1548 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1408 - accuracy: 0.9553 - val_loss: 0.1547 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1407 - accuracy: 0.9553 - val_loss: 0.1545 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 517/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1405 - accuracy: 0.9553 - val_loss: 0.1545 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 518/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.9553 - val_loss: 0.1543 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1403 - accuracy: 0.9553 - val_loss: 0.1542 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9553 - val_loss: 0.1541 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1401 - accuracy: 0.9553 - val_loss: 0.1540 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1400 - accuracy: 0.9553 - val_loss: 0.1539 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9553 - val_loss: 0.1538 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1398 - accuracy: 0.9553 - val_loss: 0.1537 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 525/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1397 - accuracy: 0.9553 - val_loss: 0.1536 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1396 - accuracy: 0.9553 - val_loss: 0.1535 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1395 - accuracy: 0.9553 - val_loss: 0.1534 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 528/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1393 - accuracy: 0.9553 - val_loss: 0.1533 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 529/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1392 - accuracy: 0.9553 - val_loss: 0.1532 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 530/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1391 - accuracy: 0.9553 - val_loss: 0.1531 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1390 - accuracy: 0.9553 - val_loss: 0.1530 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 532/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1389 - accuracy: 0.9553 - val_loss: 0.1529 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9553 - val_loss: 0.1528 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 534/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1387 - accuracy: 0.9553 - val_loss: 0.1527 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 535/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1386 - accuracy: 0.9553 - val_loss: 0.1526 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 536/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1385 - accuracy: 0.9553 - val_loss: 0.1525 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9553 - val_loss: 0.1524 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9553 - val_loss: 0.1523 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1382 - accuracy: 0.9553 - val_loss: 0.1522 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1381 - accuracy: 0.9553 - val_loss: 0.1521 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1380 - accuracy: 0.9553 - val_loss: 0.1520 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1379 - accuracy: 0.9553 - val_loss: 0.1519 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 543/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1378 - accuracy: 0.9553 - val_loss: 0.1518 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1377 - accuracy: 0.9553 - val_loss: 0.1517 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9553 - val_loss: 0.1515 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1374 - accuracy: 0.9553 - val_loss: 0.1514 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1373 - accuracy: 0.9553 - val_loss: 0.1513 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1372 - accuracy: 0.9553 - val_loss: 0.1513 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9553 - val_loss: 0.1511 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 550/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1370 - accuracy: 0.9553 - val_loss: 0.1511 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 551/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9553 - val_loss: 0.1510 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 552/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1368 - accuracy: 0.9553 - val_loss: 0.1509 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1367 - accuracy: 0.9553 - val_loss: 0.1508 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 554/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1366 - accuracy: 0.9553 - val_loss: 0.1507 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 555/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1365 - accuracy: 0.9553 - val_loss: 0.1506 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1364 - accuracy: 0.9553 - val_loss: 0.1505 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1363 - accuracy: 0.9553 - val_loss: 0.1504 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1362 - accuracy: 0.9553 - val_loss: 0.1503 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 559/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1361 - accuracy: 0.9553 - val_loss: 0.1503 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 560/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1360 - accuracy: 0.9553 - val_loss: 0.1501 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1359 - accuracy: 0.9553 - val_loss: 0.1500 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.9553 - val_loss: 0.1500 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 563/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1357 - accuracy: 0.9553 - val_loss: 0.1499 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 564/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1356 - accuracy: 0.9553 - val_loss: 0.1498 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1355 - accuracy: 0.9553 - val_loss: 0.1497 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1354 - accuracy: 0.9553 - val_loss: 0.1496 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 567/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1353 - accuracy: 0.9553 - val_loss: 0.1495 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 568/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1352 - accuracy: 0.9553 - val_loss: 0.1494 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1352 - accuracy: 0.9553 - val_loss: 0.1494 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1351 - accuracy: 0.9553 - val_loss: 0.1493 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1350 - accuracy: 0.9553 - val_loss: 0.1492 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1349 - accuracy: 0.9553 - val_loss: 0.1491 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1348 - accuracy: 0.9553 - val_loss: 0.1490 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 574/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1347 - accuracy: 0.9553 - val_loss: 0.1489 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1346 - accuracy: 0.9553 - val_loss: 0.1488 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1345 - accuracy: 0.9553 - val_loss: 0.1488 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1344 - accuracy: 0.9553 - val_loss: 0.1487 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1343 - accuracy: 0.9553 - val_loss: 0.1486 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 579/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1342 - accuracy: 0.9553 - val_loss: 0.1485 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9553 - val_loss: 0.1484 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "40/40 [==============================] - 1s 13ms/step - loss: 0.1340 - accuracy: 0.9553 - val_loss: 0.1483 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 582/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1339 - accuracy: 0.9553 - val_loss: 0.1482 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1338 - accuracy: 0.9553 - val_loss: 0.1481 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 584/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1337 - accuracy: 0.9553 - val_loss: 0.1481 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1336 - accuracy: 0.9553 - val_loss: 0.1480 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9553 - val_loss: 0.1479 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 587/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1334 - accuracy: 0.9553 - val_loss: 0.1478 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1333 - accuracy: 0.9553 - val_loss: 0.1477 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1332 - accuracy: 0.9553 - val_loss: 0.1476 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "40/40 [==============================] - 1s 18ms/step - loss: 0.1331 - accuracy: 0.9553 - val_loss: 0.1476 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 591/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1330 - accuracy: 0.9553 - val_loss: 0.1475 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1329 - accuracy: 0.9553 - val_loss: 0.1474 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1329 - accuracy: 0.9553 - val_loss: 0.1473 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1328 - accuracy: 0.9553 - val_loss: 0.1472 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 595/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1327 - accuracy: 0.9553 - val_loss: 0.1472 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1326 - accuracy: 0.9553 - val_loss: 0.1471 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1325 - accuracy: 0.9553 - val_loss: 0.1470 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 598/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1324 - accuracy: 0.9553 - val_loss: 0.1469 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 599/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1323 - accuracy: 0.9553 - val_loss: 0.1469 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 600/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1322 - accuracy: 0.9553 - val_loss: 0.1468 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 601/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1321 - accuracy: 0.9553 - val_loss: 0.1467 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9553 - val_loss: 0.1466 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1319 - accuracy: 0.9553 - val_loss: 0.1465 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1318 - accuracy: 0.9553 - val_loss: 0.1464 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1317 - accuracy: 0.9553 - val_loss: 0.1464 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1317 - accuracy: 0.9553 - val_loss: 0.1463 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1316 - accuracy: 0.9553 - val_loss: 0.1462 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 608/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1315 - accuracy: 0.9553 - val_loss: 0.1461 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 609/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1314 - accuracy: 0.9553 - val_loss: 0.1461 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1313 - accuracy: 0.9553 - val_loss: 0.1460 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1312 - accuracy: 0.9553 - val_loss: 0.1459 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1311 - accuracy: 0.9553 - val_loss: 0.1458 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1310 - accuracy: 0.9553 - val_loss: 0.1458 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1309 - accuracy: 0.9585 - val_loss: 0.1457 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 615/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1309 - accuracy: 0.9585 - val_loss: 0.1456 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9585 - val_loss: 0.1455 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 617/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1307 - accuracy: 0.9585 - val_loss: 0.1455 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 618/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1306 - accuracy: 0.9585 - val_loss: 0.1454 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 619/1000\n",
      "40/40 [==============================] - 1s 14ms/step - loss: 0.1305 - accuracy: 0.9585 - val_loss: 0.1453 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1304 - accuracy: 0.9585 - val_loss: 0.1452 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 621/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1303 - accuracy: 0.9585 - val_loss: 0.1452 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1302 - accuracy: 0.9585 - val_loss: 0.1451 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.1302 - accuracy: 0.9585 - val_loss: 0.1450 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1301 - accuracy: 0.9585 - val_loss: 0.1449 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1300 - accuracy: 0.9585 - val_loss: 0.1449 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 626/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1299 - accuracy: 0.9585 - val_loss: 0.1448 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 627/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9585 - val_loss: 0.1447 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 628/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1297 - accuracy: 0.9585 - val_loss: 0.1446 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 629/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9585 - val_loss: 0.1446 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1296 - accuracy: 0.9585 - val_loss: 0.1445 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 631/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9585 - val_loss: 0.1444 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1294 - accuracy: 0.9585 - val_loss: 0.1443 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 633/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1293 - accuracy: 0.9585 - val_loss: 0.1443 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 634/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1292 - accuracy: 0.9585 - val_loss: 0.1442 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1291 - accuracy: 0.9585 - val_loss: 0.1442 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9585 - val_loss: 0.1441 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1290 - accuracy: 0.9585 - val_loss: 0.1440 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 638/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9585 - val_loss: 0.1439 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 639/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1288 - accuracy: 0.9585 - val_loss: 0.1439 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 640/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1287 - accuracy: 0.9585 - val_loss: 0.1438 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 641/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1286 - accuracy: 0.9585 - val_loss: 0.1437 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 642/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.9585 - val_loss: 0.1436 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 643/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9585 - val_loss: 0.1436 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1284 - accuracy: 0.9585 - val_loss: 0.1435 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 645/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1283 - accuracy: 0.9585 - val_loss: 0.1434 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1282 - accuracy: 0.9585 - val_loss: 0.1434 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1281 - accuracy: 0.9585 - val_loss: 0.1433 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 648/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1280 - accuracy: 0.9585 - val_loss: 0.1432 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9585 - val_loss: 0.1431 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1279 - accuracy: 0.9585 - val_loss: 0.1431 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1278 - accuracy: 0.9585 - val_loss: 0.1430 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 652/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1277 - accuracy: 0.9585 - val_loss: 0.1429 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1276 - accuracy: 0.9585 - val_loss: 0.1429 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1275 - accuracy: 0.9585 - val_loss: 0.1428 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 655/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1275 - accuracy: 0.9585 - val_loss: 0.1428 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1274 - accuracy: 0.9585 - val_loss: 0.1427 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 657/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1273 - accuracy: 0.9585 - val_loss: 0.1426 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 658/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1272 - accuracy: 0.9585 - val_loss: 0.1426 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 659/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1271 - accuracy: 0.9585 - val_loss: 0.1425 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 660/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1270 - accuracy: 0.9585 - val_loss: 0.1424 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 661/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1270 - accuracy: 0.9585 - val_loss: 0.1424 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1269 - accuracy: 0.9585 - val_loss: 0.1424 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1268 - accuracy: 0.9585 - val_loss: 0.1423 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9585 - val_loss: 0.1422 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 665/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1267 - accuracy: 0.9585 - val_loss: 0.1421 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 666/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9585 - val_loss: 0.1421 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1265 - accuracy: 0.9585 - val_loss: 0.1420 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 668/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1264 - accuracy: 0.9585 - val_loss: 0.1419 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1263 - accuracy: 0.9585 - val_loss: 0.1419 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1262 - accuracy: 0.9585 - val_loss: 0.1418 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1262 - accuracy: 0.9585 - val_loss: 0.1417 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1261 - accuracy: 0.9585 - val_loss: 0.1417 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9585 - val_loss: 0.1416 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1259 - accuracy: 0.9585 - val_loss: 0.1415 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 675/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1258 - accuracy: 0.9585 - val_loss: 0.1415 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 676/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1258 - accuracy: 0.9585 - val_loss: 0.1414 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 677/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1257 - accuracy: 0.9585 - val_loss: 0.1413 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1256 - accuracy: 0.9585 - val_loss: 0.1413 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 679/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1255 - accuracy: 0.9585 - val_loss: 0.1412 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1254 - accuracy: 0.9585 - val_loss: 0.1411 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1253 - accuracy: 0.9585 - val_loss: 0.1411 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1253 - accuracy: 0.9585 - val_loss: 0.1410 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 683/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1252 - accuracy: 0.9585 - val_loss: 0.1410 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 684/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1251 - accuracy: 0.9585 - val_loss: 0.1410 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 0.9585 - val_loss: 0.1409 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1250 - accuracy: 0.9585 - val_loss: 0.1408 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 687/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1249 - accuracy: 0.9585 - val_loss: 0.1408 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1248 - accuracy: 0.9585 - val_loss: 0.1407 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1247 - accuracy: 0.9585 - val_loss: 0.1406 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1247 - accuracy: 0.9585 - val_loss: 0.1406 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 691/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.1246 - accuracy: 0.9585 - val_loss: 0.1406 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 692/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9585 - val_loss: 0.1405 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1244 - accuracy: 0.9585 - val_loss: 0.1404 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 694/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9585 - val_loss: 0.1404 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1243 - accuracy: 0.9585 - val_loss: 0.1403 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1242 - accuracy: 0.9585 - val_loss: 0.1402 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1241 - accuracy: 0.9585 - val_loss: 0.1402 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9585 - val_loss: 0.1401 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9617 - val_loss: 0.1400 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1239 - accuracy: 0.9617 - val_loss: 0.1400 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 701/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1238 - accuracy: 0.9617 - val_loss: 0.1399 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1237 - accuracy: 0.9617 - val_loss: 0.1398 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 703/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1237 - accuracy: 0.9617 - val_loss: 0.1398 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1236 - accuracy: 0.9617 - val_loss: 0.1397 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 705/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9617 - val_loss: 0.1397 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1234 - accuracy: 0.9617 - val_loss: 0.1396 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1234 - accuracy: 0.9617 - val_loss: 0.1395 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1233 - accuracy: 0.9617 - val_loss: 0.1395 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 709/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1232 - accuracy: 0.9617 - val_loss: 0.1394 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 710/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1231 - accuracy: 0.9617 - val_loss: 0.1393 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1231 - accuracy: 0.9617 - val_loss: 0.1393 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 712/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9617 - val_loss: 0.1392 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1229 - accuracy: 0.9617 - val_loss: 0.1392 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 714/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1228 - accuracy: 0.9617 - val_loss: 0.1391 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1228 - accuracy: 0.9617 - val_loss: 0.1390 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 716/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1227 - accuracy: 0.9617 - val_loss: 0.1390 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 717/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9617 - val_loss: 0.1389 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 718/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9617 - val_loss: 0.1389 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 719/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1225 - accuracy: 0.9617 - val_loss: 0.1388 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1224 - accuracy: 0.9617 - val_loss: 0.1387 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1223 - accuracy: 0.9617 - val_loss: 0.1387 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1222 - accuracy: 0.9617 - val_loss: 0.1387 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1222 - accuracy: 0.9617 - val_loss: 0.1386 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9617 - val_loss: 0.1386 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 725/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1220 - accuracy: 0.9617 - val_loss: 0.1385 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1219 - accuracy: 0.9617 - val_loss: 0.1384 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 727/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1219 - accuracy: 0.9617 - val_loss: 0.1384 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 728/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1218 - accuracy: 0.9617 - val_loss: 0.1384 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 729/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1217 - accuracy: 0.9617 - val_loss: 0.1383 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.9617 - val_loss: 0.1383 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 731/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1216 - accuracy: 0.9617 - val_loss: 0.1382 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 732/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9617 - val_loss: 0.1382 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 733/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9617 - val_loss: 0.1381 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 734/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1213 - accuracy: 0.9617 - val_loss: 0.1380 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 735/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1213 - accuracy: 0.9617 - val_loss: 0.1380 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 736/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1212 - accuracy: 0.9617 - val_loss: 0.1379 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 737/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1211 - accuracy: 0.9617 - val_loss: 0.1379 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9617 - val_loss: 0.1378 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1210 - accuracy: 0.9617 - val_loss: 0.1378 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1209 - accuracy: 0.9617 - val_loss: 0.1377 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 741/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9617 - val_loss: 0.1376 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 742/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1208 - accuracy: 0.9617 - val_loss: 0.1376 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 743/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9617 - val_loss: 0.1375 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 744/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1206 - accuracy: 0.9617 - val_loss: 0.1374 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 745/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1206 - accuracy: 0.9617 - val_loss: 0.1374 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 746/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1205 - accuracy: 0.9617 - val_loss: 0.1373 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 747/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1204 - accuracy: 0.9617 - val_loss: 0.1372 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 748/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1203 - accuracy: 0.9617 - val_loss: 0.1372 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 749/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1203 - accuracy: 0.9617 - val_loss: 0.1371 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1202 - accuracy: 0.9617 - val_loss: 0.1371 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1201 - accuracy: 0.9617 - val_loss: 0.1370 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.9617 - val_loss: 0.1369 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1200 - accuracy: 0.9617 - val_loss: 0.1369 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 754/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.9617 - val_loss: 0.1368 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 755/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1199 - accuracy: 0.9617 - val_loss: 0.1368 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 756/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1198 - accuracy: 0.9617 - val_loss: 0.1367 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 757/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1197 - accuracy: 0.9617 - val_loss: 0.1366 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 758/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1196 - accuracy: 0.9617 - val_loss: 0.1366 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1196 - accuracy: 0.9617 - val_loss: 0.1365 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 760/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9617 - val_loss: 0.1365 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1194 - accuracy: 0.9617 - val_loss: 0.1364 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1194 - accuracy: 0.9617 - val_loss: 0.1364 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1193 - accuracy: 0.9617 - val_loss: 0.1363 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1192 - accuracy: 0.9617 - val_loss: 0.1363 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 765/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9617 - val_loss: 0.1362 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1191 - accuracy: 0.9617 - val_loss: 0.1361 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1190 - accuracy: 0.9617 - val_loss: 0.1361 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 768/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1190 - accuracy: 0.9617 - val_loss: 0.1360 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1189 - accuracy: 0.9617 - val_loss: 0.1360 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1188 - accuracy: 0.9617 - val_loss: 0.1359 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1188 - accuracy: 0.9617 - val_loss: 0.1359 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 772/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9617 - val_loss: 0.1358 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 773/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1186 - accuracy: 0.9617 - val_loss: 0.1358 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1185 - accuracy: 0.9617 - val_loss: 0.1357 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 775/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1185 - accuracy: 0.9617 - val_loss: 0.1356 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1184 - accuracy: 0.9617 - val_loss: 0.1356 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1183 - accuracy: 0.9617 - val_loss: 0.1355 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 778/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1183 - accuracy: 0.9617 - val_loss: 0.1355 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 779/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1182 - accuracy: 0.9617 - val_loss: 0.1354 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 780/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9617 - val_loss: 0.1354 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 781/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1181 - accuracy: 0.9617 - val_loss: 0.1353 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1180 - accuracy: 0.9617 - val_loss: 0.1353 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9617 - val_loss: 0.1353 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9617 - val_loss: 0.1352 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 785/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1178 - accuracy: 0.9617 - val_loss: 0.1352 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 786/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.9617 - val_loss: 0.1351 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 787/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1176 - accuracy: 0.9617 - val_loss: 0.1350 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 788/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1176 - accuracy: 0.9617 - val_loss: 0.1350 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 789/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1175 - accuracy: 0.9617 - val_loss: 0.1349 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 790/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1174 - accuracy: 0.9617 - val_loss: 0.1349 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 791/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1174 - accuracy: 0.9617 - val_loss: 0.1349 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 792/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1173 - accuracy: 0.9617 - val_loss: 0.1348 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1172 - accuracy: 0.9617 - val_loss: 0.1348 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 794/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1172 - accuracy: 0.9617 - val_loss: 0.1347 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 795/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1171 - accuracy: 0.9617 - val_loss: 0.1347 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1170 - accuracy: 0.9617 - val_loss: 0.1346 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 797/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1169 - accuracy: 0.9617 - val_loss: 0.1346 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1169 - accuracy: 0.9617 - val_loss: 0.1345 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 799/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9617 - val_loss: 0.1344 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 800/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1168 - accuracy: 0.9617 - val_loss: 0.1344 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1167 - accuracy: 0.9617 - val_loss: 0.1343 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1166 - accuracy: 0.9617 - val_loss: 0.1343 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 803/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1166 - accuracy: 0.9617 - val_loss: 0.1342 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1165 - accuracy: 0.9617 - val_loss: 0.1342 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1164 - accuracy: 0.9617 - val_loss: 0.1341 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 806/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1164 - accuracy: 0.9617 - val_loss: 0.1341 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1163 - accuracy: 0.9617 - val_loss: 0.1340 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1162 - accuracy: 0.9617 - val_loss: 0.1340 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9617 - val_loss: 0.1339 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 810/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1161 - accuracy: 0.9617 - val_loss: 0.1339 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1160 - accuracy: 0.9617 - val_loss: 0.1338 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 812/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1160 - accuracy: 0.9617 - val_loss: 0.1338 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 813/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.9617 - val_loss: 0.1337 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 814/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9617 - val_loss: 0.1337 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 815/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1158 - accuracy: 0.9617 - val_loss: 0.1336 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 816/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1157 - accuracy: 0.9617 - val_loss: 0.1336 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 817/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1157 - accuracy: 0.9617 - val_loss: 0.1335 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1156 - accuracy: 0.9617 - val_loss: 0.1335 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 819/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1155 - accuracy: 0.9617 - val_loss: 0.1334 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 820/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9617 - val_loss: 0.1334 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 821/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1154 - accuracy: 0.9617 - val_loss: 0.1333 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1153 - accuracy: 0.9617 - val_loss: 0.1333 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 823/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1153 - accuracy: 0.9617 - val_loss: 0.1332 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 824/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1152 - accuracy: 0.9617 - val_loss: 0.1331 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 825/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1151 - accuracy: 0.9617 - val_loss: 0.1331 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 826/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1151 - accuracy: 0.9617 - val_loss: 0.1330 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 827/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1150 - accuracy: 0.9617 - val_loss: 0.1330 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 828/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1149 - accuracy: 0.9617 - val_loss: 0.1329 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 829/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1149 - accuracy: 0.9617 - val_loss: 0.1328 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9617 - val_loss: 0.1328 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1147 - accuracy: 0.9617 - val_loss: 0.1327 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1147 - accuracy: 0.9617 - val_loss: 0.1327 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 833/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1146 - accuracy: 0.9617 - val_loss: 0.1326 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 834/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1145 - accuracy: 0.9617 - val_loss: 0.1326 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 835/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1145 - accuracy: 0.9617 - val_loss: 0.1325 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 836/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1144 - accuracy: 0.9617 - val_loss: 0.1325 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1144 - accuracy: 0.9617 - val_loss: 0.1324 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1143 - accuracy: 0.9617 - val_loss: 0.1324 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 839/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1142 - accuracy: 0.9617 - val_loss: 0.1323 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 840/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1142 - accuracy: 0.9617 - val_loss: 0.1323 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1141 - accuracy: 0.9617 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1140 - accuracy: 0.9617 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1140 - accuracy: 0.9617 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 844/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1139 - accuracy: 0.9617 - val_loss: 0.1322 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1139 - accuracy: 0.9617 - val_loss: 0.1321 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1138 - accuracy: 0.9617 - val_loss: 0.1321 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1137 - accuracy: 0.9617 - val_loss: 0.1320 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 848/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1137 - accuracy: 0.9617 - val_loss: 0.1320 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 849/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9617 - val_loss: 0.1319 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 850/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1136 - accuracy: 0.9617 - val_loss: 0.1319 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 851/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1135 - accuracy: 0.9617 - val_loss: 0.1318 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1134 - accuracy: 0.9617 - val_loss: 0.1318 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1134 - accuracy: 0.9617 - val_loss: 0.1317 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 854/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1133 - accuracy: 0.9617 - val_loss: 0.1317 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1132 - accuracy: 0.9617 - val_loss: 0.1316 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1132 - accuracy: 0.9617 - val_loss: 0.1316 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 857/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1131 - accuracy: 0.9617 - val_loss: 0.1316 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9617 - val_loss: 0.1315 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 859/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9617 - val_loss: 0.1315 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1129 - accuracy: 0.9617 - val_loss: 0.1314 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 861/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1128 - accuracy: 0.9617 - val_loss: 0.1314 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 862/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1128 - accuracy: 0.9617 - val_loss: 0.1313 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1127 - accuracy: 0.9617 - val_loss: 0.1313 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 864/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1127 - accuracy: 0.9617 - val_loss: 0.1312 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 865/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9617 - val_loss: 0.1312 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1126 - accuracy: 0.9617 - val_loss: 0.1311 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1125 - accuracy: 0.9617 - val_loss: 0.1311 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1124 - accuracy: 0.9617 - val_loss: 0.1310 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 869/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1124 - accuracy: 0.9617 - val_loss: 0.1310 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 870/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1123 - accuracy: 0.9617 - val_loss: 0.1310 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1122 - accuracy: 0.9617 - val_loss: 0.1309 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 872/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1122 - accuracy: 0.9617 - val_loss: 0.1309 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 873/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1121 - accuracy: 0.9617 - val_loss: 0.1308 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1121 - accuracy: 0.9617 - val_loss: 0.1308 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1120 - accuracy: 0.9617 - val_loss: 0.1307 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 876/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1119 - accuracy: 0.9617 - val_loss: 0.1307 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 877/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1119 - accuracy: 0.9617 - val_loss: 0.1306 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1118 - accuracy: 0.9617 - val_loss: 0.1306 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 879/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9617 - val_loss: 0.1305 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1117 - accuracy: 0.9617 - val_loss: 0.1305 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 881/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9617 - val_loss: 0.1304 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1116 - accuracy: 0.9617 - val_loss: 0.1304 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1115 - accuracy: 0.9617 - val_loss: 0.1304 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9617 - val_loss: 0.1303 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1114 - accuracy: 0.9617 - val_loss: 0.1303 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1113 - accuracy: 0.9617 - val_loss: 0.1302 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9617 - val_loss: 0.1302 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 888/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1112 - accuracy: 0.9617 - val_loss: 0.1301 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 889/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9617 - val_loss: 0.1301 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 890/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9617 - val_loss: 0.1300 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1110 - accuracy: 0.9617 - val_loss: 0.1300 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 892/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9617 - val_loss: 0.1299 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1109 - accuracy: 0.9617 - val_loss: 0.1299 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 894/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1109 - accuracy: 0.9617 - val_loss: 0.1299 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1108 - accuracy: 0.9617 - val_loss: 0.1298 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1108 - accuracy: 0.9617 - val_loss: 0.1298 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 897/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1107 - accuracy: 0.9617 - val_loss: 0.1297 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 0.9617 - val_loss: 0.1297 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 899/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1106 - accuracy: 0.9617 - val_loss: 0.1297 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9617 - val_loss: 0.1296 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9617 - val_loss: 0.1296 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 902/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9617 - val_loss: 0.1295 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 903/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1103 - accuracy: 0.9617 - val_loss: 0.1295 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 904/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1103 - accuracy: 0.9617 - val_loss: 0.1294 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 905/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1102 - accuracy: 0.9617 - val_loss: 0.1294 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9617 - val_loss: 0.1293 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 907/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1101 - accuracy: 0.9617 - val_loss: 0.1293 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 908/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1100 - accuracy: 0.9617 - val_loss: 0.1292 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 909/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1100 - accuracy: 0.9617 - val_loss: 0.1292 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 910/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1099 - accuracy: 0.9617 - val_loss: 0.1291 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1098 - accuracy: 0.9617 - val_loss: 0.1291 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1098 - accuracy: 0.9617 - val_loss: 0.1291 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9617 - val_loss: 0.1290 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1097 - accuracy: 0.9617 - val_loss: 0.1290 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1096 - accuracy: 0.9617 - val_loss: 0.1289 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1096 - accuracy: 0.9617 - val_loss: 0.1289 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1095 - accuracy: 0.9617 - val_loss: 0.1288 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 918/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.1288 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.1288 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 920/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9617 - val_loss: 0.1287 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 921/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1093 - accuracy: 0.9617 - val_loss: 0.1287 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9617 - val_loss: 0.1287 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 923/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1092 - accuracy: 0.9617 - val_loss: 0.1286 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 924/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1091 - accuracy: 0.9617 - val_loss: 0.1286 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1090 - accuracy: 0.9617 - val_loss: 0.1285 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1090 - accuracy: 0.9617 - val_loss: 0.1285 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 927/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1089 - accuracy: 0.9617 - val_loss: 0.1284 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1089 - accuracy: 0.9617 - val_loss: 0.1284 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9617 - val_loss: 0.1284 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 930/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1088 - accuracy: 0.9617 - val_loss: 0.1283 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 931/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1087 - accuracy: 0.9617 - val_loss: 0.1283 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1087 - accuracy: 0.9617 - val_loss: 0.1282 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 933/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1086 - accuracy: 0.9617 - val_loss: 0.1282 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 934/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9617 - val_loss: 0.1282 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1085 - accuracy: 0.9617 - val_loss: 0.1281 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 936/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1084 - accuracy: 0.9617 - val_loss: 0.1281 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 937/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.1084 - accuracy: 0.9617 - val_loss: 0.1280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1083 - accuracy: 0.9617 - val_loss: 0.1280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1083 - accuracy: 0.9617 - val_loss: 0.1280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1082 - accuracy: 0.9617 - val_loss: 0.1279 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 941/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1082 - accuracy: 0.9617 - val_loss: 0.1279 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 942/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9617 - val_loss: 0.1279 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 943/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1080 - accuracy: 0.9617 - val_loss: 0.1278 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 944/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9617 - val_loss: 0.1278 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 945/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1079 - accuracy: 0.9617 - val_loss: 0.1277 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 946/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1079 - accuracy: 0.9617 - val_loss: 0.1277 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 947/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9617 - val_loss: 0.1276 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 948/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1077 - accuracy: 0.9617 - val_loss: 0.1276 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 0.9617 - val_loss: 0.1276 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1076 - accuracy: 0.9617 - val_loss: 0.1275 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9617 - val_loss: 0.1275 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 952/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9617 - val_loss: 0.1274 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 953/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1075 - accuracy: 0.9617 - val_loss: 0.1274 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 954/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1074 - accuracy: 0.9617 - val_loss: 0.1273 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 955/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.1074 - accuracy: 0.9617 - val_loss: 0.1273 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1073 - accuracy: 0.9617 - val_loss: 0.1272 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 957/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1072 - accuracy: 0.9617 - val_loss: 0.1272 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1072 - accuracy: 0.9617 - val_loss: 0.1272 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1071 - accuracy: 0.9617 - val_loss: 0.1271 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1071 - accuracy: 0.9617 - val_loss: 0.1271 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 961/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9617 - val_loss: 0.1270 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1070 - accuracy: 0.9617 - val_loss: 0.1270 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1069 - accuracy: 0.9617 - val_loss: 0.1270 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1068 - accuracy: 0.9617 - val_loss: 0.1269 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1068 - accuracy: 0.9617 - val_loss: 0.1269 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 966/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1067 - accuracy: 0.9617 - val_loss: 0.1268 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1067 - accuracy: 0.9617 - val_loss: 0.1268 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 968/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1066 - accuracy: 0.9617 - val_loss: 0.1268 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1066 - accuracy: 0.9617 - val_loss: 0.1267 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 970/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1065 - accuracy: 0.9617 - val_loss: 0.1267 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 971/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1065 - accuracy: 0.9617 - val_loss: 0.1267 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 0.9617 - val_loss: 0.1266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1064 - accuracy: 0.9617 - val_loss: 0.1266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1063 - accuracy: 0.9617 - val_loss: 0.1266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 975/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9617 - val_loss: 0.1266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9617 - val_loss: 0.1265 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 977/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1061 - accuracy: 0.9617 - val_loss: 0.1265 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 978/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1061 - accuracy: 0.9617 - val_loss: 0.1265 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 979/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1060 - accuracy: 0.9617 - val_loss: 0.1264 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9617 - val_loss: 0.1264 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1059 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 982/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1059 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 983/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1058 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1058 - accuracy: 0.9617 - val_loss: 0.1262 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 985/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 987/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1056 - accuracy: 0.9617 - val_loss: 0.1263 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1055 - accuracy: 0.9617 - val_loss: 0.1262 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.1055 - accuracy: 0.9617 - val_loss: 0.1262 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 990/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.9617 - val_loss: 0.1261 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.9617 - val_loss: 0.1261 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9617 - val_loss: 0.1261 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 993/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1053 - accuracy: 0.9617 - val_loss: 0.1260 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1052 - accuracy: 0.9617 - val_loss: 0.1260 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 995/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9617 - val_loss: 0.1260 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1051 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 997/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1051 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 998/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1050 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1050 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1049 - accuracy: 0.9617 - val_loss: 0.1259 - val_accuracy: 0.9643 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 64)                38656     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,721\n",
      "Trainable params: 38,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Definizione del modello LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(None, x_train.shape[-1]))) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Definisco l'ottimizzatore con il learning rate iniziale\n",
    "initial_learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "# Definisco il learning rate schedule con decay lineare\n",
    "decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "decay_rate = 0.1  # Tasso di decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "# Compilazione del modello\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "# optimizer \"adam\"\n",
    "model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Addestramento del modello con il learning rate modificato\n",
    "history = model.fit(x_train, y_train, epochs=1000, batch_size=8, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the LSTM model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9643\n",
      "Test Loss: 0.12585870921611786\n",
      "Test Accuracy: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "# Valutazione del modello\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation on LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing LSTM with k-fold\n",
      "Loss: 0.1783, Accuracy: 91.43%\n",
      "Loss: 0.1643, Accuracy: 94.29%\n",
      "Loss: 0.1497, Accuracy: 97.14%\n",
      "Loss: 0.0685, Accuracy: 98.57%\n",
      "Loss: 0.1639, Accuracy: 92.86%\n",
      "Loss: 0.1237, Accuracy: 94.29%\n",
      "Loss: 0.1354, Accuracy: 94.29%\n",
      "Loss: 0.1707, Accuracy: 92.86%\n",
      "LSTM finished in 1178.66 sec\n",
      "\n",
      "Average accuracy: 0.9446\n",
      "Average loss: 0.1443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "\n",
    "k = 8  # numero di fold\n",
    "kf = KFold(n_splits=k, shuffle = True)\n",
    "\n",
    "# Array per memorizzare le curve di apprendimento\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "f = pd.DataFrame(columns = perfInd)\n",
    "print('Implementing LSTM with k-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=(None, x_train.shape[-1]))) \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Definisco l'ottimizzatore con il learning rate iniziale\n",
    "    initial_learning_rate = 0.001\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # Definisco il learning rate schedule con decay lineare\n",
    "    decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "    decay_rate = 0.1  # Tasso di decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 1000, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Valuta il modello\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Stampa i risultati di accuracy e loss per la k-esima fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('LSTM finished in', t,'sec\\n')\n",
    "\n",
    " # Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_train_loss = np.mean(train_loss, axis=0)\n",
    "avg_train_acc = np.mean(train_acc, axis=0)\n",
    "avg_val_loss = np.mean(val_loss, axis=0)\n",
    "avg_val_acc = np.mean(train_acc, axis=0)\n",
    "print(np.shape(avg_train_loss))\n",
    "\n",
    "# Plot delle curve di apprendimento mediate sulle K fold\n",
    "\n",
    "epochs = range(1, len(train_loss[0]) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, avg_train_loss, label='Training loss')\n",
    "plt.plot(epochs, avg_val_loss, label='Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Avg_loss')\n",
    "plt.title('Average train and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, avg_train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, avg_val_acc, label='Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Avg_accuracy')\n",
    "plt.title('Average train and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM greco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "40/40 [==============================] - 3s 15ms/step - loss: 2.6474 - accuracy: 0.5112 - val_loss: 2.6343 - val_accuracy: 0.5238 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.6346 - accuracy: 0.5559 - val_loss: 2.6236 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.6199 - accuracy: 0.5847 - val_loss: 2.6133 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.6143 - accuracy: 0.6038 - val_loss: 2.6029 - val_accuracy: 0.6786 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.5996 - accuracy: 0.6869 - val_loss: 2.5929 - val_accuracy: 0.7381 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5898 - accuracy: 0.7125 - val_loss: 2.5827 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5827 - accuracy: 0.7348 - val_loss: 2.5727 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.5683 - accuracy: 0.7796 - val_loss: 2.5627 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5619 - accuracy: 0.8179 - val_loss: 2.5530 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5494 - accuracy: 0.8275 - val_loss: 2.5431 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.5402 - accuracy: 0.8371 - val_loss: 2.5335 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5309 - accuracy: 0.8307 - val_loss: 2.5240 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5247 - accuracy: 0.8594 - val_loss: 2.5146 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5115 - accuracy: 0.8818 - val_loss: 2.5054 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.5048 - accuracy: 0.8626 - val_loss: 2.4961 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.4933 - accuracy: 0.8850 - val_loss: 2.4869 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4855 - accuracy: 0.8722 - val_loss: 2.4779 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4745 - accuracy: 0.8754 - val_loss: 2.4691 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4609 - accuracy: 0.8914 - val_loss: 2.4603 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4616 - accuracy: 0.8786 - val_loss: 2.4514 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.4461 - accuracy: 0.9010 - val_loss: 2.4426 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.4374 - accuracy: 0.8946 - val_loss: 2.4340 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.4256 - accuracy: 0.8850 - val_loss: 2.4254 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.4235 - accuracy: 0.9073 - val_loss: 2.4170 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.4140 - accuracy: 0.8946 - val_loss: 2.4087 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.3999 - accuracy: 0.8946 - val_loss: 2.4004 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3936 - accuracy: 0.9042 - val_loss: 2.3922 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3858 - accuracy: 0.9073 - val_loss: 2.3841 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3820 - accuracy: 0.8850 - val_loss: 2.3762 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3698 - accuracy: 0.9137 - val_loss: 2.3682 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.3664 - accuracy: 0.8946 - val_loss: 2.3608 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3558 - accuracy: 0.9042 - val_loss: 2.3530 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3496 - accuracy: 0.9010 - val_loss: 2.3453 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.3398 - accuracy: 0.9010 - val_loss: 2.3376 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3323 - accuracy: 0.9105 - val_loss: 2.3300 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3244 - accuracy: 0.9105 - val_loss: 2.3227 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3141 - accuracy: 0.9010 - val_loss: 2.3153 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3129 - accuracy: 0.8978 - val_loss: 2.3080 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.3004 - accuracy: 0.9137 - val_loss: 2.3008 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2939 - accuracy: 0.9073 - val_loss: 2.2936 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2821 - accuracy: 0.9042 - val_loss: 2.2867 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2816 - accuracy: 0.9042 - val_loss: 2.2798 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.2682 - accuracy: 0.9105 - val_loss: 2.2727 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2653 - accuracy: 0.8946 - val_loss: 2.2658 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2560 - accuracy: 0.9201 - val_loss: 2.2589 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.2509 - accuracy: 0.9105 - val_loss: 2.2522 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.2459 - accuracy: 0.9169 - val_loss: 2.2456 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2395 - accuracy: 0.9105 - val_loss: 2.2390 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2332 - accuracy: 0.9105 - val_loss: 2.2325 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2242 - accuracy: 0.9137 - val_loss: 2.2264 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2178 - accuracy: 0.9073 - val_loss: 2.2200 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2133 - accuracy: 0.9105 - val_loss: 2.2137 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2132 - accuracy: 0.9105 - val_loss: 2.2073 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.2082 - accuracy: 0.9137 - val_loss: 2.2012 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.2027 - accuracy: 0.9073 - val_loss: 2.1951 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1879 - accuracy: 0.9073 - val_loss: 2.1890 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.1863 - accuracy: 0.9105 - val_loss: 2.1831 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1825 - accuracy: 0.9201 - val_loss: 2.1771 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.1648 - accuracy: 0.9137 - val_loss: 2.1712 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1640 - accuracy: 0.9137 - val_loss: 2.1653 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.1599 - accuracy: 0.9073 - val_loss: 2.1596 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.1546 - accuracy: 0.9169 - val_loss: 2.1538 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1510 - accuracy: 0.9169 - val_loss: 2.1482 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1441 - accuracy: 0.9169 - val_loss: 2.1426 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1420 - accuracy: 0.9105 - val_loss: 2.1371 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.1272 - accuracy: 0.9201 - val_loss: 2.1318 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1259 - accuracy: 0.9137 - val_loss: 2.1266 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1266 - accuracy: 0.9137 - val_loss: 2.1212 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.1167 - accuracy: 0.9233 - val_loss: 2.1158 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1102 - accuracy: 0.9169 - val_loss: 2.1106 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1121 - accuracy: 0.9105 - val_loss: 2.1054 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.1018 - accuracy: 0.9201 - val_loss: 2.1003 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0946 - accuracy: 0.9201 - val_loss: 2.0952 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.0930 - accuracy: 0.9105 - val_loss: 2.0902 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0814 - accuracy: 0.9233 - val_loss: 2.0852 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0822 - accuracy: 0.9201 - val_loss: 2.0806 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 2.0836 - accuracy: 0.9105 - val_loss: 2.0757 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.0718 - accuracy: 0.9201 - val_loss: 2.0708 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0695 - accuracy: 0.9137 - val_loss: 2.0662 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0601 - accuracy: 0.9169 - val_loss: 2.0614 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0527 - accuracy: 0.9265 - val_loss: 2.0567 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0595 - accuracy: 0.9201 - val_loss: 2.0522 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0445 - accuracy: 0.9233 - val_loss: 2.0476 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0393 - accuracy: 0.9169 - val_loss: 2.0430 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0414 - accuracy: 0.9137 - val_loss: 2.0384 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0353 - accuracy: 0.9169 - val_loss: 2.0339 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0325 - accuracy: 0.9201 - val_loss: 2.0295 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0321 - accuracy: 0.9201 - val_loss: 2.0251 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0238 - accuracy: 0.9201 - val_loss: 2.0207 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0145 - accuracy: 0.9233 - val_loss: 2.0163 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 2.0163 - accuracy: 0.9169 - val_loss: 2.0120 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 2.0118 - accuracy: 0.9265 - val_loss: 2.0077 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9997 - accuracy: 0.9169 - val_loss: 2.0037 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 2.0017 - accuracy: 0.9201 - val_loss: 1.9995 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9968 - accuracy: 0.9265 - val_loss: 1.9953 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9910 - accuracy: 0.9265 - val_loss: 1.9912 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9924 - accuracy: 0.9233 - val_loss: 1.9871 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9763 - accuracy: 0.9297 - val_loss: 1.9830 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9799 - accuracy: 0.9233 - val_loss: 1.9789 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9750 - accuracy: 0.9201 - val_loss: 1.9748 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9694 - accuracy: 0.9233 - val_loss: 1.9708 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9700 - accuracy: 0.9233 - val_loss: 1.9671 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9603 - accuracy: 0.9265 - val_loss: 1.9632 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9594 - accuracy: 0.9233 - val_loss: 1.9592 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.9524 - accuracy: 0.9265 - val_loss: 1.9553 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9494 - accuracy: 0.9297 - val_loss: 1.9515 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9456 - accuracy: 0.9297 - val_loss: 1.9476 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9485 - accuracy: 0.9201 - val_loss: 1.9438 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9430 - accuracy: 0.9201 - val_loss: 1.9400 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9473 - accuracy: 0.9265 - val_loss: 1.9363 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9325 - accuracy: 0.9297 - val_loss: 1.9326 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9293 - accuracy: 0.9233 - val_loss: 1.9289 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9230 - accuracy: 0.9297 - val_loss: 1.9252 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9314 - accuracy: 0.9169 - val_loss: 1.9215 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9209 - accuracy: 0.9265 - val_loss: 1.9179 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9163 - accuracy: 0.9233 - val_loss: 1.9143 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.9174 - accuracy: 0.9233 - val_loss: 1.9107 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.9115 - accuracy: 0.9265 - val_loss: 1.9072 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9122 - accuracy: 0.9169 - val_loss: 1.9036 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.9063 - accuracy: 0.9233 - val_loss: 1.9001 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8993 - accuracy: 0.9265 - val_loss: 1.8966 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8872 - accuracy: 0.9233 - val_loss: 1.8931 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8901 - accuracy: 0.9265 - val_loss: 1.8897 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8894 - accuracy: 0.9233 - val_loss: 1.8862 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8880 - accuracy: 0.9265 - val_loss: 1.8828 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8799 - accuracy: 0.9329 - val_loss: 1.8795 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8795 - accuracy: 0.9233 - val_loss: 1.8759 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8739 - accuracy: 0.9265 - val_loss: 1.8726 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8761 - accuracy: 0.9265 - val_loss: 1.8692 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8726 - accuracy: 0.9169 - val_loss: 1.8659 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.8596 - accuracy: 0.9297 - val_loss: 1.8626 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8665 - accuracy: 0.9297 - val_loss: 1.8593 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8620 - accuracy: 0.9201 - val_loss: 1.8561 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8511 - accuracy: 0.9233 - val_loss: 1.8528 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8562 - accuracy: 0.9233 - val_loss: 1.8495 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8463 - accuracy: 0.9265 - val_loss: 1.8462 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8410 - accuracy: 0.9329 - val_loss: 1.8431 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8400 - accuracy: 0.9297 - val_loss: 1.8398 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8348 - accuracy: 0.9265 - val_loss: 1.8365 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8308 - accuracy: 0.9297 - val_loss: 1.8333 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8279 - accuracy: 0.9329 - val_loss: 1.8302 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8286 - accuracy: 0.9297 - val_loss: 1.8270 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8288 - accuracy: 0.9297 - val_loss: 1.8239 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.8213 - accuracy: 0.9265 - val_loss: 1.8208 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8248 - accuracy: 0.9233 - val_loss: 1.8177 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8095 - accuracy: 0.9265 - val_loss: 1.8146 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8165 - accuracy: 0.9265 - val_loss: 1.8116 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8100 - accuracy: 0.9265 - val_loss: 1.8084 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8045 - accuracy: 0.9265 - val_loss: 1.8053 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7993 - accuracy: 0.9329 - val_loss: 1.8023 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.8101 - accuracy: 0.9329 - val_loss: 1.7992 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.8062 - accuracy: 0.9297 - val_loss: 1.7962 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7996 - accuracy: 0.9265 - val_loss: 1.7931 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7940 - accuracy: 0.9297 - val_loss: 1.7901 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7847 - accuracy: 0.9265 - val_loss: 1.7872 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7909 - accuracy: 0.9233 - val_loss: 1.7842 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.7837 - accuracy: 0.9329 - val_loss: 1.7812 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7819 - accuracy: 0.9265 - val_loss: 1.7781 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7826 - accuracy: 0.9265 - val_loss: 1.7751 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7793 - accuracy: 0.9265 - val_loss: 1.7721 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7747 - accuracy: 0.9329 - val_loss: 1.7692 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7679 - accuracy: 0.9297 - val_loss: 1.7662 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7638 - accuracy: 0.9329 - val_loss: 1.7632 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7574 - accuracy: 0.9329 - val_loss: 1.7603 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7577 - accuracy: 0.9329 - val_loss: 1.7574 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7537 - accuracy: 0.9297 - val_loss: 1.7545 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7500 - accuracy: 0.9361 - val_loss: 1.7515 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7442 - accuracy: 0.9297 - val_loss: 1.7486 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.7490 - accuracy: 0.9265 - val_loss: 1.7457 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7433 - accuracy: 0.9329 - val_loss: 1.7429 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7455 - accuracy: 0.9361 - val_loss: 1.7400 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7344 - accuracy: 0.9265 - val_loss: 1.7371 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7406 - accuracy: 0.9329 - val_loss: 1.7343 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7291 - accuracy: 0.9297 - val_loss: 1.7313 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7381 - accuracy: 0.9329 - val_loss: 1.7285 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7305 - accuracy: 0.9265 - val_loss: 1.7257 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7205 - accuracy: 0.9297 - val_loss: 1.7229 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7215 - accuracy: 0.9329 - val_loss: 1.7200 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7214 - accuracy: 0.9329 - val_loss: 1.7172 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.7227 - accuracy: 0.9329 - val_loss: 1.7145 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7162 - accuracy: 0.9329 - val_loss: 1.7116 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7101 - accuracy: 0.9361 - val_loss: 1.7089 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7034 - accuracy: 0.9361 - val_loss: 1.7061 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6981 - accuracy: 0.9361 - val_loss: 1.7033 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.7005 - accuracy: 0.9361 - val_loss: 1.7006 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.7036 - accuracy: 0.9329 - val_loss: 1.6979 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6932 - accuracy: 0.9361 - val_loss: 1.6951 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6910 - accuracy: 0.9361 - val_loss: 1.6924 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6911 - accuracy: 0.9297 - val_loss: 1.6897 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6901 - accuracy: 0.9361 - val_loss: 1.6869 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6806 - accuracy: 0.9361 - val_loss: 1.6842 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.6808 - accuracy: 0.9329 - val_loss: 1.6815 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6805 - accuracy: 0.9329 - val_loss: 1.6788 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6872 - accuracy: 0.9361 - val_loss: 1.6761 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6809 - accuracy: 0.9361 - val_loss: 1.6734 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6698 - accuracy: 0.9361 - val_loss: 1.6707 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6707 - accuracy: 0.9329 - val_loss: 1.6681 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6652 - accuracy: 0.9329 - val_loss: 1.6654 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6659 - accuracy: 0.9329 - val_loss: 1.6627 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6652 - accuracy: 0.9361 - val_loss: 1.6601 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6635 - accuracy: 0.9361 - val_loss: 1.6575 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6532 - accuracy: 0.9361 - val_loss: 1.6548 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6483 - accuracy: 0.9297 - val_loss: 1.6522 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.6559 - accuracy: 0.9361 - val_loss: 1.6495 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6500 - accuracy: 0.9425 - val_loss: 1.6469 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6448 - accuracy: 0.9329 - val_loss: 1.6442 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6386 - accuracy: 0.9329 - val_loss: 1.6416 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6426 - accuracy: 0.9393 - val_loss: 1.6390 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6370 - accuracy: 0.9361 - val_loss: 1.6364 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6348 - accuracy: 0.9329 - val_loss: 1.6338 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6334 - accuracy: 0.9329 - val_loss: 1.6313 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6308 - accuracy: 0.9361 - val_loss: 1.6287 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6339 - accuracy: 0.9393 - val_loss: 1.6261 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6314 - accuracy: 0.9329 - val_loss: 1.6236 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.6261 - accuracy: 0.9393 - val_loss: 1.6211 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6308 - accuracy: 0.9393 - val_loss: 1.6185 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6151 - accuracy: 0.9361 - val_loss: 1.6160 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6207 - accuracy: 0.9425 - val_loss: 1.6135 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6096 - accuracy: 0.9457 - val_loss: 1.6110 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6145 - accuracy: 0.9361 - val_loss: 1.6085 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6086 - accuracy: 0.9393 - val_loss: 1.6060 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6081 - accuracy: 0.9393 - val_loss: 1.6034 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.6065 - accuracy: 0.9329 - val_loss: 1.6009 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6018 - accuracy: 0.9361 - val_loss: 1.5984 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.6013 - accuracy: 0.9329 - val_loss: 1.5959 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.5932 - accuracy: 0.9393 - val_loss: 1.5934 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5934 - accuracy: 0.9329 - val_loss: 1.5909 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5893 - accuracy: 0.9297 - val_loss: 1.5884 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5868 - accuracy: 0.9425 - val_loss: 1.5859 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5825 - accuracy: 0.9361 - val_loss: 1.5834 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5815 - accuracy: 0.9393 - val_loss: 1.5809 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5833 - accuracy: 0.9361 - val_loss: 1.5784 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5733 - accuracy: 0.9457 - val_loss: 1.5759 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5751 - accuracy: 0.9393 - val_loss: 1.5735 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.5692 - accuracy: 0.9425 - val_loss: 1.5710 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5675 - accuracy: 0.9457 - val_loss: 1.5686 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5648 - accuracy: 0.9425 - val_loss: 1.5661 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5639 - accuracy: 0.9361 - val_loss: 1.5637 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5578 - accuracy: 0.9393 - val_loss: 1.5613 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5588 - accuracy: 0.9329 - val_loss: 1.5588 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5582 - accuracy: 0.9425 - val_loss: 1.5564 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5563 - accuracy: 0.9457 - val_loss: 1.5540 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5506 - accuracy: 0.9393 - val_loss: 1.5515 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5562 - accuracy: 0.9393 - val_loss: 1.5491 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5512 - accuracy: 0.9457 - val_loss: 1.5467 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.5416 - accuracy: 0.9457 - val_loss: 1.5443 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5437 - accuracy: 0.9361 - val_loss: 1.5419 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5433 - accuracy: 0.9393 - val_loss: 1.5395 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5405 - accuracy: 0.9361 - val_loss: 1.5371 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5286 - accuracy: 0.9393 - val_loss: 1.5347 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5300 - accuracy: 0.9393 - val_loss: 1.5323 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5263 - accuracy: 0.9457 - val_loss: 1.5300 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5322 - accuracy: 0.9457 - val_loss: 1.5276 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5290 - accuracy: 0.9457 - val_loss: 1.5252 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5294 - accuracy: 0.9361 - val_loss: 1.5229 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5228 - accuracy: 0.9489 - val_loss: 1.5205 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.5145 - accuracy: 0.9457 - val_loss: 1.5181 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5176 - accuracy: 0.9393 - val_loss: 1.5158 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5117 - accuracy: 0.9457 - val_loss: 1.5135 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5059 - accuracy: 0.9361 - val_loss: 1.5111 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5106 - accuracy: 0.9425 - val_loss: 1.5088 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5186 - accuracy: 0.9393 - val_loss: 1.5065 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5009 - accuracy: 0.9393 - val_loss: 1.5042 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.5034 - accuracy: 0.9393 - val_loss: 1.5019 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5053 - accuracy: 0.9393 - val_loss: 1.4995 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4974 - accuracy: 0.9425 - val_loss: 1.4972 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.4873 - accuracy: 0.9457 - val_loss: 1.4949 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4993 - accuracy: 0.9457 - val_loss: 1.4926 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.5003 - accuracy: 0.9425 - val_loss: 1.4903 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4862 - accuracy: 0.9393 - val_loss: 1.4879 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4865 - accuracy: 0.9393 - val_loss: 1.4857 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4786 - accuracy: 0.9489 - val_loss: 1.4834 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4747 - accuracy: 0.9521 - val_loss: 1.4811 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4821 - accuracy: 0.9425 - val_loss: 1.4788 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4755 - accuracy: 0.9393 - val_loss: 1.4765 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4822 - accuracy: 0.9425 - val_loss: 1.4742 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.4696 - accuracy: 0.9425 - val_loss: 1.4720 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4688 - accuracy: 0.9425 - val_loss: 1.4697 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4661 - accuracy: 0.9489 - val_loss: 1.4675 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4725 - accuracy: 0.9457 - val_loss: 1.4653 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4611 - accuracy: 0.9457 - val_loss: 1.4631 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4599 - accuracy: 0.9457 - val_loss: 1.4608 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4588 - accuracy: 0.9457 - val_loss: 1.4586 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4587 - accuracy: 0.9457 - val_loss: 1.4565 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4569 - accuracy: 0.9425 - val_loss: 1.4542 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4577 - accuracy: 0.9457 - val_loss: 1.4520 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.4520 - accuracy: 0.9489 - val_loss: 1.4498 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4464 - accuracy: 0.9489 - val_loss: 1.4476 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4453 - accuracy: 0.9457 - val_loss: 1.4454 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4427 - accuracy: 0.9489 - val_loss: 1.4432 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4464 - accuracy: 0.9457 - val_loss: 1.4410 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4316 - accuracy: 0.9489 - val_loss: 1.4388 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4404 - accuracy: 0.9361 - val_loss: 1.4366 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4404 - accuracy: 0.9393 - val_loss: 1.4344 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4339 - accuracy: 0.9457 - val_loss: 1.4323 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4384 - accuracy: 0.9489 - val_loss: 1.4302 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.4304 - accuracy: 0.9425 - val_loss: 1.4280 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4188 - accuracy: 0.9457 - val_loss: 1.4259 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4142 - accuracy: 0.9553 - val_loss: 1.4237 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4316 - accuracy: 0.9489 - val_loss: 1.4215 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4266 - accuracy: 0.9425 - val_loss: 1.4195 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4188 - accuracy: 0.9457 - val_loss: 1.4173 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4147 - accuracy: 0.9521 - val_loss: 1.4152 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4078 - accuracy: 0.9489 - val_loss: 1.4130 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4131 - accuracy: 0.9521 - val_loss: 1.4109 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 306/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4090 - accuracy: 0.9489 - val_loss: 1.4087 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.4069 - accuracy: 0.9425 - val_loss: 1.4065 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.4025 - accuracy: 0.9489 - val_loss: 1.4044 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.4048 - accuracy: 0.9457 - val_loss: 1.4023 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3983 - accuracy: 0.9489 - val_loss: 1.4002 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3956 - accuracy: 0.9425 - val_loss: 1.3981 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3948 - accuracy: 0.9457 - val_loss: 1.3960 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3922 - accuracy: 0.9457 - val_loss: 1.3939 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3911 - accuracy: 0.9521 - val_loss: 1.3918 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3925 - accuracy: 0.9521 - val_loss: 1.3897 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3858 - accuracy: 0.9457 - val_loss: 1.3876 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.3857 - accuracy: 0.9585 - val_loss: 1.3855 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 318/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3824 - accuracy: 0.9457 - val_loss: 1.3835 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3785 - accuracy: 0.9553 - val_loss: 1.3814 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3777 - accuracy: 0.9489 - val_loss: 1.3794 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3783 - accuracy: 0.9457 - val_loss: 1.3774 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3750 - accuracy: 0.9553 - val_loss: 1.3753 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3730 - accuracy: 0.9489 - val_loss: 1.3733 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3680 - accuracy: 0.9521 - val_loss: 1.3712 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 325/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3747 - accuracy: 0.9457 - val_loss: 1.3692 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.3680 - accuracy: 0.9521 - val_loss: 1.3672 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3583 - accuracy: 0.9489 - val_loss: 1.3652 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3620 - accuracy: 0.9585 - val_loss: 1.3631 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3537 - accuracy: 0.9553 - val_loss: 1.3611 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3574 - accuracy: 0.9521 - val_loss: 1.3590 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3497 - accuracy: 0.9489 - val_loss: 1.3570 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3586 - accuracy: 0.9489 - val_loss: 1.3550 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3506 - accuracy: 0.9585 - val_loss: 1.3530 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3567 - accuracy: 0.9489 - val_loss: 1.3510 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.3518 - accuracy: 0.9521 - val_loss: 1.3490 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3434 - accuracy: 0.9585 - val_loss: 1.3470 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3499 - accuracy: 0.9489 - val_loss: 1.3450 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3399 - accuracy: 0.9553 - val_loss: 1.3430 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3458 - accuracy: 0.9489 - val_loss: 1.3410 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3348 - accuracy: 0.9585 - val_loss: 1.3390 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3381 - accuracy: 0.9585 - val_loss: 1.3370 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 342/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3311 - accuracy: 0.9553 - val_loss: 1.3350 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3335 - accuracy: 0.9585 - val_loss: 1.3330 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 344/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.3295 - accuracy: 0.9585 - val_loss: 1.3311 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3354 - accuracy: 0.9521 - val_loss: 1.3291 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3254 - accuracy: 0.9553 - val_loss: 1.3271 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3333 - accuracy: 0.9521 - val_loss: 1.3252 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3253 - accuracy: 0.9553 - val_loss: 1.3232 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3252 - accuracy: 0.9489 - val_loss: 1.3213 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3168 - accuracy: 0.9553 - val_loss: 1.3193 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 351/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3143 - accuracy: 0.9553 - val_loss: 1.3174 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3136 - accuracy: 0.9553 - val_loss: 1.3155 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.3162 - accuracy: 0.9521 - val_loss: 1.3136 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3094 - accuracy: 0.9585 - val_loss: 1.3116 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 355/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.3108 - accuracy: 0.9521 - val_loss: 1.3097 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3081 - accuracy: 0.9553 - val_loss: 1.3078 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3126 - accuracy: 0.9585 - val_loss: 1.3059 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.3011 - accuracy: 0.9521 - val_loss: 1.3040 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2993 - accuracy: 0.9585 - val_loss: 1.3021 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2991 - accuracy: 0.9521 - val_loss: 1.3002 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 361/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2997 - accuracy: 0.9585 - val_loss: 1.2983 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 362/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2886 - accuracy: 0.9553 - val_loss: 1.2964 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2926 - accuracy: 0.9553 - val_loss: 1.2944 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2875 - accuracy: 0.9585 - val_loss: 1.2926 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2980 - accuracy: 0.9553 - val_loss: 1.2907 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2925 - accuracy: 0.9553 - val_loss: 1.2889 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2862 - accuracy: 0.9489 - val_loss: 1.2870 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 368/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2780 - accuracy: 0.9617 - val_loss: 1.2850 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2848 - accuracy: 0.9521 - val_loss: 1.2832 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 370/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2801 - accuracy: 0.9553 - val_loss: 1.2813 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2816 - accuracy: 0.9553 - val_loss: 1.2794 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2738 - accuracy: 0.9585 - val_loss: 1.2776 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2760 - accuracy: 0.9585 - val_loss: 1.2758 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2763 - accuracy: 0.9521 - val_loss: 1.2739 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2651 - accuracy: 0.9585 - val_loss: 1.2720 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2746 - accuracy: 0.9553 - val_loss: 1.2702 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2670 - accuracy: 0.9553 - val_loss: 1.2683 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2637 - accuracy: 0.9585 - val_loss: 1.2665 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2605 - accuracy: 0.9553 - val_loss: 1.2646 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2635 - accuracy: 0.9585 - val_loss: 1.2628 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 381/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2653 - accuracy: 0.9585 - val_loss: 1.2610 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2660 - accuracy: 0.9585 - val_loss: 1.2591 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2625 - accuracy: 0.9585 - val_loss: 1.2573 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2562 - accuracy: 0.9553 - val_loss: 1.2555 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.2461 - accuracy: 0.9585 - val_loss: 1.2536 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2483 - accuracy: 0.9585 - val_loss: 1.2518 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2504 - accuracy: 0.9553 - val_loss: 1.2499 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2438 - accuracy: 0.9617 - val_loss: 1.2481 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2470 - accuracy: 0.9521 - val_loss: 1.2464 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2434 - accuracy: 0.9585 - val_loss: 1.2446 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2435 - accuracy: 0.9585 - val_loss: 1.2427 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2337 - accuracy: 0.9617 - val_loss: 1.2409 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2414 - accuracy: 0.9585 - val_loss: 1.2392 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.2338 - accuracy: 0.9617 - val_loss: 1.2374 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2371 - accuracy: 0.9617 - val_loss: 1.2356 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2238 - accuracy: 0.9585 - val_loss: 1.2339 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2277 - accuracy: 0.9617 - val_loss: 1.2321 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2368 - accuracy: 0.9585 - val_loss: 1.2303 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2240 - accuracy: 0.9617 - val_loss: 1.2285 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2298 - accuracy: 0.9617 - val_loss: 1.2268 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2300 - accuracy: 0.9585 - val_loss: 1.2251 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2222 - accuracy: 0.9617 - val_loss: 1.2233 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2283 - accuracy: 0.9553 - val_loss: 1.2215 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2170 - accuracy: 0.9585 - val_loss: 1.2197 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2150 - accuracy: 0.9585 - val_loss: 1.2180 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2169 - accuracy: 0.9617 - val_loss: 1.2162 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2118 - accuracy: 0.9617 - val_loss: 1.2145 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2218 - accuracy: 0.9553 - val_loss: 1.2128 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2057 - accuracy: 0.9585 - val_loss: 1.2110 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.2062 - accuracy: 0.9553 - val_loss: 1.2091 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2086 - accuracy: 0.9617 - val_loss: 1.2075 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2011 - accuracy: 0.9617 - val_loss: 1.2056 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.2103 - accuracy: 0.9521 - val_loss: 1.2039 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1977 - accuracy: 0.9585 - val_loss: 1.2022 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1974 - accuracy: 0.9585 - val_loss: 1.2005 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 416/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.2001 - accuracy: 0.9617 - val_loss: 1.1988 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 417/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1959 - accuracy: 0.9617 - val_loss: 1.1971 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 418/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1955 - accuracy: 0.9553 - val_loss: 1.1954 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1959 - accuracy: 0.9617 - val_loss: 1.1937 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 420/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1918 - accuracy: 0.9585 - val_loss: 1.1920 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1852 - accuracy: 0.9585 - val_loss: 1.1903 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1912 - accuracy: 0.9585 - val_loss: 1.1886 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1906 - accuracy: 0.9649 - val_loss: 1.1870 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1901 - accuracy: 0.9585 - val_loss: 1.1853 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 425/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1865 - accuracy: 0.9585 - val_loss: 1.1836 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1755 - accuracy: 0.9585 - val_loss: 1.1819 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1754 - accuracy: 0.9585 - val_loss: 1.1803 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1668 - accuracy: 0.9617 - val_loss: 1.1786 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1689 - accuracy: 0.9617 - val_loss: 1.1770 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1724 - accuracy: 0.9585 - val_loss: 1.1753 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1743 - accuracy: 0.9553 - val_loss: 1.1737 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1748 - accuracy: 0.9617 - val_loss: 1.1720 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1662 - accuracy: 0.9649 - val_loss: 1.1703 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1631 - accuracy: 0.9585 - val_loss: 1.1686 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1651 - accuracy: 0.9585 - val_loss: 1.1668 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1617 - accuracy: 0.9585 - val_loss: 1.1652 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1591 - accuracy: 0.9617 - val_loss: 1.1635 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1605 - accuracy: 0.9585 - val_loss: 1.1618 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1630 - accuracy: 0.9553 - val_loss: 1.1602 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1550 - accuracy: 0.9617 - val_loss: 1.1585 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1509 - accuracy: 0.9553 - val_loss: 1.1569 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1537 - accuracy: 0.9585 - val_loss: 1.1552 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1511 - accuracy: 0.9617 - val_loss: 1.1536 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1440 - accuracy: 0.9681 - val_loss: 1.1520 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1418 - accuracy: 0.9649 - val_loss: 1.1503 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 446/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1513 - accuracy: 0.9617 - val_loss: 1.1487 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 447/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1415 - accuracy: 0.9617 - val_loss: 1.1471 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 448/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1353 - accuracy: 0.9617 - val_loss: 1.1455 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 449/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1442 - accuracy: 0.9585 - val_loss: 1.1438 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 450/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1407 - accuracy: 0.9617 - val_loss: 1.1422 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1382 - accuracy: 0.9617 - val_loss: 1.1406 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 452/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1404 - accuracy: 0.9617 - val_loss: 1.1389 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 453/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1351 - accuracy: 0.9585 - val_loss: 1.1373 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 454/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1330 - accuracy: 0.9617 - val_loss: 1.1357 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1328 - accuracy: 0.9617 - val_loss: 1.1342 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 456/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1321 - accuracy: 0.9617 - val_loss: 1.1325 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 457/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1254 - accuracy: 0.9585 - val_loss: 1.1310 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1288 - accuracy: 0.9649 - val_loss: 1.1294 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1255 - accuracy: 0.9585 - val_loss: 1.1278 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 460/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1243 - accuracy: 0.9617 - val_loss: 1.1262 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 461/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1193 - accuracy: 0.9617 - val_loss: 1.1247 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1116 - accuracy: 0.9617 - val_loss: 1.1231 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1108 - accuracy: 0.9617 - val_loss: 1.1215 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1110 - accuracy: 0.9649 - val_loss: 1.1200 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 465/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1187 - accuracy: 0.9617 - val_loss: 1.1184 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.1134 - accuracy: 0.9585 - val_loss: 1.1169 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 467/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1113 - accuracy: 0.9649 - val_loss: 1.1153 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1092 - accuracy: 0.9585 - val_loss: 1.1138 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 469/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1100 - accuracy: 0.9617 - val_loss: 1.1122 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1075 - accuracy: 0.9617 - val_loss: 1.1107 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1055 - accuracy: 0.9617 - val_loss: 1.1091 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 472/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1067 - accuracy: 0.9617 - val_loss: 1.1076 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.1060 - accuracy: 0.9617 - val_loss: 1.1061 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0961 - accuracy: 0.9617 - val_loss: 1.1045 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.1059 - accuracy: 0.9649 - val_loss: 1.1030 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 476/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1043 - accuracy: 0.9585 - val_loss: 1.1015 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 477/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.1020 - accuracy: 0.9617 - val_loss: 1.0999 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0971 - accuracy: 0.9617 - val_loss: 1.0984 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0946 - accuracy: 0.9649 - val_loss: 1.0968 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 480/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0902 - accuracy: 0.9617 - val_loss: 1.0954 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 481/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0902 - accuracy: 0.9617 - val_loss: 1.0938 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 482/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0890 - accuracy: 0.9649 - val_loss: 1.0923 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 483/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0888 - accuracy: 0.9681 - val_loss: 1.0907 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0781 - accuracy: 0.9617 - val_loss: 1.0892 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0925 - accuracy: 0.9553 - val_loss: 1.0877 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 486/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.0816 - accuracy: 0.9617 - val_loss: 1.0862 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0884 - accuracy: 0.9553 - val_loss: 1.0846 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0811 - accuracy: 0.9585 - val_loss: 1.0831 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 489/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0709 - accuracy: 0.9617 - val_loss: 1.0817 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 490/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0769 - accuracy: 0.9617 - val_loss: 1.0802 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0715 - accuracy: 0.9649 - val_loss: 1.0786 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0766 - accuracy: 0.9649 - val_loss: 1.0771 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 493/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0769 - accuracy: 0.9649 - val_loss: 1.0756 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 494/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0737 - accuracy: 0.9553 - val_loss: 1.0741 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0685 - accuracy: 0.9649 - val_loss: 1.0727 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 496/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0687 - accuracy: 0.9617 - val_loss: 1.0713 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0631 - accuracy: 0.9617 - val_loss: 1.0699 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0627 - accuracy: 0.9585 - val_loss: 1.0683 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0693 - accuracy: 0.9585 - val_loss: 1.0669 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 1.0643 - accuracy: 0.9585 - val_loss: 1.0654 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 501/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0559 - accuracy: 0.9681 - val_loss: 1.0639 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 502/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0616 - accuracy: 0.9585 - val_loss: 1.0625 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 503/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0546 - accuracy: 0.9649 - val_loss: 1.0610 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 504/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0551 - accuracy: 0.9617 - val_loss: 1.0596 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0526 - accuracy: 0.9617 - val_loss: 1.0580 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 506/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0552 - accuracy: 0.9649 - val_loss: 1.0566 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0470 - accuracy: 0.9681 - val_loss: 1.0552 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 508/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0505 - accuracy: 0.9585 - val_loss: 1.0537 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 509/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0481 - accuracy: 0.9585 - val_loss: 1.0523 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 510/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0455 - accuracy: 0.9617 - val_loss: 1.0508 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0485 - accuracy: 0.9617 - val_loss: 1.0494 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 512/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0398 - accuracy: 0.9553 - val_loss: 1.0479 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 513/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0404 - accuracy: 0.9649 - val_loss: 1.0465 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 514/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.0394 - accuracy: 0.9585 - val_loss: 1.0450 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0462 - accuracy: 0.9649 - val_loss: 1.0436 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0391 - accuracy: 0.9617 - val_loss: 1.0422 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 517/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0331 - accuracy: 0.9649 - val_loss: 1.0407 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 518/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0303 - accuracy: 0.9649 - val_loss: 1.0393 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0322 - accuracy: 0.9681 - val_loss: 1.0378 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0298 - accuracy: 0.9617 - val_loss: 1.0364 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0301 - accuracy: 0.9617 - val_loss: 1.0350 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0332 - accuracy: 0.9649 - val_loss: 1.0336 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0283 - accuracy: 0.9617 - val_loss: 1.0322 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0297 - accuracy: 0.9617 - val_loss: 1.0309 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 525/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0258 - accuracy: 0.9681 - val_loss: 1.0295 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0225 - accuracy: 0.9617 - val_loss: 1.0280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0222 - accuracy: 0.9617 - val_loss: 1.0266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 528/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 1.0265 - accuracy: 0.9617 - val_loss: 1.0252 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 529/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0187 - accuracy: 0.9649 - val_loss: 1.0238 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 530/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0093 - accuracy: 0.9681 - val_loss: 1.0224 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0136 - accuracy: 0.9585 - val_loss: 1.0209 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 532/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0174 - accuracy: 0.9585 - val_loss: 1.0195 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 1.0135 - accuracy: 0.9585 - val_loss: 1.0181 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 534/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0144 - accuracy: 0.9585 - val_loss: 1.0167 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 535/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0128 - accuracy: 0.9617 - val_loss: 1.0155 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 536/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0106 - accuracy: 0.9649 - val_loss: 1.0141 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9994 - accuracy: 0.9617 - val_loss: 1.0127 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0023 - accuracy: 0.9649 - val_loss: 1.0113 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0088 - accuracy: 0.9617 - val_loss: 1.0100 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0038 - accuracy: 0.9617 - val_loss: 1.0086 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0030 - accuracy: 0.9585 - val_loss: 1.0072 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0048 - accuracy: 0.9617 - val_loss: 1.0059 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 543/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0057 - accuracy: 0.9617 - val_loss: 1.0046 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 1.0014 - accuracy: 0.9649 - val_loss: 1.0033 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 1.0032 - accuracy: 0.9617 - val_loss: 1.0019 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9921 - accuracy: 0.9617 - val_loss: 1.0006 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9929 - accuracy: 0.9681 - val_loss: 0.9992 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9816 - accuracy: 0.9617 - val_loss: 0.9978 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9897 - accuracy: 0.9617 - val_loss: 0.9965 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 550/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9946 - accuracy: 0.9649 - val_loss: 0.9951 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 551/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9907 - accuracy: 0.9617 - val_loss: 0.9938 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 552/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9924 - accuracy: 0.9617 - val_loss: 0.9926 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9823 - accuracy: 0.9649 - val_loss: 0.9912 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 554/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9829 - accuracy: 0.9617 - val_loss: 0.9898 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 555/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9934 - accuracy: 0.9585 - val_loss: 0.9885 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9791 - accuracy: 0.9617 - val_loss: 0.9871 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9781 - accuracy: 0.9649 - val_loss: 0.9857 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9773 - accuracy: 0.9649 - val_loss: 0.9844 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 559/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9798 - accuracy: 0.9649 - val_loss: 0.9831 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 560/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9774 - accuracy: 0.9617 - val_loss: 0.9817 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9747 - accuracy: 0.9617 - val_loss: 0.9804 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9632 - accuracy: 0.9712 - val_loss: 0.9791 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 563/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9757 - accuracy: 0.9649 - val_loss: 0.9777 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 564/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9711 - accuracy: 0.9681 - val_loss: 0.9764 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9707 - accuracy: 0.9617 - val_loss: 0.9751 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9705 - accuracy: 0.9617 - val_loss: 0.9738 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 567/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9673 - accuracy: 0.9585 - val_loss: 0.9725 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 568/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9716 - accuracy: 0.9585 - val_loss: 0.9711 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9536 - accuracy: 0.9617 - val_loss: 0.9699 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9618 - accuracy: 0.9649 - val_loss: 0.9685 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9570 - accuracy: 0.9649 - val_loss: 0.9673 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9606 - accuracy: 0.9585 - val_loss: 0.9657 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9507 - accuracy: 0.9617 - val_loss: 0.9645 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 574/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9531 - accuracy: 0.9681 - val_loss: 0.9632 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9573 - accuracy: 0.9585 - val_loss: 0.9619 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9501 - accuracy: 0.9617 - val_loss: 0.9606 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9573 - accuracy: 0.9617 - val_loss: 0.9594 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9536 - accuracy: 0.9649 - val_loss: 0.9581 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 579/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9522 - accuracy: 0.9617 - val_loss: 0.9568 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9497 - accuracy: 0.9617 - val_loss: 0.9556 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9524 - accuracy: 0.9649 - val_loss: 0.9543 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 582/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9399 - accuracy: 0.9649 - val_loss: 0.9530 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9410 - accuracy: 0.9617 - val_loss: 0.9517 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 584/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9491 - accuracy: 0.9617 - val_loss: 0.9505 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9439 - accuracy: 0.9649 - val_loss: 0.9493 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9417 - accuracy: 0.9649 - val_loss: 0.9480 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 587/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9449 - accuracy: 0.9649 - val_loss: 0.9467 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.9346 - accuracy: 0.9617 - val_loss: 0.9454 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9386 - accuracy: 0.9617 - val_loss: 0.9441 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9361 - accuracy: 0.9617 - val_loss: 0.9428 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 591/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9389 - accuracy: 0.9649 - val_loss: 0.9415 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9289 - accuracy: 0.9617 - val_loss: 0.9403 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9251 - accuracy: 0.9681 - val_loss: 0.9391 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.9342 - accuracy: 0.9681 - val_loss: 0.9378 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 595/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9323 - accuracy: 0.9649 - val_loss: 0.9366 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9270 - accuracy: 0.9681 - val_loss: 0.9352 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9264 - accuracy: 0.9585 - val_loss: 0.9340 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 598/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9214 - accuracy: 0.9617 - val_loss: 0.9328 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 599/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9249 - accuracy: 0.9649 - val_loss: 0.9316 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 600/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.9286 - accuracy: 0.9585 - val_loss: 0.9303 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 601/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9188 - accuracy: 0.9649 - val_loss: 0.9291 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9203 - accuracy: 0.9617 - val_loss: 0.9278 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9217 - accuracy: 0.9617 - val_loss: 0.9266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9180 - accuracy: 0.9617 - val_loss: 0.9254 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9159 - accuracy: 0.9617 - val_loss: 0.9242 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9122 - accuracy: 0.9617 - val_loss: 0.9231 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9182 - accuracy: 0.9585 - val_loss: 0.9219 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 608/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9170 - accuracy: 0.9649 - val_loss: 0.9207 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 609/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9099 - accuracy: 0.9617 - val_loss: 0.9195 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9033 - accuracy: 0.9681 - val_loss: 0.9183 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.9059 - accuracy: 0.9617 - val_loss: 0.9170 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9079 - accuracy: 0.9649 - val_loss: 0.9158 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.9082 - accuracy: 0.9617 - val_loss: 0.9147 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9031 - accuracy: 0.9617 - val_loss: 0.9135 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 615/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8985 - accuracy: 0.9617 - val_loss: 0.9123 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.9057 - accuracy: 0.9649 - val_loss: 0.9111 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 617/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8989 - accuracy: 0.9617 - val_loss: 0.9098 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 618/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8962 - accuracy: 0.9617 - val_loss: 0.9086 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 619/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.9014 - accuracy: 0.9681 - val_loss: 0.9074 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8927 - accuracy: 0.9681 - val_loss: 0.9063 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 621/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8951 - accuracy: 0.9649 - val_loss: 0.9051 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8947 - accuracy: 0.9553 - val_loss: 0.9038 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8932 - accuracy: 0.9617 - val_loss: 0.9026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8910 - accuracy: 0.9585 - val_loss: 0.9014 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8946 - accuracy: 0.9617 - val_loss: 0.9003 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 626/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8942 - accuracy: 0.9649 - val_loss: 0.8991 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 627/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8840 - accuracy: 0.9617 - val_loss: 0.8979 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 628/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8833 - accuracy: 0.9617 - val_loss: 0.8967 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 629/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8868 - accuracy: 0.9649 - val_loss: 0.8955 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8841 - accuracy: 0.9617 - val_loss: 0.8942 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 631/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8849 - accuracy: 0.9681 - val_loss: 0.8929 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8878 - accuracy: 0.9617 - val_loss: 0.8918 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 633/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8762 - accuracy: 0.9649 - val_loss: 0.8907 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 634/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8742 - accuracy: 0.9649 - val_loss: 0.8895 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8833 - accuracy: 0.9617 - val_loss: 0.8883 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8817 - accuracy: 0.9712 - val_loss: 0.8872 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8765 - accuracy: 0.9617 - val_loss: 0.8860 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 638/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.8736 - accuracy: 0.9585 - val_loss: 0.8848 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 639/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8748 - accuracy: 0.9649 - val_loss: 0.8837 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 640/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8742 - accuracy: 0.9617 - val_loss: 0.8826 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 641/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8766 - accuracy: 0.9649 - val_loss: 0.8814 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 642/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8672 - accuracy: 0.9617 - val_loss: 0.8803 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 643/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8692 - accuracy: 0.9649 - val_loss: 0.8791 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8695 - accuracy: 0.9617 - val_loss: 0.8780 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 645/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8649 - accuracy: 0.9712 - val_loss: 0.8767 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8609 - accuracy: 0.9617 - val_loss: 0.8755 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8688 - accuracy: 0.9649 - val_loss: 0.8744 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 648/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8636 - accuracy: 0.9617 - val_loss: 0.8732 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8635 - accuracy: 0.9649 - val_loss: 0.8721 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8720 - accuracy: 0.9617 - val_loss: 0.8710 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8641 - accuracy: 0.9649 - val_loss: 0.8699 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 652/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8531 - accuracy: 0.9649 - val_loss: 0.8688 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8600 - accuracy: 0.9649 - val_loss: 0.8677 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8599 - accuracy: 0.9585 - val_loss: 0.8664 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 655/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8523 - accuracy: 0.9649 - val_loss: 0.8653 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8602 - accuracy: 0.9649 - val_loss: 0.8641 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 657/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8551 - accuracy: 0.9649 - val_loss: 0.8630 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 658/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8543 - accuracy: 0.9585 - val_loss: 0.8618 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 659/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8500 - accuracy: 0.9617 - val_loss: 0.8607 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 660/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8471 - accuracy: 0.9681 - val_loss: 0.8595 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 661/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8462 - accuracy: 0.9681 - val_loss: 0.8585 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.8482 - accuracy: 0.9617 - val_loss: 0.8574 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8521 - accuracy: 0.9617 - val_loss: 0.8563 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8389 - accuracy: 0.9681 - val_loss: 0.8553 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 665/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8434 - accuracy: 0.9681 - val_loss: 0.8541 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 666/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8443 - accuracy: 0.9617 - val_loss: 0.8530 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8365 - accuracy: 0.9681 - val_loss: 0.8519 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 668/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8473 - accuracy: 0.9617 - val_loss: 0.8508 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8386 - accuracy: 0.9617 - val_loss: 0.8497 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8360 - accuracy: 0.9649 - val_loss: 0.8486 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8417 - accuracy: 0.9585 - val_loss: 0.8475 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8447 - accuracy: 0.9617 - val_loss: 0.8466 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8467 - accuracy: 0.9617 - val_loss: 0.8454 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.8288 - accuracy: 0.9681 - val_loss: 0.8444 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 675/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8325 - accuracy: 0.9649 - val_loss: 0.8432 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 676/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8265 - accuracy: 0.9776 - val_loss: 0.8421 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 677/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8345 - accuracy: 0.9617 - val_loss: 0.8411 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8274 - accuracy: 0.9585 - val_loss: 0.8400 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 679/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8329 - accuracy: 0.9649 - val_loss: 0.8389 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8293 - accuracy: 0.9681 - val_loss: 0.8378 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8236 - accuracy: 0.9617 - val_loss: 0.8368 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8171 - accuracy: 0.9649 - val_loss: 0.8357 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 683/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8200 - accuracy: 0.9649 - val_loss: 0.8346 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 684/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8218 - accuracy: 0.9649 - val_loss: 0.8335 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8289 - accuracy: 0.9617 - val_loss: 0.8324 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8181 - accuracy: 0.9585 - val_loss: 0.8314 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 687/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8176 - accuracy: 0.9649 - val_loss: 0.8303 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8231 - accuracy: 0.9649 - val_loss: 0.8293 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8169 - accuracy: 0.9649 - val_loss: 0.8283 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8080 - accuracy: 0.9649 - val_loss: 0.8273 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 691/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8127 - accuracy: 0.9681 - val_loss: 0.8260 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 692/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8161 - accuracy: 0.9681 - val_loss: 0.8250 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8144 - accuracy: 0.9649 - val_loss: 0.8239 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 694/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8092 - accuracy: 0.9681 - val_loss: 0.8227 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8061 - accuracy: 0.9617 - val_loss: 0.8217 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8088 - accuracy: 0.9681 - val_loss: 0.8206 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8083 - accuracy: 0.9681 - val_loss: 0.8195 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8041 - accuracy: 0.9617 - val_loss: 0.8184 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8124 - accuracy: 0.9617 - val_loss: 0.8174 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.8061 - accuracy: 0.9681 - val_loss: 0.8163 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 701/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8069 - accuracy: 0.9617 - val_loss: 0.8152 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.8082 - accuracy: 0.9649 - val_loss: 0.8141 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 703/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8091 - accuracy: 0.9681 - val_loss: 0.8131 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8008 - accuracy: 0.9649 - val_loss: 0.8120 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 705/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7974 - accuracy: 0.9681 - val_loss: 0.8109 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.8036 - accuracy: 0.9617 - val_loss: 0.8099 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.8031 - accuracy: 0.9649 - val_loss: 0.8089 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7963 - accuracy: 0.9617 - val_loss: 0.8079 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 709/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.7968 - accuracy: 0.9649 - val_loss: 0.8068 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 710/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.8032 - accuracy: 0.9617 - val_loss: 0.8058 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.7978 - accuracy: 0.9617 - val_loss: 0.8049 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 712/1000\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 0.8002 - accuracy: 0.9649 - val_loss: 0.8038 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7978 - accuracy: 0.9617 - val_loss: 0.8029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 714/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7949 - accuracy: 0.9681 - val_loss: 0.8019 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7849 - accuracy: 0.9649 - val_loss: 0.8009 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 716/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7881 - accuracy: 0.9681 - val_loss: 0.7999 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 717/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7851 - accuracy: 0.9649 - val_loss: 0.7988 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 718/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7803 - accuracy: 0.9617 - val_loss: 0.7978 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 719/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7844 - accuracy: 0.9617 - val_loss: 0.7968 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.7879 - accuracy: 0.9649 - val_loss: 0.7958 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7889 - accuracy: 0.9649 - val_loss: 0.7948 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7839 - accuracy: 0.9649 - val_loss: 0.7938 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7874 - accuracy: 0.9649 - val_loss: 0.7928 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7788 - accuracy: 0.9585 - val_loss: 0.7917 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 725/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7861 - accuracy: 0.9617 - val_loss: 0.7908 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7810 - accuracy: 0.9617 - val_loss: 0.7897 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 727/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7855 - accuracy: 0.9681 - val_loss: 0.7887 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 728/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7826 - accuracy: 0.9649 - val_loss: 0.7877 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 729/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7816 - accuracy: 0.9617 - val_loss: 0.7867 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7710 - accuracy: 0.9681 - val_loss: 0.7857 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 731/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7667 - accuracy: 0.9712 - val_loss: 0.7847 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 732/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7705 - accuracy: 0.9617 - val_loss: 0.7838 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 733/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7708 - accuracy: 0.9649 - val_loss: 0.7828 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 734/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7753 - accuracy: 0.9585 - val_loss: 0.7818 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 735/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.7750 - accuracy: 0.9617 - val_loss: 0.7809 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 736/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7687 - accuracy: 0.9649 - val_loss: 0.7798 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 737/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7634 - accuracy: 0.9681 - val_loss: 0.7789 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7735 - accuracy: 0.9649 - val_loss: 0.7779 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7713 - accuracy: 0.9649 - val_loss: 0.7769 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7664 - accuracy: 0.9649 - val_loss: 0.7758 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 741/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7665 - accuracy: 0.9649 - val_loss: 0.7749 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 742/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7592 - accuracy: 0.9681 - val_loss: 0.7739 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 743/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7636 - accuracy: 0.9681 - val_loss: 0.7729 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 744/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7639 - accuracy: 0.9681 - val_loss: 0.7719 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 745/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7652 - accuracy: 0.9649 - val_loss: 0.7710 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 746/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7523 - accuracy: 0.9712 - val_loss: 0.7701 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 747/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7552 - accuracy: 0.9681 - val_loss: 0.7691 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 748/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7564 - accuracy: 0.9649 - val_loss: 0.7681 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 749/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7541 - accuracy: 0.9585 - val_loss: 0.7671 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7556 - accuracy: 0.9649 - val_loss: 0.7662 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7520 - accuracy: 0.9617 - val_loss: 0.7652 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7543 - accuracy: 0.9649 - val_loss: 0.7642 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7497 - accuracy: 0.9649 - val_loss: 0.7633 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 754/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7561 - accuracy: 0.9585 - val_loss: 0.7624 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 755/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7481 - accuracy: 0.9681 - val_loss: 0.7615 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 756/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7486 - accuracy: 0.9649 - val_loss: 0.7606 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 757/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7479 - accuracy: 0.9649 - val_loss: 0.7596 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 758/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7397 - accuracy: 0.9681 - val_loss: 0.7586 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7496 - accuracy: 0.9649 - val_loss: 0.7577 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 760/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7374 - accuracy: 0.9712 - val_loss: 0.7568 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7366 - accuracy: 0.9649 - val_loss: 0.7558 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7452 - accuracy: 0.9649 - val_loss: 0.7548 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7455 - accuracy: 0.9681 - val_loss: 0.7539 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7354 - accuracy: 0.9681 - val_loss: 0.7529 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 765/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7421 - accuracy: 0.9617 - val_loss: 0.7520 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7450 - accuracy: 0.9585 - val_loss: 0.7512 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7345 - accuracy: 0.9681 - val_loss: 0.7502 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 768/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7411 - accuracy: 0.9649 - val_loss: 0.7493 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7350 - accuracy: 0.9712 - val_loss: 0.7484 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7370 - accuracy: 0.9617 - val_loss: 0.7475 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7352 - accuracy: 0.9712 - val_loss: 0.7466 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 772/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7394 - accuracy: 0.9617 - val_loss: 0.7458 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 773/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7348 - accuracy: 0.9681 - val_loss: 0.7449 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7318 - accuracy: 0.9649 - val_loss: 0.7440 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 775/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7257 - accuracy: 0.9617 - val_loss: 0.7430 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7281 - accuracy: 0.9681 - val_loss: 0.7421 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7347 - accuracy: 0.9681 - val_loss: 0.7412 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 778/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7327 - accuracy: 0.9649 - val_loss: 0.7402 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 779/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7323 - accuracy: 0.9649 - val_loss: 0.7393 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 780/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7268 - accuracy: 0.9649 - val_loss: 0.7384 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 781/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7271 - accuracy: 0.9585 - val_loss: 0.7375 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7202 - accuracy: 0.9681 - val_loss: 0.7366 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7183 - accuracy: 0.9649 - val_loss: 0.7356 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7135 - accuracy: 0.9649 - val_loss: 0.7347 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 785/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7214 - accuracy: 0.9649 - val_loss: 0.7339 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 786/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7201 - accuracy: 0.9712 - val_loss: 0.7330 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 787/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7229 - accuracy: 0.9649 - val_loss: 0.7321 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 788/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7201 - accuracy: 0.9681 - val_loss: 0.7312 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 789/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7222 - accuracy: 0.9649 - val_loss: 0.7302 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 790/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7200 - accuracy: 0.9617 - val_loss: 0.7293 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 791/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7159 - accuracy: 0.9617 - val_loss: 0.7284 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 792/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7177 - accuracy: 0.9585 - val_loss: 0.7275 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7058 - accuracy: 0.9712 - val_loss: 0.7264 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 794/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.7056 - accuracy: 0.9744 - val_loss: 0.7255 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 795/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7008 - accuracy: 0.9681 - val_loss: 0.7247 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7081 - accuracy: 0.9681 - val_loss: 0.7238 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 797/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.7098 - accuracy: 0.9681 - val_loss: 0.7229 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7136 - accuracy: 0.9617 - val_loss: 0.7219 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 799/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7148 - accuracy: 0.9681 - val_loss: 0.7211 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 800/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7100 - accuracy: 0.9681 - val_loss: 0.7202 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6994 - accuracy: 0.9681 - val_loss: 0.7194 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6997 - accuracy: 0.9681 - val_loss: 0.7184 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 803/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7062 - accuracy: 0.9681 - val_loss: 0.7175 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7054 - accuracy: 0.9649 - val_loss: 0.7166 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7008 - accuracy: 0.9617 - val_loss: 0.7157 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 806/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.7102 - accuracy: 0.9681 - val_loss: 0.7149 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7032 - accuracy: 0.9649 - val_loss: 0.7140 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7033 - accuracy: 0.9681 - val_loss: 0.7131 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6992 - accuracy: 0.9649 - val_loss: 0.7123 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 810/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.7000 - accuracy: 0.9617 - val_loss: 0.7114 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6925 - accuracy: 0.9649 - val_loss: 0.7105 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 812/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.9712 - val_loss: 0.7097 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 813/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.7007 - accuracy: 0.9681 - val_loss: 0.7088 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 814/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.9585 - val_loss: 0.7079 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 815/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.9712 - val_loss: 0.7070 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 816/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6952 - accuracy: 0.9649 - val_loss: 0.7061 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 817/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.9712 - val_loss: 0.7052 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6904 - accuracy: 0.9585 - val_loss: 0.7043 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 819/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6850 - accuracy: 0.9712 - val_loss: 0.7035 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 820/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.9649 - val_loss: 0.7026 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 821/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6952 - accuracy: 0.9681 - val_loss: 0.7018 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 0.9681 - val_loss: 0.7009 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 823/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.9617 - val_loss: 0.7000 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 824/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6855 - accuracy: 0.9681 - val_loss: 0.6993 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 825/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6883 - accuracy: 0.9585 - val_loss: 0.6985 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 826/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6875 - accuracy: 0.9744 - val_loss: 0.6976 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 827/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6852 - accuracy: 0.9617 - val_loss: 0.6966 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 828/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.9681 - val_loss: 0.6957 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 829/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6816 - accuracy: 0.9712 - val_loss: 0.6950 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6801 - accuracy: 0.9617 - val_loss: 0.6940 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6761 - accuracy: 0.9617 - val_loss: 0.6932 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6828 - accuracy: 0.9712 - val_loss: 0.6924 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 833/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6824 - accuracy: 0.9649 - val_loss: 0.6916 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 834/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6727 - accuracy: 0.9712 - val_loss: 0.6908 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 835/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6709 - accuracy: 0.9681 - val_loss: 0.6899 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 836/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6775 - accuracy: 0.9617 - val_loss: 0.6893 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6745 - accuracy: 0.9617 - val_loss: 0.6885 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6707 - accuracy: 0.9712 - val_loss: 0.6876 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 839/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6706 - accuracy: 0.9744 - val_loss: 0.6868 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 840/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6776 - accuracy: 0.9585 - val_loss: 0.6859 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6640 - accuracy: 0.9617 - val_loss: 0.6851 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6721 - accuracy: 0.9649 - val_loss: 0.6843 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6639 - accuracy: 0.9712 - val_loss: 0.6835 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 844/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6668 - accuracy: 0.9681 - val_loss: 0.6826 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6686 - accuracy: 0.9617 - val_loss: 0.6816 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6750 - accuracy: 0.9649 - val_loss: 0.6808 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6667 - accuracy: 0.9649 - val_loss: 0.6800 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 848/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6611 - accuracy: 0.9649 - val_loss: 0.6792 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 849/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6652 - accuracy: 0.9681 - val_loss: 0.6784 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 850/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6631 - accuracy: 0.9712 - val_loss: 0.6776 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 851/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6605 - accuracy: 0.9712 - val_loss: 0.6767 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6609 - accuracy: 0.9681 - val_loss: 0.6758 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6616 - accuracy: 0.9712 - val_loss: 0.6750 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 854/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6552 - accuracy: 0.9681 - val_loss: 0.6742 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6570 - accuracy: 0.9649 - val_loss: 0.6734 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6522 - accuracy: 0.9744 - val_loss: 0.6726 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 857/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6642 - accuracy: 0.9617 - val_loss: 0.6718 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6524 - accuracy: 0.9649 - val_loss: 0.6711 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 859/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6502 - accuracy: 0.9649 - val_loss: 0.6702 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6526 - accuracy: 0.9712 - val_loss: 0.6694 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 861/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6555 - accuracy: 0.9681 - val_loss: 0.6686 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 862/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6465 - accuracy: 0.9712 - val_loss: 0.6678 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6513 - accuracy: 0.9649 - val_loss: 0.6670 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 864/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6485 - accuracy: 0.9681 - val_loss: 0.6662 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 865/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6483 - accuracy: 0.9681 - val_loss: 0.6654 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6499 - accuracy: 0.9681 - val_loss: 0.6645 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6510 - accuracy: 0.9681 - val_loss: 0.6638 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6507 - accuracy: 0.9681 - val_loss: 0.6630 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 869/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6446 - accuracy: 0.9681 - val_loss: 0.6621 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 870/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6410 - accuracy: 0.9712 - val_loss: 0.6613 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6509 - accuracy: 0.9681 - val_loss: 0.6606 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 872/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6469 - accuracy: 0.9617 - val_loss: 0.6598 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 873/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6440 - accuracy: 0.9681 - val_loss: 0.6590 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6398 - accuracy: 0.9681 - val_loss: 0.6583 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6466 - accuracy: 0.9617 - val_loss: 0.6574 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 876/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6448 - accuracy: 0.9681 - val_loss: 0.6565 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 877/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6411 - accuracy: 0.9681 - val_loss: 0.6557 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6433 - accuracy: 0.9712 - val_loss: 0.6550 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 879/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6331 - accuracy: 0.9649 - val_loss: 0.6543 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6399 - accuracy: 0.9649 - val_loss: 0.6535 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 881/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6295 - accuracy: 0.9744 - val_loss: 0.6527 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6365 - accuracy: 0.9617 - val_loss: 0.6519 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6356 - accuracy: 0.9681 - val_loss: 0.6512 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6391 - accuracy: 0.9681 - val_loss: 0.6504 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6340 - accuracy: 0.9744 - val_loss: 0.6497 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.6299 - accuracy: 0.9681 - val_loss: 0.6489 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6365 - accuracy: 0.9681 - val_loss: 0.6482 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 888/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6327 - accuracy: 0.9681 - val_loss: 0.6474 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 889/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6355 - accuracy: 0.9681 - val_loss: 0.6467 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 890/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6346 - accuracy: 0.9649 - val_loss: 0.6460 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6282 - accuracy: 0.9744 - val_loss: 0.6451 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 892/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6319 - accuracy: 0.9681 - val_loss: 0.6444 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6280 - accuracy: 0.9681 - val_loss: 0.6436 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 894/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6315 - accuracy: 0.9681 - val_loss: 0.6429 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6216 - accuracy: 0.9681 - val_loss: 0.6421 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6301 - accuracy: 0.9681 - val_loss: 0.6413 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 897/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6208 - accuracy: 0.9681 - val_loss: 0.6406 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6290 - accuracy: 0.9649 - val_loss: 0.6398 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 899/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6266 - accuracy: 0.9649 - val_loss: 0.6390 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6215 - accuracy: 0.9681 - val_loss: 0.6383 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6255 - accuracy: 0.9649 - val_loss: 0.6375 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 902/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6209 - accuracy: 0.9681 - val_loss: 0.6368 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 903/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6157 - accuracy: 0.9681 - val_loss: 0.6360 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 904/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6206 - accuracy: 0.9681 - val_loss: 0.6352 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 905/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6213 - accuracy: 0.9712 - val_loss: 0.6345 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6199 - accuracy: 0.9681 - val_loss: 0.6338 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 907/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6227 - accuracy: 0.9712 - val_loss: 0.6331 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 908/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6146 - accuracy: 0.9681 - val_loss: 0.6323 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 909/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6161 - accuracy: 0.9681 - val_loss: 0.6315 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 910/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6185 - accuracy: 0.9681 - val_loss: 0.6308 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6195 - accuracy: 0.9617 - val_loss: 0.6300 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6137 - accuracy: 0.9712 - val_loss: 0.6296 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6162 - accuracy: 0.9681 - val_loss: 0.6288 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6173 - accuracy: 0.9649 - val_loss: 0.6280 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6067 - accuracy: 0.9649 - val_loss: 0.6273 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6146 - accuracy: 0.9617 - val_loss: 0.6266 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5991 - accuracy: 0.9744 - val_loss: 0.6258 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 918/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6052 - accuracy: 0.9712 - val_loss: 0.6251 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6068 - accuracy: 0.9712 - val_loss: 0.6244 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 920/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6031 - accuracy: 0.9681 - val_loss: 0.6236 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 921/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6077 - accuracy: 0.9712 - val_loss: 0.6229 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6087 - accuracy: 0.9681 - val_loss: 0.6223 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 923/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6116 - accuracy: 0.9681 - val_loss: 0.6215 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 924/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6075 - accuracy: 0.9649 - val_loss: 0.6208 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.6049 - accuracy: 0.9712 - val_loss: 0.6200 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6092 - accuracy: 0.9681 - val_loss: 0.6193 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 927/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6012 - accuracy: 0.9681 - val_loss: 0.6185 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.6005 - accuracy: 0.9681 - val_loss: 0.6178 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6048 - accuracy: 0.9681 - val_loss: 0.6170 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 930/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.6030 - accuracy: 0.9681 - val_loss: 0.6163 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 931/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.6015 - accuracy: 0.9649 - val_loss: 0.6156 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5936 - accuracy: 0.9712 - val_loss: 0.6148 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 933/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.6042 - accuracy: 0.9649 - val_loss: 0.6141 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 934/1000\n",
      "40/40 [==============================] - 0s 10ms/step - loss: 0.5986 - accuracy: 0.9681 - val_loss: 0.6135 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5939 - accuracy: 0.9681 - val_loss: 0.6128 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 936/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5972 - accuracy: 0.9681 - val_loss: 0.6120 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 937/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5951 - accuracy: 0.9649 - val_loss: 0.6112 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5904 - accuracy: 0.9712 - val_loss: 0.6105 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5949 - accuracy: 0.9649 - val_loss: 0.6099 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5854 - accuracy: 0.9744 - val_loss: 0.6091 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 941/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5927 - accuracy: 0.9649 - val_loss: 0.6085 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 942/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5876 - accuracy: 0.9712 - val_loss: 0.6077 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 943/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5892 - accuracy: 0.9681 - val_loss: 0.6070 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 944/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5873 - accuracy: 0.9681 - val_loss: 0.6063 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 945/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5860 - accuracy: 0.9681 - val_loss: 0.6056 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 946/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5901 - accuracy: 0.9649 - val_loss: 0.6049 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 947/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5850 - accuracy: 0.9649 - val_loss: 0.6042 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 948/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5896 - accuracy: 0.9712 - val_loss: 0.6035 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5866 - accuracy: 0.9681 - val_loss: 0.6029 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5902 - accuracy: 0.9712 - val_loss: 0.6022 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5808 - accuracy: 0.9681 - val_loss: 0.6015 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 952/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5819 - accuracy: 0.9712 - val_loss: 0.6008 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 953/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5880 - accuracy: 0.9649 - val_loss: 0.6001 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 954/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5865 - accuracy: 0.9712 - val_loss: 0.5994 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 955/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5892 - accuracy: 0.9649 - val_loss: 0.5986 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5869 - accuracy: 0.9681 - val_loss: 0.5982 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 957/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5830 - accuracy: 0.9712 - val_loss: 0.5975 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5766 - accuracy: 0.9744 - val_loss: 0.5968 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5812 - accuracy: 0.9681 - val_loss: 0.5962 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5797 - accuracy: 0.9649 - val_loss: 0.5955 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 961/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5712 - accuracy: 0.9681 - val_loss: 0.5948 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5779 - accuracy: 0.9744 - val_loss: 0.5941 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5854 - accuracy: 0.9681 - val_loss: 0.5934 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5785 - accuracy: 0.9712 - val_loss: 0.5927 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5771 - accuracy: 0.9712 - val_loss: 0.5919 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 966/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5684 - accuracy: 0.9712 - val_loss: 0.5912 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5747 - accuracy: 0.9681 - val_loss: 0.5906 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 968/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5671 - accuracy: 0.9712 - val_loss: 0.5898 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5748 - accuracy: 0.9681 - val_loss: 0.5892 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 970/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5727 - accuracy: 0.9681 - val_loss: 0.5885 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 971/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5753 - accuracy: 0.9649 - val_loss: 0.5883 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5632 - accuracy: 0.9712 - val_loss: 0.5874 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5768 - accuracy: 0.9712 - val_loss: 0.5867 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5727 - accuracy: 0.9712 - val_loss: 0.5861 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 975/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5714 - accuracy: 0.9681 - val_loss: 0.5855 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5667 - accuracy: 0.9649 - val_loss: 0.5845 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 977/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5697 - accuracy: 0.9681 - val_loss: 0.5838 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 978/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5775 - accuracy: 0.9681 - val_loss: 0.5834 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 979/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5623 - accuracy: 0.9681 - val_loss: 0.5828 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5627 - accuracy: 0.9617 - val_loss: 0.5821 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5663 - accuracy: 0.9712 - val_loss: 0.5814 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 982/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5624 - accuracy: 0.9712 - val_loss: 0.5808 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 983/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5569 - accuracy: 0.9681 - val_loss: 0.5800 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5647 - accuracy: 0.9712 - val_loss: 0.5793 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 985/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5643 - accuracy: 0.9681 - val_loss: 0.5786 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "40/40 [==============================] - 0s 9ms/step - loss: 0.5713 - accuracy: 0.9712 - val_loss: 0.5779 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 987/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5574 - accuracy: 0.9744 - val_loss: 0.5772 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5560 - accuracy: 0.9712 - val_loss: 0.5766 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5488 - accuracy: 0.9744 - val_loss: 0.5759 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 990/1000\n",
      "40/40 [==============================] - 0s 11ms/step - loss: 0.5591 - accuracy: 0.9649 - val_loss: 0.5753 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5620 - accuracy: 0.9712 - val_loss: 0.5747 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5632 - accuracy: 0.9681 - val_loss: 0.5740 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 993/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5548 - accuracy: 0.9681 - val_loss: 0.5734 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "40/40 [==============================] - 1s 13ms/step - loss: 0.5628 - accuracy: 0.9681 - val_loss: 0.5727 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 995/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5608 - accuracy: 0.9712 - val_loss: 0.5720 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5570 - accuracy: 0.9712 - val_loss: 0.5714 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 997/1000\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.5625 - accuracy: 0.9649 - val_loss: 0.5707 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 998/1000\n",
      "40/40 [==============================] - 0s 8ms/step - loss: 0.5442 - accuracy: 0.9681 - val_loss: 0.5699 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.5513 - accuracy: 0.9776 - val_loss: 0.5692 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.5562 - accuracy: 0.9617 - val_loss: 0.5686 - val_accuracy: 0.9643 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGzCAYAAAD9pBdvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABiHklEQVR4nO3dd3QUZd/G8e/sppNGTQKE0HvvRZqUgAiCDRAVsCsIWJEH5QVUsKACFlAfARVBRJqgIr136S2A9BI6CSF9d94/8rAaaUlIMgm5PufsOezMPTO/mSzZKzP3PWOYpmkiIiIiYhGb1QWIiIhI3qYwIiIiIpZSGBERERFLKYyIiIiIpRRGRERExFIKIyIiImIphRERERGxlMKIiIiIWEphRERERCylMCKSRr169aJkyZIZWnbo0KEYhpG5BeUwhw8fxjAMJk2alK3bXbZsGYZhsGzZMte0tP6ssqrmkiVL0qtXr0xdZ1pMmjQJwzA4fPhwtm9b5HYojEiuZxhGml7//LISuV1r1qxh6NChXLp0yepSRHI9N6sLELld33//far33333HQsXLrxmeqVKlW5rO19//TVOpzNDy7755pu88cYbt7V9Sbvb+Vml1Zo1axg2bBi9evUiMDAw1byIiAhsNv2tJ5JWCiOS6z366KOp3q9bt46FCxdeM/3fYmNj8fHxSfN23N3dM1QfgJubG25u+u+WXW7nZ5UZPD09Ld2+SG6j6C55QosWLahatSp//vknzZo1w8fHh//85z8AzJkzhw4dOlC0aFE8PT0pU6YMb7/9Ng6HI9U6/t0P4Wp/g1GjRvHVV19RpkwZPD09qVevHhs3bky17PX6jBiGQd++fZk9ezZVq1bF09OTKlWqMH/+/GvqX7ZsGXXr1sXLy4syZcrw5ZdfprkfysqVK3nooYcoUaIEnp6ehIaG8tJLLxEXF3fN/vn6+nLixAk6d+6Mr68vhQsX5tVXX73mWFy6dIlevXoREBBAYGAgPXv2TNPlik2bNmEYBt9+++018/744w8Mw2DevHkAHDlyhBdeeIEKFSrg7e1NwYIFeeihh9LUH+J6fUbSWvP27dvp1asXpUuXxsvLi+DgYJ544gnOnz/vajN06FBee+01AEqVKuW6FHi1tuv1GTl48CAPPfQQBQoUwMfHh4YNG/Lrr7+manO1/8tPP/3Eu+++S/HixfHy8qJVq1YcOHDglvt9I1988QVVqlTB09OTokWL0qdPn2v2ff/+/TzwwAMEBwfj5eVF8eLF6datG1FRUa42Cxcu5K677iIwMBBfX18qVKjg+n8kcjv0p5rkGefPn6d9+/Z069aNRx99lKCgICCl05+vry8vv/wyvr6+LFmyhCFDhhAdHc2HH354y/VOmTKFy5cv8+yzz2IYBh988AH3338/Bw8evOVf6KtWrWLmzJm88MIL+Pn5MXbsWB544AGOHj1KwYIFAdiyZQvt2rUjJCSEYcOG4XA4GD58OIULF07Tfk+fPp3Y2Fief/55ChYsyIYNG/j00085fvw406dPT9XW4XAQHh5OgwYNGDVqFIsWLeKjjz6iTJkyPP/88wCYpsl9993HqlWreO6556hUqRKzZs2iZ8+et6ylbt26lC5dmp9++uma9tOmTSN//vyEh4cDsHHjRtasWUO3bt0oXrw4hw8fZty4cbRo0YLdu3en66xWempeuHAhBw8epHfv3gQHB7Nr1y6++uordu3axbp16zAMg/vvv599+/YxdepUPvnkEwoVKgRww5/J6dOnady4MbGxsfTr14+CBQvy7bff0qlTJ37++We6dOmSqv17772HzWbj1VdfJSoqig8++IAePXqwfv36NO/zVUOHDmXYsGG0bt2a559/noiICMaNG8fGjRtZvXo17u7uJCYmEh4eTkJCAi+++CLBwcGcOHGCefPmcenSJQICAti1axf33nsv1atXZ/jw4Xh6enLgwAFWr16d7ppErmGK3GH69Olj/vuj3bx5cxMwx48ff0372NjYa6Y9++yzpo+PjxkfH++a1rNnTzMsLMz1/tChQyZgFixY0Lxw4YJr+pw5c0zAnDt3rmva//3f/11TE2B6eHiYBw4ccE3btm2bCZiffvqpa1rHjh1NHx8f88SJE65p+/fvN93c3K5Z5/Vcb/9GjhxpGoZhHjlyJNX+Aebw4cNTta1Vq5ZZp04d1/vZs2ebgPnBBx+4piUnJ5tNmzY1AXPixIk3rWfQoEGmu7t7qmOWkJBgBgYGmk888cRN6167dq0JmN99951r2tKlS03AXLp0aap9+efPKj01X2+7U6dONQFzxYoVrmkffvihCZiHDh26pn1YWJjZs2dP1/sBAwaYgLly5UrXtMuXL5ulSpUyS5YsaTocjlT7UqlSJTMhIcHVdsyYMSZg7tix45pt/dPEiRNT1XTmzBnTw8PDbNu2rWsbpmman332mQmYEyZMME3TNLds2WIC5vTp02+47k8++cQEzLNnz960BpGM0GUayTM8PT3p3bv3NdO9vb1d/758+TLnzp2jadOmxMbGsnfv3luut2vXruTPn9/1vmnTpkDKaflbad26NWXKlHG9r169Ov7+/q5lHQ4HixYtonPnzhQtWtTVrmzZsrRv3/6W64fU+3flyhXOnTtH48aNMU2TLVu2XNP+ueeeS/W+adOmqfblt99+w83NzXWmBMBut/Piiy+mqZ6uXbuSlJTEzJkzXdMWLFjApUuX6Nq163XrTkpK4vz585QtW5bAwEA2b96cpm1lpOZ/bjc+Pp5z587RsGFDgHRv95/br1+/PnfddZdrmq+vL8888wyHDx9m9+7dqdr37t0bDw8P1/v0fKb+adGiRSQmJjJgwIBUHWqffvpp/P39XZeJAgICgJRLZbGxsddd19VOunPmzMnyzsGS9yiMSJ5RrFixVL/gr9q1axddunQhICAAf39/Chcu7Or8+s/r5TdSokSJVO+vBpOLFy+me9mry19d9syZM8TFxVG2bNlr2l1v2vUcPXqUXr16UaBAAVc/kObNmwPX7p+Xl9c1lxr+WQ+k9OUICQnB19c3VbsKFSqkqZ4aNWpQsWJFpk2b5po2bdo0ChUqxN133+2aFhcXx5AhQwgNDcXT05NChQpRuHBhLl26lKafyz+lp+YLFy7Qv39/goKC8Pb2pnDhwpQqVQpI2+fhRtu/3raujvA6cuRIqum385n693bh2v308PCgdOnSrvmlSpXi5Zdf5r///S+FChUiPDyczz//PNX+du3alSZNmvDUU08RFBREt27d+OmnnxRMJFOoz4jkGf/8i/eqS5cu0bx5c/z9/Rk+fDhlypTBy8uLzZs3M3DgwDT9orXb7dedbppmli6bFg6HgzZt2nDhwgUGDhxIxYoVyZcvHydOnKBXr17X7N+N6slsXbt25d133+XcuXP4+fnxyy+/0L1791Qjjl588UUmTpzIgAEDaNSoEQEBARiGQbdu3bL0C/Dhhx9mzZo1vPbaa9SsWRNfX1+cTift2rXLti/erP5cXM9HH31Er169mDNnDgsWLKBfv36MHDmSdevWUbx4cby9vVmxYgVLly7l119/Zf78+UybNo27776bBQsWZNtnR+5MCiOSpy1btozz588zc+ZMmjVr5pp+6NAhC6v6W5EiRfDy8rruSIq0jK7YsWMH+/bt49tvv+Xxxx93TV+4cGGGawoLC2Px4sXExMSkOtMQERGR5nV07dqVYcOGMWPGDIKCgoiOjqZbt26p2vz888/07NmTjz76yDUtPj4+QzcZS2vNFy9eZPHixQwbNowhQ4a4pu/fv/+adabnjrphYWHXPT5XLwOGhYWleV3pcXW9ERERlC5d2jU9MTGRQ4cO0bp161Ttq1WrRrVq1XjzzTdZs2YNTZo0Yfz48bzzzjsA2Gw2WrVqRatWrfj4448ZMWIEgwcPZunSpdesSyQ9dJlG8rSrf8398y/OxMREvvjiC6tKSsVut9O6dWtmz57NyZMnXdMPHDjA77//nqblIfX+mabJmDFjMlzTPffcQ3JyMuPGjXNNczgcfPrpp2leR6VKlahWrRrTpk1j2rRphISEpAqDV2v/95mATz/99JphxplZ8/WOF8Do0aOvWWe+fPkA0hSO7rnnHjZs2MDatWtd065cucJXX31FyZIlqVy5clp3JV1at26Nh4cHY8eOTbVP33zzDVFRUXTo0AGA6OhokpOTUy1brVo1bDYbCQkJQMrlq3+rWbMmgKuNSEbpzIjkaY0bNyZ//vz07NmTfv36YRgG33//fZaeDk+voUOHsmDBApo0acLzzz+Pw+Hgs88+o2rVqmzduvWmy1asWJEyZcrw6quvcuLECfz9/ZkxY0a6+x78U8eOHWnSpAlvvPEGhw8fpnLlysycOTPd/Sm6du3KkCFD8PLy4sknn7zmjqX33nsv33//PQEBAVSuXJm1a9eyaNEi15DnrKjZ39+fZs2a8cEHH5CUlESxYsVYsGDBdc+U1alTB4DBgwfTrVs33N3d6dixoyuk/NMbb7zB1KlTad++Pf369aNAgQJ8++23HDp0iBkzZmTZ3VoLFy7MoEGDGDZsGO3ataNTp05ERETwxRdfUK9ePVffqCVLltC3b18eeughypcvT3JyMt9//z12u50HHngAgOHDh7NixQo6dOhAWFgYZ86c4YsvvqB48eKpOuaKZITCiORpBQsWZN68ebzyyiu8+eab5M+fn0cffZRWrVq57ndhtTp16vD777/z6quv8tZbbxEaGsrw4cPZs2fPLUf7uLu7M3fuXNf1fy8vL7p06ULfvn2pUaNGhuqx2Wz88ssvDBgwgMmTJ2MYBp06deKjjz6iVq1aaV5P165defPNN4mNjU01iuaqMWPGYLfb+eGHH4iPj6dJkyYsWrQoQz+X9NQ8ZcoUXnzxRT7//HNM06Rt27b8/vvvqUYzAdSrV4+3336b8ePHM3/+fJxOJ4cOHbpuGAkKCmLNmjUMHDiQTz/9lPj4eKpXr87cuXNdZyeyytChQylcuDCfffYZL730EgUKFOCZZ55hxIgRrvvg1KhRg/DwcObOncuJEyfw8fGhRo0a/P77766RRJ06deLw4cNMmDCBc+fOUahQIZo3b86wYcNco3FEMsowc9KfgCKSZp07d2bXrl3X7c8gIpKbqM+ISC7w71u379+/n99++40WLVpYU5CISCbSmRGRXCAkJMT1vJQjR44wbtw4EhIS2LJlC+XKlbO6PBGR26I+IyK5QLt27Zg6dSqRkZF4enrSqFEjRowYoSAiIncEnRkRERERS6nPiIiIiFhKYUREREQslSv6jDidTk6ePImfn1+6bsEsIiIi1jFNk8uXL1O0aNGb3twvV4SRkydPEhoaanUZIiIikgHHjh2jePHiN5yfK8KIn58fkLIz/v7+FlcjIiIiaREdHU1oaKjre/xGckUYuXppxt/fX2FEREQkl7lVFwt1YBURERFLKYyIiIiIpRRGRERExFK5os+IiIhkHtM0SU5OxuFwWF2K5HJ2ux03N7fbvu2GwoiISB6SmJjIqVOniI2NtboUuUP4+PgQEhKCh4dHhtehMCIikkc4nU4OHTqE3W6naNGieHh46EaSkmGmaZKYmMjZs2c5dOgQ5cqVu+mNzW5GYUREJI9ITEzE6XQSGhqKj4+P1eXIHcDb2xt3d3eOHDlCYmIiXl5eGVqPOrCKiOQxGf3rVeR6MuPzpE+kiIiIWEphRERERCylMCIiInlOyZIlGT16dJrbL1u2DMMwuHTpUpbVBDBp0iQCAwOzdBs5kcKIiIjkWIZh3PQ1dOjQDK1348aNPPPMM2lu37hxY06dOkVAQECGtic3l6dH00xed4Q/j1zk1fAKFAv0trocERH5l1OnTrn+PW3aNIYMGUJERIRrmq+vr+vfpmnicDhwc7v1V1vhwoXTVYeHhwfBwcHpWkbSLk+fGflp0zFmbTnBlqMXrS5FRCTbmaZJbGKyJS/TNNNUY3BwsOsVEBCAYRiu93v37sXPz4/ff/+dOnXq4OnpyapVq/jrr7+47777CAoKwtfXl3r16rFo0aJU6/33ZRrDMPjvf/9Lly5d8PHxoVy5cvzyyy+u+f++THP1csoff/xBpUqV8PX1pV27dqnCU3JyMv369SMwMJCCBQsycOBAevbsSefOndP1cxo3bhxlypTBw8ODChUq8P3336f6GQ4dOpQSJUrg6elJ0aJF6devn2v+F198Qbly5fDy8iIoKIgHH3wwXdvOLnn3zIhp8rDvdh50W8L+wwWhelGrKxIRyVZxSQ4qD/nDkm3vHh6Oj0fmfAW98cYbjBo1itKlS5M/f36OHTvGPffcw7vvvounpyffffcdHTt2JCIighIlStxwPcOGDeODDz7gww8/5NNPP6VHjx4cOXKEAgUKXLd9bGwso0aN4vvvv8dms/Hoo4/y6quv8sMPPwDw/vvv88MPPzBx4kQqVarEmDFjmD17Ni1btkzzvs2aNYv+/fszevRoWrduzbx58+jduzfFixenZcuWzJgxg08++YQff/yRKlWqEBkZybZt2wDYtGkT/fr14/vvv6dx48ZcuHCBlStXpuPIZp+8G0YMg47n/kuA2198cqg+0NDqikREJAOGDx9OmzZtXO8LFChAjRo1XO/ffvttZs2axS+//ELfvn1vuJ5evXrRvXt3AEaMGMHYsWPZsGED7dq1u277pKQkxo8fT5kyZQDo27cvw4cPd83/9NNPGTRoEF26dAHgs88+47fffkvXvo0aNYpevXrxwgsvAPDyyy+zbt06Ro0aRcuWLTl69CjBwcG0bt0ad3d3SpQoQf369QE4evQo+fLl495778XPz4+wsDBq1aqVru1nl7wbRoDkUi1gx1+EXliL02lis+m2yCKSd3i729k9PNyybWeWunXrpnofExPD0KFD+fXXXzl16hTJycnExcVx9OjRm66nevXqrn/ny5cPf39/zpw5c8P2Pj4+riACEBIS4mofFRXF6dOnXcEAUh4qV6dOHZxOZ5r3bc+ePdd0tG3SpAljxowB4KGHHmL06NGULl2adu3acc8999CxY0fc3Nxo06YNYWFhrnnt2rVzXYbKafJ0n5HAqilptyHb2X0yyuJqRESyl2EY+Hi4WfLKzGfi5MuXL9X7V199lVmzZjFixAhWrlzJ1q1bqVatGomJiTddj7u7+zXH52bB4Xrt09oXJrOEhoYSERHBF198gbe3Ny+88ALNmjUjKSkJPz8/Nm/ezNSpUwkJCWHIkCHUqFEjy4cnZ0SeDiP2UneRhDvFjXPMX7HK6nJERCQTrF69ml69etGlSxeqVatGcHAwhw8fztYaAgICCAoKYuPGja5pDoeDzZs3p2s9lSpVYvXq1ammrV69msqVK7vee3t707FjR8aOHcuyZctYu3YtO3bsAMDNzY3WrVvzwQcfsH37dg4fPsySJUtuY8+yRp6+TIOHD9FF6lLwzFp8ji4D7rW6IhERuU3lypVj5syZdOzYEcMweOutt9J1aSSzvPjii4wcOZKyZctSsWJFPv30Uy5evJius0KvvfYaDz/8MLVq1aJ169bMnTuXmTNnukYHTZo0CYfDQYMGDfDx8WHy5Ml4e3sTFhbGvHnzOHjwIM2aNSN//vz89ttvOJ1OKlSokFW7nGF5+swIgHuF1gBUjN1EfJLD4mpEROR2ffzxx+TPn5/GjRvTsWNHwsPDqV27drbXMXDgQLp3787jjz9Oo0aN8PX1JTw8PF1Ptu3cuTNjxoxh1KhRVKlShS+//JKJEyfSokULAAIDA/n6669p0qQJ1atXZ9GiRcydO5eCBQsSGBjIzJkzufvuu6lUqRLjx49n6tSpVKlSJYv2OOMMM7svcGVAdHQ0AQEBREVF4e/vn6nrNk9tx/iyKbGmJ1u6b6FJxWKZun4RkZwiPj6eQ4cOUapUqQw/6l0yzul0UqlSJR5++GHefvttq8vJNDf7XKX1+zvPnxkxgqtx2a0APkYCERsX3XoBERGRNDhy5Ahff/01+/btY8eOHTz//PMcOnSIRx55xOrScpw8H0YwDGKLNwPA+dcSkh3Zf11RRETuPDabjUmTJlGvXj2aNGnCjh07WLRoEZUqVbK6tBwnb3dg/Z/CNdvD4dk0dG7l4LkrlA/ys7okERHJ5UJDQ68ZCSPXpzMjgK3s3QBUtR0m4sABi6sRERHJWxRGAHyLcCpfymmzU5vmWlyMiIhI3qIw8j8+Ve4BoMT5lcQmJltcjYiISN6hMPI/ATU6ANDE2MG2wzd+FoGIiIhkLoWRq0JqEW3Pj58Rx5Eti62uRkREJM9QGLnKZuNSsZYAuP210OJiRERE8g6FkX8oWLsjALXj13PwbIzF1YiISGZp0aIFAwYMcL0vWbIko0ePvukyhmEwe/bs2952Zq3nZoYOHUrNmjWzdBtZSWHkH/JVbE0ybpS2RfL7co0NFxGxWseOHWnXrt11561cuRLDMNi+fXu617tx40aeeeaZ2y0vlRsFglOnTtG+fftM3dadJl1hZOTIkdSrVw8/Pz+KFClC586diYiIuOkykyZNwjCMVK8c+0wEL3+iCtdN+ff+P6ytRUREePLJJ1m4cCHHjx+/Zt7EiROpW7cu1atXT/d6CxcujI+PT2aUeEvBwcF4enpmy7Zyq3SFkeXLl9OnTx/WrVvHwoULSUpKom3btly5cuWmy/n7+3Pq1CnX68iRI7dVdFby+t8Q3+qx64mKS7K4GhGRLGSakHjFmlcan9F67733UrhwYSZNmpRqekxMDNOnT+fJJ5/k/PnzdO/enWLFiuHj40O1atWYOnXqTdf778s0+/fvp1mzZnh5eVG5cmUWLry27+DAgQMpX748Pj4+lC5dmrfeeoukpJTviUmTJjFs2DC2bdvm+sP7as3/vkyzY8cO7r77bry9vSlYsCDPPPMMMTF/dw3o1asXnTt3ZtSoUYSEhFCwYEH69Onj2lZaOJ1Ohg8fTvHixfH09KRmzZrMnz/fNT8xMZG+ffsSEhKCl5cXYWFhjBw5EgDTNBk6dCglSpTA09OTokWL0q9fvzRvOyPSdTv4f+4IpBz8IkWK8Oeff9KsWbMbLmcYBsHBwRmrMJvlq9oBlg2hgW0Pczbv56Emla0uSUQkayTFwoii1mz7PyfBI98tm7m5ufH4448zadIkBg8ejGEYAEyfPh2Hw0H37t2JiYmhTp06DBw4EH9/f3799Vcee+wxypQpQ/369W+5DafTyf33309QUBDr168nKioqVf+Sq/z8/Jg0aRJFixZlx44dPP300/j5+fH666/TtWtXdu7cyfz581m0KOWhqwEBAdes48qVK4SHh9OoUSM2btzImTNneOqpp+jbt2+qwLV06VJCQkJYunQpBw4coGvXrtSsWZOnn376lvsDMGbMGD766CO+/PJLatWqxYQJE+jUqRO7du2iXLlyjB07ll9++YWffvqJEiVKcOzYMY4dOwbAjBkz+OSTT/jxxx+pUqUKkZGRbNu2LU3bzajbejZNVFQUAAUKFLhpu5iYGMLCwnA6ndSuXZsRI0ZQpUqVG7ZPSEggISHB9T46Ovp2ykyfQmW55F2CwLijnN78KyiMiIhY6oknnuDDDz9k+fLltGjRAki5RPPAAw8QEBBAQEAAr776qqv9iy++yB9//MFPP/2UpjCyaNEi9u7dyx9//EHRoinhbMSIEdf083jzzTdd/y5ZsiSvvvoqP/74I6+//jre3t74+vri5uZ20z++p0yZQnx8PN999x358qWEsc8++4yOHTvy/vvvExQUBED+/Pn57LPPsNvtVKxYkQ4dOrB48eI0h5FRo0YxcOBAunXrBsD777/P0qVLGT16NJ9//jlHjx6lXLly3HXXXRiGQVhYmGvZo0ePEhwcTOvWrXF3d6dEiRJpOo63I8NhxOl0MmDAAJo0aULVqlVv2K5ChQpMmDCB6tWrExUVxahRo2jcuDG7du2iePHi111m5MiRDBs2LKOl3bbkcvfA9vGUPreEJMcruNvVz1dE7kDuPilnKKzadhpVrFiRxo0bM2HCBFq0aMGBAwdYuXIlw4cPB8DhcDBixAh++uknTpw4QWJiIgkJCWnuE7Jnzx5CQ0NdQQSgUaNG17SbNm0aY8eO5a+//iImJobk5GT8/f3TvB9Xt1WjRg1XEAFo0qQJTqeTiIgIVxipUqUKdrvd1SYkJIQdO3akaRvR0dGcPHmSJk2apJrepEkT1xmOXr160aZNGypUqEC7du249957adu2LQAPPfQQo0ePpnTp0rRr14577rmHjh074uaWdc/WzfC3bJ8+fdi5cyc//vjjTds1atSIxx9/nJo1a9K8eXNmzpxJ4cKF+fLLL2+4zKBBg4iKinK9rp46yi4F6twPQFO2sGrviWzdtohItjGMlEslVrz+d7klrZ588klmzJjB5cuXmThxImXKlKF58+YAfPjhh4wZM4aBAweydOlStm7dSnh4OImJiZl2qNauXUuPHj245557mDdvHlu2bGHw4MGZuo1/cnd3T/XeMAycTmemrb927docOnSIt99+m7i4OB5++GEefPBBIOVpwxEREXzxxRd4e3vzwgsv0KxZs3T1WUmvDIWRvn37Mm/ePJYuXXrDsxs34u7uTq1atThwk6fjenp64u/vn+qVnWyh9bjsXgg/I47Dm+bfegEREclSDz/8MDabjSlTpvDdd9/xxBNPuPqPrF69mvvuu49HH32UGjVqULp0afbt25fmdVeqVIljx45x6tQp17R169alarNmzRrCwsIYPHgwdevWpVy5ctcMxvDw8MDhcNxyW9u2bUs18GP16tXYbDYqVKiQ5ppvxt/fn6JFi7J6depbVKxevZrKlSunate1a1e+/vprpk2bxowZM7hw4QIA3t7edOzYkbFjx7Js2TLWrl2b5jMzGZGuMGKaJn379mXWrFksWbKEUqVKpXuDDoeDHTt2EBISku5ls43NxsXQ1gAUOaG7sYqIWM3X15euXbsyaNAgTp06Ra9evVzzypUrx8KFC1mzZg179uzh2Wef5fTp02led+vWrSlfvjw9e/Zk27ZtrFy5ksGDB6dqU65cOY4ePcqPP/7IX3/9xdixY5k1a1aqNiVLluTQoUNs3bqVc+fOper7eFWPHj3w8vKiZ8+e7Ny5k6VLl/Liiy/y2GOPuS7RZIbXXnuN999/n2nTphEREcEbb7zB1q1b6d+/PwAff/wxU6dOZe/evezbt4/p06cTHBxMYGAgkyZN4ptvvmHnzp0cPHiQyZMn4+3tnapfSWZLVxjp06cPkydPZsqUKfj5+REZGUlkZCRxcXGuNo8//jiDBg1yvR8+fDgLFizg4MGDbN68mUcffZQjR47w1FNPZd5eZAH/Wl0AqJ+wjqgr8RZXIyIiTz75JBcvXiQ8PDxV/44333yT2rVrEx4eTosWLQgODqZz585pXq/NZmPWrFnExcVRv359nnrqKd59991UbTp16sRLL71E3759qVmzJmvWrOGtt95K1eaBBx6gXbt2tGzZksKFC193eLGPjw9//PEHFy5coF69ejz44IO0atWKzz77LH0H4xb69evHyy+/zCuvvEK1atWYP38+v/zyC+XKlQNSRgZ98MEH1K1bl3r16nH48GF+++03bDYbgYGBfP311zRp0oTq1auzaNEi5s6dS8GCBTO1xn8yTDONg73BdUrs3yZOnOhKqS1atKBkyZKuIUovvfQSM2fOJDIykvz581OnTh3eeecdatWqleYio6OjCQgIICoqKvsu2SQncvmdkvhxhbeDPuGt55/Inu2KiGSR+Ph4Dh06RKlSpXLuzScl17nZ5yqt39/p6hqbltyybNmyVO8/+eQTPvnkk/RsJmdw8yDCvzF1oxdS7NRiHM7e2G3p63AlIiIit6YxqzdRpXUPAFqxgSPn9OA8ERGRrKAwchPeFduSgAdhtjMc2r3B6nJERETuSAojN+ORj6P5GwIQt32OxcWIiIjcmRRGbsG9aicAyl9YitOZ5r6+IiI5VjrGLYjcUmZ8nhRGbqFYgwdINO2U5yiLVqywuhwRkQy7elfP2NhYiyuRO8nVz9O/7xqbHll3o/k7hLtvAfb416fS5bWcWTcVWjS3uiQRkQyx2+0EBgZy5swZIOWeFze6ZYPIrZimSWxsLGfOnCEwMDDVs3TSS2EkDYrf1QN+X0vD2OWcjoojKMDb6pJERDLk6hNlrwYSkdsVGBh40ycVp4XCSBr41biPxN9foqztJGt2rieoSQurSxIRyRDDMAgJCaFIkSJZ+uAzyRvc3d1v64zIVQojaeHlzz6/hlS9vBJj1yxQGBGRXM5ut2fKl4hIZlAH1jSKK38fAMVPzic+MdniakRERO4cCiNpVKNVV+LxIJRIlizVk3xFREQyi8JIGnn4+HMqKGUkzeWNUzROX0REJJMojKRDSNOeALRMWsGJC5ctrkZEROTOoDCSDl4Vw4ky/CliXGLL0llWlyMiInJHUBhJDzcPToV2AMB91zTdHl5ERCQTKIykU5k2zwDQwrmBvUeOW1yNiIhI7qcwkk7uxWtx0iMMLyOJPYsnW12OiIhIrqcwkl6GQVKVrgCUPD6HxGSnxQWJiIjkbgojGVC8eS+cGNRhD1u2b7G6HBERkVxNYSQD7IHFOOhXF4CodbpUIyIicjsURjLIrN4dgIpnfiUhSbeHFxERySiFkQwq07QrV/CiBKfZtXa+1eWIiIjkWgojGWTz8mV7YGsAzq/4SvccERERySCFkdtQos0LADRLWsOhY8csrkZERCR3Uhi5DcUqN+awW2k8jSQurP3e6nJERERyJYWR22EYHCv9MACF908DPclXREQk3RRGblPl8KeIMz0o6TjCpf1rrC5HREQk11EYuU0FCxZmhcddAGyZNdraYkRERHIhhZFMEF25BwANYpfjiL1kbTEiIiK5jMJIJri/0/3sN4vjYyRweMlEq8sRERHJVRRGMoHdbmNL0P0AeG75Rh1ZRURE0kFhJJM0vv9FLpveFHcc4+KuBVaXIyIikmsojGSS4sFFWOmTckfWM4s+s7gaERGR3ENhJBMVursvAGUvruTs0X0WVyMiIpI7KIxkonp1G7DdoxZ2wyTi1zFWlyMiIpIrKIxkIsMwsDd4BoCqkbOJvxJtcUUiIiI5n8JIJqvc4iGOE0SgEcORRV9aXY6IiEiOpzCSyQy7O1tLPA5A/m1fkpAQb3FFIiIiOZvCSBaods/znDUDKOI8y8QvR2HqviMiIiI3pDCSBcKCC3KobE8A7j43hZX7zlhckYiISM6lMJJF6j/0KvG2fJS3nWDb4mlWlyMiIpJjKYxkFa8ALlZJ6TvS4uz3ukW8iIjIDSiMZCH/Fi+SYLpTzdzHikVzrC5HREQkR1IYyUL5ChZjc4F7ADBWjSYx2WlxRSIiIjmPwkgWq99jKA7ToKmxhZMRG60uR0REJMdRGMli9kKlWeXVDIDkFR9bXI2IiEjOozCSDTYXTxnmWyryD84f22txNSIiIjmLwkg26NK+HWuMlAfo7f75XavLERERyVEURrJByUL5CO7wHwDqX/qdjTv3WFyRiIhIzqEwkk1K12nDEZ9qeBpJnPjlXY2sERER+R+FkexiGHi1HQJA+4Tf2bpju8UFiYiI5AwKI9koqGZb9nrXxtNIJm7RCBKSHVaXJCIiYjmFkWy2q1J/AO6KWcCUXxdbXI2IiIj1FEayWYu727OEutgNk9I7PsHUM2tERCSPUxjJZgV9Panb+yOcpkHz5DUM+Pgbq0sSERGxlMKIBfzDarIxoC0Aj0R9zdnoeIsrEhERsY7CiEVq9fqIeNOdBra97F3+o9XliIiIWEZhxCIeBULZGfYYAMU3vcfZSzEWVyQiImINhRELle3yJudNf0oZp/j563e4cCXR6pJERESyncKIhQLzF2RrmecAeCTmWz6fu8biikRERLKfwojFyt/zIjucJQkwYqkX8SEOp4b6iohI3qIwYrHQQv6UeeIbHKZBO3MVi+ZOsbokERGRbKUwkgP4lKzLjuLdAKj05/+xavdRiysSERHJPgojOUTYgyM4aRaghO0s26cMtrocERGRbKMwkkPkz1+ACf4vAPC0/VfOH9xscUUiIiLZQ2EkB3nyqb7Md9TD3XBw/LtniU9MsrokERGRLKcwkoOEBHhzqN5bXDa9qcE+Fk9+n2SH0+qyREREspTCSA7Ts10TvrB1B6Dpkc/55ve1FlckIiKStRRGchgfDzdeG/whhzzK42/EUm7Lu5im7j0iIiJ3rnSFkZEjR1KvXj38/PwoUqQInTt3JiIi4pbLTZ8+nYoVK+Ll5UW1atX47bffMlxwXmBzcyO4x5ckmzbudqxm0qSvrC5JREQky6QrjCxfvpw+ffqwbt06Fi5cSFJSEm3btuXKlSs3XGbNmjV0796dJ598ki1bttC5c2c6d+7Mzp07b7v4O5l3WG2WBD4AQJvDH7DtrxMWVyQiIpI1DPM2rgGcPXuWIkWKsHz5cpo1a3bdNl27duXKlSvMmzfPNa1hw4bUrFmT8ePHp2k70dHRBAQEEBUVhb+/f0bLzXVOnTmH4/MGFDfOMcvnAbq8PsHqkkRERNIsrd/ft9VnJCoqCoACBQrcsM3atWtp3bp1qmnh4eGsXXvjjpkJCQlER0eneuVFIUUKcbDeUAA6XpnF8mULrS1IREQkC2Q4jDidTgYMGECTJk2oWrXqDdtFRkYSFBSUalpQUBCRkZE3XGbkyJEEBAS4XqGhoRktM9er3OJh5jka4mY4KbakH0u2H7a6JBERkUyV4TDSp08fdu7cyY8//piZ9QAwaNAgoqKiXK9jx45l+jZyi0K+ntz9yvecNgMpazuJY8FbVpckIiKSqTIURvr27cu8efNYunQpxYsXv2nb4OBgTp8+nWra6dOnCQ4OvuEynp6e+Pv7p3rlZT6BRTjUZBQAbWJ+YcfS6RZXJCIiknnSFUZM06Rv377MmjWLJUuWUKpUqVsu06hRIxYvXpxq2sKFC2nUqFH6Ks3jqjbvwoTkdgAEL3uZE8f1ZF8REbkzpCuM9OnTh8mTJzNlyhT8/PyIjIwkMjKSuLg4V5vHH3+cQYMGud7379+f+fPn89FHH7F3716GDh3Kpk2b6Nu3b+btRR7g6+lGUsu3iHAWp7ARzV/fPIHp1K3iRUQk90tXGBk3bhxRUVG0aNGCkJAQ12vatGmuNkePHuXUqVOu940bN2bKlCl89dVX1KhRg59//pnZs2fftNOrXN+zrapyuvVnJJhuNDM3snXOWKtLEhERuW23dZ+R7JJX7zNyIz+NfZ2HL3xJHJ7Yn1+JR1AFq0sSERG5RrbcZ0SsEf7k26w1q+JNAn99/gD1h/7C8YuxVpclIiKSIQojuVBAPk+u3DuOs2YAlWzHeCl5Ap8v/cvqskRERDJEYSSXal2vOnsbf4zTNOjutpTA/TO5FJtodVkiIiLppjCSizUNf5DzdV8C4MXYz3ng7UnsOhllcVUiIiLpozCSyxW6ZzDb3GviYyTwhfsYvl+x1+qSRERE0kVhJJcz7G54dZ3AGTOQCrbjNNo1lO/WHLK6LBERkTRTGLkDVChbhuT7J5Bk2rnPvoajv37I5HVHrC5LREQkTRRG7hBFa7QivtXbAAxym8K6RT+T5NAdWkVEJOdTGLmD+DV9AUf17tgNk7eTPmb5+k1WlyQiInJLCiN3EsPA3nE0p/JVIr8RQ9H5T7HrSKTVVYmIiNyUwsidxt2LQ3d/yVnTn8q2Ixz6phfbj120uioREZEbUhi5A9WqVpVPAgeTZNq517aWpV++zI7juv+IiIjkTAojdyBvDzsjXnqOE3eNAKC/20wmfDGSLUd1hkRERHIehZE7WMk2zxFR7ikA3nf/ipHjvuGvszEWVyUiIpKawsgdrkL3D4ks1hYPw8GXHp/w6viZJGvIr4iI5CAKI3c6mw33B79iq7MM+Y0YPkp6lwETl1hdlYiIiIvCSB5QMH9+8vX6ieNmIUrbInn06JvExcVZXZaIiAigMJJnlCtdllMdvuOy6U1D2x72ft0b06nLNSIiYj2FkTykXv0mrK/7EcmmjVoXfmfvT29ZXZKIiIjCSF7T6t5HmF/iFQAq7f2MiWOGcODMZYurEhGRvExhJI8xDIM2PQcxxasrAI9fGMuHo0cpkIiIiGUURvIgTzc77ft+ygr/DtgNk7Hun/Hm6PHEJiZbXZqIiORBCiN5VH5fT5r2/47F1MfTSOYr94948P++ZMRve3A6TavLExGRPERhJA8z7G74dJ/EOmcl/I04vvV4n/kr1/Lzn8etLk1ERPIQhZE8rlGFYphdp7LbGUZhI4rv3N/jxyUbcejsiIiIZBOFEaFRlVKcve8HjplFKGk7zbtXhtBu5GziEh1WlyYiInmAwogA0LxONUL7/0G0W0Eq2Y7xYcLb9Pt2OVFxSVaXJiIidziFEflbgdIc7ziVC6YvNW1/8dSxQTz13+Uk6cF6IiKShRRGJJXKNRpw+r6pxNny0cC2lz5nhjNlzX6ryxIRkTuYwohco1LtZnj3nEGyzYsW9m2ELe1HfEKC1WWJiMgdSmFEri+sEcldJ5OIGy2c61j/SXecDnVoFRGRzKcwIjfkVaENG+t+TLJpo3n8YnZ8/TSYGvIrIiKZS2FEbqp++8eYUOQNnKZBjcgZTHirG/N3nLS6LBERuYMojMhNudttPNNnIBMLvgTAE27zOTbtFYbO2alhvyIikikURiRNwh97jSHOpwF42u03QjaOoOv4NZi6bCMiIrdJYUTSpHh+H4YPH8XvJV8H4Fm3X+l8/mt2nYiyuDIREcntFEYkXVo88gZjPJ8F4Dm3uSwf/yIDp28jWTdGExGRDFIYkXTx9rDz/GvvsaT0awD0cfuF4ts+5vcdpyyuTEREciuFEUk3DzcbTXv8h5VlXgHgRbfZJMwfgunU2REREUk/hRHJEHe7jaaPDeHK3e8C8GD8z/ww/FFmbDpmcWUiIpLbKIzIbcnXrC9zQ18F4FF+JXHuy7pTq4iIpIvCiNy2ml1eYZDjWZymQXdjAYtHPULEqSguXEm0ujQREckFFEbktoUW8OH1N95hRtibOEyDNnHz2fnFI9R/Zz4LdkVaXZ6IiORwCiOSKfLn8+DB3q+wuub7JJs2HrCv4nO3Mbw1Y7PVpYmISA6nMCKZxjAMmnZ+hs+L/B8Jphvh9k2MSnqXjRFHrS5NRERyMIURyVSGYdC/zwA8e84kDi+a2ndi/6ELH/+yjrhEdWwVEZFrKYxI1ijdnF1tJ3PR9KW27QD3bHqKsXNWEp+kQCIiIqkpjEiWqdu4DTHd5xDlVpCKtmN03fEML3w+S7eOFxGRVBRGJEuFVqyL7cn5HHUWpqTtNCMuvsqyVSusLktERHIQhRHJcn4h5Vnc+Hv2OYsRbFyk9pIeTPhpptVliYhIDqEwItmid7tGBLywiK3O0hQwYnho1wu8OHIs3609TFRcktXliYiIhRRGJNsEBRcl+uEZrHFUxs+IY1T8cFbM/Y4awxaw80SU1eWJiIhFFEYkWzWrWppGQ5ZwpFALPI0kvnT/mEfsi+k9aSOmaVpdnoiIWEBhRLKd4e5N2PMzcNZ8FLthMsL9Gx6Nm8zL07YqkIiI5EEKI2INuxu2+z7jRI1+APR3m0XDnUMZOXcHTqcCiYhIXqIwItYxDIp1eZvZxV/DYRp0dVtGw40v0vfbVSQm614kIiJ5hcKIWK7zU2+yr+V4Eg1P7rZv5ZlD/fji17VWlyUiItlEYURyhEotuuHeey4xNn9q2g5y35+9Wb1ho9VliYhINlAYkRzDKNEAs/cfnKAwpWynqfDr/Yz57kcio+KtLk1ERLKQwojkKH6hlSk0YAWH3ctSyIjm6b/68e3Ez60uS0REspDCiOQ4noFF8X9hIWuNmvgYCbx28W3WT33X6rJERCSLKIxIjlQgfwFqv/EHU5LvxmaYNIj4gN0Tnsd0JFtdmoiIZDKFEcmxPD29uH/IdH7O/zQAlY9OYcU77Vm79yiXYhMtrk5ERDKLwojkaF4ebtz/4od8EzKEBNOd5uYGfKbcR/g7PxMdrwfsiYjcCRRGJMez2Qx6PDGAuTXHc9H0o4btIDPc32L8T3NJdujmaCIiuZ3CiOQKXu52HuzyIJd6/MZBZzDFjXM8/9cLjBgzloRkh9XliYjIbVAYkVylVPnq+PRZyjqzCn5GHIOjhrJ68jtsO3rR6tJERCSDFEYk1wkOKkqFVxawPvBe7IbJ3Yc/ZsdXT9J/iu7YKiKSGymMSK6U39+XxHs+4Z2kHjhNg0fdFvPQnv7sPniEy/FJGm0jIpKLpDuMrFixgo4dO1K0aFEMw2D27Nk3bb9s2TIMw7jmFRkZmdGaRQBoWr4I7Z5+h6llRhJrenKXfReek8LpNOxbWn20nMsabSMikiukO4xcuXKFGjVq8Pnn6btFd0REBKdOnXK9ihQpkt5Ni1yjbskC9Hj8eXa2n84JsyBlbKeY4/EWVeM2suXoJavLExGRNHBL7wLt27enffv26d5QkSJFCAwMTPdyImlRr0EzPj39HY3/fIm6tn1McP+A9789yuEOA2lYphDlg/ysLlFERG4g2/qM1KxZk5CQENq0acPq1atv2jYhIYHo6OhUL5GbMQyDfvfdxerGE/gxuQV2w+Q/7lPx//0FOo1exK6TUVaXKCIiN5DlYSQkJITx48czY8YMZsyYQWhoKC1atGDz5s03XGbkyJEEBAS4XqGhoVldptwh+rerRpOXpjApsC/Jpo3O9jX85D6MTdt2WF2aiIjcgGGappnhhQ2DWbNm0blz53Qt17x5c0qUKMH3339/3fkJCQkkJCS43kdHRxMaGkpUVBT+/v4ZLVfymG7/+ZAv3EdTwIjhrOnPmjqjcSvZmPZVg7HZDKvLExG540VHRxMQEHDL729LhvbWr1+fAwcO3HC+p6cn/v7+qV4i6dXrkUd52vMj9jhLUNiIpv2fT7Ny2iim/3nM6tJEROQfLAkjW7duJSQkxIpNSx7SrmoIM/7Tje3h05nnaICH4eA99/8SP+dlnp20ltjEZKtLFBERMjCaJiYmJtVZjUOHDrF161YKFChAiRIlGDRoECdOnOC7774DYPTo0ZQqVYoqVaoQHx/Pf//7X5YsWcKCBQsyby9EbuLBRhV4/fgH7N7+Oa+6Taen20IqHDzOoo3j6NSkltXliYjkeekOI5s2baJly5au9y+//DIAPXv2ZNKkSZw6dYqjR4+65icmJvLKK69w4sQJfHx8qF69OosWLUq1DpGsZLcZfNS1Jo6HvuL7b2tz/+FhNLTt4fSCLnzx17vUb9aeuiULWF2miEiedVsdWLNLWjvAiKTFklWrCF3wDOVsJ0gy7XxgPsozr35AYX8vq0sTEbmj5OgOrCJWuvuuuwjsv5KDQeG4Gw4G275l7YedeX/OJpzOHJ/NRUTuOAojkicVLliQ0s9N40DtN0ky7XSyr6XLn4/TZvDXfLQggmSH0+oSRUTyDIURybsMg7KdXmNji++JNPNT3naCOR5vcWDZD3y4IILdJ3XnXxGR7KAwInle45Yd+KrSRNY6KuNrxDPOYwyFVg+n09ilzN95yuryRETueAojIkCfexszo8qnbA7tBcDTbr8x1eMdPvp5ibWFiYjkAQojIkBBX09GdatL7SfH0J9XiTa9qWfbx0/m60yf8l8Sk9WHREQkqyiMiPxLySZduTdxBNudpchvxPDQvldY9tlzJCUm3HphERFJN4URkX/p16ocfR9ow/muc5mYHA5A20vT2P5OEw4d2GtxdSIidx6FEZF/sdsMHq4bSssqoZR89DO+KDKUaNOHOrb9FPqhFef/nE0uuFegiEiuoTAichMtKxbhhRdeYnHzn9nqLI2fGUPBuT2Z9V5PEhLirC5PROSOoDAikgZd7m6Cs9fvzPToBMD9CXM4/GEzEs8esrgyEZHcT2FEJI1qlw7mvje+Y3rZ94kyfaiQvI+EzxuzevaXHDwbo0s3IiIZpDAikg52m8FDjz7Hn+3nstlZFj9iabL1dbaPfYgJi7ZZXZ6ISK6kMCKSAXc3rIvfcwuZnq8HyaaNzvY1tF/Vhdi9i60uTUQk11EYEcmgckUL8OCrnzMm7FMOO4MoalzA58f7+ePjJxm/eBfxSQ6rSxQRyRUURkRug2EYvPLEo4wtP5EpyS0BCI/+mebLuzJo3FTOxehGaSIit6IwIpIJBneuy3+Sn+apxFc4Z/pTyXaM987358uRA1gecdrq8kREcjSFEZFMUNDXk/3vtqdKy260S3ifRY5aeBrJDHafgsfkzpw5dsDqEkVEcizDzAXjEaOjowkICCAqKgp/f3+ryxG5pT8PX2D61+8yxO17fIwEok0f3kzqzd7C4Ux7phH583lYXaKISJZL6/e3zoyIZIE6JQswaPB73JM4gi3OsvgbsYz1+Jy+F0byyGfzdU8SEZF/UBgRySIBPu788XZv5taZwCdJD5Bs2uhkX8s3sf1Zt2im1eWJiOQYukwjkg2cTpPZv/5CzY2vU9oWCcCv+e6ndNf3CQzwIyTA2+IKRUQyny7TiOQgNpvB/R3vw/2F1Ux1tgagw5WZ2P7bkgGjvyfJ4bS4QhER6yiMiGSj0OBCVHt2An15g7OmPxVsx/ne+QY7pg0jNl73JBGRvEmXaUQssnnPfs5NeZa29j8B2GhWYoRHf564tzkdaxS1uDoRkdunyzQiOVztSuUo++IcRri9wBXTk3rGHiYn9mfDT+8Tn5hkdXkiItlGYUTEQqWL+DFo8Age5EM2OCuQz0jgbfdJbHvnLn5dusLq8kREsoXCiIjFDMNgWK+OvOjxDm8l9eKK6UkD215aLbufyR+9zMqIU7oviYjc0dRnRCQHiYyK54GRPzLS/b80s+8AYJuzNLvrjaB7x/YWVycikj5p/f5WGBHJYY5diOXs5XimfjWSN90mE2DEkmjame3bjZn5ujKqez2K5/exukwRkVtSB1aRXCq0gA+1wwowcvj7PGgbzR+OungYDh6+8gNDI/vw9Y8zrC5RRCRTKYyI5FBudhuT+nfCfPh7Bttf4QL+VLQdY0jki+ya1A9HwhWrSxQRyRS6TCOSCyQ5nLjFX2D1509zV+wSAA45gzjZ/EOatLrP4upERK5Pl2lE7iDudhtGvkL8ddcnPJH4KqfMApSynabJyseZ/Ob97D54lMioeI26EZFcSWdGRHKRJIeT5RFn+XXjXurtH80jbilnSc6aAbyd9Bi17nmSno1LYbMZFlcqIqLRNCJ3vHUHz/PJ1xN51/0bytpOArDCUY23knsz4OFwutQqbnGFIpLXKYyI5AF/HrnII+OX87RtHi+6zcbTSCLBdOez5PtIbPAiXt4+vNSmvNVlikgepT4jInlAnbD8TH2+ObP9H6Ft4vuscFTD00jiFfefeXhTN9YvncOFK4lWlykiclM6MyJyB0lMcvDluA/pdv4LChtRAPzsaMZEnyf47sV7KOjraXGFIpKX6MyISB7k4W7nuT6v83Oj2Xyf3BqnafCgfQWT4/syZ8J7JCYlW12iiMg1FEZE7jDudhvPt6tNmV5fcn/iMHY7w8hvxPDEhY/Z9e5dzP1jvtUlioikoss0Incwp9Pk1WmbyL9zIi+7/Uw+IwGHaTDF0YqIyv1444Em+Hq6WV2miNyhNJpGRFwuXEnkxNEDnJr+Gm2dqwC4ZOZjVehz3PvEYLDZLa5QRO5E6jMiIi4F8nlQrVJlqvT7mSeNYexxliDQuMK9xz8ieVxT9qz9nYsadSMiFlEYEclDigV6M/7NfoQO2shQxxNcMvPhdnYXlf7oxo6xD0LUCatLFJE8SGFEJI9xt9vw9faCek/RIuFjJie3wmkaNEtYTvzoWhycMYxdR05bXaaI5CEKIyJ51OAOlSgaUoz/BrzIC74fs8FZAS8zgdI7PsZvQlNid/wCOb9LmYjcAdSBVSQPM00T04TL8cm88tMWfPbN4T/uPxBsXATgSP7GHKg9mFZN77K4UhHJjTSaRkTS7Ux0PBOW7sRv4xiesv+Gp5FMkmlnU3BXaj46Am+//FaXKCK5iEbTiEi6FfH34vWOdbC3GUrbxA9Y5KiFu+Gg0ekpXB5VkxU/jQWn0+oyReQOozMjInJds7Yc56Vp22hh28IQt+8pbYsE4IRPJdzveZciVVtZXKGI5HQ6MyIit6VLreJEvNOOM0HNCU/8gJFJ3YkxvSgWu4ciP9/Psc87YZ7Za3WZInIH0JkREbkpp9PEMCDZaTJ58UbcVnxAd/sS3AwnTmycK98VrzZv4l+4uNWlikgOozMjIpIpbDYDwzBwt9vo3bYBnp1H0zbxA/5w1MWGkyL7puL+RR2cS0ZAQozV5YpILqQzIyKSbk6nyV3vL6Fo9Fb+4z6F2rYDAETZ83O0Wn/c6vWkUrECFlcpIlbTmRERyTI2m8GQjlWI8KjK/YnDeD6xP4edQQQ4LlJt61Dcv2zCpAmf4XBo5I2I3JrOjIhIhpmmiWEYRMUm8d/le4le9TX93GZS0LgMwMmAWhR9aBTxQbVwt9uw2wyLKxaR7KSbnolIttt4+AJPjF/Ms25zecr+G15GEgDzHA34xNmNKQMfIcjfy+IqRSS7KIyIiCW2H79EsUBvvvhlBRV3j+UB+0pshkmSaWeJd1vCX/gY/ItaXaaIZAP1GRERS1QvHkhBX08GPtwK365fcU/iSJY6auBuOAiP/534j6qz4rNnOXf6pNWlikgOoTAiIlnCw81G+2ohVKrZiN5JA3koYUjKk4GNJJqd+xGvcbU5PGMIGyKOWF2qiFhMl2lEJEvFJzmIiktixubjfDB/Ly1s23jNbRpVbCkh5Lzpx5kafajU8SVwV38SkTuJ+oyISI6S5HCyPOIsBX092HXiEmvnTuBlt+mUsZ0C4IpnEI6mr+LfsBe4eVhbrIhkCoUREcnR5u88xXu/7qR+9B/0d5tJMeM8ADGewbi3eAXPej3BzdPiKkXkdiiMiEiO53SaDJq5g9mb/qK7fQnPu/1CkHEJgItuhVlaqAetH30Vf18/awsVkQxRGBGRXOXL5X/x8e/b6WpfyvNucwkxLgBw2b0wfq1fg9o91adEJJdRGBGRXOdUVBwbD1/ktanreci+nBfc5lDUFUoKkdiwHwWbPQPu3hZXKiJpoTAiIrnWnlPR7D4ZzaDpm3jIvpzn3X6huHEOgGi3Auwt3Zt6D76C4ZHP4kpF5GYURkQk1/vzyEWem/wnly5f4UH7cvq4zXGFkgSvQtia9MO9wVOgUCKSI2XZHVhXrFhBx44dKVq0KIZhMHv27Fsus2zZMmrXro2npydly5Zl0qRJ6d2siORBdcLys3Fwa/a/dx+v/+c93gz9loFJT3PMWRjP+HO4Lx5C3IdVcK4cDQkxVpcrIhmU7jBy5coVatSoweeff56m9ocOHaJDhw60bNmSrVu3MmDAAJ566in++OOPdBcrInlX/nweTHyyCdU7vUjLxI94LekZjjiL4J10Edvi/yPmgyqcmjeCs+fPWV2qiKTTbV2mMQyDWbNm0blz5xu2GThwIL/++is7d+50TevWrRuXLl1i/vz5adqOLtOIyFWJyU5ajlrGiUtxVCzsRdULf9DXPpuSttMAXDB92VXiMZr2+A946feFiJXS+v3tltWFrF27ltatW6eaFh4ezoABA264TEJCAgkJCa730dHRWVWeiOQyHm42fuvfFIfTJMDbnTL/iWeW4y7us62mr9tsStsiaXpsHEkfTeZQya4UatWfk44AqhYLsLp0EbmBLH9QXmRkJEFBQammBQUFER0dTVxc3HWXGTlyJAEBAa5XaGhoVpcpIrlIgLc7BfJ5YLcZvNulKm2rFqP3C/8h+fl19E98gQPOorgnXab8/v+Sb1wtdozrycZN660uW0RuIEc+tXfQoEFERUW5XseOHbO6JBHJoXo0CGPco3WoVjyA8iH58ajVjTaJH/BU4itscpbH00imu9tS6swLJ3nKI3Bso9Uli8i/ZHkYCQ4O5vTp06mmnT59Gn9/f7y9r3/jIk9PT/z9/VO9RETS4r0HqmNiY5GzDg8mDuWBhP9joaMONkzc9v0K37Tm8rg2sO8PcDqtLldEyIY+I40aNeK3335LNW3hwoU0atQoqzctInmQ3WYwsXc9IiIvc1/NooxbFsbTaytQJvkEz9rn0dm+Cr/TG2DKwxx1CyO4/UA8ajykJwWLWCjdo2liYmI4cOAAALVq1eLjjz+mZcuWFChQgBIlSjBo0CBOnDjBd999B6QM7a1atSp9+vThiSeeYMmSJfTr149ff/2V8PDwNG1To2lE5HZcSUhmw+EL9J64kSAu0NttPj3si/EzUvqtmf7FMBq+AHV6gqceyieSWbLsDqzLli2jZcuW10zv2bMnkyZNolevXhw+fJhly5alWuall15i9+7dFC9enLfeeotevXpl+s6IiNxM1y/Xsv5QyrNu/LnCI/bFPOE2nyL/e1Jwgpsfbg2ext7wOfALusmaRCQtdDt4EZF/OR+TwJT1R3m8UUkGztjO/F2ReJBEZ/sqnrXPo4ztFABJhgexlR4i4O6XoVBZi6sWyb0URkREbmLrsUt0/ny1672Bkza2P3nObS61bSmXop0YRIeFE9jmdShex6pSRXIthRERkVtIdjg5FRXPtI3HOBkVR+lC+Ri1IIK6RgTPus2jjX2zq21c0UZ4t3gZyrUBw7CwapHcQ2FERCQDvll1iLfn7QagnHGcZ+zzuM++Gg/DAYCjYHnsDZ6B6l11u3mRW1AYERHJAIfTZMeJKMoV8eXV6dv4fWckwZx3jcDxNeIBSLL74F77Eaj3NBSpaHHVIjmTwoiIyG1yOE1i4pOp9fYCnCb4Esv99pU8bl9IWdtJV7uE4o3xbPwcVOgA9iy/fZNIrqEwIiKSSQ6cuczE1Yfx83Jn/PK/AJNGtt30tC+gjW0TduN/v0b9ikLd3lC7p4YGi6AwIiKSJQ6cucymwxeZvP4IO09EE8J5HnFbTDf7EgobKU8YN23uGJU7pVzCKdFQHV4lz1IYERHJYiv2neU/s3Zw/GIcHiTRzraBnm4LqGPb/3ejoGpQ/ymo9hB45LOuWBELKIyIiGQD0zR59Jv1rD5w3jWtinGYx+wL6Oy2Bi8SU9p5+mPUehTqPQUFy1hVrki2UhgREckmickpT/+dvyuSflO3uKb7E8ND9uU8Zl9ESds/nl5e5u6USzjlw8Fmz+5yRbKNwoiIiEVW7T/HhwsiiI5L4tC5Kxg4aW7bzmP2hbS0bcV2tcNrQAmo9wTUehzyFbS2aJEsoDAiIpID7DkVzb2frsLhTPlVG2qcpod9MV3ty8hvxABg2j0xqt6fcrZEt52XO4jCiIhIDvHX2RgOnb3CnG0nmbst5f4kniTSyb6Gx+wLqW475Gp7wL08IW1eJF+th8Hdy6qSRTKFwoiISA5zOT6J4XN3ExWXxJnLCWw9dgkwqWn8xWNuC7jXtg5PIzmlsXcBqP0Y1H0S8odZWbZIhimMiIjkcNuPX6LXxI1cuJIy4qYA0XS1L6OH2yKKG+cAMDEwyraCGt2hYgdw97awYpH0URgREcklpm86xusztlOlqD87T0Rjw0kr22Yesy+kmX2Hq53p6Z/St6RmDyheTzdTkxxPYUREJJcxTZP7x61hy9FLrmlhRiT321fygH2l62wJwAlbUeKqdKVs66cgoLgF1YrcmsKIiEgulJjsJD7ZwZFzsRw+f4WXpm0l2Wli4KShbQ8P2lfQ3rYBHyPhf0sYULo51HgEKt2ru7xKjqIwIiJyBzgVFcfohfuZtumYa1o+4mhv38ADtpU0su/+u7GHL1TpnBJMwhrrMo5YTmFEROQOEZfo4Lcdp1iy9wzzd0VSsqAPf529AkBx4wz321bxqPdqiiSf+nuh/CVTOr3W6JbybxELKIyIiNyhTNOk1tsLuRSb9M+p1DMieNC+gnvs6/Ez4v6eFXYX1OwOle8DT79sr1fyLoUREZE73KbDFzgXk8iEVYfYcPiCa7o38YTbNtHDazV1ndsxSPk1b7r7YFTqBDUfgZJNwWazqnTJIxRGRETyiH2nL9Pnh82UKpSP7cejiIyOd80L4Txd/jcap4ztH5dxAkJTLuHU6K6nCEuWURgREcmjHh6/NtWZkhQmtY39PGBfSUf7WvyN2L/nhDbEqNkdqnQBr4DsLVbuaAojIiJ51JnL8Rw8e4VCvh58vHAfv+2ITDXfk0Ta2P7kQfsKmtq2Y7/6FGE3L6h4b8plnNItwGbP/uLljqIwIiIiqVy8kkjXr9ay73SMa1oRLtLFvopnAtZTMPaga7rDNwRb9a4YtR6BwhWsKFfuAAojIiJyDYfTJNnpZP/pGMoW8eXJbzey+sB5wKS6cZAH7Cu4z76GQOPK3wsVrQVVH4Sq94N/Uctql9xHYURERG5p4e7TPP3dplTTPEjibtsWHrCvoKV9K244gZSH9jlLNMFe/cGUYcI+BawoWXIRhREREUmTuEQHg2fvoFigN2EF8/Hq9G2ueQWJ4h77ejrZ11DPtu/vhWxuKf1KqnRJeZqwd/7sL1xyPIURERG5Lc98t4kFu0+73hfjLB3ta+lkX0tl25G/G9rcoczdKZdxKrTXiBxxURgREZHbcvBsDL0mbqRKUX8io+NTPU24tHGSe2zr6ZZvE8UTD/29kN0DyrRKeUaOgkmepzAiIiKZJiouiTlbT1CmsC+PT9iAw/n3V0dZ4zgdbOu5176OcrYTfy9k94DSLVP6l1Rorz4meZDCiIiIZIkLVxL5ZesJhs7dfc28csZx7rWvo4N9HWWNk3/PsLml3IK+cieo0AH8grKxYrGKwoiIiGQZh9MkIvIyFYL92HDoAoNn7eDguX8MB8aknHGC9rYNtLevp5Lt2D/mGVCiYcoN1irdq6cK38EURkREJFudj0mgzjuLrjuvlHGKcNtG7nHbSHXjr9Qzg6tBxY4pwaRIZTCMbKhWsoPCiIiIZLvjF2P5aME+dp2M4sTFOK4kOq5pE8J52to30c62kYZuezFM598zC5T+3xmTjlCsrp4snMspjIiIiOWcThOHaeJut/H+/L2MW5b6rEh+omlt30y4bSPN7DvxIOnvmb7BKfcwqXRvSn8Tu3s2Vy+3S2FERERylMRkJzM2H6dxmYLM3XaSUQv2pZqfjzia27YxvPxB/I8txcPxdx8U0ysAo3z7lGBSphV4+GR3+ZIBCiMiIpKjDf1lF5PWHAagd5OSTFx92DXPgyQa23bRyeNP2tr/xNdx6e8F3byhbKuUSznlw3X31xxMYURERHI0h9Nk7raT1CtVgGKB3vy08Rivz9h+TTsbTuoY+wi3b6Sz12YKJf99V1gMO4Q2gPJtoVw4FKmkDrA5iMKIiIjkKnGJDjqMXcnBc1eoX6oACUkOth2P+lcrkyrGEdraN/KI33YKx/1rZE5Aib+DSamm4O6dbfXLtRRGREQk10lMdjJz83GalS/MsQuxfLPqEGWK+FIhyI93ft3NuZjEVO2LG2doadvK3bYt3OW2G3fzHx1g3byhVLO/w0lgaDbvjSiMiIjIHSUh2cFL07by247I6873Jp67PffSPXAPVa6sI3/y2dQNilT5O5gUrwd2t2yoOm9TGBERkTuWaZp88EfENUOF/9GCisYxniyyj5rx6ymTsAcb/7ifiVcglG0N5duldIbVc3OyhMKIiIjc8RKSHQz4cSu1SgTy5F2l+XrlQd77fe817QK5TDPbdlrZt9DKbTu+ZszfMw0bFK//91mToCrqBJtJFEZERCRPWrX/HF+tPMiKfWevO9+Ogy6FT/JcyAFCz6/C8/ye1A38QqDM3Smv0i0gX6GsL/oOpTAiIiJ5lmmaHDgTg5e7ne5fr+P4xbgbti3KOVrat/JKycPkP70WI/lfbYOr/x1OSjQEN88srv7OoTAiIiICxCQk88fOSPy93Ska6EXPCRs5F5Nw3bY+tiTeqR1DkTOrKReziaDY1HeJxc0bSjb5O5wUrqhLOjehMCIiInITc7ed5MWpW27aphBRNLHtoIt/BI3Zjkf8vy79+IVA6ZZ/X9LxLZx1BedCCiMiIiK3cDk+iR0novh1+yl+WH8UgEAfdy7FJl2ntUlD3zN08t1LU9t2ikVtxub41xmW4OpQ5n/hJLQhuHtl/U7kYAojIiIi6RAdn4S7zYa3h53v1h5m2NzdOJw3/or0JJE6tn2099pNW6/dN76kU7ollG4ORSqDzZ7Fe5GzKIyIiIjcBofTJNnp5PjFONYcOMdbc3bdtH0homhs28lD+ffRhO3YrpxO3cAzICWclGoGpZrniefoKIyIiIhkEqfT5LOlB/htxylealOeH9Yfxcfdzvxd178bLJiUN47T1LaDbgUOUDJ2O+6O2NRN8hX+XzD5XzgpUCrL9yO7KYyIiIhkMYfTZM+paO79dNVN29lxUNk4QguPPfQvfRL7sXXXDiEOLAFhTSCsMZRoDAXL5PozJwojIiIi2SQmIZljF2Ip4ufJF8v+YsLqQwR436gjLOSzJ1PFuZ/G9l3cH/gXobG7MJzJ/2pUJCWYXH3lwj4nCiMiIiIWME2ThGQnXu52DpyJoWA+D8Ys3s+kNYdvuIwP8dS1RdDO7xDdihzDdnIz/HukjmdAyk3XwhqlnEEJqQluHlm6L7dLYURERCSHME2T+z5fzfbjUQAU8vWgc81i/HfVoeu2D/O38UaNONr6/oX96Fo4th4SY1I3cvOG4nX/d2mnUcqTiD3yZfWupIvCiIiISA4Sm5jM/83ZRdsqwTQvXxgPNxtHzl/h952RjPojguSbDCO246CScYTHQk5Q/PJWarMH76RLqRvZ3FLOlly9rFOiIXjnz9J9uhWFERERkVwiMiqeb1YdxGYz+HL5wTQsYdKq0CXuy38Y5+HVtM13EJ/4f4/sMVL6mYQ1TjlzUqIx+IdkRfk3pDAiIiKSCyU5nLz80zbmbjtJ3bD8bDpyMQ1LmfSt5UHo5a3ki9xAI7d9FIw/cm2zAqVTQsnVgJK/VJaO2FEYERERuYNM33SMA2dieKFlWbp9tY49p6Jv2r4QUdSz7eWF0meokrQTTu/Exr++8v1CoESjlHBS8d5MP3OiMCIiInKHOns5gfHL/+KbVYcoW8SXA2dibrmMP1eobdvH+3VjSPhrFcVi92A3/zGc+LHZKc/VyUQKIyIiIne44xdjCQnw5t1f9zBh9SFsBtykH2wqXiRQ0/YX9Y29PFzkOEWeno6HT+Z+xyqMiIiI5DHxSQ7mbD1B+2oheLrZGLNoP18s+ytNy37wYHUerhuaqfUojIiIiAgxCcnM2XqCumEFCB+9AgA/LzealS/Mr9tPudodeLc9bnZbpm47rd/fbpm6VREREclRfD3d6NEgDIAPH6zO2oPnGdGlGl7udiIil3PgTAwD21XM9CCSHjozIiIikkcdPBvD5qOXuL9WMWy2zB/iqzMjIiIiclOlC/tSurCv1WVg3TkZERERETIYRj7//HNKliyJl5cXDRo0YMOGDTdsO2nSJAzDSPXy8vLKcMEiIiJyZ0l3GJk2bRovv/wy//d//8fmzZupUaMG4eHhnDlz5obL+Pv7c+rUKdfryJHr3KJWRERE8qR0h5GPP/6Yp59+mt69e1O5cmXGjx+Pj48PEyZMuOEyhmEQHBzsegUFBd1W0SIiInLnSFcYSUxM5M8//6R169Z/r8Bmo3Xr1qxdu/aGy8XExBAWFkZoaCj33Xcfu3btuul2EhISiI6OTvUSERGRO1O6wsi5c+dwOBzXnNkICgoiMvLfjy5OUaFCBSZMmMCcOXOYPHkyTqeTxo0bc/z48RtuZ+TIkQQEBLheoaGZe0c4ERERyTmyfDRNo0aNePzxx6lZsybNmzdn5syZFC5cmC+//PKGywwaNIioqCjX69ixY1ldpoiIiFgkXfcZKVSoEHa7ndOnT6eafvr0aYKDg9O0Dnd3d2rVqsWBAwdu2MbT0xNPT8/0lCYiIiK5VLrOjHh4eFCnTh0WL17smuZ0Olm8eDGNGjVK0zocDgc7duwgJCQkfZWKiIjIHSndd2B9+eWX6dmzJ3Xr1qV+/fqMHj2aK1eu0Lt3bwAef/xxihUrxsiRIwEYPnw4DRs2pGzZsly6dIkPP/yQI0eO8NRTT2XunoiIiEiulO4w0rVrV86ePcuQIUOIjIykZs2azJ8/39Wp9ejRo9hsf59wuXjxIk8//TSRkZHkz5+fOnXqsGbNGipXrpx5eyEiIiK5lh6UJyIiIlkird/fejaNiIiIWCpXPLX36skb3fxMREQk97j6vX2rizC5IoxcvnwZQDc/ExERyYUuX75MQEDADefnij4jTqeTkydP4ufnh2EYmbbe6OhoQkNDOXbsmPqiZDEd6+yh45w9dJyzh45z9smqY22aJpcvX6Zo0aKpBrf8W644M2Kz2ShevHiWrd/f318f9GyiY509dJyzh45z9tBxzj5ZcaxvdkbkKnVgFREREUspjIiIiIil8nQY8fT05P/+7//0HJxsoGOdPXScs4eOc/bQcc4+Vh/rXNGBVURERO5cefrMiIiIiFhPYUREREQspTAiIiIillIYEREREUspjIiIiIil8nQY+fzzzylZsiReXl40aNCADRs2WF1SrjFy5Ejq1auHn58fRYoUoXPnzkRERKRqEx8fT58+fShYsCC+vr488MADnD59OlWbo0eP0qFDB3x8fChSpAivvfYaycnJ2bkrucp7772HYRgMGDDANU3HOfOcOHGCRx99lIIFC+Lt7U21atXYtGmTa75pmgwZMoSQkBC8vb1p3bo1+/fvT7WOCxcu0KNHD/z9/QkMDOTJJ58kJiYmu3clx3I4HLz11luUKlUKb29vypQpw9tvv53qQWo6zhmzYsUKOnbsSNGiRTEMg9mzZ6ean1nHdfv27TRt2hQvLy9CQ0P54IMPbr94M4/68ccfTQ8PD3PChAnmrl27zKefftoMDAw0T58+bXVpuUJ4eLg5ceJEc+fOnebWrVvNe+65xyxRooQZExPjavPcc8+ZoaGh5uLFi81NmzaZDRs2NBs3buyan5ycbFatWtVs3bq1uWXLFvO3334zCxUqZA4aNMiKXcrxNmzYYJYsWdKsXr262b9/f9d0HefMceHCBTMsLMzs1auXuX79evPgwYPmH3/8YR44cMDV5r333jMDAgLM2bNnm9u2bTM7depklipVyoyLi3O1adeunVmjRg1z3bp15sqVK82yZcua3bt3t2KXcqR3333XLFiwoDlv3jzz0KFD5vTp001fX19zzJgxrjY6zhnz22+/mYMHDzZnzpxpAuasWbNSzc+M4xoVFWUGBQWZPXr0MHfu3GlOnTrV9Pb2Nr/88svbqj3PhpH69eubffr0cb13OBxm0aJFzZEjR1pYVe515swZEzCXL19umqZpXrp0yXR3dzenT5/uarNnzx4TMNeuXWuaZsp/HJvNZkZGRrrajBs3zvT39zcTEhKydwdyuMuXL5vlypUzFy5caDZv3twVRnScM8/AgQPNu+6664bznU6nGRwcbH744YeuaZcuXTI9PT3NqVOnmqZpmrt37zYBc+PGja42v//+u2kYhnnixImsKz4X6dChg/nEE0+kmnb//febPXr0ME1Txzmz/DuMZNZx/eKLL8z8+fOn+t0xcOBAs0KFCrdVb568TJOYmMiff/5J69atXdNsNhutW7dm7dq1FlaWe0VFRQFQoEABAP7880+SkpJSHeOKFStSokQJ1zFeu3Yt1apVIygoyNUmPDyc6Ohodu3alY3V53x9+vShQ4cOqY4n6Dhnpl9++YW6devy0EMPUaRIEWrVqsXXX3/tmn/o0CEiIyNTHeuAgAAaNGiQ6lgHBgZSt25dV5vWrVtjs9lYv3599u1MDta4cWMWL17Mvn37ANi2bRurVq2iffv2gI5zVsms47p27VqaNWuGh4eHq014eDgRERFcvHgxw/Xliqf2ZrZz587hcDhS/XIGCAoKYu/evRZVlXs5nU4GDBhAkyZNqFq1KgCRkZF4eHgQGBiYqm1QUBCRkZGuNtf7GVydJyl+/PFHNm/ezMaNG6+Zp+OceQ4ePMi4ceN4+eWX+c9//sPGjRvp168fHh4e9OzZ03Wsrncs/3msixQpkmq+m5sbBQoU0LH+nzfeeIPo6GgqVqyI3W7H4XDw7rvv0qNHDwAd5yySWcc1MjKSUqVKXbOOq/Py58+fofryZBiRzNWnTx927tzJqlWrrC7ljnPs2DH69+/PwoUL8fLysrqcO5rT6aRu3bqMGDECgFq1arFz507Gjx9Pz549La7uzvHTTz/xww8/MGXKFKpUqcLWrVsZMGAARYsW1XHOw/LkZZpChQpht9uvGXFw+vRpgoODLaoqd+rbty/z5s1j6dKlFC9e3DU9ODiYxMRELl26lKr9P49xcHDwdX8GV+dJymWYM2fOULt2bdzc3HBzc2P58uWMHTsWNzc3goKCdJwzSUhICJUrV041rVKlShw9ehT4+1jd7PdGcHAwZ86cSTU/OTmZCxcu6Fj/z2uvvcYbb7xBt27dqFatGo899hgvvfQSI0eOBHScs0pmHdes+n2SJ8OIh4cHderUYfHixa5pTqeTxYsX06hRIwsryz1M06Rv377MmjWLJUuWXHPark6dOri7u6c6xhERERw9etR1jBs1asSOHTtSffgXLlyIv7//NV8KeVWrVq3YsWMHW7dudb3q1q1Ljx49XP/Wcc4cTZo0uWZ4+r59+wgLCwOgVKlSBAcHpzrW0dHRrF+/PtWxvnTpEn/++aerzZIlS3A6nTRo0CAb9iLni42NxWZL/dVjt9txOp2AjnNWyazj2qhRI1asWEFSUpKrzcKFC6lQoUKGL9EAeXtor6enpzlp0iRz9+7d5jPPPGMGBgamGnEgN/b888+bAQEB5rJly8xTp065XrGxsa42zz33nFmiRAlzyZIl5qZNm8xGjRqZjRo1cs2/OuS0bdu25tatW8358+ebhQsX1pDTW/jnaBrT1HHOLBs2bDDd3NzMd99919y/f7/5ww8/mD4+PubkyZNdbd577z0zMDDQnDNnjrl9+3bzvvvuu+7QyFq1apnr1683V61aZZYrVy7PDzn9p549e5rFihVzDe2dOXOmWahQIfP11193tdFxzpjLly+bW7ZsMbds2WIC5scff2xu2bLFPHLkiGmamXNcL126ZAYFBZmPPfaYuXPnTvPHH380fXx8NLT3dnz66admiRIlTA8PD7N+/frmunXrrC4p1wCu+5o4caKrTVxcnPnCCy+Y+fPnN318fMwuXbqYp06dSrWew4cPm+3btze9vb3NQoUKma+88oqZlJSUzXuTu/w7jOg4Z565c+eaVatWNT09Pc2KFSuaX331Var5TqfTfOutt8ygoCDT09PTbNWqlRkREZGqzfnz583u3bubvr6+pr+/v9m7d2/z8uXL2bkbOVp0dLTZv39/s0SJEqaXl5dZunRpc/DgwamGiuo4Z8zSpUuv+3u5Z8+epmlm3nHdtm2bedddd5menp5msWLFzPfee++2azdM8x+3vRMRERHJZnmyz4iIiIjkHAojIiIiYimFEREREbGUwoiIiIhYSmFERERELKUwIiIiIpZSGBERERFLKYyIiIiIpRRGRERExFIKIyIiImIphRERERGx1P8DPcLurCS0e08AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_13 (LSTM)              (None, 128)               110080    \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 114,241\n",
      "Trainable params: 114,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 1s 2ms/step - loss: 0.5686 - accuracy: 0.9643\n",
      "Test Loss: 0.5685984492301941\n",
      "Test Accuracy: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "# Definisco l'ottimizzatore con il learning rate iniziale\n",
    "initial_learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "# Definisco il learning rate schedule con decay lineare\n",
    "decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "decay_rate = 0.1  # Tasso di decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "                                                             \n",
    "model = Sequential()\n",
    "lstm_units = 128\n",
    "dense_units = 32\n",
    "\n",
    "# kernel_regularizer=regularizers.l2(1e-3)\n",
    "\n",
    "model.add(LSTM(lstm_units, kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(dropout_percentage))\n",
    "model.add(Dense(dense_units, activation = 'relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(dropout_percentage/2))\n",
    "model.add(Dense(1, activation = 'sigmoid', kernel_regularizer=regularizers.l2(1e-3)))\n",
    "\n",
    "# Compilazione del modello\n",
    "# model.compile(optimizer = Adam(learning_rate = 1e-3), loss = loss_function, metrics = metric)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestramento del modello con il learning rate modificato\n",
    "history = model.fit(x_train, y_train, epochs=1000, batch_size=8, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Valutazione del modello\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold con early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing vanilla RNN with k-fold\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 1s 9ms/step - loss: 0.5190 - accuracy: 0.7522 - val_loss: 0.5247 - val_accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4813 - accuracy: 0.7679 - val_loss: 0.4928 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4486 - accuracy: 0.7857 - val_loss: 0.4674 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4210 - accuracy: 0.8259 - val_loss: 0.4473 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4030 - accuracy: 0.8348 - val_loss: 0.4309 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8281 - val_loss: 0.4176 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3604 - accuracy: 0.8549 - val_loss: 0.4064 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3799 - accuracy: 0.8571 - val_loss: 0.3964 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8616 - val_loss: 0.3875 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3619 - accuracy: 0.8527 - val_loss: 0.3789 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3483 - accuracy: 0.8527 - val_loss: 0.3713 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8862 - val_loss: 0.3646 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8862 - val_loss: 0.3586 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8795 - val_loss: 0.3533 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8750 - val_loss: 0.3482 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3048 - accuracy: 0.8973 - val_loss: 0.3436 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3092 - accuracy: 0.8884 - val_loss: 0.3396 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3138 - accuracy: 0.8862 - val_loss: 0.3355 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8862 - val_loss: 0.3316 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3015 - accuracy: 0.8795 - val_loss: 0.3282 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2978 - accuracy: 0.8951 - val_loss: 0.3248 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8973 - val_loss: 0.3218 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3037 - accuracy: 0.8862 - val_loss: 0.3189 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2893 - accuracy: 0.8929 - val_loss: 0.3161 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2880 - accuracy: 0.8929 - val_loss: 0.3135 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2840 - accuracy: 0.8862 - val_loss: 0.3115 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2744 - accuracy: 0.9040 - val_loss: 0.3094 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2752 - accuracy: 0.8973 - val_loss: 0.3071 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2827 - accuracy: 0.8862 - val_loss: 0.3048 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2739 - accuracy: 0.8951 - val_loss: 0.3025 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2801 - accuracy: 0.8817 - val_loss: 0.3001 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2695 - accuracy: 0.9040 - val_loss: 0.2983 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2710 - accuracy: 0.8929 - val_loss: 0.2963 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2729 - accuracy: 0.9018 - val_loss: 0.2944 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2684 - accuracy: 0.9062 - val_loss: 0.2930 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2682 - accuracy: 0.9018 - val_loss: 0.2915 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2690 - accuracy: 0.8996 - val_loss: 0.2897 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2526 - accuracy: 0.9040 - val_loss: 0.2881 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2602 - accuracy: 0.9129 - val_loss: 0.2867 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2484 - accuracy: 0.9129 - val_loss: 0.2851 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2505 - accuracy: 0.9129 - val_loss: 0.2836 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9085 - val_loss: 0.2820 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2630 - accuracy: 0.9085 - val_loss: 0.2804 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2368 - accuracy: 0.9085 - val_loss: 0.2791 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2461 - accuracy: 0.9040 - val_loss: 0.2780 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.9062 - val_loss: 0.2765 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2483 - accuracy: 0.9085 - val_loss: 0.2752 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2388 - accuracy: 0.9107 - val_loss: 0.2741 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2343 - accuracy: 0.9174 - val_loss: 0.2731 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.9085 - val_loss: 0.2718 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9062 - val_loss: 0.2706 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2462 - accuracy: 0.9107 - val_loss: 0.2694 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2310 - accuracy: 0.9174 - val_loss: 0.2681 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2320 - accuracy: 0.9219 - val_loss: 0.2670 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2424 - accuracy: 0.9152 - val_loss: 0.2660 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2431 - accuracy: 0.9152 - val_loss: 0.2650 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2307 - accuracy: 0.9085 - val_loss: 0.2639 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2303 - accuracy: 0.9196 - val_loss: 0.2630 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2330 - accuracy: 0.9152 - val_loss: 0.2621 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9219 - val_loss: 0.2610 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2434 - accuracy: 0.9062 - val_loss: 0.2599 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2290 - accuracy: 0.9196 - val_loss: 0.2591 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2299 - accuracy: 0.9263 - val_loss: 0.2582 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.9219 - val_loss: 0.2572 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2146 - accuracy: 0.9263 - val_loss: 0.2565 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.9174 - val_loss: 0.2557 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.9107 - val_loss: 0.2548 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2288 - accuracy: 0.9219 - val_loss: 0.2540 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2193 - accuracy: 0.9263 - val_loss: 0.2531 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9174 - val_loss: 0.2522 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9174 - val_loss: 0.2512 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.9196 - val_loss: 0.2503 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2049 - accuracy: 0.9308 - val_loss: 0.2496 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2188 - accuracy: 0.9196 - val_loss: 0.2489 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2226 - accuracy: 0.9107 - val_loss: 0.2482 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.9219 - val_loss: 0.2474 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2132 - accuracy: 0.9286 - val_loss: 0.2465 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2131 - accuracy: 0.9241 - val_loss: 0.2459 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2283 - accuracy: 0.9129 - val_loss: 0.2451 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2080 - accuracy: 0.9353 - val_loss: 0.2442 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2155 - accuracy: 0.9219 - val_loss: 0.2435 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2106 - accuracy: 0.9308 - val_loss: 0.2427 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9330 - val_loss: 0.2419 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2072 - accuracy: 0.9375 - val_loss: 0.2411 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2050 - accuracy: 0.9353 - val_loss: 0.2403 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.9174 - val_loss: 0.2395 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9196 - val_loss: 0.2388 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1990 - accuracy: 0.9286 - val_loss: 0.2382 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2037 - accuracy: 0.9375 - val_loss: 0.2376 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9286 - val_loss: 0.2369 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2106 - accuracy: 0.9308 - val_loss: 0.2362 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9330 - val_loss: 0.2357 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2059 - accuracy: 0.9375 - val_loss: 0.2350 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2068 - accuracy: 0.9241 - val_loss: 0.2342 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2103 - accuracy: 0.9263 - val_loss: 0.2335 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2007 - accuracy: 0.9286 - val_loss: 0.2328 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9353 - val_loss: 0.2322 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2030 - accuracy: 0.9286 - val_loss: 0.2316 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9464 - val_loss: 0.2309 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2040 - accuracy: 0.9308 - val_loss: 0.2304 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1951 - accuracy: 0.9353 - val_loss: 0.2299 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2052 - accuracy: 0.9375 - val_loss: 0.2292 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2016 - accuracy: 0.9308 - val_loss: 0.2286 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2018 - accuracy: 0.9353 - val_loss: 0.2282 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9330 - val_loss: 0.2276 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2042 - accuracy: 0.9397 - val_loss: 0.2268 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1945 - accuracy: 0.9375 - val_loss: 0.2263 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1986 - accuracy: 0.9353 - val_loss: 0.2258 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1989 - accuracy: 0.9308 - val_loss: 0.2252 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9353 - val_loss: 0.2247 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1883 - accuracy: 0.9464 - val_loss: 0.2241 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9464 - val_loss: 0.2235 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9308 - val_loss: 0.2228 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1979 - accuracy: 0.9397 - val_loss: 0.2225 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1806 - accuracy: 0.9442 - val_loss: 0.2220 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1845 - accuracy: 0.9442 - val_loss: 0.2215 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.9330 - val_loss: 0.2209 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1947 - accuracy: 0.9353 - val_loss: 0.2204 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1862 - accuracy: 0.9464 - val_loss: 0.2198 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1983 - accuracy: 0.9375 - val_loss: 0.2194 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.9330 - val_loss: 0.2189 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1904 - accuracy: 0.9375 - val_loss: 0.2184 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1892 - accuracy: 0.9397 - val_loss: 0.2177 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1936 - accuracy: 0.9442 - val_loss: 0.2172 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1931 - accuracy: 0.9397 - val_loss: 0.2167 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.9420 - val_loss: 0.2164 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1875 - accuracy: 0.9397 - val_loss: 0.2159 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1932 - accuracy: 0.9353 - val_loss: 0.2155 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.9420 - val_loss: 0.2151 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.9375 - val_loss: 0.2146 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1835 - accuracy: 0.9330 - val_loss: 0.2141 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9442 - val_loss: 0.2136 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9420 - val_loss: 0.2132 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.9420 - val_loss: 0.2126 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9442 - val_loss: 0.2122 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.9397 - val_loss: 0.2117 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1828 - accuracy: 0.9353 - val_loss: 0.2113 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9442 - val_loss: 0.2108 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1865 - accuracy: 0.9397 - val_loss: 0.2104 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9531 - val_loss: 0.2099 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9397 - val_loss: 0.2097 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.9464 - val_loss: 0.2092 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.9375 - val_loss: 0.2086 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1765 - accuracy: 0.9464 - val_loss: 0.2081 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.9487 - val_loss: 0.2077 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.9487 - val_loss: 0.2074 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9375 - val_loss: 0.2069 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.9375 - val_loss: 0.2066 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.9420 - val_loss: 0.2062 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9554 - val_loss: 0.2059 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1892 - accuracy: 0.9442 - val_loss: 0.2056 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1817 - accuracy: 0.9442 - val_loss: 0.2052 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9487 - val_loss: 0.2049 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1802 - accuracy: 0.9464 - val_loss: 0.2044 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9464 - val_loss: 0.2042 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9375 - val_loss: 0.2038 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1805 - accuracy: 0.9464 - val_loss: 0.2034 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9442 - val_loss: 0.2031 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.9442 - val_loss: 0.2028 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9442 - val_loss: 0.2025 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1678 - accuracy: 0.9464 - val_loss: 0.2020 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9464 - val_loss: 0.2016 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1717 - accuracy: 0.9509 - val_loss: 0.2013 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1735 - accuracy: 0.9397 - val_loss: 0.2009 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1703 - accuracy: 0.9531 - val_loss: 0.2004 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9554 - val_loss: 0.2001 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9509 - val_loss: 0.1998 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1677 - accuracy: 0.9531 - val_loss: 0.1994 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1742 - accuracy: 0.9464 - val_loss: 0.1990 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1760 - accuracy: 0.9487 - val_loss: 0.1987 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9554 - val_loss: 0.1984 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9397 - val_loss: 0.1981 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.9420 - val_loss: 0.1978 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1770 - accuracy: 0.9397 - val_loss: 0.1975 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1776 - accuracy: 0.9420 - val_loss: 0.1971 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.9442 - val_loss: 0.1968 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1690 - accuracy: 0.9464 - val_loss: 0.1966 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1637 - accuracy: 0.9531 - val_loss: 0.1962 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1644 - accuracy: 0.9531 - val_loss: 0.1958 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.9531 - val_loss: 0.1955 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1675 - accuracy: 0.9420 - val_loss: 0.1953 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9420 - val_loss: 0.1950 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9487 - val_loss: 0.1945 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1636 - accuracy: 0.9464 - val_loss: 0.1942 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.1622 - accuracy: 0.9487 - val_loss: 0.1940 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1472 - accuracy: 0.9509 - val_loss: 0.1936 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9554 - val_loss: 0.1932 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1704 - accuracy: 0.9487 - val_loss: 0.1929 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9375 - val_loss: 0.1927 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1655 - accuracy: 0.9487 - val_loss: 0.1923 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1636 - accuracy: 0.9576 - val_loss: 0.1921 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9487 - val_loss: 0.1917 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1605 - accuracy: 0.9464 - val_loss: 0.1914 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9464 - val_loss: 0.1911 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9487 - val_loss: 0.1908 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9464 - val_loss: 0.1905 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.9509 - val_loss: 0.1902 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9487 - val_loss: 0.1898 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.9554 - val_loss: 0.1895 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1656 - accuracy: 0.9509 - val_loss: 0.1892 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Loss: 0.1892, Accuracy: 92.86%\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 1s 7ms/step - loss: 0.7091 - accuracy: 0.6451 - val_loss: 0.5647 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6337 - accuracy: 0.6830 - val_loss: 0.5085 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5980 - accuracy: 0.7098 - val_loss: 0.4673 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5537 - accuracy: 0.7500 - val_loss: 0.4369 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5088 - accuracy: 0.7746 - val_loss: 0.4142 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5003 - accuracy: 0.7612 - val_loss: 0.3952 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4707 - accuracy: 0.7924 - val_loss: 0.3797 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4561 - accuracy: 0.8080 - val_loss: 0.3671 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4459 - accuracy: 0.8103 - val_loss: 0.3567 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8036 - val_loss: 0.3484 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8192 - val_loss: 0.3417 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8304 - val_loss: 0.3359 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8259 - val_loss: 0.3305 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3762 - accuracy: 0.8326 - val_loss: 0.3260 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3596 - accuracy: 0.8549 - val_loss: 0.3223 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8638 - val_loss: 0.3188 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3598 - accuracy: 0.8638 - val_loss: 0.3159 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3477 - accuracy: 0.8638 - val_loss: 0.3131 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3505 - accuracy: 0.8527 - val_loss: 0.3107 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8594 - val_loss: 0.3084 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8728 - val_loss: 0.3063 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8750 - val_loss: 0.3045 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8795 - val_loss: 0.3028 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3233 - accuracy: 0.8817 - val_loss: 0.3012 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8750 - val_loss: 0.2996 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8884 - val_loss: 0.2981 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8772 - val_loss: 0.2968 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8817 - val_loss: 0.2955 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3027 - accuracy: 0.8839 - val_loss: 0.2944 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2985 - accuracy: 0.8862 - val_loss: 0.2933 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2929 - accuracy: 0.8973 - val_loss: 0.2922 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3086 - accuracy: 0.8839 - val_loss: 0.2911 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.9018 - val_loss: 0.2903 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2798 - accuracy: 0.9018 - val_loss: 0.2894 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.8906 - val_loss: 0.2885 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2929 - accuracy: 0.8884 - val_loss: 0.2877 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2823 - accuracy: 0.8973 - val_loss: 0.2868 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8973 - val_loss: 0.2861 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2631 - accuracy: 0.9040 - val_loss: 0.2854 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2750 - accuracy: 0.8996 - val_loss: 0.2847 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2760 - accuracy: 0.9040 - val_loss: 0.2841 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.9107 - val_loss: 0.2834 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2770 - accuracy: 0.8929 - val_loss: 0.2828 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2659 - accuracy: 0.9152 - val_loss: 0.2821 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.9018 - val_loss: 0.2815 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2772 - accuracy: 0.9152 - val_loss: 0.2808 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2547 - accuracy: 0.9107 - val_loss: 0.2803 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2668 - accuracy: 0.9040 - val_loss: 0.2797 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2682 - accuracy: 0.8996 - val_loss: 0.2789 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2561 - accuracy: 0.9062 - val_loss: 0.2784 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2522 - accuracy: 0.9286 - val_loss: 0.2778 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2496 - accuracy: 0.9129 - val_loss: 0.2773 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2431 - accuracy: 0.9241 - val_loss: 0.2768 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2549 - accuracy: 0.9152 - val_loss: 0.2762 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2537 - accuracy: 0.9107 - val_loss: 0.2757 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.9129 - val_loss: 0.2752 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2415 - accuracy: 0.9152 - val_loss: 0.2747 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2451 - accuracy: 0.9018 - val_loss: 0.2742 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2294 - accuracy: 0.9174 - val_loss: 0.2737 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2295 - accuracy: 0.9219 - val_loss: 0.2733 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.9219 - val_loss: 0.2729 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2403 - accuracy: 0.9241 - val_loss: 0.2726 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.9219 - val_loss: 0.2721 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.9219 - val_loss: 0.2717 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.9286 - val_loss: 0.2713 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2412 - accuracy: 0.9174 - val_loss: 0.2709 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.9196 - val_loss: 0.2705 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2272 - accuracy: 0.9263 - val_loss: 0.2701 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.9196 - val_loss: 0.2697 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.9263 - val_loss: 0.2692 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.9286 - val_loss: 0.2689 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2215 - accuracy: 0.9308 - val_loss: 0.2686 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.9263 - val_loss: 0.2682 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2258 - accuracy: 0.9196 - val_loss: 0.2678 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2320 - accuracy: 0.9219 - val_loss: 0.2674 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.9219 - val_loss: 0.2671 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2301 - accuracy: 0.9353 - val_loss: 0.2668 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2337 - accuracy: 0.9196 - val_loss: 0.2664 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2355 - accuracy: 0.9308 - val_loss: 0.2660 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.9263 - val_loss: 0.2656 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2284 - accuracy: 0.9219 - val_loss: 0.2653 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2216 - accuracy: 0.9286 - val_loss: 0.2649 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9375 - val_loss: 0.2645 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9286 - val_loss: 0.2641 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9375 - val_loss: 0.2638 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2206 - accuracy: 0.9286 - val_loss: 0.2635 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2105 - accuracy: 0.9397 - val_loss: 0.2632 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2064 - accuracy: 0.9375 - val_loss: 0.2629 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.9219 - val_loss: 0.2626 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2050 - accuracy: 0.9375 - val_loss: 0.2623 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2107 - accuracy: 0.9330 - val_loss: 0.2619 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9330 - val_loss: 0.2616 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1990 - accuracy: 0.9375 - val_loss: 0.2613 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2086 - accuracy: 0.9375 - val_loss: 0.2611 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2083 - accuracy: 0.9353 - val_loss: 0.2607 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9397 - val_loss: 0.2604 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2091 - accuracy: 0.9330 - val_loss: 0.2601 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2059 - accuracy: 0.9375 - val_loss: 0.2598 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2041 - accuracy: 0.9330 - val_loss: 0.2595 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2053 - accuracy: 0.9420 - val_loss: 0.2591 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2121 - accuracy: 0.9330 - val_loss: 0.2589 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1976 - accuracy: 0.9375 - val_loss: 0.2585 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1940 - accuracy: 0.9420 - val_loss: 0.2582 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1995 - accuracy: 0.9397 - val_loss: 0.2579 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1949 - accuracy: 0.9420 - val_loss: 0.2576 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1926 - accuracy: 0.9397 - val_loss: 0.2573 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1961 - accuracy: 0.9420 - val_loss: 0.2570 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2040 - accuracy: 0.9442 - val_loss: 0.2567 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1963 - accuracy: 0.9442 - val_loss: 0.2564 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2036 - accuracy: 0.9353 - val_loss: 0.2563 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1947 - accuracy: 0.9442 - val_loss: 0.2561 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1902 - accuracy: 0.9397 - val_loss: 0.2558 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9464 - val_loss: 0.2555 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9420 - val_loss: 0.2552 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9442 - val_loss: 0.2549 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1957 - accuracy: 0.9420 - val_loss: 0.2546 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1878 - accuracy: 0.9554 - val_loss: 0.2543 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.9397 - val_loss: 0.2540 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9509 - val_loss: 0.2537 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1880 - accuracy: 0.9442 - val_loss: 0.2535 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.9464 - val_loss: 0.2533 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1961 - accuracy: 0.9308 - val_loss: 0.2530 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1829 - accuracy: 0.9487 - val_loss: 0.2528 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1835 - accuracy: 0.9487 - val_loss: 0.2526 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1914 - accuracy: 0.9442 - val_loss: 0.2523 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9464 - val_loss: 0.2520 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1867 - accuracy: 0.9509 - val_loss: 0.2517 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9442 - val_loss: 0.2515 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.9397 - val_loss: 0.2511 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9442 - val_loss: 0.2509 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1881 - accuracy: 0.9464 - val_loss: 0.2507 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9397 - val_loss: 0.2504 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9464 - val_loss: 0.2501 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1846 - accuracy: 0.9397 - val_loss: 0.2499 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1787 - accuracy: 0.9442 - val_loss: 0.2497 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1815 - accuracy: 0.9442 - val_loss: 0.2494 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1774 - accuracy: 0.9442 - val_loss: 0.2492 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1746 - accuracy: 0.9554 - val_loss: 0.2489 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1663 - accuracy: 0.9509 - val_loss: 0.2487 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1756 - accuracy: 0.9487 - val_loss: 0.2484 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1810 - accuracy: 0.9442 - val_loss: 0.2481 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9487 - val_loss: 0.2478 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1836 - accuracy: 0.9487 - val_loss: 0.2475 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.9442 - val_loss: 0.2472 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1706 - accuracy: 0.9487 - val_loss: 0.2469 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9487 - val_loss: 0.2467 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1776 - accuracy: 0.9487 - val_loss: 0.2465 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1762 - accuracy: 0.9464 - val_loss: 0.2463 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.9509 - val_loss: 0.2461 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1755 - accuracy: 0.9464 - val_loss: 0.2458 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1740 - accuracy: 0.9509 - val_loss: 0.2456 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.9487 - val_loss: 0.2454 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1720 - accuracy: 0.9509 - val_loss: 0.2453 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1723 - accuracy: 0.9487 - val_loss: 0.2451 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9531 - val_loss: 0.2448 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1745 - accuracy: 0.9509 - val_loss: 0.2445 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9509 - val_loss: 0.2443 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9531 - val_loss: 0.2441 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.9509 - val_loss: 0.2439 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.9464 - val_loss: 0.2437 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1788 - accuracy: 0.9509 - val_loss: 0.2434 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9464 - val_loss: 0.2432 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9420 - val_loss: 0.2431 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.9509 - val_loss: 0.2428 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1749 - accuracy: 0.9464 - val_loss: 0.2426 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1682 - accuracy: 0.9464 - val_loss: 0.2424 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9442 - val_loss: 0.2422 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1614 - accuracy: 0.9531 - val_loss: 0.2420 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9442 - val_loss: 0.2418 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.9464 - val_loss: 0.2416 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1674 - accuracy: 0.9509 - val_loss: 0.2414 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9576 - val_loss: 0.2411 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1614 - accuracy: 0.9554 - val_loss: 0.2409 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1617 - accuracy: 0.9531 - val_loss: 0.2407 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9531 - val_loss: 0.2406 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1622 - accuracy: 0.9531 - val_loss: 0.2405 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1729 - accuracy: 0.9464 - val_loss: 0.2403 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1638 - accuracy: 0.9509 - val_loss: 0.2400 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1697 - accuracy: 0.9509 - val_loss: 0.2398 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1608 - accuracy: 0.9531 - val_loss: 0.2396 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9531 - val_loss: 0.2394 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9531 - val_loss: 0.2392 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1685 - accuracy: 0.9576 - val_loss: 0.2390 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9509 - val_loss: 0.2388 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1596 - accuracy: 0.9621 - val_loss: 0.2386 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1600 - accuracy: 0.9464 - val_loss: 0.2385 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1542 - accuracy: 0.9554 - val_loss: 0.2383 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.9576 - val_loss: 0.2382 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.9554 - val_loss: 0.2380 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9554 - val_loss: 0.2378 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.9442 - val_loss: 0.2377 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1588 - accuracy: 0.9509 - val_loss: 0.2375 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1575 - accuracy: 0.9576 - val_loss: 0.2374 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1536 - accuracy: 0.9554 - val_loss: 0.2372 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1586 - accuracy: 0.9554 - val_loss: 0.2370 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.9531 - val_loss: 0.2369 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1598 - accuracy: 0.9509 - val_loss: 0.2368 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.9531 - val_loss: 0.2366 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1475 - accuracy: 0.9643 - val_loss: 0.2365 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.9554 - val_loss: 0.2363 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Loss: 0.2363, Accuracy: 91.96%\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 2s 11ms/step - loss: 0.8080 - accuracy: 0.5000 - val_loss: 0.7664 - val_accuracy: 0.4732 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6746 - accuracy: 0.6339 - val_loss: 0.6839 - val_accuracy: 0.5804 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6316 - accuracy: 0.6719 - val_loss: 0.6205 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5786 - accuracy: 0.7232 - val_loss: 0.5730 - val_accuracy: 0.6964 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5297 - accuracy: 0.7433 - val_loss: 0.5361 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.4939 - accuracy: 0.7902 - val_loss: 0.5052 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4720 - accuracy: 0.7969 - val_loss: 0.4783 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.4324 - accuracy: 0.8125 - val_loss: 0.4568 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.8125 - val_loss: 0.4382 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4098 - accuracy: 0.8170 - val_loss: 0.4220 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4068 - accuracy: 0.8438 - val_loss: 0.4088 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3916 - accuracy: 0.8482 - val_loss: 0.3966 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8616 - val_loss: 0.3858 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3625 - accuracy: 0.8594 - val_loss: 0.3768 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8527 - val_loss: 0.3684 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3505 - accuracy: 0.8683 - val_loss: 0.3612 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8661 - val_loss: 0.3548 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8683 - val_loss: 0.3488 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8817 - val_loss: 0.3429 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3313 - accuracy: 0.8906 - val_loss: 0.3377 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8795 - val_loss: 0.3327 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8750 - val_loss: 0.3282 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3135 - accuracy: 0.8839 - val_loss: 0.3241 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3089 - accuracy: 0.8728 - val_loss: 0.3203 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3016 - accuracy: 0.8951 - val_loss: 0.3165 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3088 - accuracy: 0.8973 - val_loss: 0.3132 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8906 - val_loss: 0.3099 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3104 - accuracy: 0.8839 - val_loss: 0.3069 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2929 - accuracy: 0.8906 - val_loss: 0.3042 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2942 - accuracy: 0.8973 - val_loss: 0.3016 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2788 - accuracy: 0.8996 - val_loss: 0.2989 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2846 - accuracy: 0.8951 - val_loss: 0.2964 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2996 - accuracy: 0.8906 - val_loss: 0.2940 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2786 - accuracy: 0.9062 - val_loss: 0.2918 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2783 - accuracy: 0.9018 - val_loss: 0.2897 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2922 - accuracy: 0.8973 - val_loss: 0.2876 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.8951 - val_loss: 0.2856 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2800 - accuracy: 0.8973 - val_loss: 0.2839 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2732 - accuracy: 0.9018 - val_loss: 0.2820 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2828 - accuracy: 0.9107 - val_loss: 0.2801 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2735 - accuracy: 0.9129 - val_loss: 0.2785 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2675 - accuracy: 0.9129 - val_loss: 0.2767 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.9107 - val_loss: 0.2751 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.9018 - val_loss: 0.2734 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2559 - accuracy: 0.9107 - val_loss: 0.2718 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2659 - accuracy: 0.9107 - val_loss: 0.2702 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2608 - accuracy: 0.9085 - val_loss: 0.2688 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2675 - accuracy: 0.8929 - val_loss: 0.2674 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2642 - accuracy: 0.8996 - val_loss: 0.2658 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2548 - accuracy: 0.9219 - val_loss: 0.2643 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2522 - accuracy: 0.9085 - val_loss: 0.2629 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2611 - accuracy: 0.9085 - val_loss: 0.2616 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2569 - accuracy: 0.9107 - val_loss: 0.2603 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.9196 - val_loss: 0.2590 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2453 - accuracy: 0.9174 - val_loss: 0.2577 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2432 - accuracy: 0.9129 - val_loss: 0.2565 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2377 - accuracy: 0.9241 - val_loss: 0.2554 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9085 - val_loss: 0.2542 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2349 - accuracy: 0.9219 - val_loss: 0.2531 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2495 - accuracy: 0.9129 - val_loss: 0.2520 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2383 - accuracy: 0.9107 - val_loss: 0.2510 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9174 - val_loss: 0.2501 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2445 - accuracy: 0.9107 - val_loss: 0.2491 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2421 - accuracy: 0.9152 - val_loss: 0.2481 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2438 - accuracy: 0.9174 - val_loss: 0.2471 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.9040 - val_loss: 0.2462 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.9107 - val_loss: 0.2453 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.9241 - val_loss: 0.2442 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2363 - accuracy: 0.9107 - val_loss: 0.2434 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2359 - accuracy: 0.9219 - val_loss: 0.2425 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.9152 - val_loss: 0.2416 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2314 - accuracy: 0.9286 - val_loss: 0.2407 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2307 - accuracy: 0.9330 - val_loss: 0.2398 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2412 - accuracy: 0.9196 - val_loss: 0.2390 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.9196 - val_loss: 0.2383 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2333 - accuracy: 0.9174 - val_loss: 0.2376 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2394 - accuracy: 0.9196 - val_loss: 0.2367 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2412 - accuracy: 0.9196 - val_loss: 0.2360 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.9241 - val_loss: 0.2352 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.9308 - val_loss: 0.2345 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2309 - accuracy: 0.9219 - val_loss: 0.2337 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2287 - accuracy: 0.9263 - val_loss: 0.2329 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2294 - accuracy: 0.9286 - val_loss: 0.2322 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2291 - accuracy: 0.9174 - val_loss: 0.2314 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2266 - accuracy: 0.9219 - val_loss: 0.2306 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9263 - val_loss: 0.2300 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.9174 - val_loss: 0.2293 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2167 - accuracy: 0.9286 - val_loss: 0.2286 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2095 - accuracy: 0.9330 - val_loss: 0.2279 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2231 - accuracy: 0.9241 - val_loss: 0.2273 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2184 - accuracy: 0.9263 - val_loss: 0.2266 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2211 - accuracy: 0.9263 - val_loss: 0.2261 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9330 - val_loss: 0.2255 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2228 - accuracy: 0.9241 - val_loss: 0.2248 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2185 - accuracy: 0.9219 - val_loss: 0.2241 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.9241 - val_loss: 0.2235 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9263 - val_loss: 0.2230 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2152 - accuracy: 0.9241 - val_loss: 0.2224 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2092 - accuracy: 0.9353 - val_loss: 0.2218 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2138 - accuracy: 0.9263 - val_loss: 0.2212 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2066 - accuracy: 0.9420 - val_loss: 0.2207 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9308 - val_loss: 0.2200 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2028 - accuracy: 0.9308 - val_loss: 0.2195 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2060 - accuracy: 0.9353 - val_loss: 0.2189 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9397 - val_loss: 0.2184 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2124 - accuracy: 0.9330 - val_loss: 0.2179 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9397 - val_loss: 0.2174 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2090 - accuracy: 0.9308 - val_loss: 0.2169 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2106 - accuracy: 0.9308 - val_loss: 0.2163 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2022 - accuracy: 0.9397 - val_loss: 0.2158 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1911 - accuracy: 0.9330 - val_loss: 0.2152 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.9308 - val_loss: 0.2147 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2068 - accuracy: 0.9442 - val_loss: 0.2141 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1999 - accuracy: 0.9353 - val_loss: 0.2137 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1929 - accuracy: 0.9353 - val_loss: 0.2132 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2006 - accuracy: 0.9375 - val_loss: 0.2128 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1990 - accuracy: 0.9353 - val_loss: 0.2123 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9420 - val_loss: 0.2117 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.9263 - val_loss: 0.2112 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9353 - val_loss: 0.2108 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2001 - accuracy: 0.9397 - val_loss: 0.2103 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1993 - accuracy: 0.9375 - val_loss: 0.2098 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1973 - accuracy: 0.9375 - val_loss: 0.2093 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1899 - accuracy: 0.9375 - val_loss: 0.2088 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9263 - val_loss: 0.2084 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2012 - accuracy: 0.9353 - val_loss: 0.2079 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1899 - accuracy: 0.9375 - val_loss: 0.2075 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1920 - accuracy: 0.9397 - val_loss: 0.2070 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1992 - accuracy: 0.9397 - val_loss: 0.2066 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1938 - accuracy: 0.9375 - val_loss: 0.2062 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.9375 - val_loss: 0.2057 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1942 - accuracy: 0.9442 - val_loss: 0.2054 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.9487 - val_loss: 0.2050 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1920 - accuracy: 0.9420 - val_loss: 0.2046 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9464 - val_loss: 0.2041 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1845 - accuracy: 0.9487 - val_loss: 0.2037 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1900 - accuracy: 0.9487 - val_loss: 0.2034 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1942 - accuracy: 0.9442 - val_loss: 0.2030 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1865 - accuracy: 0.9464 - val_loss: 0.2026 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1928 - accuracy: 0.9375 - val_loss: 0.2021 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1958 - accuracy: 0.9397 - val_loss: 0.2017 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9420 - val_loss: 0.2013 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9464 - val_loss: 0.2008 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.9397 - val_loss: 0.2005 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.9442 - val_loss: 0.2001 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1804 - accuracy: 0.9487 - val_loss: 0.1997 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1891 - accuracy: 0.9375 - val_loss: 0.1994 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9420 - val_loss: 0.1990 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1763 - accuracy: 0.9442 - val_loss: 0.1987 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1892 - accuracy: 0.9420 - val_loss: 0.1984 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1866 - accuracy: 0.9420 - val_loss: 0.1981 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9442 - val_loss: 0.1977 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1836 - accuracy: 0.9464 - val_loss: 0.1974 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1848 - accuracy: 0.9420 - val_loss: 0.1970 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1867 - accuracy: 0.9442 - val_loss: 0.1966 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1890 - accuracy: 0.9442 - val_loss: 0.1963 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9420 - val_loss: 0.1961 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.9420 - val_loss: 0.1957 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1820 - accuracy: 0.9509 - val_loss: 0.1953 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9375 - val_loss: 0.1950 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1804 - accuracy: 0.9420 - val_loss: 0.1947 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9464 - val_loss: 0.1943 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1809 - accuracy: 0.9487 - val_loss: 0.1940 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1907 - accuracy: 0.9420 - val_loss: 0.1936 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1802 - accuracy: 0.9420 - val_loss: 0.1933 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1709 - accuracy: 0.9442 - val_loss: 0.1930 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.9442 - val_loss: 0.1926 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1786 - accuracy: 0.9464 - val_loss: 0.1923 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1709 - accuracy: 0.9576 - val_loss: 0.1919 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1798 - accuracy: 0.9487 - val_loss: 0.1916 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1723 - accuracy: 0.9464 - val_loss: 0.1913 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1757 - accuracy: 0.9442 - val_loss: 0.1909 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1807 - accuracy: 0.9397 - val_loss: 0.1906 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9487 - val_loss: 0.1904 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9397 - val_loss: 0.1901 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1764 - accuracy: 0.9509 - val_loss: 0.1898 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1700 - accuracy: 0.9487 - val_loss: 0.1895 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1731 - accuracy: 0.9509 - val_loss: 0.1892 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.9509 - val_loss: 0.1889 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1689 - accuracy: 0.9442 - val_loss: 0.1886 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1742 - accuracy: 0.9420 - val_loss: 0.1884 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.9420 - val_loss: 0.1880 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9464 - val_loss: 0.1877 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1741 - accuracy: 0.9509 - val_loss: 0.1874 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9487 - val_loss: 0.1871 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1765 - accuracy: 0.9397 - val_loss: 0.1868 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1712 - accuracy: 0.9464 - val_loss: 0.1865 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9487 - val_loss: 0.1862 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1635 - accuracy: 0.9487 - val_loss: 0.1858 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1668 - accuracy: 0.9509 - val_loss: 0.1856 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1722 - accuracy: 0.9442 - val_loss: 0.1853 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1699 - accuracy: 0.9487 - val_loss: 0.1850 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1611 - accuracy: 0.9554 - val_loss: 0.1847 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1719 - accuracy: 0.9487 - val_loss: 0.1844 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1758 - accuracy: 0.9442 - val_loss: 0.1841 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9509 - val_loss: 0.1839 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1599 - accuracy: 0.9487 - val_loss: 0.1837 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1689 - accuracy: 0.9464 - val_loss: 0.1834 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1738 - accuracy: 0.9509 - val_loss: 0.1831 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.9531 - val_loss: 0.1828 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Loss: 0.1828, Accuracy: 92.86%\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 1s 8ms/step - loss: 0.7557 - accuracy: 0.5714 - val_loss: 0.7241 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.6238 - accuracy: 0.6853 - val_loss: 0.6347 - val_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.5369 - accuracy: 0.7455 - val_loss: 0.5690 - val_accuracy: 0.6607 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.5022 - accuracy: 0.7879 - val_loss: 0.5182 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4647 - accuracy: 0.8058 - val_loss: 0.4811 - val_accuracy: 0.7232 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4297 - accuracy: 0.8170 - val_loss: 0.4519 - val_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4167 - accuracy: 0.8371 - val_loss: 0.4287 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3913 - accuracy: 0.8571 - val_loss: 0.4104 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3755 - accuracy: 0.8571 - val_loss: 0.3941 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3778 - accuracy: 0.8549 - val_loss: 0.3813 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3572 - accuracy: 0.8661 - val_loss: 0.3702 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3352 - accuracy: 0.8817 - val_loss: 0.3608 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3517 - accuracy: 0.8661 - val_loss: 0.3524 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8817 - val_loss: 0.3453 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3183 - accuracy: 0.8839 - val_loss: 0.3390 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3223 - accuracy: 0.8884 - val_loss: 0.3336 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3272 - accuracy: 0.8705 - val_loss: 0.3286 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.3172 - accuracy: 0.8862 - val_loss: 0.3240 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.8772 - val_loss: 0.3202 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2944 - accuracy: 0.8839 - val_loss: 0.3161 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3095 - accuracy: 0.8795 - val_loss: 0.3126 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2961 - accuracy: 0.8929 - val_loss: 0.3097 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2970 - accuracy: 0.8884 - val_loss: 0.3070 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2965 - accuracy: 0.8929 - val_loss: 0.3042 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2790 - accuracy: 0.8929 - val_loss: 0.3018 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2927 - accuracy: 0.8906 - val_loss: 0.2995 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2778 - accuracy: 0.8973 - val_loss: 0.2974 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.9062 - val_loss: 0.2952 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8839 - val_loss: 0.2936 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2833 - accuracy: 0.8862 - val_loss: 0.2918 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2922 - accuracy: 0.8884 - val_loss: 0.2903 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2761 - accuracy: 0.9018 - val_loss: 0.2888 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2829 - accuracy: 0.9040 - val_loss: 0.2873 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2681 - accuracy: 0.9085 - val_loss: 0.2857 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2669 - accuracy: 0.8996 - val_loss: 0.2844 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2723 - accuracy: 0.9040 - val_loss: 0.2830 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.8951 - val_loss: 0.2818 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2599 - accuracy: 0.8951 - val_loss: 0.2807 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2590 - accuracy: 0.9107 - val_loss: 0.2797 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2680 - accuracy: 0.9085 - val_loss: 0.2786 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2550 - accuracy: 0.9107 - val_loss: 0.2776 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2630 - accuracy: 0.9107 - val_loss: 0.2764 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2544 - accuracy: 0.9062 - val_loss: 0.2753 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2606 - accuracy: 0.8973 - val_loss: 0.2742 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2665 - accuracy: 0.8996 - val_loss: 0.2732 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2475 - accuracy: 0.9085 - val_loss: 0.2724 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.9085 - val_loss: 0.2715 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2568 - accuracy: 0.9174 - val_loss: 0.2707 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2530 - accuracy: 0.9040 - val_loss: 0.2699 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2567 - accuracy: 0.9062 - val_loss: 0.2692 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2574 - accuracy: 0.9018 - val_loss: 0.2684 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2510 - accuracy: 0.9107 - val_loss: 0.2676 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2504 - accuracy: 0.9129 - val_loss: 0.2669 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2363 - accuracy: 0.9129 - val_loss: 0.2663 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2500 - accuracy: 0.9107 - val_loss: 0.2656 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2474 - accuracy: 0.9152 - val_loss: 0.2650 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2546 - accuracy: 0.9062 - val_loss: 0.2644 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2501 - accuracy: 0.9085 - val_loss: 0.2637 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2455 - accuracy: 0.9085 - val_loss: 0.2631 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2340 - accuracy: 0.9040 - val_loss: 0.2624 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.9107 - val_loss: 0.2618 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9062 - val_loss: 0.2611 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2410 - accuracy: 0.9152 - val_loss: 0.2604 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2378 - accuracy: 0.9174 - val_loss: 0.2598 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2286 - accuracy: 0.9129 - val_loss: 0.2592 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2359 - accuracy: 0.9129 - val_loss: 0.2587 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2294 - accuracy: 0.9152 - val_loss: 0.2581 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2255 - accuracy: 0.9152 - val_loss: 0.2575 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2198 - accuracy: 0.9129 - val_loss: 0.2569 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2297 - accuracy: 0.9174 - val_loss: 0.2564 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2267 - accuracy: 0.9219 - val_loss: 0.2558 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2297 - accuracy: 0.9129 - val_loss: 0.2553 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2237 - accuracy: 0.9241 - val_loss: 0.2547 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2210 - accuracy: 0.9263 - val_loss: 0.2543 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2219 - accuracy: 0.9196 - val_loss: 0.2537 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2276 - accuracy: 0.9196 - val_loss: 0.2533 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2051 - accuracy: 0.9286 - val_loss: 0.2527 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2171 - accuracy: 0.9241 - val_loss: 0.2522 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.9241 - val_loss: 0.2518 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9308 - val_loss: 0.2513 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2177 - accuracy: 0.9263 - val_loss: 0.2509 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2093 - accuracy: 0.9241 - val_loss: 0.2505 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2125 - accuracy: 0.9219 - val_loss: 0.2500 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2185 - accuracy: 0.9263 - val_loss: 0.2496 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9353 - val_loss: 0.2492 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2133 - accuracy: 0.9196 - val_loss: 0.2487 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2142 - accuracy: 0.9174 - val_loss: 0.2483 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2103 - accuracy: 0.9375 - val_loss: 0.2480 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9286 - val_loss: 0.2476 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9286 - val_loss: 0.2471 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.9308 - val_loss: 0.2468 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2173 - accuracy: 0.9353 - val_loss: 0.2463 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2061 - accuracy: 0.9353 - val_loss: 0.2459 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2095 - accuracy: 0.9263 - val_loss: 0.2455 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1998 - accuracy: 0.9286 - val_loss: 0.2452 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1945 - accuracy: 0.9308 - val_loss: 0.2448 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2024 - accuracy: 0.9308 - val_loss: 0.2445 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2016 - accuracy: 0.9353 - val_loss: 0.2441 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1953 - accuracy: 0.9330 - val_loss: 0.2438 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2016 - accuracy: 0.9330 - val_loss: 0.2435 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2084 - accuracy: 0.9353 - val_loss: 0.2432 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1973 - accuracy: 0.9375 - val_loss: 0.2429 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2020 - accuracy: 0.9330 - val_loss: 0.2425 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2043 - accuracy: 0.9330 - val_loss: 0.2422 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9241 - val_loss: 0.2418 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1928 - accuracy: 0.9308 - val_loss: 0.2415 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1935 - accuracy: 0.9397 - val_loss: 0.2412 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1990 - accuracy: 0.9308 - val_loss: 0.2409 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1972 - accuracy: 0.9330 - val_loss: 0.2406 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2033 - accuracy: 0.9241 - val_loss: 0.2402 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2016 - accuracy: 0.9308 - val_loss: 0.2399 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1969 - accuracy: 0.9308 - val_loss: 0.2395 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2010 - accuracy: 0.9286 - val_loss: 0.2392 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1887 - accuracy: 0.9353 - val_loss: 0.2390 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9397 - val_loss: 0.2387 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.9330 - val_loss: 0.2384 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1961 - accuracy: 0.9420 - val_loss: 0.2381 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1895 - accuracy: 0.9420 - val_loss: 0.2378 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1979 - accuracy: 0.9353 - val_loss: 0.2374 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1909 - accuracy: 0.9375 - val_loss: 0.2371 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2009 - accuracy: 0.9330 - val_loss: 0.2368 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1920 - accuracy: 0.9397 - val_loss: 0.2365 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1880 - accuracy: 0.9330 - val_loss: 0.2362 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9420 - val_loss: 0.2359 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1965 - accuracy: 0.9353 - val_loss: 0.2357 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1856 - accuracy: 0.9397 - val_loss: 0.2354 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1819 - accuracy: 0.9397 - val_loss: 0.2351 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1845 - accuracy: 0.9308 - val_loss: 0.2348 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1885 - accuracy: 0.9375 - val_loss: 0.2345 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9397 - val_loss: 0.2343 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1803 - accuracy: 0.9442 - val_loss: 0.2340 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9464 - val_loss: 0.2338 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1890 - accuracy: 0.9353 - val_loss: 0.2335 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9375 - val_loss: 0.2332 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.9420 - val_loss: 0.2329 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9375 - val_loss: 0.2327 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9487 - val_loss: 0.2325 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.9442 - val_loss: 0.2322 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1783 - accuracy: 0.9397 - val_loss: 0.2319 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9420 - val_loss: 0.2317 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9487 - val_loss: 0.2315 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1782 - accuracy: 0.9375 - val_loss: 0.2313 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1795 - accuracy: 0.9442 - val_loss: 0.2311 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9330 - val_loss: 0.2309 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1809 - accuracy: 0.9375 - val_loss: 0.2307 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1805 - accuracy: 0.9375 - val_loss: 0.2304 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9420 - val_loss: 0.2302 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1745 - accuracy: 0.9442 - val_loss: 0.2299 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1617 - accuracy: 0.9487 - val_loss: 0.2297 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1836 - accuracy: 0.9464 - val_loss: 0.2295 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1857 - accuracy: 0.9442 - val_loss: 0.2293 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1697 - accuracy: 0.9442 - val_loss: 0.2290 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1743 - accuracy: 0.9442 - val_loss: 0.2288 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9487 - val_loss: 0.2286 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9442 - val_loss: 0.2284 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1761 - accuracy: 0.9442 - val_loss: 0.2282 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1691 - accuracy: 0.9464 - val_loss: 0.2280 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.9464 - val_loss: 0.2278 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9442 - val_loss: 0.2276 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1700 - accuracy: 0.9420 - val_loss: 0.2274 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1616 - accuracy: 0.9487 - val_loss: 0.2272 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1708 - accuracy: 0.9464 - val_loss: 0.2271 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1783 - accuracy: 0.9420 - val_loss: 0.2268 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1781 - accuracy: 0.9375 - val_loss: 0.2267 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1568 - accuracy: 0.9464 - val_loss: 0.2265 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1551 - accuracy: 0.9509 - val_loss: 0.2262 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1726 - accuracy: 0.9509 - val_loss: 0.2261 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.9420 - val_loss: 0.2258 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.9420 - val_loss: 0.2256 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1616 - accuracy: 0.9576 - val_loss: 0.2255 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1736 - accuracy: 0.9464 - val_loss: 0.2253 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1744 - accuracy: 0.9375 - val_loss: 0.2251 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1662 - accuracy: 0.9420 - val_loss: 0.2248 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1642 - accuracy: 0.9531 - val_loss: 0.2247 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1576 - accuracy: 0.9554 - val_loss: 0.2245 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1547 - accuracy: 0.9554 - val_loss: 0.2243 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9464 - val_loss: 0.2241 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1702 - accuracy: 0.9487 - val_loss: 0.2240 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1633 - accuracy: 0.9464 - val_loss: 0.2238 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9509 - val_loss: 0.2236 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1571 - accuracy: 0.9487 - val_loss: 0.2234 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1579 - accuracy: 0.9531 - val_loss: 0.2233 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1627 - accuracy: 0.9442 - val_loss: 0.2231 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1769 - accuracy: 0.9487 - val_loss: 0.2229 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1644 - accuracy: 0.9464 - val_loss: 0.2227 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9509 - val_loss: 0.2225 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1536 - accuracy: 0.9576 - val_loss: 0.2223 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1570 - accuracy: 0.9442 - val_loss: 0.2222 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1644 - accuracy: 0.9509 - val_loss: 0.2220 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1607 - accuracy: 0.9442 - val_loss: 0.2218 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9397 - val_loss: 0.2216 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 1s 16ms/step - loss: 0.1589 - accuracy: 0.9464 - val_loss: 0.2215 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.1560 - accuracy: 0.9464 - val_loss: 0.2213 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1553 - accuracy: 0.9509 - val_loss: 0.2212 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1602 - accuracy: 0.9464 - val_loss: 0.2210 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.1590 - accuracy: 0.9464 - val_loss: 0.2208 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1491 - accuracy: 0.9509 - val_loss: 0.2207 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1521 - accuracy: 0.9464 - val_loss: 0.2205 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1590 - accuracy: 0.9554 - val_loss: 0.2204 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1498 - accuracy: 0.9509 - val_loss: 0.2203 - val_accuracy: 0.9375 - lr: 0.0010\n",
      "Loss: 0.2203, Accuracy: 93.75%\n",
      "(448, 1, 86)\n",
      "(112, 1, 86)\n",
      "(448,)\n",
      "(112,)\n",
      "Epoch 1/200\n",
      "45/45 [==============================] - 2s 11ms/step - loss: 0.8109 - accuracy: 0.5603 - val_loss: 0.6803 - val_accuracy: 0.6161 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.6429 - val_loss: 0.6218 - val_accuracy: 0.6875 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.6505 - accuracy: 0.6719 - val_loss: 0.5764 - val_accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.5860 - accuracy: 0.7009 - val_loss: 0.5419 - val_accuracy: 0.7321 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.5671 - accuracy: 0.7321 - val_loss: 0.5135 - val_accuracy: 0.7232 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.5249 - accuracy: 0.7344 - val_loss: 0.4908 - val_accuracy: 0.7321 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.4997 - accuracy: 0.7589 - val_loss: 0.4724 - val_accuracy: 0.7411 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.4782 - accuracy: 0.7768 - val_loss: 0.4572 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.7991 - val_loss: 0.4442 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4430 - accuracy: 0.7879 - val_loss: 0.4328 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4112 - accuracy: 0.8259 - val_loss: 0.4241 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4058 - accuracy: 0.8237 - val_loss: 0.4158 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.4017 - accuracy: 0.8348 - val_loss: 0.4088 - val_accuracy: 0.7768 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3916 - accuracy: 0.8393 - val_loss: 0.4024 - val_accuracy: 0.7946 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.4000 - accuracy: 0.8304 - val_loss: 0.3967 - val_accuracy: 0.8125 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3593 - accuracy: 0.8527 - val_loss: 0.3915 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3577 - accuracy: 0.8482 - val_loss: 0.3869 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3573 - accuracy: 0.8504 - val_loss: 0.3827 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3389 - accuracy: 0.8638 - val_loss: 0.3791 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3384 - accuracy: 0.8705 - val_loss: 0.3755 - val_accuracy: 0.8482 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3533 - accuracy: 0.8504 - val_loss: 0.3721 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3381 - accuracy: 0.8661 - val_loss: 0.3691 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3395 - accuracy: 0.8616 - val_loss: 0.3661 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3311 - accuracy: 0.8728 - val_loss: 0.3634 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3180 - accuracy: 0.8750 - val_loss: 0.3609 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.3248 - accuracy: 0.8661 - val_loss: 0.3584 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3136 - accuracy: 0.8795 - val_loss: 0.3561 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.8661 - val_loss: 0.3540 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3242 - accuracy: 0.8795 - val_loss: 0.3520 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3075 - accuracy: 0.8817 - val_loss: 0.3500 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3064 - accuracy: 0.8795 - val_loss: 0.3481 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3029 - accuracy: 0.8795 - val_loss: 0.3463 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2967 - accuracy: 0.8817 - val_loss: 0.3447 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2898 - accuracy: 0.8817 - val_loss: 0.3430 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2918 - accuracy: 0.8906 - val_loss: 0.3413 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.3022 - accuracy: 0.8817 - val_loss: 0.3397 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8906 - val_loss: 0.3382 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.8862 - val_loss: 0.3365 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2896 - accuracy: 0.8951 - val_loss: 0.3351 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8906 - val_loss: 0.3336 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2852 - accuracy: 0.9040 - val_loss: 0.3321 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2903 - accuracy: 0.8929 - val_loss: 0.3306 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2773 - accuracy: 0.8929 - val_loss: 0.3293 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2804 - accuracy: 0.8906 - val_loss: 0.3280 - val_accuracy: 0.8661 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2809 - accuracy: 0.8839 - val_loss: 0.3267 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2754 - accuracy: 0.8951 - val_loss: 0.3255 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2768 - accuracy: 0.9018 - val_loss: 0.3243 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2747 - accuracy: 0.8906 - val_loss: 0.3231 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.8906 - val_loss: 0.3219 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2738 - accuracy: 0.8951 - val_loss: 0.3207 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2719 - accuracy: 0.8929 - val_loss: 0.3196 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2522 - accuracy: 0.9040 - val_loss: 0.3185 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2549 - accuracy: 0.9040 - val_loss: 0.3173 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2607 - accuracy: 0.9018 - val_loss: 0.3162 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2660 - accuracy: 0.8884 - val_loss: 0.3151 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2662 - accuracy: 0.8996 - val_loss: 0.3140 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2602 - accuracy: 0.9062 - val_loss: 0.3129 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2566 - accuracy: 0.8951 - val_loss: 0.3119 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2594 - accuracy: 0.9018 - val_loss: 0.3108 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2563 - accuracy: 0.9062 - val_loss: 0.3096 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2547 - accuracy: 0.9129 - val_loss: 0.3087 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2501 - accuracy: 0.9062 - val_loss: 0.3077 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2573 - accuracy: 0.8996 - val_loss: 0.3068 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2567 - accuracy: 0.9062 - val_loss: 0.3058 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2502 - accuracy: 0.8996 - val_loss: 0.3049 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2433 - accuracy: 0.9129 - val_loss: 0.3039 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2533 - accuracy: 0.9040 - val_loss: 0.3030 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2449 - accuracy: 0.9062 - val_loss: 0.3021 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2472 - accuracy: 0.9085 - val_loss: 0.3011 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2384 - accuracy: 0.9107 - val_loss: 0.3002 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2423 - accuracy: 0.9152 - val_loss: 0.2994 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2392 - accuracy: 0.9085 - val_loss: 0.2985 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2394 - accuracy: 0.9040 - val_loss: 0.2975 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2341 - accuracy: 0.9107 - val_loss: 0.2967 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2401 - accuracy: 0.9085 - val_loss: 0.2958 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2303 - accuracy: 0.9107 - val_loss: 0.2950 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2196 - accuracy: 0.9174 - val_loss: 0.2941 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2314 - accuracy: 0.9040 - val_loss: 0.2933 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2395 - accuracy: 0.9107 - val_loss: 0.2923 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2374 - accuracy: 0.9196 - val_loss: 0.2914 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2317 - accuracy: 0.9219 - val_loss: 0.2905 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2268 - accuracy: 0.9107 - val_loss: 0.2898 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2399 - accuracy: 0.9062 - val_loss: 0.2888 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2302 - accuracy: 0.9241 - val_loss: 0.2880 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2209 - accuracy: 0.9174 - val_loss: 0.2872 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2386 - accuracy: 0.9085 - val_loss: 0.2863 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2301 - accuracy: 0.9085 - val_loss: 0.2856 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2240 - accuracy: 0.9263 - val_loss: 0.2848 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2190 - accuracy: 0.9152 - val_loss: 0.2840 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2265 - accuracy: 0.9085 - val_loss: 0.2833 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2321 - accuracy: 0.9129 - val_loss: 0.2825 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2198 - accuracy: 0.9107 - val_loss: 0.2817 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2186 - accuracy: 0.9107 - val_loss: 0.2809 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9241 - val_loss: 0.2802 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2226 - accuracy: 0.9174 - val_loss: 0.2793 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2219 - accuracy: 0.9263 - val_loss: 0.2786 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2193 - accuracy: 0.9241 - val_loss: 0.2778 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2213 - accuracy: 0.9129 - val_loss: 0.2771 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2129 - accuracy: 0.9152 - val_loss: 0.2764 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2150 - accuracy: 0.9241 - val_loss: 0.2757 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2174 - accuracy: 0.9196 - val_loss: 0.2750 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2159 - accuracy: 0.9219 - val_loss: 0.2743 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2290 - accuracy: 0.9241 - val_loss: 0.2735 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2091 - accuracy: 0.9196 - val_loss: 0.2729 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2195 - accuracy: 0.9152 - val_loss: 0.2722 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2138 - accuracy: 0.9263 - val_loss: 0.2715 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2126 - accuracy: 0.9308 - val_loss: 0.2708 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.9152 - val_loss: 0.2701 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2133 - accuracy: 0.9308 - val_loss: 0.2695 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1945 - accuracy: 0.9375 - val_loss: 0.2689 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2166 - accuracy: 0.9152 - val_loss: 0.2682 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2117 - accuracy: 0.9174 - val_loss: 0.2676 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2047 - accuracy: 0.9263 - val_loss: 0.2670 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9219 - val_loss: 0.2663 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2146 - accuracy: 0.9174 - val_loss: 0.2655 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2055 - accuracy: 0.9263 - val_loss: 0.2649 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2100 - accuracy: 0.9219 - val_loss: 0.2643 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2048 - accuracy: 0.9286 - val_loss: 0.2638 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2135 - accuracy: 0.9263 - val_loss: 0.2632 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2063 - accuracy: 0.9330 - val_loss: 0.2626 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.2005 - accuracy: 0.9353 - val_loss: 0.2621 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2001 - accuracy: 0.9308 - val_loss: 0.2616 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2025 - accuracy: 0.9286 - val_loss: 0.2610 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2007 - accuracy: 0.9286 - val_loss: 0.2604 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.2064 - accuracy: 0.9263 - val_loss: 0.2598 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1958 - accuracy: 0.9353 - val_loss: 0.2593 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2110 - accuracy: 0.9286 - val_loss: 0.2587 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1953 - accuracy: 0.9286 - val_loss: 0.2581 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2084 - accuracy: 0.9196 - val_loss: 0.2575 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2009 - accuracy: 0.9353 - val_loss: 0.2570 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2017 - accuracy: 0.9241 - val_loss: 0.2563 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1922 - accuracy: 0.9353 - val_loss: 0.2558 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2002 - accuracy: 0.9263 - val_loss: 0.2552 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1969 - accuracy: 0.9353 - val_loss: 0.2546 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1973 - accuracy: 0.9353 - val_loss: 0.2541 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1930 - accuracy: 0.9330 - val_loss: 0.2535 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2020 - accuracy: 0.9397 - val_loss: 0.2529 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1940 - accuracy: 0.9353 - val_loss: 0.2524 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.2013 - accuracy: 0.9286 - val_loss: 0.2519 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1927 - accuracy: 0.9353 - val_loss: 0.2513 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1989 - accuracy: 0.9308 - val_loss: 0.2507 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1812 - accuracy: 0.9487 - val_loss: 0.2502 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1983 - accuracy: 0.9353 - val_loss: 0.2497 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.9397 - val_loss: 0.2492 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1962 - accuracy: 0.9263 - val_loss: 0.2486 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.9397 - val_loss: 0.2481 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9442 - val_loss: 0.2477 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1925 - accuracy: 0.9330 - val_loss: 0.2472 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1926 - accuracy: 0.9420 - val_loss: 0.2467 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1814 - accuracy: 0.9420 - val_loss: 0.2462 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1859 - accuracy: 0.9375 - val_loss: 0.2457 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1839 - accuracy: 0.9397 - val_loss: 0.2453 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1905 - accuracy: 0.9308 - val_loss: 0.2448 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1872 - accuracy: 0.9330 - val_loss: 0.2443 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1907 - accuracy: 0.9397 - val_loss: 0.2437 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1827 - accuracy: 0.9397 - val_loss: 0.2433 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1881 - accuracy: 0.9330 - val_loss: 0.2428 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1823 - accuracy: 0.9397 - val_loss: 0.2422 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9397 - val_loss: 0.2417 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1885 - accuracy: 0.9464 - val_loss: 0.2413 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.9420 - val_loss: 0.2408 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1732 - accuracy: 0.9375 - val_loss: 0.2404 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9487 - val_loss: 0.2400 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1751 - accuracy: 0.9420 - val_loss: 0.2396 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1748 - accuracy: 0.9464 - val_loss: 0.2391 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1834 - accuracy: 0.9353 - val_loss: 0.2386 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9464 - val_loss: 0.2382 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9308 - val_loss: 0.2377 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9420 - val_loss: 0.2372 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9375 - val_loss: 0.2366 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1749 - accuracy: 0.9464 - val_loss: 0.2362 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1837 - accuracy: 0.9397 - val_loss: 0.2358 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1799 - accuracy: 0.9420 - val_loss: 0.2353 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1730 - accuracy: 0.9531 - val_loss: 0.2350 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9308 - val_loss: 0.2345 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1762 - accuracy: 0.9464 - val_loss: 0.2341 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1743 - accuracy: 0.9442 - val_loss: 0.2337 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1875 - accuracy: 0.9330 - val_loss: 0.2333 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1695 - accuracy: 0.9464 - val_loss: 0.2328 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "45/45 [==============================] - 0s 10ms/step - loss: 0.1842 - accuracy: 0.9353 - val_loss: 0.2324 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "45/45 [==============================] - 0s 9ms/step - loss: 0.1757 - accuracy: 0.9330 - val_loss: 0.2320 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "45/45 [==============================] - 0s 6ms/step - loss: 0.1834 - accuracy: 0.9420 - val_loss: 0.2316 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1766 - accuracy: 0.9353 - val_loss: 0.2312 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "45/45 [==============================] - 0s 8ms/step - loss: 0.1698 - accuracy: 0.9487 - val_loss: 0.2308 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1719 - accuracy: 0.9420 - val_loss: 0.2304 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1711 - accuracy: 0.9420 - val_loss: 0.2301 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1770 - accuracy: 0.9442 - val_loss: 0.2297 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/45 [==============================] - 0s 7ms/step - loss: 0.1718 - accuracy: 0.9487 - val_loss: 0.2293 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1709 - accuracy: 0.9442 - val_loss: 0.2290 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1707 - accuracy: 0.9442 - val_loss: 0.2286 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1688 - accuracy: 0.9531 - val_loss: 0.2283 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.9509 - val_loss: 0.2280 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "45/45 [==============================] - 0s 5ms/step - loss: 0.1752 - accuracy: 0.9397 - val_loss: 0.2276 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1744 - accuracy: 0.9487 - val_loss: 0.2272 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1693 - accuracy: 0.9487 - val_loss: 0.2268 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9420 - val_loss: 0.2265 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1629 - accuracy: 0.9509 - val_loss: 0.2261 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9487 - val_loss: 0.2258 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9464 - val_loss: 0.2255 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 0.1657 - accuracy: 0.9509 - val_loss: 0.2251 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Loss: 0.2251, Accuracy: 91.96%\n",
      "RNN finished in 187.83 sec\n",
      "\n",
      "Average accuracy: 0.9268\n",
      "Average loss: 0.2107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\"\"\"\n",
    "\n",
    "k = 5  # numero di fold\n",
    "kf = KFold(n_splits=k, shuffle = True)\n",
    "\n",
    "# Array per memorizzare le curve di apprendimento\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "f = pd.DataFrame(columns = perfInd)\n",
    "print('Implementing vanilla RNN with k-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    print(x_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  # \"timesteps\" rappresenta il numero di istanti temporali nel segnale EEG, \"features\"  il numero di caratteristiche per ciascun istante temporale\n",
    "    model.add(Dropout(0.18))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Definisco l'ottimizzatore con il learning rate iniziale\n",
    "    initial_learning_rate = 0.001\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # Definisco il learning rate schedule con decay lineare\n",
    "    decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "    decay_rate = 0.1  # Tasso di decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "    # Compilazione del modello\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(x_train, y_train, batch_size = 10, epochs = 200, verbose = 1, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), early_stopping])\n",
    "    # Valuta il modello\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Stampa i risultati di accuracy e loss per la k-esima fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "    # train_loss.append(history.history['loss'])\n",
    "    # train_acc.append(history.history['accuracy'])\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('RNN finished in', t,'sec\\n')\n",
    "\n",
    " # Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting the results\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modello Vanilla RNN con dropout e early stopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(32, input_shape=(None, x_train.shape[-1])))  # \"timesteps\" rappresenta il numero di istanti temporali nel segnale EEG, \"features\"  il numero di caratteristiche per ciascun istante temporale\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilazione del modello\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Definizione dell'early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Addestramento del modello con early stopping\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=8, validation_data=(x_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazione del modello\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "test_loss = history.history['loss']\n",
    "test_acc = history.history['accuracy']\n",
    "\n",
    "epochs = range(len(test_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, test_loss, label='Test loss')\n",
    "plt.plot(epochs, test_acc, label='Test accuracy')\n",
    "plt.title('Test loss and accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
