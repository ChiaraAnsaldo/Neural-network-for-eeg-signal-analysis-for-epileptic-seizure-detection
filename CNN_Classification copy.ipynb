{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import  Dense, Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization, TimeDistributed, Bidirectional, GRU\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from random import shuffle\n",
    "import math\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from EEGModels import EEGNet  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "PathSpectogramFolder='out'\n",
    "OutputPath='model/3/path'\n",
    "OutputPathModels='model/3/'\n",
    "interictalSpectograms=[]\n",
    "preictalSpectograms=[]  #This array contains syntetic data, it's created to have a balance dataset and it's used for training\n",
    "preictalRealSpectograms=[]  #This array containt the real preictal data, it's used for testing\n",
    "#patients = [\"01\"]\n",
    "patients = [\"01\",\"02\",\"03\",\"05\",\"09\"]\n",
    "nSeizure=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSpectogramData(indexPat):\n",
    "    global interictalSpectograms\n",
    "    global preictalSpectograms\n",
    "    global preictalRealSpectograms\n",
    "    global nSeizure\n",
    "    f = open(PathSpectogramFolder+'/paz'+patients[indexPat]+'/legendAllData.txt', 'r')\n",
    "    line=f.readline()\n",
    "    while(not \"SEIZURE\" in line):\n",
    "        line=f.readline()\n",
    "    nSeizure=int(line.split(\":\")[1].strip())\n",
    "    line=f.readline()\n",
    "    line=f.readline()#legge il numero di spectogrammi. non lo salvo dato che non mi serve\n",
    "    nSpectograms=int(line.strip())\n",
    "    nFileForSeizure=math.ceil(math.ceil(nSpectograms/50)/nSeizure)\n",
    "    line=f.readline()#leggo il percorso del primo file\n",
    "    \n",
    "    #Lettura path files Interictal\n",
    "    cont=-1\n",
    "    indFilePathRead=0\n",
    "    while(\"npy\" in line and indFilePathRead<nSeizure*nFileForSeizure):\n",
    "        if(indFilePathRead%nFileForSeizure==0):\n",
    "            interictalSpectograms.append([])\n",
    "            cont=cont+1\n",
    "            interictalSpectograms[cont].append(line.split(' ')[2].rstrip())#.rstrip() remove \\n\n",
    "            indFilePathRead=indFilePathRead+1\n",
    "        else:\n",
    "            if(len(line.split(' '))>=3):\n",
    "                interictalSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "            indFilePathRead=indFilePathRead+1\n",
    "            \n",
    "        line=f.readline()\n",
    "    line=f.readline()#leggo PREICTAL\n",
    "    line=f.readline()#leggo n° spectogram\n",
    "    line=f.readline()#leggo n°seizure(SEIZURE X)\n",
    "\n",
    "    #Lettura path files Preictal\n",
    "    cont=-1\n",
    "    indFilePathRead=0   \n",
    "    #while(line and indFilePathRead<nSeizure*nFileForSeizure):    \n",
    "    while(line.strip()!=\"\"):\n",
    "        if(\"SEIZURE\" in line):\n",
    "            line=f.readline()#ho letto n°seizure(SEIZURE X) perciò scorro in avanti\n",
    "            if(len(line.split(' '))>=3):\n",
    "                preictalSpectograms.append([])\n",
    "                cont=cont+1\n",
    "                preictalSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "                indFilePathRead=indFilePathRead+1\n",
    "        else:\n",
    "            if(len(line.split(' '))>=3):\n",
    "                preictalSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "            indFilePathRead=indFilePathRead+1\n",
    "            \n",
    "        line=f.readline()\n",
    "        \n",
    "    line=f.readline()#leggo REAL_PREICTAL\n",
    "    line=f.readline()#leggo n° spectogram\n",
    "    line=f.readline()#leggo n°seizure(SEIZURE X)\n",
    "\n",
    "    #Lettura path files Real Preictal\n",
    "    cont=-1\n",
    "    while(line):\n",
    "        if(\"SEIZURE\" in line):\n",
    "            line=f.readline()#ho letto n°seizure(SEIZURE X) perciò scorro in avanti\n",
    "            preictalRealSpectograms.append([])\n",
    "            cont=cont+1\n",
    "            preictalRealSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "        else:\n",
    "            preictalRealSpectograms[cont].append(line.split(' ')[2].rstrip())\n",
    "            \n",
    "        line=f.readline()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesPathWithoutSeizure(indexSeizure):\n",
    "    filesPath=[]\n",
    "    for i in range(0, nSeizure):\n",
    "        if(i!=indexSeizure):\n",
    "            filesPath.extend(interictalSpectograms[i])\n",
    "            filesPath.extend(preictalSpectograms[i])\n",
    "    shuffle(filesPath)\n",
    "    return filesPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays_for_training(paths, start=0, end=100):\n",
    "    while True:\n",
    "        from_=int(len(paths)/100*start)\n",
    "        to_=int(len(paths)/100*end)\n",
    "        for i in range(from_, int(to_)):\n",
    "            f=paths[i]\n",
    "            x = np.load(PathSpectogramFolder+f)\n",
    "            x=np.array([x])\n",
    "            x=x.swapaxes(0,1)\n",
    "            x=x[0]\n",
    "            if('P' in f):\n",
    "                y = np.repeat([[0,1]], x.shape[0], axis=0)\n",
    "            else:\n",
    "                y = np.repeat([[1,0]], x.shape[0], axis=0)\n",
    "            yield(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arrays_for_predict(indexPat, paths, start=0, end=100):\n",
    "    print(paths)\n",
    "    while True:\n",
    "        f = open(PathSpectogramFolder+'/paz'+patients[indexPat]+'/legendAllData.txt', 'r')\n",
    "        line=f.readline()\n",
    "        while(not \"SEIZURE\" in line):\n",
    "            line=f.readline()\n",
    "        nSeizure=int(line.split(\":\")[1].strip())\n",
    "        from_=int(len(paths)/100*start)\n",
    "        to_=int(len(paths)/100*end)\n",
    "        for j in 0, nSeizure:\n",
    "            for i in range(from_, int(to_)):\n",
    "                f=paths[j][i]\n",
    "                x = np.load(PathSpectogramFolder+f)\n",
    "                x=np.array([x])\n",
    "                x=x.swapaxes(0,1)\n",
    "                x=x[0]\n",
    "                yield(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "Parameters loaded\n",
      "Patient 01\n",
      "Patient 02\n",
      "Patient 03\n",
      "Patient 05\n",
      "Patient 09\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The `data_format` argument must be one of \"channels_first\", \"channels_last\". Received: channel_first",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m decay_rate \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m  \u001b[39m# Tasso di decay\u001b[39;00m\n\u001b[0;32m     17\u001b[0m lr_schedule \u001b[39m=\u001b[39m ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 19\u001b[0m model \u001b[39m=\u001b[39m EEGNet(nb_classes \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, Chans \u001b[39m=\u001b[39;49m \u001b[39m22\u001b[39;49m, Samples \u001b[39m=\u001b[39;49m \u001b[39m114\u001b[39;49m)\n\u001b[0;32m     21\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m     22\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mRecall(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msen\u001b[39m\u001b[39m'\u001b[39m)])\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\Desktop\\Chiara\\fuzzy\\Fuzzy-Project\\EEGModels.py:129\u001b[0m, in \u001b[0;36mEEGNet\u001b[1;34m(nb_classes, Chans, Samples, dropoutRate, kernLength, F1, D, F2, norm_rate, dropoutType)\u001b[0m\n\u001b[0;32m    126\u001b[0m input1   \u001b[39m=\u001b[39m Input(shape \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m, Chans, Samples))\n\u001b[0;32m    128\u001b[0m \u001b[39m##################################################################\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m block1       \u001b[39m=\u001b[39m Conv2D(F1, (\u001b[39m1\u001b[39;49m, kernLength), padding \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39msame\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    130\u001b[0m                                input_shape \u001b[39m=\u001b[39;49m (\u001b[39m1\u001b[39;49m, Chans, Samples),\n\u001b[0;32m    131\u001b[0m                                use_bias \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, data_format\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mchannel_first\u001b[39;49m\u001b[39m'\u001b[39;49m)(input1)\n\u001b[0;32m    132\u001b[0m block1       \u001b[39m=\u001b[39m BatchNormalization(axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)(block1)\n\u001b[0;32m    133\u001b[0m block1       \u001b[39m=\u001b[39m DepthwiseConv2D((Chans, \u001b[39m1\u001b[39m), use_bias \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \n\u001b[0;32m    134\u001b[0m                                depth_multiplier \u001b[39m=\u001b[39m D,\n\u001b[0;32m    135\u001b[0m                                depthwise_constraint \u001b[39m=\u001b[39m max_norm(\u001b[39m1.\u001b[39m))(block1)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\dtensor\\utils.py:96\u001b[0m, in \u001b[0;36mallow_initializer_layout.<locals>._wrap_function\u001b[1;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[39mif\u001b[39;00m layout:\n\u001b[0;32m     94\u001b[0m             layout_args[variable_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_layout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m layout\n\u001b[1;32m---> 96\u001b[0m init_method(layer_instance, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     98\u001b[0m \u001b[39m# Inject the layout parameter after the invocation of __init__()\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m layout_param_name, layout \u001b[39min\u001b[39;00m layout_args\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\layers\\convolutional\\conv2d.py:179\u001b[0m, in \u001b[0;36mConv2D.__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mallow_initializer_layout\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    160\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    178\u001b[0m ):\n\u001b[1;32m--> 179\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    180\u001b[0m         rank\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m    181\u001b[0m         filters\u001b[39m=\u001b[39mfilters,\n\u001b[0;32m    182\u001b[0m         kernel_size\u001b[39m=\u001b[39mkernel_size,\n\u001b[0;32m    183\u001b[0m         strides\u001b[39m=\u001b[39mstrides,\n\u001b[0;32m    184\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m    185\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[0;32m    186\u001b[0m         dilation_rate\u001b[39m=\u001b[39mdilation_rate,\n\u001b[0;32m    187\u001b[0m         groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m    188\u001b[0m         activation\u001b[39m=\u001b[39mactivations\u001b[39m.\u001b[39mget(activation),\n\u001b[0;32m    189\u001b[0m         use_bias\u001b[39m=\u001b[39muse_bias,\n\u001b[0;32m    190\u001b[0m         kernel_initializer\u001b[39m=\u001b[39minitializers\u001b[39m.\u001b[39mget(kernel_initializer),\n\u001b[0;32m    191\u001b[0m         bias_initializer\u001b[39m=\u001b[39minitializers\u001b[39m.\u001b[39mget(bias_initializer),\n\u001b[0;32m    192\u001b[0m         kernel_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39mget(kernel_regularizer),\n\u001b[0;32m    193\u001b[0m         bias_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39mget(bias_regularizer),\n\u001b[0;32m    194\u001b[0m         activity_regularizer\u001b[39m=\u001b[39mregularizers\u001b[39m.\u001b[39mget(activity_regularizer),\n\u001b[0;32m    195\u001b[0m         kernel_constraint\u001b[39m=\u001b[39mconstraints\u001b[39m.\u001b[39mget(kernel_constraint),\n\u001b[0;32m    196\u001b[0m         bias_constraint\u001b[39m=\u001b[39mconstraints\u001b[39m.\u001b[39mget(bias_constraint),\n\u001b[0;32m    197\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\layers\\convolutional\\base_conv.py:143\u001b[0m, in \u001b[0;36mConv.__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, conv_op, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrides \u001b[39m=\u001b[39m conv_utils\u001b[39m.\u001b[39mnormalize_tuple(\n\u001b[0;32m    140\u001b[0m     strides, rank, \u001b[39m\"\u001b[39m\u001b[39mstrides\u001b[39m\u001b[39m\"\u001b[39m, allow_zero\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    141\u001b[0m )\n\u001b[0;32m    142\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding \u001b[39m=\u001b[39m conv_utils\u001b[39m.\u001b[39mnormalize_padding(padding)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_format \u001b[39m=\u001b[39m conv_utils\u001b[39m.\u001b[39;49mnormalize_data_format(data_format)\n\u001b[0;32m    144\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation_rate \u001b[39m=\u001b[39m conv_utils\u001b[39m.\u001b[39mnormalize_tuple(\n\u001b[0;32m    145\u001b[0m     dilation_rate, rank, \u001b[39m\"\u001b[39m\u001b[39mdilation_rate\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m )\n\u001b[0;32m    148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39m=\u001b[39m activations\u001b[39m.\u001b[39mget(activation)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\conv_utils.py:225\u001b[0m, in \u001b[0;36mnormalize_data_format\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    223\u001b[0m data_format \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mlower()\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m data_format \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mchannels_first\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mchannels_last\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m--> 225\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe `data_format` argument must be one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchannels_first\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchannels_last\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m. Received: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    228\u001b[0m     )\n\u001b[0;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m data_format\n",
      "\u001b[1;31mValueError\u001b[0m: The `data_format` argument must be one of \"channels_first\", \"channels_last\". Received: channel_first"
     ]
    }
   ],
   "source": [
    "print(\"START\")\n",
    "if not os.path.exists(OutputPathModels):\n",
    "    os.makedirs(OutputPathModels)\n",
    "print(\"Parameters loaded\")\n",
    "nSeizure=0\n",
    "\n",
    "for indexPat in range(0, len(patients)):\n",
    "    print('Patient '+patients[indexPat])\n",
    "    loadSpectogramData(indexPat) \n",
    "       \n",
    "result='Out Seizure, True Positive, False Positive, False negative, Second of Inter in Test, Sensitivity, FPR \\n'\n",
    "filesPath = getFilesPathWithoutSeizure(indexPat)\n",
    "    \n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000  # Numero di passi di addestramento dopo i quali applicare il decay\n",
    "decay_rate = 0.1  # Tasso di decay\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "model = EEGNet(nb_classes = 2, Chans = 22, Samples = 114)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', keras.metrics.Recall(name='sen')])\n",
    "                \n",
    "model.summary()\n",
    "\n",
    "history = model.fit(generate_arrays_for_training(filesPath, end=75), #end=75),#It take the first 75%\n",
    "                        validation_data=generate_arrays_for_training(filesPath, start=75),#start=75), #It take the last 25%\n",
    "                        #steps_per_epoch=10000, epochs=10)\n",
    "                        steps_per_epoch=int((len(filesPath)-int(len(filesPath)/100*25))),#*25), \n",
    "                        validation_steps=int((len(filesPath)-int(len(filesPath)/100*75))),#*75),\n",
    "                        epochs=60, max_queue_size=2, shuffle=True, callbacks=[LearningRateScheduler(lr_schedule)])# 100 epochs è meglio #aggiungere criterio di stop in base accuratezza\n",
    "\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Creates a HDF5 file \n",
    "model.save(OutputPathModels+\"ModelPat\"+patients[indexPat]+\"/\"+'ModelOutSeizure'+str(i+1)+'.h5')\n",
    "print(\"Model saved\")\n",
    "        \n",
    "epochs = range(len(train_loss))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_loss, label='Training loss')\n",
    "plt.plot(epochs, val_loss, label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Training end')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
