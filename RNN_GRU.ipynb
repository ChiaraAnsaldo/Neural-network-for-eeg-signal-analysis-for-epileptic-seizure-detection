{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.models import Model, Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.DatasetManage import read_and_store_data\n",
    "from ipynb.fs.full.FeatureExtraction import feature_extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = ['FP1-F7', 'F7-T7','T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', 'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'seizure']\n",
    "\n",
    "dataset = 'CHB_MIT'\n",
    "csvImportFile = 'CHB.csv'\n",
    "csvExportFile = 'CHB.csv'\n",
    "sample_rate = 256\n",
    "time_window = 2\n",
    "step = time_window * sample_rate\n",
    "\n",
    "test_ratio = 0.3 # ratio to split dataset into training and testing sets\n",
    "val_ratio = 0.2 # ratio to split dataset into validation and training sets\n",
    "\n",
    "pca_tolerance = 0.9 # desired percentage of variation in the data preserved\n",
    "\n",
    "undersampling_rate = 0.2 # undersampling rate for Cluster Centroids\n",
    "\n",
    "oversampling_neighbors = 11 # size of neighbourhood for K-nearest neighbours method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a method to split the dataset into training and testing sets, also returning the train/test\n",
    "# indexes for splitting the dataset into K folds for the K-fold cross validation\n",
    "\n",
    "def trainTestData_1 (features, test_ratio, k_fold):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_tr, x_ts, y_tr, y_ts = train_test_split(x, y, test_size = test_ratio, shuffle = True, random_state=42)\n",
    "    kf = KFold(n_splits = k_fold, shuffle = True)\n",
    "    x_train = np.reshape(x_tr.values, (x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "    y_train = y_tr.values.astype(int)\n",
    "    x_test = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_test = y_ts.values.astype(int)\n",
    "    return x_train, x_test, y_train, y_test, kf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define a method to split the dataset into training, validationa and testing sets\n",
    "\n",
    "def trainTestData_2 (features, test_ratio, val_ratio):\n",
    "    x = features.loc[:, features.columns != 'seizure']\n",
    "    y = features['seizure']\n",
    "    x_1, x_ts, y_1, y_ts = train_test_split(x, y, test_size = test_ratio, random_state=42)\n",
    "    x_tr, x_v, y_tr, y_v = train_test_split(x_1, y_1, test_size = val_ratio, random_state=42)\n",
    "    x_train = np.reshape(x_tr.values, (x_tr.shape[0], 1, x_tr.shape[1]))\n",
    "    y_train = y_tr.values.astype(int)\n",
    "    x_val = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_val = y_ts.values.astype(int)\n",
    "    x_test = np.reshape(x_ts.values, (x_ts.shape[0], 1, x_ts.shape[1]))\n",
    "    y_test = y_ts.values.astype(int)\n",
    "    return x_train, x_test, y_train, y_test, x_val, y_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and store data from the .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from CHB.csv\n"
     ]
    }
   ],
   "source": [
    "print('Reading data from', csvImportFile)\n",
    "df = pd.read_csv(csvImportFile, delimiter = ',', header = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_extraction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\RNN_GRU.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/RNN_GRU.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We compute the features and save them in the Feature.csv file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/RNN_GRU.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ft \u001b[39m=\u001b[39m feature_extraction(df, sample_rate, step, pca_tolerance, undersampling_rate, oversampling_neighbors)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_extraction' is not defined"
     ]
    }
   ],
   "source": [
    "# We compute the features and save them in the Feature.csv file\n",
    "\n",
    "ft = feature_extraction(df, sample_rate, step, pca_tolerance, undersampling_rate, oversampling_neighbors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Test process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = pd.read_csv(\"Features.csv\", delimiter = ',', header = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset splitting without validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 5\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset splitting with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, x_val, y_val = trainTestData_2 (ft, test_ratio, val_ratio)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "save_path = os.path.join(dir_name, 'Vanilla_RNN.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_SGD (initial_learning_rate, decay_steps, decay_rate):\n",
    "    # We define the optimizer with an initial learning rate\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate)\n",
    "\n",
    "    # We define the larning rate schedule with an exponential decay, specifying the number of decay steps and the decay rate\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps, decay_rate, staircase=True)\n",
    "    \n",
    "    return optimizer, lr_schedule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2 (history):\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(train_loss))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_loss, label='Training loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot method for K-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1 (train_loss, train_acc, val_loss, val_acc):\n",
    "    \n",
    "    avg_train_loss = np.mean(train_loss, axis=0)\n",
    "    avg_train_acc = np.mean(train_acc, axis=0)\n",
    "    avg_val_loss = np.mean(val_loss, axis=0)\n",
    "    avg_val_acc = np.mean(train_acc, axis=0)\n",
    "\n",
    "    # Plot delle curve di apprendimento mediate sulle K fold\n",
    "\n",
    "    epochs = range(1, len(train_loss[0]) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_loss, label='Training loss')\n",
    "    plt.plot(epochs, avg_val_loss, label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg_loss')\n",
    "    plt.title('Average train and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, avg_train_acc, label='Training accuracy')\n",
    "    plt.plot(epochs, avg_val_acc, label='Validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Avg_accuracy')\n",
    "    plt.title('Average train and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\irene\\OneDrive\\Documenti\\GitHub\\Fuzzy-Project\\RNN_GRU.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/RNN_GRU.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/RNN_GRU.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/RNN_GRU.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     plt\u001b[39m.\u001b[39mplot(train_loss[i])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/RNN_GRU.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mTraining Loss - All Folds\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/irene/OneDrive/Documenti/GitHub/Fuzzy-Project/RNN_GRU.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mEpochs\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the average loss curve\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(train_loss[i])\n",
    "plt.title('Training Loss - All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for i in range(5):\n",
    "    plt.plot(train_acc[i])\n",
    "plt.title('Training Accuracy - All Folds')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 1 layer SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "43/63 [===================>..........] - ETA: 0s - loss: 0.7200 - accuracy: 0.6186\n",
      "Epoch 1: val_loss improved from inf to 0.69626, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 3s 11ms/step - loss: 0.7307 - accuracy: 0.6006 - val_loss: 0.6963 - val_accuracy: 0.6726 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.6764 - accuracy: 0.6290\n",
      "Epoch 2: val_loss improved from 0.69626 to 0.65035, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6777 - accuracy: 0.6262 - val_loss: 0.6504 - val_accuracy: 0.7143 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.6355 - accuracy: 0.6873\n",
      "Epoch 3: val_loss improved from 0.65035 to 0.61083, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6335 - accuracy: 0.6869 - val_loss: 0.6108 - val_accuracy: 0.7500 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.5972 - accuracy: 0.7556\n",
      "Epoch 4: val_loss improved from 0.61083 to 0.57649, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5961 - accuracy: 0.7380 - val_loss: 0.5765 - val_accuracy: 0.7679 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.5679 - accuracy: 0.7571\n",
      "Epoch 5: val_loss improved from 0.57649 to 0.54773, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5638 - accuracy: 0.7604 - val_loss: 0.5477 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.5292 - accuracy: 0.7927\n",
      "Epoch 6: val_loss improved from 0.54773 to 0.52294, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5364 - accuracy: 0.7859 - val_loss: 0.5229 - val_accuracy: 0.8214 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.5182 - accuracy: 0.7927\n",
      "Epoch 7: val_loss improved from 0.52294 to 0.50111, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5127 - accuracy: 0.8019 - val_loss: 0.5011 - val_accuracy: 0.8274 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.4834 - accuracy: 0.8145\n",
      "Epoch 8: val_loss improved from 0.50111 to 0.48172, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4918 - accuracy: 0.8147 - val_loss: 0.4817 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.4777 - accuracy: 0.8269\n",
      "Epoch 9: val_loss improved from 0.48172 to 0.46419, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4733 - accuracy: 0.8371 - val_loss: 0.4642 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.4578 - accuracy: 0.8453\n",
      "Epoch 10: val_loss improved from 0.46419 to 0.44848, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4566 - accuracy: 0.8466 - val_loss: 0.4485 - val_accuracy: 0.8631 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.4377 - accuracy: 0.8679\n",
      "Epoch 11: val_loss improved from 0.44848 to 0.43417, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4416 - accuracy: 0.8530 - val_loss: 0.4342 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.4395 - accuracy: 0.8480\n",
      "Epoch 12: val_loss improved from 0.43417 to 0.42129, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4278 - accuracy: 0.8530 - val_loss: 0.4213 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.4244 - accuracy: 0.8490\n",
      "Epoch 13: val_loss improved from 0.42129 to 0.40980, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4153 - accuracy: 0.8626 - val_loss: 0.4098 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.3915 - accuracy: 0.8870\n",
      "Epoch 14: val_loss improved from 0.40980 to 0.39959, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4037 - accuracy: 0.8690 - val_loss: 0.3996 - val_accuracy: 0.8810 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3990 - accuracy: 0.8655\n",
      "Epoch 15: val_loss improved from 0.39959 to 0.39027, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3934 - accuracy: 0.8722 - val_loss: 0.3903 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.3943 - accuracy: 0.8694\n",
      "Epoch 16: val_loss improved from 0.39027 to 0.38172, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3841 - accuracy: 0.8786 - val_loss: 0.3817 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.3666 - accuracy: 0.8885\n",
      "Epoch 17: val_loss improved from 0.38172 to 0.37385, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3757 - accuracy: 0.8786 - val_loss: 0.3738 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3679 - accuracy: 0.8727\n",
      "Epoch 18: val_loss improved from 0.37385 to 0.36650, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3679 - accuracy: 0.8818 - val_loss: 0.3665 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3573 - accuracy: 0.8945\n",
      "Epoch 19: val_loss improved from 0.36650 to 0.35963, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3607 - accuracy: 0.8882 - val_loss: 0.3596 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3497 - accuracy: 0.8889\n",
      "Epoch 20: val_loss improved from 0.35963 to 0.35323, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3540 - accuracy: 0.8882 - val_loss: 0.3532 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3505 - accuracy: 0.8909\n",
      "Epoch 21: val_loss improved from 0.35323 to 0.34726, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3478 - accuracy: 0.8914 - val_loss: 0.3473 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3511 - accuracy: 0.8945\n",
      "Epoch 22: val_loss improved from 0.34726 to 0.34164, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3419 - accuracy: 0.8946 - val_loss: 0.3416 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3289 - accuracy: 0.8981\n",
      "Epoch 23: val_loss improved from 0.34164 to 0.33640, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3364 - accuracy: 0.8946 - val_loss: 0.3364 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.3346 - accuracy: 0.8917\n",
      "Epoch 24: val_loss improved from 0.33640 to 0.33150, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3312 - accuracy: 0.8946 - val_loss: 0.3315 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.3453 - accuracy: 0.8816\n",
      "Epoch 25: val_loss improved from 0.33150 to 0.32687, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8946 - val_loss: 0.3269 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3201 - accuracy: 0.9055\n",
      "Epoch 26: val_loss improved from 0.32687 to 0.32253, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3218 - accuracy: 0.8946 - val_loss: 0.3225 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3123 - accuracy: 0.9036\n",
      "Epoch 27: val_loss improved from 0.32253 to 0.31843, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8978 - val_loss: 0.3184 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3148 - accuracy: 0.8926\n",
      "Epoch 28: val_loss improved from 0.31843 to 0.31451, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3133 - accuracy: 0.8978 - val_loss: 0.3145 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.3107 - accuracy: 0.9040\n",
      "Epoch 29: val_loss improved from 0.31451 to 0.31081, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3094 - accuracy: 0.8978 - val_loss: 0.3108 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.3217 - accuracy: 0.8846\n",
      "Epoch 30: val_loss improved from 0.31081 to 0.30729, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.8978 - val_loss: 0.3073 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2871 - accuracy: 0.9055\n",
      "Epoch 31: val_loss improved from 0.30729 to 0.30400, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3021 - accuracy: 0.8978 - val_loss: 0.3040 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 32/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.3038 - accuracy: 0.8852\n",
      "Epoch 32: val_loss improved from 0.30400 to 0.30078, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2987 - accuracy: 0.8978 - val_loss: 0.3008 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 33/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3059 - accuracy: 0.8945\n",
      "Epoch 33: val_loss improved from 0.30078 to 0.29771, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2954 - accuracy: 0.9010 - val_loss: 0.2977 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 34/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2874 - accuracy: 0.9034\n",
      "Epoch 34: val_loss improved from 0.29771 to 0.29482, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2923 - accuracy: 0.9010 - val_loss: 0.2948 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 35/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2798 - accuracy: 0.9036\n",
      "Epoch 35: val_loss improved from 0.29482 to 0.29202, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2893 - accuracy: 0.9010 - val_loss: 0.2920 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 36/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.2870 - accuracy: 0.9021\n",
      "Epoch 36: val_loss improved from 0.29202 to 0.28932, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2864 - accuracy: 0.9010 - val_loss: 0.2893 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 37/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2921 - accuracy: 0.8981\n",
      "Epoch 37: val_loss improved from 0.28932 to 0.28675, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2836 - accuracy: 0.9042 - val_loss: 0.2867 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 38/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2771 - accuracy: 0.9091\n",
      "Epoch 38: val_loss improved from 0.28675 to 0.28429, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2809 - accuracy: 0.9042 - val_loss: 0.2843 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 39/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2755 - accuracy: 0.9074\n",
      "Epoch 39: val_loss improved from 0.28429 to 0.28192, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2783 - accuracy: 0.9042 - val_loss: 0.2819 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 40/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.2790 - accuracy: 0.9000\n",
      "Epoch 40: val_loss improved from 0.28192 to 0.27965, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2758 - accuracy: 0.9073 - val_loss: 0.2796 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 41/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2709 - accuracy: 0.9107\n",
      "Epoch 41: val_loss improved from 0.27965 to 0.27746, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2734 - accuracy: 0.9073 - val_loss: 0.2775 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 42/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2853 - accuracy: 0.8923\n",
      "Epoch 42: val_loss improved from 0.27746 to 0.27534, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2711 - accuracy: 0.9073 - val_loss: 0.2753 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 43/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2680 - accuracy: 0.9000\n",
      "Epoch 43: val_loss improved from 0.27534 to 0.27330, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2688 - accuracy: 0.9073 - val_loss: 0.2733 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 44/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2733 - accuracy: 0.9000\n",
      "Epoch 44: val_loss improved from 0.27330 to 0.27134, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2666 - accuracy: 0.9073 - val_loss: 0.2713 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 45/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2596 - accuracy: 0.9111\n",
      "Epoch 45: val_loss improved from 0.27134 to 0.26943, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2645 - accuracy: 0.9073 - val_loss: 0.2694 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 46/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2606 - accuracy: 0.9074\n",
      "Epoch 46: val_loss improved from 0.26943 to 0.26758, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2624 - accuracy: 0.9073 - val_loss: 0.2676 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 47/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2486 - accuracy: 0.9127\n",
      "Epoch 47: val_loss improved from 0.26758 to 0.26583, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2603 - accuracy: 0.9073 - val_loss: 0.2658 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 48/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2592 - accuracy: 0.9111\n",
      "Epoch 48: val_loss improved from 0.26583 to 0.26411, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2584 - accuracy: 0.9105 - val_loss: 0.2641 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 49/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2611 - accuracy: 0.9132\n",
      "Epoch 49: val_loss improved from 0.26411 to 0.26248, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2564 - accuracy: 0.9137 - val_loss: 0.2625 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 50/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.2257 - accuracy: 0.9289\n",
      "Epoch 50: val_loss improved from 0.26248 to 0.26083, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2546 - accuracy: 0.9169 - val_loss: 0.2608 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 51/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2545 - accuracy: 0.9164\n",
      "Epoch 51: val_loss improved from 0.26083 to 0.25924, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2528 - accuracy: 0.9169 - val_loss: 0.2592 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 52/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2430 - accuracy: 0.9192\n",
      "Epoch 52: val_loss improved from 0.25924 to 0.25770, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2510 - accuracy: 0.9169 - val_loss: 0.2577 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 53/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2490 - accuracy: 0.9222\n",
      "Epoch 53: val_loss improved from 0.25770 to 0.25620, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2493 - accuracy: 0.9201 - val_loss: 0.2562 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 54/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2396 - accuracy: 0.9245\n",
      "Epoch 54: val_loss improved from 0.25620 to 0.25474, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2476 - accuracy: 0.9201 - val_loss: 0.2547 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 55/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2476 - accuracy: 0.9273\n",
      "Epoch 55: val_loss improved from 0.25474 to 0.25332, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2460 - accuracy: 0.9233 - val_loss: 0.2533 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 56/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2491 - accuracy: 0.9228\n",
      "Epoch 56: val_loss improved from 0.25332 to 0.25193, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2444 - accuracy: 0.9233 - val_loss: 0.2519 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 57/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2600 - accuracy: 0.9132\n",
      "Epoch 57: val_loss improved from 0.25193 to 0.25058, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2428 - accuracy: 0.9233 - val_loss: 0.2506 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 58/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2459 - accuracy: 0.9200\n",
      "Epoch 58: val_loss improved from 0.25058 to 0.24927, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2413 - accuracy: 0.9233 - val_loss: 0.2493 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 59/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2382 - accuracy: 0.9250\n",
      "Epoch 59: val_loss improved from 0.24927 to 0.24799, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2398 - accuracy: 0.9233 - val_loss: 0.2480 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 60/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2433 - accuracy: 0.9186\n",
      "Epoch 60: val_loss improved from 0.24799 to 0.24674, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2384 - accuracy: 0.9233 - val_loss: 0.2467 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 61/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.2239 - accuracy: 0.9304\n",
      "Epoch 61: val_loss improved from 0.24674 to 0.24552, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2370 - accuracy: 0.9233 - val_loss: 0.2455 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 62/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2435 - accuracy: 0.9132\n",
      "Epoch 62: val_loss improved from 0.24552 to 0.24433, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2356 - accuracy: 0.9233 - val_loss: 0.2443 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 63/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2362 - accuracy: 0.9245\n",
      "Epoch 63: val_loss improved from 0.24433 to 0.24316, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2342 - accuracy: 0.9233 - val_loss: 0.2432 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 64/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2274 - accuracy: 0.9269\n",
      "Epoch 64: val_loss improved from 0.24316 to 0.24202, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2329 - accuracy: 0.9233 - val_loss: 0.2420 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 65/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2218 - accuracy: 0.9345\n",
      "Epoch 65: val_loss improved from 0.24202 to 0.24091, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2316 - accuracy: 0.9233 - val_loss: 0.2409 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 66/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2299 - accuracy: 0.9222\n",
      "Epoch 66: val_loss improved from 0.24091 to 0.23983, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2303 - accuracy: 0.9233 - val_loss: 0.2398 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 67/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2120 - accuracy: 0.9286\n",
      "Epoch 67: val_loss improved from 0.23983 to 0.23876, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2290 - accuracy: 0.9233 - val_loss: 0.2388 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 68/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9258\n",
      "Epoch 68: val_loss improved from 0.23876 to 0.23773, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2278 - accuracy: 0.9265 - val_loss: 0.2377 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 69/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2375 - accuracy: 0.9208\n",
      "Epoch 69: val_loss improved from 0.23773 to 0.23670, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2266 - accuracy: 0.9265 - val_loss: 0.2367 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 70/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2280 - accuracy: 0.9245\n",
      "Epoch 70: val_loss improved from 0.23670 to 0.23571, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2254 - accuracy: 0.9297 - val_loss: 0.2357 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 71/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.2095 - accuracy: 0.9333\n",
      "Epoch 71: val_loss improved from 0.23571 to 0.23475, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2242 - accuracy: 0.9297 - val_loss: 0.2347 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 72/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2429 - accuracy: 0.9231\n",
      "Epoch 72: val_loss improved from 0.23475 to 0.23381, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2230 - accuracy: 0.9297 - val_loss: 0.2338 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 73/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2259 - accuracy: 0.9296\n",
      "Epoch 73: val_loss improved from 0.23381 to 0.23288, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2219 - accuracy: 0.9297 - val_loss: 0.2329 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 74/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2250 - accuracy: 0.9294\n",
      "Epoch 74: val_loss improved from 0.23288 to 0.23196, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2208 - accuracy: 0.9297 - val_loss: 0.2320 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 75/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2249 - accuracy: 0.9288\n",
      "Epoch 75: val_loss improved from 0.23196 to 0.23109, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2197 - accuracy: 0.9297 - val_loss: 0.2311 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 76/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2072 - accuracy: 0.9333\n",
      "Epoch 76: val_loss improved from 0.23109 to 0.23020, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2186 - accuracy: 0.9297 - val_loss: 0.2302 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 77/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2272 - accuracy: 0.9192\n",
      "Epoch 77: val_loss improved from 0.23020 to 0.22932, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2176 - accuracy: 0.9297 - val_loss: 0.2293 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 78/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2094 - accuracy: 0.9280\n",
      "Epoch 78: val_loss improved from 0.22932 to 0.22850, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.9297 - val_loss: 0.2285 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 79/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9297\n",
      "Epoch 79: val_loss improved from 0.22850 to 0.22767, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2156 - accuracy: 0.9297 - val_loss: 0.2277 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 80/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9279\n",
      "Epoch 80: val_loss improved from 0.22767 to 0.22686, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2146 - accuracy: 0.9297 - val_loss: 0.2269 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 81/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2195 - accuracy: 0.9241\n",
      "Epoch 81: val_loss improved from 0.22686 to 0.22606, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2136 - accuracy: 0.9297 - val_loss: 0.2261 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 82/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2080 - accuracy: 0.9294\n",
      "Epoch 82: val_loss improved from 0.22606 to 0.22528, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2126 - accuracy: 0.9297 - val_loss: 0.2253 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 83/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2162 - accuracy: 0.9306\n",
      "Epoch 83: val_loss improved from 0.22528 to 0.22451, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2117 - accuracy: 0.9297 - val_loss: 0.2245 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 84/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2156 - accuracy: 0.9236\n",
      "Epoch 84: val_loss improved from 0.22451 to 0.22375, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2107 - accuracy: 0.9297 - val_loss: 0.2237 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 85/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.2143 - accuracy: 0.9280\n",
      "Epoch 85: val_loss improved from 0.22375 to 0.22301, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2098 - accuracy: 0.9297 - val_loss: 0.2230 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 86/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.2016 - accuracy: 0.9292\n",
      "Epoch 86: val_loss improved from 0.22301 to 0.22227, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2089 - accuracy: 0.9297 - val_loss: 0.2223 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 87/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.2177 - accuracy: 0.9250\n",
      "Epoch 87: val_loss improved from 0.22227 to 0.22154, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2080 - accuracy: 0.9297 - val_loss: 0.2215 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 88/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1988 - accuracy: 0.9321\n",
      "Epoch 88: val_loss improved from 0.22154 to 0.22086, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2071 - accuracy: 0.9297 - val_loss: 0.2209 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 89/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2151 - accuracy: 0.9265\n",
      "Epoch 89: val_loss improved from 0.22086 to 0.22016, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2063 - accuracy: 0.9297 - val_loss: 0.2202 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 90/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2118 - accuracy: 0.9241\n",
      "Epoch 90: val_loss improved from 0.22016 to 0.21947, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2054 - accuracy: 0.9297 - val_loss: 0.2195 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 91/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.2130 - accuracy: 0.9234\n",
      "Epoch 91: val_loss improved from 0.21947 to 0.21880, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2045 - accuracy: 0.9297 - val_loss: 0.2188 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 92/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2037 - accuracy: 0.9297\n",
      "Epoch 92: val_loss improved from 0.21880 to 0.21812, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2037 - accuracy: 0.9297 - val_loss: 0.2181 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 93/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2145 - accuracy: 0.9176\n",
      "Epoch 93: val_loss improved from 0.21812 to 0.21747, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2029 - accuracy: 0.9297 - val_loss: 0.2175 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 94/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.2106 - accuracy: 0.9216\n",
      "Epoch 94: val_loss improved from 0.21747 to 0.21682, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2021 - accuracy: 0.9297 - val_loss: 0.2168 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 95/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1963 - accuracy: 0.9321\n",
      "Epoch 95: val_loss improved from 0.21682 to 0.21619, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2013 - accuracy: 0.9297 - val_loss: 0.2162 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 96/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1893 - accuracy: 0.9375\n",
      "Epoch 96: val_loss improved from 0.21619 to 0.21554, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2005 - accuracy: 0.9297 - val_loss: 0.2155 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 97/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1894 - accuracy: 0.9360\n",
      "Epoch 97: val_loss improved from 0.21554 to 0.21492, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1997 - accuracy: 0.9329 - val_loss: 0.2149 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 98/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1871 - accuracy: 0.9385\n",
      "Epoch 98: val_loss improved from 0.21492 to 0.21433, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1989 - accuracy: 0.9329 - val_loss: 0.2143 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 99/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2064 - accuracy: 0.9265\n",
      "Epoch 99: val_loss improved from 0.21433 to 0.21372, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1981 - accuracy: 0.9329 - val_loss: 0.2137 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 100/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1980 - accuracy: 0.9322\n",
      "Epoch 100: val_loss improved from 0.21372 to 0.21313, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1974 - accuracy: 0.9329 - val_loss: 0.2131 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 101/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.2025 - accuracy: 0.9345\n",
      "Epoch 101: val_loss improved from 0.21313 to 0.21254, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9361 - val_loss: 0.2125 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 102/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9361\n",
      "Epoch 102: val_loss improved from 0.21254 to 0.21196, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9361 - val_loss: 0.2120 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 103/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2052 - accuracy: 0.9306\n",
      "Epoch 103: val_loss improved from 0.21196 to 0.21139, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1952 - accuracy: 0.9361 - val_loss: 0.2114 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 104/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1961 - accuracy: 0.9373\n",
      "Epoch 104: val_loss improved from 0.21139 to 0.21083, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1945 - accuracy: 0.9361 - val_loss: 0.2108 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 105/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1889 - accuracy: 0.9414\n",
      "Epoch 105: val_loss improved from 0.21083 to 0.21028, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1938 - accuracy: 0.9361 - val_loss: 0.2103 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 106/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2003 - accuracy: 0.9333\n",
      "Epoch 106: val_loss improved from 0.21028 to 0.20973, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1931 - accuracy: 0.9361 - val_loss: 0.2097 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 107/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1870 - accuracy: 0.9458\n",
      "Epoch 107: val_loss improved from 0.20973 to 0.20919, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1923 - accuracy: 0.9361 - val_loss: 0.2092 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 108/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1809 - accuracy: 0.9424\n",
      "Epoch 108: val_loss improved from 0.20919 to 0.20866, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1917 - accuracy: 0.9361 - val_loss: 0.2087 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 109/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1920 - accuracy: 0.9355\n",
      "Epoch 109: val_loss improved from 0.20866 to 0.20814, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1910 - accuracy: 0.9361 - val_loss: 0.2081 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 110/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1911 - accuracy: 0.9388\n",
      "Epoch 110: val_loss improved from 0.20814 to 0.20764, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1903 - accuracy: 0.9361 - val_loss: 0.2076 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 111/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1909 - accuracy: 0.9333\n",
      "Epoch 111: val_loss improved from 0.20764 to 0.20712, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1896 - accuracy: 0.9361 - val_loss: 0.2071 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 112/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9361\n",
      "Epoch 112: val_loss improved from 0.20712 to 0.20661, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1890 - accuracy: 0.9361 - val_loss: 0.2066 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 113/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1952 - accuracy: 0.9333\n",
      "Epoch 113: val_loss improved from 0.20661 to 0.20611, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1883 - accuracy: 0.9361 - val_loss: 0.2061 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 114/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9355\n",
      "Epoch 114: val_loss improved from 0.20611 to 0.20561, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1877 - accuracy: 0.9361 - val_loss: 0.2056 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 115/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2016 - accuracy: 0.9283\n",
      "Epoch 115: val_loss improved from 0.20561 to 0.20512, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1870 - accuracy: 0.9361 - val_loss: 0.2051 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 116/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2026 - accuracy: 0.9306\n",
      "Epoch 116: val_loss improved from 0.20512 to 0.20463, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1864 - accuracy: 0.9393 - val_loss: 0.2046 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 117/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1836 - accuracy: 0.9396\n",
      "Epoch 117: val_loss improved from 0.20463 to 0.20415, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1858 - accuracy: 0.9393 - val_loss: 0.2041 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 118/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1779 - accuracy: 0.9462\n",
      "Epoch 118: val_loss improved from 0.20415 to 0.20369, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1852 - accuracy: 0.9393 - val_loss: 0.2037 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 119/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1999 - accuracy: 0.9320\n",
      "Epoch 119: val_loss improved from 0.20369 to 0.20322, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1846 - accuracy: 0.9393 - val_loss: 0.2032 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 120/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1852 - accuracy: 0.9404\n",
      "Epoch 120: val_loss improved from 0.20322 to 0.20275, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1839 - accuracy: 0.9393 - val_loss: 0.2028 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 121/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1774 - accuracy: 0.9429\n",
      "Epoch 121: val_loss improved from 0.20275 to 0.20230, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1833 - accuracy: 0.9393 - val_loss: 0.2023 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 122/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1781 - accuracy: 0.9439\n",
      "Epoch 122: val_loss improved from 0.20230 to 0.20187, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1828 - accuracy: 0.9393 - val_loss: 0.2019 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 123/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1874 - accuracy: 0.9367\n",
      "Epoch 123: val_loss improved from 0.20187 to 0.20142, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1822 - accuracy: 0.9393 - val_loss: 0.2014 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 124/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1823 - accuracy: 0.9379\n",
      "Epoch 124: val_loss improved from 0.20142 to 0.20097, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.1816 - accuracy: 0.9393 - val_loss: 0.2010 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 125/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1846 - accuracy: 0.9429\n",
      "Epoch 125: val_loss improved from 0.20097 to 0.20052, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1810 - accuracy: 0.9393 - val_loss: 0.2005 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 126/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1913 - accuracy: 0.9333\n",
      "Epoch 126: val_loss improved from 0.20052 to 0.20008, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1804 - accuracy: 0.9393 - val_loss: 0.2001 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 127/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1746 - accuracy: 0.9443\n",
      "Epoch 127: val_loss improved from 0.20008 to 0.19965, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1799 - accuracy: 0.9393 - val_loss: 0.1996 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 128/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1783 - accuracy: 0.9423\n",
      "Epoch 128: val_loss improved from 0.19965 to 0.19922, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1793 - accuracy: 0.9393 - val_loss: 0.1992 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9393\n",
      "Epoch 129: val_loss improved from 0.19922 to 0.19880, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1787 - accuracy: 0.9393 - val_loss: 0.1988 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 130/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1691 - accuracy: 0.9412\n",
      "Epoch 130: val_loss improved from 0.19880 to 0.19837, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1782 - accuracy: 0.9393 - val_loss: 0.1984 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 131/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1725 - accuracy: 0.9373\n",
      "Epoch 131: val_loss improved from 0.19837 to 0.19798, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9393 - val_loss: 0.1980 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 132/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1792 - accuracy: 0.9377\n",
      "Epoch 132: val_loss improved from 0.19798 to 0.19757, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1771 - accuracy: 0.9393 - val_loss: 0.1976 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 133/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1777 - accuracy: 0.9375\n",
      "Epoch 133: val_loss improved from 0.19757 to 0.19717, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9393 - val_loss: 0.1972 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 134/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1729 - accuracy: 0.9410\n",
      "Epoch 134: val_loss improved from 0.19717 to 0.19679, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1760 - accuracy: 0.9393 - val_loss: 0.1968 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 135/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1712 - accuracy: 0.9467\n",
      "Epoch 135: val_loss improved from 0.19679 to 0.19639, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.1755 - accuracy: 0.9393 - val_loss: 0.1964 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 136/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9410\n",
      "Epoch 136: val_loss improved from 0.19639 to 0.19599, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1750 - accuracy: 0.9393 - val_loss: 0.1960 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 137/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1536 - accuracy: 0.9517\n",
      "Epoch 137: val_loss improved from 0.19599 to 0.19562, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1745 - accuracy: 0.9393 - val_loss: 0.1956 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 138/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1666 - accuracy: 0.9404\n",
      "Epoch 138: val_loss improved from 0.19562 to 0.19523, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1739 - accuracy: 0.9393 - val_loss: 0.1952 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 139/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1653 - accuracy: 0.9404\n",
      "Epoch 139: val_loss improved from 0.19523 to 0.19484, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1735 - accuracy: 0.9393 - val_loss: 0.1948 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 140/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1615 - accuracy: 0.9393\n",
      "Epoch 140: val_loss improved from 0.19484 to 0.19446, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1729 - accuracy: 0.9393 - val_loss: 0.1945 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 141/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1601 - accuracy: 0.9439\n",
      "Epoch 141: val_loss improved from 0.19446 to 0.19408, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1724 - accuracy: 0.9393 - val_loss: 0.1941 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 142/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9387\n",
      "Epoch 142: val_loss improved from 0.19408 to 0.19370, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1719 - accuracy: 0.9393 - val_loss: 0.1937 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 143/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9387\n",
      "Epoch 143: val_loss improved from 0.19370 to 0.19333, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1714 - accuracy: 0.9393 - val_loss: 0.1933 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 144/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1774 - accuracy: 0.9333\n",
      "Epoch 144: val_loss improved from 0.19333 to 0.19296, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1710 - accuracy: 0.9393 - val_loss: 0.1930 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 145/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1798 - accuracy: 0.9333\n",
      "Epoch 145: val_loss improved from 0.19296 to 0.19260, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1705 - accuracy: 0.9393 - val_loss: 0.1926 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 146/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1719 - accuracy: 0.9429\n",
      "Epoch 146: val_loss improved from 0.19260 to 0.19223, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1700 - accuracy: 0.9393 - val_loss: 0.1922 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 147/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1706 - accuracy: 0.9404\n",
      "Epoch 147: val_loss improved from 0.19223 to 0.19187, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1695 - accuracy: 0.9393 - val_loss: 0.1919 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 148/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1720 - accuracy: 0.9407\n",
      "Epoch 148: val_loss improved from 0.19187 to 0.19151, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1690 - accuracy: 0.9393 - val_loss: 0.1915 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 149/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1676 - accuracy: 0.9418\n",
      "Epoch 149: val_loss improved from 0.19151 to 0.19116, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1685 - accuracy: 0.9393 - val_loss: 0.1912 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 150/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1627 - accuracy: 0.9429\n",
      "Epoch 150: val_loss improved from 0.19116 to 0.19082, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9393 - val_loss: 0.1908 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 151/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.9387\n",
      "Epoch 151: val_loss improved from 0.19082 to 0.19047, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.9393 - val_loss: 0.1905 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 152/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1679 - accuracy: 0.9419\n",
      "Epoch 152: val_loss improved from 0.19047 to 0.19013, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1672 - accuracy: 0.9425 - val_loss: 0.1901 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 153/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9443\n",
      "Epoch 153: val_loss improved from 0.19013 to 0.18978, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1667 - accuracy: 0.9393 - val_loss: 0.1898 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 154/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1537 - accuracy: 0.9464\n",
      "Epoch 154: val_loss improved from 0.18978 to 0.18944, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1662 - accuracy: 0.9425 - val_loss: 0.1894 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 155/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1761 - accuracy: 0.9404\n",
      "Epoch 155: val_loss improved from 0.18944 to 0.18911, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1658 - accuracy: 0.9457 - val_loss: 0.1891 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 156/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1758 - accuracy: 0.9396\n",
      "Epoch 156: val_loss improved from 0.18911 to 0.18878, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.9457 - val_loss: 0.1888 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 157/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1681 - accuracy: 0.9448\n",
      "Epoch 157: val_loss improved from 0.18878 to 0.18845, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1649 - accuracy: 0.9457 - val_loss: 0.1884 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 158/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9443\n",
      "Epoch 158: val_loss improved from 0.18845 to 0.18813, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1645 - accuracy: 0.9457 - val_loss: 0.1881 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 159/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1681 - accuracy: 0.9424\n",
      "Epoch 159: val_loss improved from 0.18813 to 0.18781, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1641 - accuracy: 0.9457 - val_loss: 0.1878 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 160/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1708 - accuracy: 0.9448\n",
      "Epoch 160: val_loss improved from 0.18781 to 0.18749, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1636 - accuracy: 0.9489 - val_loss: 0.1875 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 161/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1618 - accuracy: 0.9492\n",
      "Epoch 161: val_loss improved from 0.18749 to 0.18718, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1632 - accuracy: 0.9489 - val_loss: 0.1872 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 162/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1676 - accuracy: 0.9474\n",
      "Epoch 162: val_loss improved from 0.18718 to 0.18686, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1628 - accuracy: 0.9489 - val_loss: 0.1869 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 163/200\n",
      "44/63 [===================>..........] - ETA: 0s - loss: 0.1540 - accuracy: 0.9545\n",
      "Epoch 163: val_loss improved from 0.18686 to 0.18655, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9489 - val_loss: 0.1865 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 164/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1579 - accuracy: 0.9517\n",
      "Epoch 164: val_loss improved from 0.18655 to 0.18624, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1620 - accuracy: 0.9489 - val_loss: 0.1862 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 165/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1662 - accuracy: 0.9440\n",
      "Epoch 165: val_loss improved from 0.18624 to 0.18594, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1616 - accuracy: 0.9489 - val_loss: 0.1859 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 166/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1537 - accuracy: 0.9536\n",
      "Epoch 166: val_loss improved from 0.18594 to 0.18564, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1612 - accuracy: 0.9489 - val_loss: 0.1856 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 167/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1659 - accuracy: 0.9467\n",
      "Epoch 167: val_loss improved from 0.18564 to 0.18534, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1608 - accuracy: 0.9489 - val_loss: 0.1853 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 168/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1668 - accuracy: 0.9458\n",
      "Epoch 168: val_loss improved from 0.18534 to 0.18504, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1604 - accuracy: 0.9489 - val_loss: 0.1850 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 169/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1565 - accuracy: 0.9491\n",
      "Epoch 169: val_loss improved from 0.18504 to 0.18475, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.9489 - val_loss: 0.1848 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 170/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1664 - accuracy: 0.9455\n",
      "Epoch 170: val_loss improved from 0.18475 to 0.18445, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1596 - accuracy: 0.9489 - val_loss: 0.1845 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 171/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1622 - accuracy: 0.9474\n",
      "Epoch 171: val_loss improved from 0.18445 to 0.18417, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1592 - accuracy: 0.9489 - val_loss: 0.1842 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 172/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1577 - accuracy: 0.9492\n",
      "Epoch 172: val_loss improved from 0.18417 to 0.18388, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1588 - accuracy: 0.9489 - val_loss: 0.1839 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 173/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1613 - accuracy: 0.9509\n",
      "Epoch 173: val_loss improved from 0.18388 to 0.18360, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1584 - accuracy: 0.9489 - val_loss: 0.1836 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 174/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1611 - accuracy: 0.9481\n",
      "Epoch 174: val_loss improved from 0.18360 to 0.18332, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1581 - accuracy: 0.9521 - val_loss: 0.1833 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 175/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1568 - accuracy: 0.9586\n",
      "Epoch 175: val_loss improved from 0.18332 to 0.18302, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1577 - accuracy: 0.9553 - val_loss: 0.1830 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 176/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1525 - accuracy: 0.9527\n",
      "Epoch 176: val_loss improved from 0.18302 to 0.18277, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1573 - accuracy: 0.9553 - val_loss: 0.1828 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 177/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1476 - accuracy: 0.9586\n",
      "Epoch 177: val_loss improved from 0.18277 to 0.18252, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1569 - accuracy: 0.9553 - val_loss: 0.1825 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 178/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9553\n",
      "Epoch 178: val_loss improved from 0.18252 to 0.18224, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1566 - accuracy: 0.9553 - val_loss: 0.1822 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 179/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1606 - accuracy: 0.9559\n",
      "Epoch 179: val_loss improved from 0.18224 to 0.18196, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1562 - accuracy: 0.9553 - val_loss: 0.1820 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 180/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1630 - accuracy: 0.9517\n",
      "Epoch 180: val_loss improved from 0.18196 to 0.18170, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1559 - accuracy: 0.9553 - val_loss: 0.1817 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 181/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1425 - accuracy: 0.9614\n",
      "Epoch 181: val_loss improved from 0.18170 to 0.18143, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1555 - accuracy: 0.9553 - val_loss: 0.1814 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 182/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1536 - accuracy: 0.9592\n",
      "Epoch 182: val_loss improved from 0.18143 to 0.18117, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1551 - accuracy: 0.9553 - val_loss: 0.1812 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 183/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1595 - accuracy: 0.9536\n",
      "Epoch 183: val_loss improved from 0.18117 to 0.18091, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1548 - accuracy: 0.9553 - val_loss: 0.1809 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 184/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1613 - accuracy: 0.9509\n",
      "Epoch 184: val_loss improved from 0.18091 to 0.18065, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1544 - accuracy: 0.9553 - val_loss: 0.1807 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 185/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1433 - accuracy: 0.9636\n",
      "Epoch 185: val_loss improved from 0.18065 to 0.18039, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1541 - accuracy: 0.9553 - val_loss: 0.1804 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 186/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1522 - accuracy: 0.9547\n",
      "Epoch 186: val_loss improved from 0.18039 to 0.18014, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1537 - accuracy: 0.9553 - val_loss: 0.1801 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 187/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1494 - accuracy: 0.9579\n",
      "Epoch 187: val_loss improved from 0.18014 to 0.17989, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1534 - accuracy: 0.9553 - val_loss: 0.1799 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 188/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.1640 - accuracy: 0.9511\n",
      "Epoch 188: val_loss improved from 0.17989 to 0.17964, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1531 - accuracy: 0.9553 - val_loss: 0.1796 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 189/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1602 - accuracy: 0.9536\n",
      "Epoch 189: val_loss improved from 0.17964 to 0.17939, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9553 - val_loss: 0.1794 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 190/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1378 - accuracy: 0.9623\n",
      "Epoch 190: val_loss improved from 0.17939 to 0.17914, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.9553 - val_loss: 0.1791 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 191/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1537 - accuracy: 0.9544\n",
      "Epoch 191: val_loss improved from 0.17914 to 0.17890, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.9553 - val_loss: 0.1789 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 192/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1698 - accuracy: 0.9462\n",
      "Epoch 192: val_loss improved from 0.17890 to 0.17866, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1517 - accuracy: 0.9553 - val_loss: 0.1787 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 193/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1471 - accuracy: 0.9564\n",
      "Epoch 193: val_loss improved from 0.17866 to 0.17841, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1514 - accuracy: 0.9553 - val_loss: 0.1784 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 194/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1493 - accuracy: 0.9519\n",
      "Epoch 194: val_loss improved from 0.17841 to 0.17817, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1511 - accuracy: 0.9553 - val_loss: 0.1782 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 195/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1577 - accuracy: 0.9517\n",
      "Epoch 195: val_loss improved from 0.17817 to 0.17794, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.9553 - val_loss: 0.1779 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 196/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1532 - accuracy: 0.9525\n",
      "Epoch 196: val_loss improved from 0.17794 to 0.17768, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.9553 - val_loss: 0.1777 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 197/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1520 - accuracy: 0.9541\n",
      "Epoch 197: val_loss improved from 0.17768 to 0.17744, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1501 - accuracy: 0.9553 - val_loss: 0.1774 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 198/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1423 - accuracy: 0.9577\n",
      "Epoch 198: val_loss improved from 0.17744 to 0.17722, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1498 - accuracy: 0.9553 - val_loss: 0.1772 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 199/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1354 - accuracy: 0.9621\n",
      "Epoch 199: val_loss improved from 0.17722 to 0.17700, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9553 - val_loss: 0.1770 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 200/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1511 - accuracy: 0.9544\n",
      "Epoch 200: val_loss improved from 0.17700 to 0.17678, saving model to model_checkpoint\\GRU_1 layer_SGD.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9553 - val_loss: 0.1768 - val_accuracy: 0.9464 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_5 (GRU)                 (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9464\n",
      "Test Loss: 0.1767757534980774\n",
      "Test Accuracy: 0.9464285969734192\n"
     ]
    }
   ],
   "source": [
    "# This code illustrates the implementation of an enhanced GRU model with a single layer and Stochastic Gradient Descent optimization. \n",
    "# The model is trained to solve a binary classification problem\n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_SGD.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule),callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 1 layer Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.6981 - accuracy: 0.5542\n",
      "Epoch 1: val_loss improved from inf to 0.70889, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 3s 10ms/step - loss: 0.6830 - accuracy: 0.5815 - val_loss: 0.7089 - val_accuracy: 0.5476\n",
      "Epoch 2/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.6357 - accuracy: 0.6311\n",
      "Epoch 2: val_loss improved from 0.70889 to 0.64839, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.6180 - accuracy: 0.6486 - val_loss: 0.6484 - val_accuracy: 0.6310\n",
      "Epoch 3/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.5851 - accuracy: 0.6981\n",
      "Epoch 3: val_loss improved from 0.64839 to 0.59753, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.5635 - accuracy: 0.7284 - val_loss: 0.5975 - val_accuracy: 0.6845\n",
      "Epoch 4/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.5188 - accuracy: 0.7821\n",
      "Epoch 4: val_loss improved from 0.59753 to 0.55538, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5167 - accuracy: 0.7764 - val_loss: 0.5554 - val_accuracy: 0.7262\n",
      "Epoch 5/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.4764 - accuracy: 0.8068\n",
      "Epoch 5: val_loss improved from 0.55538 to 0.51824, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4766 - accuracy: 0.8083 - val_loss: 0.5182 - val_accuracy: 0.7798\n",
      "Epoch 6/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.4444 - accuracy: 0.8444\n",
      "Epoch 6: val_loss improved from 0.51824 to 0.48603, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4423 - accuracy: 0.8371 - val_loss: 0.4860 - val_accuracy: 0.8333\n",
      "Epoch 7/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.4201 - accuracy: 0.8690\n",
      "Epoch 7: val_loss improved from 0.48603 to 0.45910, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4124 - accuracy: 0.8722 - val_loss: 0.4591 - val_accuracy: 0.8512\n",
      "Epoch 8/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.3767 - accuracy: 0.8982\n",
      "Epoch 8: val_loss improved from 0.45910 to 0.43459, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3866 - accuracy: 0.8978 - val_loss: 0.4346 - val_accuracy: 0.8631\n",
      "Epoch 9/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.3693 - accuracy: 0.8983\n",
      "Epoch 9: val_loss improved from 0.43459 to 0.41306, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3639 - accuracy: 0.9042 - val_loss: 0.4131 - val_accuracy: 0.8690\n",
      "Epoch 10/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.3488 - accuracy: 0.9067\n",
      "Epoch 10: val_loss improved from 0.41306 to 0.39407, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3441 - accuracy: 0.9073 - val_loss: 0.3941 - val_accuracy: 0.8690\n",
      "Epoch 11/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3264 - accuracy: 0.9088\n",
      "Epoch 11: val_loss improved from 0.39407 to 0.37695, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3266 - accuracy: 0.9105 - val_loss: 0.3769 - val_accuracy: 0.8690\n",
      "Epoch 12/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.3088 - accuracy: 0.9283\n",
      "Epoch 12: val_loss improved from 0.37695 to 0.36161, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3109 - accuracy: 0.9233 - val_loss: 0.3616 - val_accuracy: 0.8750\n",
      "Epoch 13/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.3002 - accuracy: 0.9207\n",
      "Epoch 13: val_loss improved from 0.36161 to 0.34796, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2970 - accuracy: 0.9265 - val_loss: 0.3480 - val_accuracy: 0.8750\n",
      "Epoch 14/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2832 - accuracy: 0.9276\n",
      "Epoch 14: val_loss improved from 0.34796 to 0.33540, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2844 - accuracy: 0.9265 - val_loss: 0.3354 - val_accuracy: 0.8810\n",
      "Epoch 15/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2781 - accuracy: 0.9254\n",
      "Epoch 15: val_loss improved from 0.33540 to 0.32427, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2730 - accuracy: 0.9297 - val_loss: 0.3243 - val_accuracy: 0.8869\n",
      "Epoch 16/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2606 - accuracy: 0.9368\n",
      "Epoch 16: val_loss improved from 0.32427 to 0.31374, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2628 - accuracy: 0.9329 - val_loss: 0.3137 - val_accuracy: 0.8988\n",
      "Epoch 17/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2603 - accuracy: 0.9356\n",
      "Epoch 17: val_loss improved from 0.31374 to 0.30474, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2532 - accuracy: 0.9393 - val_loss: 0.3047 - val_accuracy: 0.8988\n",
      "Epoch 18/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2517 - accuracy: 0.9368\n",
      "Epoch 18: val_loss improved from 0.30474 to 0.29623, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2446 - accuracy: 0.9393 - val_loss: 0.2962 - val_accuracy: 0.8988\n",
      "Epoch 19/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2388 - accuracy: 0.9434\n",
      "Epoch 19: val_loss improved from 0.29623 to 0.28835, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2368 - accuracy: 0.9393 - val_loss: 0.2883 - val_accuracy: 0.9048\n",
      "Epoch 20/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2373 - accuracy: 0.9308\n",
      "Epoch 20: val_loss improved from 0.28835 to 0.28115, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2295 - accuracy: 0.9393 - val_loss: 0.2812 - val_accuracy: 0.9048\n",
      "Epoch 21/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2297 - accuracy: 0.9390\n",
      "Epoch 21: val_loss improved from 0.28115 to 0.27439, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2227 - accuracy: 0.9425 - val_loss: 0.2744 - val_accuracy: 0.9048\n",
      "Epoch 22/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.2167 - accuracy: 0.9458\n",
      "Epoch 22: val_loss improved from 0.27439 to 0.26799, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2165 - accuracy: 0.9425 - val_loss: 0.2680 - val_accuracy: 0.9048\n",
      "Epoch 23/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.9425\n",
      "Epoch 23: val_loss improved from 0.26799 to 0.26240, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.2107 - accuracy: 0.9425 - val_loss: 0.2624 - val_accuracy: 0.9048\n",
      "Epoch 24/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9457\n",
      "Epoch 24: val_loss improved from 0.26240 to 0.25684, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9457 - val_loss: 0.2568 - val_accuracy: 0.9048\n",
      "Epoch 25/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.2046 - accuracy: 0.9433\n",
      "Epoch 25: val_loss improved from 0.25684 to 0.25190, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.2002 - accuracy: 0.9457 - val_loss: 0.2519 - val_accuracy: 0.9107\n",
      "Epoch 26/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1948 - accuracy: 0.9418\n",
      "Epoch 26: val_loss improved from 0.25190 to 0.24704, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1954 - accuracy: 0.9457 - val_loss: 0.2470 - val_accuracy: 0.9107\n",
      "Epoch 27/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1937 - accuracy: 0.9412\n",
      "Epoch 27: val_loss improved from 0.24704 to 0.24263, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1908 - accuracy: 0.9457 - val_loss: 0.2426 - val_accuracy: 0.9107\n",
      "Epoch 28/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1877 - accuracy: 0.9396\n",
      "Epoch 28: val_loss improved from 0.24263 to 0.23867, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1866 - accuracy: 0.9457 - val_loss: 0.2387 - val_accuracy: 0.9107\n",
      "Epoch 29/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1859 - accuracy: 0.9455\n",
      "Epoch 29: val_loss improved from 0.23867 to 0.23472, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1826 - accuracy: 0.9457 - val_loss: 0.2347 - val_accuracy: 0.9107\n",
      "Epoch 30/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1708 - accuracy: 0.9538\n",
      "Epoch 30: val_loss improved from 0.23472 to 0.23071, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1786 - accuracy: 0.9457 - val_loss: 0.2307 - val_accuracy: 0.9107\n",
      "Epoch 31/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1670 - accuracy: 0.9481\n",
      "Epoch 31: val_loss improved from 0.23071 to 0.22728, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9457 - val_loss: 0.2273 - val_accuracy: 0.9167\n",
      "Epoch 32/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1807 - accuracy: 0.9455\n",
      "Epoch 32: val_loss improved from 0.22728 to 0.22416, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1717 - accuracy: 0.9489 - val_loss: 0.2242 - val_accuracy: 0.9107\n",
      "Epoch 33/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9475\n",
      "Epoch 33: val_loss improved from 0.22416 to 0.22090, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1683 - accuracy: 0.9489 - val_loss: 0.2209 - val_accuracy: 0.9167\n",
      "Epoch 34/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1730 - accuracy: 0.9455\n",
      "Epoch 34: val_loss improved from 0.22090 to 0.21796, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1652 - accuracy: 0.9489 - val_loss: 0.2180 - val_accuracy: 0.9107\n",
      "Epoch 35/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1707 - accuracy: 0.9439\n",
      "Epoch 35: val_loss improved from 0.21796 to 0.21495, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9489 - val_loss: 0.2150 - val_accuracy: 0.9167\n",
      "Epoch 36/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1626 - accuracy: 0.9467\n",
      "Epoch 36: val_loss improved from 0.21495 to 0.21239, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1593 - accuracy: 0.9489 - val_loss: 0.2124 - val_accuracy: 0.9167\n",
      "Epoch 37/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1550 - accuracy: 0.9525\n",
      "Epoch 37: val_loss improved from 0.21239 to 0.20975, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9521 - val_loss: 0.2098 - val_accuracy: 0.9167\n",
      "Epoch 38/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1508 - accuracy: 0.9536\n",
      "Epoch 38: val_loss improved from 0.20975 to 0.20726, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1538 - accuracy: 0.9521 - val_loss: 0.2073 - val_accuracy: 0.9167\n",
      "Epoch 39/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1515 - accuracy: 0.9491\n",
      "Epoch 39: val_loss improved from 0.20726 to 0.20486, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1513 - accuracy: 0.9521 - val_loss: 0.2049 - val_accuracy: 0.9167\n",
      "Epoch 40/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1371 - accuracy: 0.9600\n",
      "Epoch 40: val_loss improved from 0.20486 to 0.20262, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.9553 - val_loss: 0.2026 - val_accuracy: 0.9167\n",
      "Epoch 41/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1554 - accuracy: 0.9491\n",
      "Epoch 41: val_loss improved from 0.20262 to 0.20059, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1464 - accuracy: 0.9553 - val_loss: 0.2006 - val_accuracy: 0.9167\n",
      "Epoch 42/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1408 - accuracy: 0.9615\n",
      "Epoch 42: val_loss improved from 0.20059 to 0.19838, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9585 - val_loss: 0.1984 - val_accuracy: 0.9286\n",
      "Epoch 43/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1290 - accuracy: 0.9673\n",
      "Epoch 43: val_loss improved from 0.19838 to 0.19620, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1419 - accuracy: 0.9585 - val_loss: 0.1962 - val_accuracy: 0.9286\n",
      "Epoch 44/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1353 - accuracy: 0.9586\n",
      "Epoch 44: val_loss improved from 0.19620 to 0.19457, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1396 - accuracy: 0.9585 - val_loss: 0.1946 - val_accuracy: 0.9345\n",
      "Epoch 45/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1392 - accuracy: 0.9586\n",
      "Epoch 45: val_loss improved from 0.19457 to 0.19271, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1376 - accuracy: 0.9585 - val_loss: 0.1927 - val_accuracy: 0.9345\n",
      "Epoch 46/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1380 - accuracy: 0.9567\n",
      "Epoch 46: val_loss improved from 0.19271 to 0.19105, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1355 - accuracy: 0.9585 - val_loss: 0.1911 - val_accuracy: 0.9345\n",
      "Epoch 47/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1341 - accuracy: 0.9593\n",
      "Epoch 47: val_loss improved from 0.19105 to 0.18935, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1335 - accuracy: 0.9585 - val_loss: 0.1893 - val_accuracy: 0.9345\n",
      "Epoch 48/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1303 - accuracy: 0.9564\n",
      "Epoch 48: val_loss improved from 0.18935 to 0.18766, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.9585 - val_loss: 0.1877 - val_accuracy: 0.9345\n",
      "Epoch 49/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1319 - accuracy: 0.9574\n",
      "Epoch 49: val_loss improved from 0.18766 to 0.18625, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9585 - val_loss: 0.1862 - val_accuracy: 0.9345\n",
      "Epoch 50/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1268 - accuracy: 0.9600\n",
      "Epoch 50: val_loss improved from 0.18625 to 0.18460, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1279 - accuracy: 0.9585 - val_loss: 0.1846 - val_accuracy: 0.9345\n",
      "Epoch 51/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9574\n",
      "Epoch 51: val_loss improved from 0.18460 to 0.18336, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1262 - accuracy: 0.9585 - val_loss: 0.1834 - val_accuracy: 0.9345\n",
      "Epoch 52/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1269 - accuracy: 0.9567\n",
      "Epoch 52: val_loss improved from 0.18336 to 0.18197, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1243 - accuracy: 0.9585 - val_loss: 0.1820 - val_accuracy: 0.9345\n",
      "Epoch 53/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1271 - accuracy: 0.9547\n",
      "Epoch 53: val_loss improved from 0.18197 to 0.18069, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9585 - val_loss: 0.1807 - val_accuracy: 0.9405\n",
      "Epoch 54/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9607\n",
      "Epoch 54: val_loss improved from 0.18069 to 0.17914, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1210 - accuracy: 0.9585 - val_loss: 0.1791 - val_accuracy: 0.9405\n",
      "Epoch 55/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9581\n",
      "Epoch 55: val_loss improved from 0.17914 to 0.17807, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9585 - val_loss: 0.1781 - val_accuracy: 0.9405\n",
      "Epoch 56/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9567\n",
      "Epoch 56: val_loss improved from 0.17807 to 0.17664, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9585 - val_loss: 0.1766 - val_accuracy: 0.9405\n",
      "Epoch 57/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1216 - accuracy: 0.9552\n",
      "Epoch 57: val_loss improved from 0.17664 to 0.17566, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1161 - accuracy: 0.9585 - val_loss: 0.1757 - val_accuracy: 0.9405\n",
      "Epoch 58/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9613\n",
      "Epoch 58: val_loss improved from 0.17566 to 0.17458, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9617 - val_loss: 0.1746 - val_accuracy: 0.9405\n",
      "Epoch 59/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9617\n",
      "Epoch 59: val_loss improved from 0.17458 to 0.17351, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1131 - accuracy: 0.9617 - val_loss: 0.1735 - val_accuracy: 0.9405\n",
      "Epoch 60/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1065 - accuracy: 0.9655\n",
      "Epoch 60: val_loss improved from 0.17351 to 0.17225, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1115 - accuracy: 0.9617 - val_loss: 0.1723 - val_accuracy: 0.9405\n",
      "Epoch 61/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1100 - accuracy: 0.9569\n",
      "Epoch 61: val_loss improved from 0.17225 to 0.17136, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1101 - accuracy: 0.9617 - val_loss: 0.1714 - val_accuracy: 0.9405\n",
      "Epoch 62/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.1121 - accuracy: 0.9614\n",
      "Epoch 62: val_loss improved from 0.17136 to 0.17027, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1087 - accuracy: 0.9617 - val_loss: 0.1703 - val_accuracy: 0.9405\n",
      "Epoch 63/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1104 - accuracy: 0.9621\n",
      "Epoch 63: val_loss improved from 0.17027 to 0.16936, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1073 - accuracy: 0.9617 - val_loss: 0.1694 - val_accuracy: 0.9405\n",
      "Epoch 64/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1007 - accuracy: 0.9615\n",
      "Epoch 64: val_loss improved from 0.16936 to 0.16843, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1058 - accuracy: 0.9617 - val_loss: 0.1684 - val_accuracy: 0.9405\n",
      "Epoch 65/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1070 - accuracy: 0.9627\n",
      "Epoch 65: val_loss improved from 0.16843 to 0.16741, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1045 - accuracy: 0.9649 - val_loss: 0.1674 - val_accuracy: 0.9405\n",
      "Epoch 66/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1040 - accuracy: 0.9645\n",
      "Epoch 66: val_loss improved from 0.16741 to 0.16659, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9649 - val_loss: 0.1666 - val_accuracy: 0.9405\n",
      "Epoch 67/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1039 - accuracy: 0.9633\n",
      "Epoch 67: val_loss improved from 0.16659 to 0.16575, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1018 - accuracy: 0.9649 - val_loss: 0.1658 - val_accuracy: 0.9405\n",
      "Epoch 68/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0984 - accuracy: 0.9690\n",
      "Epoch 68: val_loss improved from 0.16575 to 0.16479, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.1006 - accuracy: 0.9681 - val_loss: 0.1648 - val_accuracy: 0.9405\n",
      "Epoch 69/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1020 - accuracy: 0.9667\n",
      "Epoch 69: val_loss improved from 0.16479 to 0.16413, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0993 - accuracy: 0.9681 - val_loss: 0.1641 - val_accuracy: 0.9405\n",
      "Epoch 70/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0970 - accuracy: 0.9700\n",
      "Epoch 70: val_loss improved from 0.16413 to 0.16314, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0979 - accuracy: 0.9681 - val_loss: 0.1631 - val_accuracy: 0.9405\n",
      "Epoch 71/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1004 - accuracy: 0.9640\n",
      "Epoch 71: val_loss improved from 0.16314 to 0.16235, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0967 - accuracy: 0.9681 - val_loss: 0.1624 - val_accuracy: 0.9405\n",
      "Epoch 72/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0955 - accuracy: 0.9714\n",
      "Epoch 72: val_loss improved from 0.16235 to 0.16169, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9681 - val_loss: 0.1617 - val_accuracy: 0.9405\n",
      "Epoch 73/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0908 - accuracy: 0.9667\n",
      "Epoch 73: val_loss improved from 0.16169 to 0.16080, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0943 - accuracy: 0.9681 - val_loss: 0.1608 - val_accuracy: 0.9405\n",
      "Epoch 74/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0889 - accuracy: 0.9709\n",
      "Epoch 74: val_loss improved from 0.16080 to 0.16004, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0931 - accuracy: 0.9681 - val_loss: 0.1600 - val_accuracy: 0.9405\n",
      "Epoch 75/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0920 - accuracy: 0.9680\n",
      "Epoch 75: val_loss improved from 0.16004 to 0.15933, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0920 - accuracy: 0.9681 - val_loss: 0.1593 - val_accuracy: 0.9405\n",
      "Epoch 76/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0961 - accuracy: 0.9667\n",
      "Epoch 76: val_loss improved from 0.15933 to 0.15859, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9681 - val_loss: 0.1586 - val_accuracy: 0.9405\n",
      "Epoch 77/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0965 - accuracy: 0.9660\n",
      "Epoch 77: val_loss improved from 0.15859 to 0.15789, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9681 - val_loss: 0.1579 - val_accuracy: 0.9405\n",
      "Epoch 78/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0895 - accuracy: 0.9667\n",
      "Epoch 78: val_loss improved from 0.15789 to 0.15688, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0885 - accuracy: 0.9681 - val_loss: 0.1569 - val_accuracy: 0.9405\n",
      "Epoch 79/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0899 - accuracy: 0.9667\n",
      "Epoch 79: val_loss improved from 0.15688 to 0.15645, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0876 - accuracy: 0.9681 - val_loss: 0.1565 - val_accuracy: 0.9405\n",
      "Epoch 80/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0835 - accuracy: 0.9698\n",
      "Epoch 80: val_loss improved from 0.15645 to 0.15570, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0864 - accuracy: 0.9681 - val_loss: 0.1557 - val_accuracy: 0.9405\n",
      "Epoch 81/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0859 - accuracy: 0.9660\n",
      "Epoch 81: val_loss improved from 0.15570 to 0.15527, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0853 - accuracy: 0.9681 - val_loss: 0.1553 - val_accuracy: 0.9405\n",
      "Epoch 82/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0848 - accuracy: 0.9695\n",
      "Epoch 82: val_loss improved from 0.15527 to 0.15431, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0843 - accuracy: 0.9681 - val_loss: 0.1543 - val_accuracy: 0.9405\n",
      "Epoch 83/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0807 - accuracy: 0.9714\n",
      "Epoch 83: val_loss improved from 0.15431 to 0.15366, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0831 - accuracy: 0.9681 - val_loss: 0.1537 - val_accuracy: 0.9405\n",
      "Epoch 84/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0746 - accuracy: 0.9709\n",
      "Epoch 84: val_loss improved from 0.15366 to 0.15290, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9681 - val_loss: 0.1529 - val_accuracy: 0.9405\n",
      "Epoch 85/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0810 - accuracy: 0.9700\n",
      "Epoch 85: val_loss improved from 0.15290 to 0.15265, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.9712 - val_loss: 0.1526 - val_accuracy: 0.9405\n",
      "Epoch 86/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0837 - accuracy: 0.9709\n",
      "Epoch 86: val_loss improved from 0.15265 to 0.15160, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.9712 - val_loss: 0.1516 - val_accuracy: 0.9405\n",
      "Epoch 87/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0840 - accuracy: 0.9709\n",
      "Epoch 87: val_loss improved from 0.15160 to 0.15102, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0790 - accuracy: 0.9744 - val_loss: 0.1510 - val_accuracy: 0.9405\n",
      "Epoch 88/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0837 - accuracy: 0.9745\n",
      "Epoch 88: val_loss improved from 0.15102 to 0.15058, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9776 - val_loss: 0.1506 - val_accuracy: 0.9405\n",
      "Epoch 89/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0771 - accuracy: 0.9797\n",
      "Epoch 89: val_loss improved from 0.15058 to 0.14981, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.9776 - val_loss: 0.1498 - val_accuracy: 0.9405\n",
      "Epoch 90/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0817 - accuracy: 0.9750\n",
      "Epoch 90: val_loss improved from 0.14981 to 0.14932, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0762 - accuracy: 0.9776 - val_loss: 0.1493 - val_accuracy: 0.9405\n",
      "Epoch 91/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0808 - accuracy: 0.9755\n",
      "Epoch 91: val_loss improved from 0.14932 to 0.14858, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0751 - accuracy: 0.9776 - val_loss: 0.1486 - val_accuracy: 0.9405\n",
      "Epoch 92/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0758 - accuracy: 0.9793\n",
      "Epoch 92: val_loss improved from 0.14858 to 0.14803, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9776 - val_loss: 0.1480 - val_accuracy: 0.9405\n",
      "Epoch 93/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9770\n",
      "Epoch 93: val_loss improved from 0.14803 to 0.14720, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9776 - val_loss: 0.1472 - val_accuracy: 0.9405\n",
      "Epoch 94/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0718 - accuracy: 0.9767\n",
      "Epoch 94: val_loss improved from 0.14720 to 0.14662, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.9776 - val_loss: 0.1466 - val_accuracy: 0.9405\n",
      "Epoch 95/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0729 - accuracy: 0.9797\n",
      "Epoch 95: val_loss improved from 0.14662 to 0.14619, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9808 - val_loss: 0.1462 - val_accuracy: 0.9405\n",
      "Epoch 96/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0630 - accuracy: 0.9796\n",
      "Epoch 96: val_loss improved from 0.14619 to 0.14547, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.9808 - val_loss: 0.1455 - val_accuracy: 0.9405\n",
      "Epoch 97/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0653 - accuracy: 0.9831\n",
      "Epoch 97: val_loss improved from 0.14547 to 0.14466, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.9808 - val_loss: 0.1447 - val_accuracy: 0.9405\n",
      "Epoch 98/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0776 - accuracy: 0.9804\n",
      "Epoch 98: val_loss improved from 0.14466 to 0.14420, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9840 - val_loss: 0.1442 - val_accuracy: 0.9405\n",
      "Epoch 99/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0723 - accuracy: 0.9811\n",
      "Epoch 99: val_loss improved from 0.14420 to 0.14350, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0681 - accuracy: 0.9840 - val_loss: 0.1435 - val_accuracy: 0.9405\n",
      "Epoch 100/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0706 - accuracy: 0.9828\n",
      "Epoch 100: val_loss improved from 0.14350 to 0.14280, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9840 - val_loss: 0.1428 - val_accuracy: 0.9405\n",
      "Epoch 101/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0541 - accuracy: 0.9900\n",
      "Epoch 101: val_loss improved from 0.14280 to 0.14261, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9840 - val_loss: 0.1426 - val_accuracy: 0.9405\n",
      "Epoch 102/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0724 - accuracy: 0.9815\n",
      "Epoch 102: val_loss improved from 0.14261 to 0.14175, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0658 - accuracy: 0.9840 - val_loss: 0.1418 - val_accuracy: 0.9405\n",
      "Epoch 103/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0609 - accuracy: 0.9867\n",
      "Epoch 103: val_loss improved from 0.14175 to 0.14088, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9840 - val_loss: 0.1409 - val_accuracy: 0.9405\n",
      "Epoch 104/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0701 - accuracy: 0.9840\n",
      "Epoch 104: val_loss improved from 0.14088 to 0.14044, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9840 - val_loss: 0.1404 - val_accuracy: 0.9405\n",
      "Epoch 105/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0663 - accuracy: 0.9828\n",
      "Epoch 105: val_loss improved from 0.14044 to 0.14008, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0633 - accuracy: 0.9840 - val_loss: 0.1401 - val_accuracy: 0.9405\n",
      "Epoch 106/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0574 - accuracy: 0.9893\n",
      "Epoch 106: val_loss improved from 0.14008 to 0.13903, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.9872 - val_loss: 0.1390 - val_accuracy: 0.9405\n",
      "Epoch 107/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0641 - accuracy: 0.9864\n",
      "Epoch 107: val_loss improved from 0.13903 to 0.13879, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.9872 - val_loss: 0.1388 - val_accuracy: 0.9405\n",
      "Epoch 108/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0638 - accuracy: 0.9864\n",
      "Epoch 108: val_loss improved from 0.13879 to 0.13787, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9872 - val_loss: 0.1379 - val_accuracy: 0.9405\n",
      "Epoch 109/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0575 - accuracy: 0.9898\n",
      "Epoch 109: val_loss improved from 0.13787 to 0.13704, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9872 - val_loss: 0.1370 - val_accuracy: 0.9405\n",
      "Epoch 110/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0646 - accuracy: 0.9857\n",
      "Epoch 110: val_loss improved from 0.13704 to 0.13665, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9872 - val_loss: 0.1367 - val_accuracy: 0.9405\n",
      "Epoch 111/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0561 - accuracy: 0.9895\n",
      "Epoch 111: val_loss improved from 0.13665 to 0.13579, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9872 - val_loss: 0.1358 - val_accuracy: 0.9405\n",
      "Epoch 112/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0587 - accuracy: 0.9852\n",
      "Epoch 112: val_loss improved from 0.13579 to 0.13516, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0581 - accuracy: 0.9872 - val_loss: 0.1352 - val_accuracy: 0.9405\n",
      "Epoch 113/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0595 - accuracy: 0.9867\n",
      "Epoch 113: val_loss improved from 0.13516 to 0.13459, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.9872 - val_loss: 0.1346 - val_accuracy: 0.9405\n",
      "Epoch 114/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0594 - accuracy: 0.9862\n",
      "Epoch 114: val_loss improved from 0.13459 to 0.13411, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9872 - val_loss: 0.1341 - val_accuracy: 0.9405\n",
      "Epoch 115/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0553 - accuracy: 0.9869\n",
      "Epoch 115: val_loss improved from 0.13411 to 0.13362, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9872 - val_loss: 0.1336 - val_accuracy: 0.9405\n",
      "Epoch 116/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9869\n",
      "Epoch 116: val_loss improved from 0.13362 to 0.13259, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0554 - accuracy: 0.9872 - val_loss: 0.1326 - val_accuracy: 0.9405\n",
      "Epoch 117/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0547 - accuracy: 0.9862\n",
      "Epoch 117: val_loss improved from 0.13259 to 0.13168, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0549 - accuracy: 0.9872 - val_loss: 0.1317 - val_accuracy: 0.9405\n",
      "Epoch 118/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0566 - accuracy: 0.9852\n",
      "Epoch 118: val_loss improved from 0.13168 to 0.13136, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9872 - val_loss: 0.1314 - val_accuracy: 0.9405\n",
      "Epoch 119/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0489 - accuracy: 0.9902\n",
      "Epoch 119: val_loss improved from 0.13136 to 0.13049, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9872 - val_loss: 0.1305 - val_accuracy: 0.9405\n",
      "Epoch 120/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0550 - accuracy: 0.9864\n",
      "Epoch 120: val_loss improved from 0.13049 to 0.12982, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.9872 - val_loss: 0.1298 - val_accuracy: 0.9405\n",
      "Epoch 121/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0528 - accuracy: 0.9864\n",
      "Epoch 121: val_loss improved from 0.12982 to 0.12958, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0521 - accuracy: 0.9872 - val_loss: 0.1296 - val_accuracy: 0.9405\n",
      "Epoch 122/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0535 - accuracy: 0.9860\n",
      "Epoch 122: val_loss improved from 0.12958 to 0.12861, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0515 - accuracy: 0.9872 - val_loss: 0.1286 - val_accuracy: 0.9405\n",
      "Epoch 123/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0537 - accuracy: 0.9862\n",
      "Epoch 123: val_loss improved from 0.12861 to 0.12813, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.9872 - val_loss: 0.1281 - val_accuracy: 0.9405\n",
      "Epoch 124/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0524 - accuracy: 0.9864\n",
      "Epoch 124: val_loss improved from 0.12813 to 0.12732, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.9872 - val_loss: 0.1273 - val_accuracy: 0.9405\n",
      "Epoch 125/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0476 - accuracy: 0.9887\n",
      "Epoch 125: val_loss improved from 0.12732 to 0.12670, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0497 - accuracy: 0.9872 - val_loss: 0.1267 - val_accuracy: 0.9405\n",
      "Epoch 126/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0506 - accuracy: 0.9846\n",
      "Epoch 126: val_loss improved from 0.12670 to 0.12598, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.9872 - val_loss: 0.1260 - val_accuracy: 0.9405\n",
      "Epoch 127/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0298 - accuracy: 0.9962\n",
      "Epoch 127: val_loss improved from 0.12598 to 0.12530, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0486 - accuracy: 0.9872 - val_loss: 0.1253 - val_accuracy: 0.9464\n",
      "Epoch 128/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0460 - accuracy: 0.9889\n",
      "Epoch 128: val_loss improved from 0.12530 to 0.12456, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.9872 - val_loss: 0.1246 - val_accuracy: 0.9464\n",
      "Epoch 129/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0460 - accuracy: 0.9893\n",
      "Epoch 129: val_loss improved from 0.12456 to 0.12392, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0472 - accuracy: 0.9872 - val_loss: 0.1239 - val_accuracy: 0.9464\n",
      "Epoch 130/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0428 - accuracy: 0.9895\n",
      "Epoch 130: val_loss improved from 0.12392 to 0.12308, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0466 - accuracy: 0.9872 - val_loss: 0.1231 - val_accuracy: 0.9524\n",
      "Epoch 131/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0431 - accuracy: 0.9891\n",
      "Epoch 131: val_loss improved from 0.12308 to 0.12279, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 0.9872 - val_loss: 0.1228 - val_accuracy: 0.9524\n",
      "Epoch 132/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0489 - accuracy: 0.9849\n",
      "Epoch 132: val_loss improved from 0.12279 to 0.12224, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0455 - accuracy: 0.9872 - val_loss: 0.1222 - val_accuracy: 0.9524\n",
      "Epoch 133/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0497 - accuracy: 0.9837\n",
      "Epoch 133: val_loss improved from 0.12224 to 0.12160, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0449 - accuracy: 0.9872 - val_loss: 0.1216 - val_accuracy: 0.9524\n",
      "Epoch 134/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0370 - accuracy: 0.9933\n",
      "Epoch 134: val_loss improved from 0.12160 to 0.12057, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0442 - accuracy: 0.9872 - val_loss: 0.1206 - val_accuracy: 0.9524\n",
      "Epoch 135/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0484 - accuracy: 0.9849\n",
      "Epoch 135: val_loss improved from 0.12057 to 0.12056, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0438 - accuracy: 0.9872 - val_loss: 0.1206 - val_accuracy: 0.9524\n",
      "Epoch 136/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0439 - accuracy: 0.9869\n",
      "Epoch 136: val_loss improved from 0.12056 to 0.11975, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 0.9872 - val_loss: 0.1197 - val_accuracy: 0.9524\n",
      "Epoch 137/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0429 - accuracy: 0.9882\n",
      "Epoch 137: val_loss improved from 0.11975 to 0.11905, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0426 - accuracy: 0.9872 - val_loss: 0.1190 - val_accuracy: 0.9524\n",
      "Epoch 138/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0433 - accuracy: 0.9898\n",
      "Epoch 138: val_loss improved from 0.11905 to 0.11853, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.9904 - val_loss: 0.1185 - val_accuracy: 0.9524\n",
      "Epoch 139/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0439 - accuracy: 0.9885\n",
      "Epoch 139: val_loss improved from 0.11853 to 0.11808, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 0.9904 - val_loss: 0.1181 - val_accuracy: 0.9524\n",
      "Epoch 140/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0463 - accuracy: 0.9882\n",
      "Epoch 140: val_loss improved from 0.11808 to 0.11716, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0410 - accuracy: 0.9904 - val_loss: 0.1172 - val_accuracy: 0.9524\n",
      "Epoch 141/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9904\n",
      "Epoch 141: val_loss improved from 0.11716 to 0.11633, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 0.9904 - val_loss: 0.1163 - val_accuracy: 0.9524\n",
      "Epoch 142/200\n",
      "45/63 [====================>.........] - ETA: 0s - loss: 0.0425 - accuracy: 0.9911\n",
      "Epoch 142: val_loss improved from 0.11633 to 0.11589, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0400 - accuracy: 0.9904 - val_loss: 0.1159 - val_accuracy: 0.9524\n",
      "Epoch 143/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0411 - accuracy: 0.9882\n",
      "Epoch 143: val_loss improved from 0.11589 to 0.11547, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9904 - val_loss: 0.1155 - val_accuracy: 0.9524\n",
      "Epoch 144/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0408 - accuracy: 0.9895\n",
      "Epoch 144: val_loss improved from 0.11547 to 0.11499, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0390 - accuracy: 0.9904 - val_loss: 0.1150 - val_accuracy: 0.9524\n",
      "Epoch 145/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0306 - accuracy: 0.9931\n",
      "Epoch 145: val_loss improved from 0.11499 to 0.11464, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9904 - val_loss: 0.1146 - val_accuracy: 0.9524\n",
      "Epoch 146/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0397 - accuracy: 0.9897\n",
      "Epoch 146: val_loss improved from 0.11464 to 0.11361, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0379 - accuracy: 0.9904 - val_loss: 0.1136 - val_accuracy: 0.9524\n",
      "Epoch 147/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0393 - accuracy: 0.9875\n",
      "Epoch 147: val_loss improved from 0.11361 to 0.11311, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 0.9904 - val_loss: 0.1131 - val_accuracy: 0.9524\n",
      "Epoch 148/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0393 - accuracy: 0.9891\n",
      "Epoch 148: val_loss improved from 0.11311 to 0.11265, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.9904 - val_loss: 0.1127 - val_accuracy: 0.9524\n",
      "Epoch 149/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0397 - accuracy: 0.9893\n",
      "Epoch 149: val_loss improved from 0.11265 to 0.11202, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.9904 - val_loss: 0.1120 - val_accuracy: 0.9524\n",
      "Epoch 150/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0354 - accuracy: 0.9930\n",
      "Epoch 150: val_loss improved from 0.11202 to 0.11116, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 0.9904 - val_loss: 0.1112 - val_accuracy: 0.9524\n",
      "Epoch 151/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0251 - accuracy: 0.9965\n",
      "Epoch 151: val_loss improved from 0.11116 to 0.11106, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9904 - val_loss: 0.1111 - val_accuracy: 0.9524\n",
      "Epoch 152/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0364 - accuracy: 0.9897\n",
      "Epoch 152: val_loss improved from 0.11106 to 0.11027, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 0.9904 - val_loss: 0.1103 - val_accuracy: 0.9524\n",
      "Epoch 153/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0370 - accuracy: 0.9895\n",
      "Epoch 153: val_loss improved from 0.11027 to 0.10990, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9904 - val_loss: 0.1099 - val_accuracy: 0.9524\n",
      "Epoch 154/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0387 - accuracy: 0.9882\n",
      "Epoch 154: val_loss improved from 0.10990 to 0.10915, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.9904 - val_loss: 0.1092 - val_accuracy: 0.9524\n",
      "Epoch 155/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0276 - accuracy: 0.9920\n",
      "Epoch 155: val_loss improved from 0.10915 to 0.10909, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.9904 - val_loss: 0.1091 - val_accuracy: 0.9524\n",
      "Epoch 156/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0363 - accuracy: 0.9887\n",
      "Epoch 156: val_loss improved from 0.10909 to 0.10822, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0331 - accuracy: 0.9904 - val_loss: 0.1082 - val_accuracy: 0.9524\n",
      "Epoch 157/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0328 - accuracy: 0.9903\n",
      "Epoch 157: val_loss improved from 0.10822 to 0.10736, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0327 - accuracy: 0.9904 - val_loss: 0.1074 - val_accuracy: 0.9524\n",
      "Epoch 158/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0332 - accuracy: 0.9889\n",
      "Epoch 158: val_loss improved from 0.10736 to 0.10713, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 0.9904 - val_loss: 0.1071 - val_accuracy: 0.9524\n",
      "Epoch 159/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0303 - accuracy: 0.9927\n",
      "Epoch 159: val_loss improved from 0.10713 to 0.10637, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 0.9904 - val_loss: 0.1064 - val_accuracy: 0.9524\n",
      "Epoch 160/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0330 - accuracy: 0.9897\n",
      "Epoch 160: val_loss improved from 0.10637 to 0.10614, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.9904 - val_loss: 0.1061 - val_accuracy: 0.9583\n",
      "Epoch 161/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0307 - accuracy: 0.9889\n",
      "Epoch 161: val_loss improved from 0.10614 to 0.10582, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0309 - accuracy: 0.9904 - val_loss: 0.1058 - val_accuracy: 0.9583\n",
      "Epoch 162/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0327 - accuracy: 0.9889\n",
      "Epoch 162: val_loss improved from 0.10582 to 0.10538, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0304 - accuracy: 0.9904 - val_loss: 0.1054 - val_accuracy: 0.9583\n",
      "Epoch 163/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0272 - accuracy: 0.9929\n",
      "Epoch 163: val_loss improved from 0.10538 to 0.10462, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.1046 - val_accuracy: 0.9583\n",
      "Epoch 164/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0311 - accuracy: 0.9895\n",
      "Epoch 164: val_loss improved from 0.10462 to 0.10460, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0297 - accuracy: 0.9904 - val_loss: 0.1046 - val_accuracy: 0.9583\n",
      "Epoch 165/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0300 - accuracy: 0.9933\n",
      "Epoch 165: val_loss improved from 0.10460 to 0.10422, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 0.9936 - val_loss: 0.1042 - val_accuracy: 0.9583\n",
      "Epoch 166/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0299 - accuracy: 0.9920\n",
      "Epoch 166: val_loss improved from 0.10422 to 0.10374, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0287 - accuracy: 0.9936 - val_loss: 0.1037 - val_accuracy: 0.9583\n",
      "Epoch 167/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0294 - accuracy: 0.9932\n",
      "Epoch 167: val_loss improved from 0.10374 to 0.10318, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.9936 - val_loss: 0.1032 - val_accuracy: 0.9583\n",
      "Epoch 168/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0288 - accuracy: 0.9931\n",
      "Epoch 168: val_loss improved from 0.10318 to 0.10318, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9936 - val_loss: 0.1032 - val_accuracy: 0.9583\n",
      "Epoch 169/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0197 - accuracy: 0.9964\n",
      "Epoch 169: val_loss improved from 0.10318 to 0.10272, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9936 - val_loss: 0.1027 - val_accuracy: 0.9583\n",
      "Epoch 170/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0198 - accuracy: 0.9965\n",
      "Epoch 170: val_loss improved from 0.10272 to 0.10237, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.9936 - val_loss: 0.1024 - val_accuracy: 0.9583\n",
      "Epoch 171/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0250 - accuracy: 0.9963\n",
      "Epoch 171: val_loss improved from 0.10237 to 0.10138, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0267 - accuracy: 0.9936 - val_loss: 0.1014 - val_accuracy: 0.9583\n",
      "Epoch 172/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0279 - accuracy: 0.9925\n",
      "Epoch 172: val_loss did not improve from 0.10138\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.9936 - val_loss: 0.1017 - val_accuracy: 0.9583\n",
      "Epoch 173/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0241 - accuracy: 0.9931\n",
      "Epoch 173: val_loss improved from 0.10138 to 0.10103, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0259 - accuracy: 0.9936 - val_loss: 0.1010 - val_accuracy: 0.9583\n",
      "Epoch 174/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0271 - accuracy: 0.9929\n",
      "Epoch 174: val_loss improved from 0.10103 to 0.10097, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9936 - val_loss: 0.1010 - val_accuracy: 0.9583\n",
      "Epoch 175/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0273 - accuracy: 0.9923\n",
      "Epoch 175: val_loss improved from 0.10097 to 0.10063, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0251 - accuracy: 0.9936 - val_loss: 0.1006 - val_accuracy: 0.9583\n",
      "Epoch 176/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0249 - accuracy: 0.9935\n",
      "Epoch 176: val_loss improved from 0.10063 to 0.10010, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0248 - accuracy: 0.9936 - val_loss: 0.1001 - val_accuracy: 0.9583\n",
      "Epoch 177/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0267 - accuracy: 0.9926\n",
      "Epoch 177: val_loss improved from 0.10010 to 0.09984, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 0.9936 - val_loss: 0.0998 - val_accuracy: 0.9583\n",
      "Epoch 178/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0255 - accuracy: 0.9923\n",
      "Epoch 178: val_loss improved from 0.09984 to 0.09970, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 0.9936 - val_loss: 0.0997 - val_accuracy: 0.9583\n",
      "Epoch 179/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0262 - accuracy: 0.9918\n",
      "Epoch 179: val_loss improved from 0.09970 to 0.09934, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0236 - accuracy: 0.9936 - val_loss: 0.0993 - val_accuracy: 0.9583\n",
      "Epoch 180/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0228 - accuracy: 0.9927\n",
      "Epoch 180: val_loss improved from 0.09934 to 0.09865, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0232 - accuracy: 0.9936 - val_loss: 0.0987 - val_accuracy: 0.9583\n",
      "Epoch 181/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0237 - accuracy: 0.9931\n",
      "Epoch 181: val_loss did not improve from 0.09865\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.0988 - val_accuracy: 0.9583\n",
      "Epoch 182/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0231 - accuracy: 0.9933\n",
      "Epoch 182: val_loss improved from 0.09865 to 0.09864, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0226 - accuracy: 0.9936 - val_loss: 0.0986 - val_accuracy: 0.9583\n",
      "Epoch 183/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0233 - accuracy: 0.9926\n",
      "Epoch 183: val_loss improved from 0.09864 to 0.09824, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 0.9936 - val_loss: 0.0982 - val_accuracy: 0.9583\n",
      "Epoch 184/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0211 - accuracy: 0.9962\n",
      "Epoch 184: val_loss improved from 0.09824 to 0.09780, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9936 - val_loss: 0.0978 - val_accuracy: 0.9583\n",
      "Epoch 185/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0223 - accuracy: 0.9929\n",
      "Epoch 185: val_loss did not improve from 0.09780\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.9936 - val_loss: 0.0979 - val_accuracy: 0.9583\n",
      "Epoch 186/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0217 - accuracy: 0.9934\n",
      "Epoch 186: val_loss improved from 0.09780 to 0.09754, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9936 - val_loss: 0.0975 - val_accuracy: 0.9583\n",
      "Epoch 187/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0164 - accuracy: 0.9965\n",
      "Epoch 187: val_loss improved from 0.09754 to 0.09740, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.9936 - val_loss: 0.0974 - val_accuracy: 0.9583\n",
      "Epoch 188/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0224 - accuracy: 0.9925\n",
      "Epoch 188: val_loss improved from 0.09740 to 0.09688, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0206 - accuracy: 0.9936 - val_loss: 0.0969 - val_accuracy: 0.9583\n",
      "Epoch 189/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0199 - accuracy: 0.9929\n",
      "Epoch 189: val_loss improved from 0.09688 to 0.09669, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.9936 - val_loss: 0.0967 - val_accuracy: 0.9583\n",
      "Epoch 190/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0195 - accuracy: 0.9931\n",
      "Epoch 190: val_loss improved from 0.09669 to 0.09648, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.9936 - val_loss: 0.0965 - val_accuracy: 0.9583\n",
      "Epoch 191/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0211 - accuracy: 0.9962\n",
      "Epoch 191: val_loss did not improve from 0.09648\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.9968 - val_loss: 0.0965 - val_accuracy: 0.9583\n",
      "Epoch 192/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0192 - accuracy: 0.9962\n",
      "Epoch 192: val_loss improved from 0.09648 to 0.09615, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9968 - val_loss: 0.0961 - val_accuracy: 0.9583\n",
      "Epoch 193/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 193: val_loss did not improve from 0.09615\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.9968 - val_loss: 0.0962 - val_accuracy: 0.9583\n",
      "Epoch 194/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0173 - accuracy: 0.9962\n",
      "Epoch 194: val_loss improved from 0.09615 to 0.09567, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 0.9968 - val_loss: 0.0957 - val_accuracy: 0.9643\n",
      "Epoch 195/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0196 - accuracy: 0.9960\n",
      "Epoch 195: val_loss did not improve from 0.09567\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0184 - accuracy: 0.9968 - val_loss: 0.0957 - val_accuracy: 0.9643\n",
      "Epoch 196/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0204 - accuracy: 0.9961\n",
      "Epoch 196: val_loss improved from 0.09567 to 0.09563, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0181 - accuracy: 0.9968 - val_loss: 0.0956 - val_accuracy: 0.9643\n",
      "Epoch 197/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9968\n",
      "Epoch 197: val_loss improved from 0.09563 to 0.09561, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9968 - val_loss: 0.0956 - val_accuracy: 0.9643\n",
      "Epoch 198/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0178 - accuracy: 0.9967\n",
      "Epoch 198: val_loss improved from 0.09561 to 0.09547, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0174 - accuracy: 0.9968 - val_loss: 0.0955 - val_accuracy: 0.9643\n",
      "Epoch 199/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0180 - accuracy: 0.9966\n",
      "Epoch 199: val_loss improved from 0.09547 to 0.09526, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.9968 - val_loss: 0.0953 - val_accuracy: 0.9643\n",
      "Epoch 200/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0176 - accuracy: 0.9966\n",
      "Epoch 200: val_loss improved from 0.09526 to 0.09505, saving model to model_checkpoint\\GRU_1 layer_Adam.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.9968 - val_loss: 0.0950 - val_accuracy: 0.9643\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_1 (GRU)                 (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0950 - accuracy: 0.9643\n",
      "Test Loss: 0.09504835307598114\n",
      "Test Accuracy: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 1 layer RMS optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.7930 - accuracy: 0.3565\n",
      "Epoch 1: val_loss improved from inf to 0.72199, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 3s 13ms/step - loss: 0.7833 - accuracy: 0.3834 - val_loss: 0.7220 - val_accuracy: 0.5060\n",
      "Epoch 2/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.7248 - accuracy: 0.4691\n",
      "Epoch 2: val_loss improved from 0.72199 to 0.66277, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.7133 - accuracy: 0.4856 - val_loss: 0.6628 - val_accuracy: 0.6190\n",
      "Epoch 3/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.6561 - accuracy: 0.5745\n",
      "Epoch 3: val_loss improved from 0.66277 to 0.60938, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.6522 - accuracy: 0.5974 - val_loss: 0.6094 - val_accuracy: 0.6845\n",
      "Epoch 4/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.5990 - accuracy: 0.7143\n",
      "Epoch 4: val_loss improved from 0.60938 to 0.56258, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5975 - accuracy: 0.7252 - val_loss: 0.5626 - val_accuracy: 0.7440\n",
      "Epoch 5/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.5410 - accuracy: 0.8038\n",
      "Epoch 5: val_loss improved from 0.56258 to 0.52054, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5488 - accuracy: 0.7891 - val_loss: 0.5205 - val_accuracy: 0.7976\n",
      "Epoch 6/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.5083 - accuracy: 0.8426\n",
      "Epoch 6: val_loss improved from 0.52054 to 0.48333, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.5057 - accuracy: 0.8211 - val_loss: 0.4833 - val_accuracy: 0.8452\n",
      "Epoch 7/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.4673 - accuracy: 0.8346\n",
      "Epoch 7: val_loss improved from 0.48333 to 0.45016, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4678 - accuracy: 0.8275 - val_loss: 0.4502 - val_accuracy: 0.8690\n",
      "Epoch 8/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.8626\n",
      "Epoch 8: val_loss improved from 0.45016 to 0.42061, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4337 - accuracy: 0.8626 - val_loss: 0.4206 - val_accuracy: 0.8690\n",
      "Epoch 9/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.4006 - accuracy: 0.8746\n",
      "Epoch 9: val_loss improved from 0.42061 to 0.39449, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4036 - accuracy: 0.8754 - val_loss: 0.3945 - val_accuracy: 0.8869\n",
      "Epoch 10/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3870 - accuracy: 0.8842\n",
      "Epoch 10: val_loss improved from 0.39449 to 0.37126, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3767 - accuracy: 0.8882 - val_loss: 0.3713 - val_accuracy: 0.8929\n",
      "Epoch 11/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3447 - accuracy: 0.9088\n",
      "Epoch 11: val_loss improved from 0.37126 to 0.35038, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3529 - accuracy: 0.8978 - val_loss: 0.3504 - val_accuracy: 0.9107\n",
      "Epoch 12/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.3391 - accuracy: 0.8964\n",
      "Epoch 12: val_loss improved from 0.35038 to 0.33215, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3318 - accuracy: 0.9042 - val_loss: 0.3322 - val_accuracy: 0.9107\n",
      "Epoch 13/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.3086 - accuracy: 0.9088\n",
      "Epoch 13: val_loss improved from 0.33215 to 0.31552, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3132 - accuracy: 0.9073 - val_loss: 0.3155 - val_accuracy: 0.9167\n",
      "Epoch 14/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.2956 - accuracy: 0.9207\n",
      "Epoch 14: val_loss improved from 0.31552 to 0.30078, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2964 - accuracy: 0.9169 - val_loss: 0.3008 - val_accuracy: 0.9167\n",
      "Epoch 15/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.2875 - accuracy: 0.9143\n",
      "Epoch 15: val_loss improved from 0.30078 to 0.28772, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2816 - accuracy: 0.9233 - val_loss: 0.2877 - val_accuracy: 0.9286\n",
      "Epoch 16/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2667 - accuracy: 0.9259\n",
      "Epoch 16: val_loss improved from 0.28772 to 0.27548, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2679 - accuracy: 0.9297 - val_loss: 0.2755 - val_accuracy: 0.9286\n",
      "Epoch 17/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2587 - accuracy: 0.9298\n",
      "Epoch 17: val_loss improved from 0.27548 to 0.26509, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2556 - accuracy: 0.9329 - val_loss: 0.2651 - val_accuracy: 0.9286\n",
      "Epoch 18/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.2381 - accuracy: 0.9407\n",
      "Epoch 18: val_loss improved from 0.26509 to 0.25548, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.2447 - accuracy: 0.9361 - val_loss: 0.2555 - val_accuracy: 0.9286\n",
      "Epoch 19/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2353 - accuracy: 0.9346\n",
      "Epoch 19: val_loss improved from 0.25548 to 0.24662, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2346 - accuracy: 0.9361 - val_loss: 0.2466 - val_accuracy: 0.9286\n",
      "Epoch 20/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.2207 - accuracy: 0.9385\n",
      "Epoch 20: val_loss improved from 0.24662 to 0.23855, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2254 - accuracy: 0.9329 - val_loss: 0.2385 - val_accuracy: 0.9345\n",
      "Epoch 21/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.2159 - accuracy: 0.9358\n",
      "Epoch 21: val_loss improved from 0.23855 to 0.23149, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2169 - accuracy: 0.9393 - val_loss: 0.2315 - val_accuracy: 0.9405\n",
      "Epoch 22/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.2061 - accuracy: 0.9474\n",
      "Epoch 22: val_loss improved from 0.23149 to 0.22489, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2094 - accuracy: 0.9425 - val_loss: 0.2249 - val_accuracy: 0.9405\n",
      "Epoch 23/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.2146 - accuracy: 0.9375\n",
      "Epoch 23: val_loss improved from 0.22489 to 0.21906, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.2023 - accuracy: 0.9457 - val_loss: 0.2191 - val_accuracy: 0.9405\n",
      "Epoch 24/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1963 - accuracy: 0.9484\n",
      "Epoch 24: val_loss improved from 0.21906 to 0.21357, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1958 - accuracy: 0.9489 - val_loss: 0.2136 - val_accuracy: 0.9405\n",
      "Epoch 25/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1698 - accuracy: 0.9625\n",
      "Epoch 25: val_loss improved from 0.21357 to 0.20850, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1897 - accuracy: 0.9489 - val_loss: 0.2085 - val_accuracy: 0.9405\n",
      "Epoch 26/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.9639\n",
      "Epoch 26: val_loss improved from 0.20850 to 0.20407, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1840 - accuracy: 0.9553 - val_loss: 0.2041 - val_accuracy: 0.9405\n",
      "Epoch 27/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1708 - accuracy: 0.9469\n",
      "Epoch 27: val_loss improved from 0.20407 to 0.19986, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1787 - accuracy: 0.9521 - val_loss: 0.1999 - val_accuracy: 0.9405\n",
      "Epoch 28/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1723 - accuracy: 0.9574\n",
      "Epoch 28: val_loss improved from 0.19986 to 0.19599, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.9553 - val_loss: 0.1960 - val_accuracy: 0.9405\n",
      "Epoch 29/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9548\n",
      "Epoch 29: val_loss improved from 0.19599 to 0.19237, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1691 - accuracy: 0.9553 - val_loss: 0.1924 - val_accuracy: 0.9405\n",
      "Epoch 30/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.1662 - accuracy: 0.9536\n",
      "Epoch 30: val_loss improved from 0.19237 to 0.18917, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.9553 - val_loss: 0.1892 - val_accuracy: 0.9405\n",
      "Epoch 31/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1597 - accuracy: 0.9529\n",
      "Epoch 31: val_loss improved from 0.18917 to 0.18612, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1606 - accuracy: 0.9553 - val_loss: 0.1861 - val_accuracy: 0.9405\n",
      "Epoch 32/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.1624 - accuracy: 0.9517\n",
      "Epoch 32: val_loss improved from 0.18612 to 0.18328, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1568 - accuracy: 0.9553 - val_loss: 0.1833 - val_accuracy: 0.9405\n",
      "Epoch 33/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.1570 - accuracy: 0.9533\n",
      "Epoch 33: val_loss improved from 0.18328 to 0.18075, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1532 - accuracy: 0.9553 - val_loss: 0.1807 - val_accuracy: 0.9464\n",
      "Epoch 34/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.1572 - accuracy: 0.9542\n",
      "Epoch 34: val_loss improved from 0.18075 to 0.17833, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1497 - accuracy: 0.9553 - val_loss: 0.1783 - val_accuracy: 0.9464\n",
      "Epoch 35/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.1370 - accuracy: 0.9593\n",
      "Epoch 35: val_loss improved from 0.17833 to 0.17603, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1465 - accuracy: 0.9553 - val_loss: 0.1760 - val_accuracy: 0.9464\n",
      "Epoch 36/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1443 - accuracy: 0.9547\n",
      "Epoch 36: val_loss improved from 0.17603 to 0.17404, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9553 - val_loss: 0.1740 - val_accuracy: 0.9464\n",
      "Epoch 37/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9553\n",
      "Epoch 37: val_loss improved from 0.17404 to 0.17197, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1406 - accuracy: 0.9553 - val_loss: 0.1720 - val_accuracy: 0.9464\n",
      "Epoch 38/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1260 - accuracy: 0.9654\n",
      "Epoch 38: val_loss improved from 0.17197 to 0.17001, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1378 - accuracy: 0.9553 - val_loss: 0.1700 - val_accuracy: 0.9524\n",
      "Epoch 39/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1400 - accuracy: 0.9538\n",
      "Epoch 39: val_loss improved from 0.17001 to 0.16826, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1353 - accuracy: 0.9553 - val_loss: 0.1683 - val_accuracy: 0.9524\n",
      "Epoch 40/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1257 - accuracy: 0.9640\n",
      "Epoch 40: val_loss improved from 0.16826 to 0.16655, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1328 - accuracy: 0.9553 - val_loss: 0.1665 - val_accuracy: 0.9524\n",
      "Epoch 41/200\n",
      "46/63 [====================>.........] - ETA: 0s - loss: 0.1330 - accuracy: 0.9522\n",
      "Epoch 41: val_loss improved from 0.16655 to 0.16499, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1302 - accuracy: 0.9553 - val_loss: 0.1650 - val_accuracy: 0.9524\n",
      "Epoch 42/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.1332 - accuracy: 0.9547\n",
      "Epoch 42: val_loss improved from 0.16499 to 0.16348, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1280 - accuracy: 0.9585 - val_loss: 0.1635 - val_accuracy: 0.9524\n",
      "Epoch 43/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.1191 - accuracy: 0.9680\n",
      "Epoch 43: val_loss improved from 0.16348 to 0.16200, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1258 - accuracy: 0.9585 - val_loss: 0.1620 - val_accuracy: 0.9524\n",
      "Epoch 44/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1085 - accuracy: 0.9686\n",
      "Epoch 44: val_loss improved from 0.16200 to 0.16065, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9617 - val_loss: 0.1606 - val_accuracy: 0.9524\n",
      "Epoch 45/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9607\n",
      "Epoch 45: val_loss improved from 0.16065 to 0.15941, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1218 - accuracy: 0.9617 - val_loss: 0.1594 - val_accuracy: 0.9524\n",
      "Epoch 46/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.1140 - accuracy: 0.9627\n",
      "Epoch 46: val_loss improved from 0.15941 to 0.15825, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1198 - accuracy: 0.9617 - val_loss: 0.1583 - val_accuracy: 0.9524\n",
      "Epoch 47/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1085 - accuracy: 0.9647\n",
      "Epoch 47: val_loss improved from 0.15825 to 0.15714, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9617 - val_loss: 0.1571 - val_accuracy: 0.9524\n",
      "Epoch 48/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1165 - accuracy: 0.9608\n",
      "Epoch 48: val_loss improved from 0.15714 to 0.15604, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1163 - accuracy: 0.9617 - val_loss: 0.1560 - val_accuracy: 0.9524\n",
      "Epoch 49/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.1153 - accuracy: 0.9645\n",
      "Epoch 49: val_loss improved from 0.15604 to 0.15489, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.1144 - accuracy: 0.9649 - val_loss: 0.1549 - val_accuracy: 0.9524\n",
      "Epoch 50/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1230 - accuracy: 0.9633\n",
      "Epoch 50: val_loss improved from 0.15489 to 0.15376, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9649 - val_loss: 0.1538 - val_accuracy: 0.9524\n",
      "Epoch 51/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1048 - accuracy: 0.9673\n",
      "Epoch 51: val_loss improved from 0.15376 to 0.15283, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9649 - val_loss: 0.1528 - val_accuracy: 0.9524\n",
      "Epoch 52/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9649\n",
      "Epoch 52: val_loss improved from 0.15283 to 0.15197, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1094 - accuracy: 0.9649 - val_loss: 0.1520 - val_accuracy: 0.9524\n",
      "Epoch 53/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.1111 - accuracy: 0.9633\n",
      "Epoch 53: val_loss improved from 0.15197 to 0.15115, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1078 - accuracy: 0.9649 - val_loss: 0.1511 - val_accuracy: 0.9524\n",
      "Epoch 54/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.1053 - accuracy: 0.9615\n",
      "Epoch 54: val_loss improved from 0.15115 to 0.15032, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1061 - accuracy: 0.9649 - val_loss: 0.1503 - val_accuracy: 0.9524\n",
      "Epoch 55/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0955 - accuracy: 0.9625\n",
      "Epoch 55: val_loss improved from 0.15032 to 0.14951, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1046 - accuracy: 0.9649 - val_loss: 0.1495 - val_accuracy: 0.9524\n",
      "Epoch 56/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0881 - accuracy: 0.9686\n",
      "Epoch 56: val_loss improved from 0.14951 to 0.14876, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.1033 - accuracy: 0.9649 - val_loss: 0.1488 - val_accuracy: 0.9524\n",
      "Epoch 57/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.1064 - accuracy: 0.9647\n",
      "Epoch 57: val_loss improved from 0.14876 to 0.14808, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.1018 - accuracy: 0.9649 - val_loss: 0.1481 - val_accuracy: 0.9524\n",
      "Epoch 58/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.1055 - accuracy: 0.9636\n",
      "Epoch 58: val_loss improved from 0.14808 to 0.14737, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.1003 - accuracy: 0.9649 - val_loss: 0.1474 - val_accuracy: 0.9524\n",
      "Epoch 59/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0941 - accuracy: 0.9654\n",
      "Epoch 59: val_loss improved from 0.14737 to 0.14665, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9649 - val_loss: 0.1466 - val_accuracy: 0.9524\n",
      "Epoch 60/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0984 - accuracy: 0.9615\n",
      "Epoch 60: val_loss improved from 0.14665 to 0.14608, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0976 - accuracy: 0.9649 - val_loss: 0.1461 - val_accuracy: 0.9524\n",
      "Epoch 61/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0900 - accuracy: 0.9667\n",
      "Epoch 61: val_loss improved from 0.14608 to 0.14544, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0963 - accuracy: 0.9649 - val_loss: 0.1454 - val_accuracy: 0.9524\n",
      "Epoch 62/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0945 - accuracy: 0.9633\n",
      "Epoch 62: val_loss improved from 0.14544 to 0.14477, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0950 - accuracy: 0.9649 - val_loss: 0.1448 - val_accuracy: 0.9524\n",
      "Epoch 63/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0971 - accuracy: 0.9627\n",
      "Epoch 63: val_loss improved from 0.14477 to 0.14408, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0940 - accuracy: 0.9649 - val_loss: 0.1441 - val_accuracy: 0.9524\n",
      "Epoch 64/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0886 - accuracy: 0.9660\n",
      "Epoch 64: val_loss improved from 0.14408 to 0.14335, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0926 - accuracy: 0.9649 - val_loss: 0.1433 - val_accuracy: 0.9524\n",
      "Epoch 65/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9649\n",
      "Epoch 65: val_loss improved from 0.14335 to 0.14283, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0914 - accuracy: 0.9649 - val_loss: 0.1428 - val_accuracy: 0.9524\n",
      "Epoch 66/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0956 - accuracy: 0.9655\n",
      "Epoch 66: val_loss improved from 0.14283 to 0.14224, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.9681 - val_loss: 0.1422 - val_accuracy: 0.9524\n",
      "Epoch 67/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0914 - accuracy: 0.9667\n",
      "Epoch 67: val_loss improved from 0.14224 to 0.14184, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0893 - accuracy: 0.9681 - val_loss: 0.1418 - val_accuracy: 0.9524\n",
      "Epoch 68/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0906 - accuracy: 0.9679\n",
      "Epoch 68: val_loss improved from 0.14184 to 0.14129, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0881 - accuracy: 0.9681 - val_loss: 0.1413 - val_accuracy: 0.9524\n",
      "Epoch 69/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.0984 - accuracy: 0.9617\n",
      "Epoch 69: val_loss improved from 0.14129 to 0.14094, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0868 - accuracy: 0.9681 - val_loss: 0.1409 - val_accuracy: 0.9524\n",
      "Epoch 70/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0820 - accuracy: 0.9704\n",
      "Epoch 70: val_loss improved from 0.14094 to 0.14066, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0859 - accuracy: 0.9681 - val_loss: 0.1407 - val_accuracy: 0.9524\n",
      "Epoch 71/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0739 - accuracy: 0.9754\n",
      "Epoch 71: val_loss improved from 0.14066 to 0.13999, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0848 - accuracy: 0.9681 - val_loss: 0.1400 - val_accuracy: 0.9524\n",
      "Epoch 72/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0861 - accuracy: 0.9667\n",
      "Epoch 72: val_loss improved from 0.13999 to 0.13964, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9681 - val_loss: 0.1396 - val_accuracy: 0.9524\n",
      "Epoch 73/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0885 - accuracy: 0.9649\n",
      "Epoch 73: val_loss improved from 0.13964 to 0.13916, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0826 - accuracy: 0.9681 - val_loss: 0.1392 - val_accuracy: 0.9524\n",
      "Epoch 74/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9681\n",
      "Epoch 74: val_loss improved from 0.13916 to 0.13862, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9681 - val_loss: 0.1386 - val_accuracy: 0.9524\n",
      "Epoch 75/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0757 - accuracy: 0.9686\n",
      "Epoch 75: val_loss improved from 0.13862 to 0.13815, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0806 - accuracy: 0.9681 - val_loss: 0.1381 - val_accuracy: 0.9524\n",
      "Epoch 76/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0807 - accuracy: 0.9725\n",
      "Epoch 76: val_loss improved from 0.13815 to 0.13771, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0795 - accuracy: 0.9681 - val_loss: 0.1377 - val_accuracy: 0.9524\n",
      "Epoch 77/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0832 - accuracy: 0.9643\n",
      "Epoch 77: val_loss improved from 0.13771 to 0.13729, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0785 - accuracy: 0.9681 - val_loss: 0.1373 - val_accuracy: 0.9524\n",
      "Epoch 78/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0829 - accuracy: 0.9643\n",
      "Epoch 78: val_loss improved from 0.13729 to 0.13690, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0778 - accuracy: 0.9681 - val_loss: 0.1369 - val_accuracy: 0.9524\n",
      "Epoch 79/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0750 - accuracy: 0.9733\n",
      "Epoch 79: val_loss improved from 0.13690 to 0.13668, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0771 - accuracy: 0.9681 - val_loss: 0.1367 - val_accuracy: 0.9524\n",
      "Epoch 80/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0616 - accuracy: 0.9846\n",
      "Epoch 80: val_loss improved from 0.13668 to 0.13626, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0761 - accuracy: 0.9712 - val_loss: 0.1363 - val_accuracy: 0.9524\n",
      "Epoch 81/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9710\n",
      "Epoch 81: val_loss improved from 0.13626 to 0.13582, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0750 - accuracy: 0.9712 - val_loss: 0.1358 - val_accuracy: 0.9524\n",
      "Epoch 82/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0721 - accuracy: 0.9729\n",
      "Epoch 82: val_loss improved from 0.13582 to 0.13548, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0741 - accuracy: 0.9712 - val_loss: 0.1355 - val_accuracy: 0.9524\n",
      "Epoch 83/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0683 - accuracy: 0.9733\n",
      "Epoch 83: val_loss improved from 0.13548 to 0.13527, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0733 - accuracy: 0.9712 - val_loss: 0.1353 - val_accuracy: 0.9524\n",
      "Epoch 84/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9712\n",
      "Epoch 84: val_loss improved from 0.13527 to 0.13502, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.0723 - accuracy: 0.9712 - val_loss: 0.1350 - val_accuracy: 0.9524\n",
      "Epoch 85/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0624 - accuracy: 0.9704\n",
      "Epoch 85: val_loss improved from 0.13502 to 0.13461, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0716 - accuracy: 0.9712 - val_loss: 0.1346 - val_accuracy: 0.9524\n",
      "Epoch 86/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0713 - accuracy: 0.9719\n",
      "Epoch 86: val_loss improved from 0.13461 to 0.13425, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0706 - accuracy: 0.9712 - val_loss: 0.1342 - val_accuracy: 0.9524\n",
      "Epoch 87/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0701 - accuracy: 0.9742\n",
      "Epoch 87: val_loss improved from 0.13425 to 0.13398, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.0697 - accuracy: 0.9744 - val_loss: 0.1340 - val_accuracy: 0.9524\n",
      "Epoch 88/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0701 - accuracy: 0.9725\n",
      "Epoch 88: val_loss improved from 0.13398 to 0.13372, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0690 - accuracy: 0.9744 - val_loss: 0.1337 - val_accuracy: 0.9524\n",
      "Epoch 89/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0623 - accuracy: 0.9731\n",
      "Epoch 89: val_loss improved from 0.13372 to 0.13336, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0682 - accuracy: 0.9744 - val_loss: 0.1334 - val_accuracy: 0.9524\n",
      "Epoch 90/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0678 - accuracy: 0.9774\n",
      "Epoch 90: val_loss improved from 0.13336 to 0.13319, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9776 - val_loss: 0.1332 - val_accuracy: 0.9524\n",
      "Epoch 91/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0685 - accuracy: 0.9833\n",
      "Epoch 91: val_loss improved from 0.13319 to 0.13284, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9776 - val_loss: 0.1328 - val_accuracy: 0.9524\n",
      "Epoch 92/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9806\n",
      "Epoch 92: val_loss improved from 0.13284 to 0.13259, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0657 - accuracy: 0.9808 - val_loss: 0.1326 - val_accuracy: 0.9524\n",
      "Epoch 93/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0655 - accuracy: 0.9828\n",
      "Epoch 93: val_loss improved from 0.13259 to 0.13233, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0651 - accuracy: 0.9808 - val_loss: 0.1323 - val_accuracy: 0.9524\n",
      "Epoch 94/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0665 - accuracy: 0.9800\n",
      "Epoch 94: val_loss improved from 0.13233 to 0.13208, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0643 - accuracy: 0.9808 - val_loss: 0.1321 - val_accuracy: 0.9524\n",
      "Epoch 95/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0692 - accuracy: 0.9796\n",
      "Epoch 95: val_loss improved from 0.13208 to 0.13176, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0635 - accuracy: 0.9808 - val_loss: 0.1318 - val_accuracy: 0.9583\n",
      "Epoch 96/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9808\n",
      "Epoch 96: val_loss improved from 0.13176 to 0.13146, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9808 - val_loss: 0.1315 - val_accuracy: 0.9583\n",
      "Epoch 97/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0480 - accuracy: 0.9885\n",
      "Epoch 97: val_loss improved from 0.13146 to 0.13118, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0625 - accuracy: 0.9808 - val_loss: 0.1312 - val_accuracy: 0.9583\n",
      "Epoch 98/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0695 - accuracy: 0.9769\n",
      "Epoch 98: val_loss improved from 0.13118 to 0.13096, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.9808 - val_loss: 0.1310 - val_accuracy: 0.9583\n",
      "Epoch 99/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0664 - accuracy: 0.9808\n",
      "Epoch 99: val_loss improved from 0.13096 to 0.13076, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0609 - accuracy: 0.9808 - val_loss: 0.1308 - val_accuracy: 0.9583\n",
      "Epoch 100/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0635 - accuracy: 0.9808\n",
      "Epoch 100: val_loss improved from 0.13076 to 0.13060, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0601 - accuracy: 0.9808 - val_loss: 0.1306 - val_accuracy: 0.9583\n",
      "Epoch 101/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0641 - accuracy: 0.9804\n",
      "Epoch 101: val_loss improved from 0.13060 to 0.13042, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0594 - accuracy: 0.9840 - val_loss: 0.1304 - val_accuracy: 0.9583\n",
      "Epoch 102/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0615 - accuracy: 0.9808\n",
      "Epoch 102: val_loss improved from 0.13042 to 0.13020, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9808 - val_loss: 0.1302 - val_accuracy: 0.9583\n",
      "Epoch 103/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9871\n",
      "Epoch 103: val_loss improved from 0.13020 to 0.12996, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9872 - val_loss: 0.1300 - val_accuracy: 0.9583\n",
      "Epoch 104/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0579 - accuracy: 0.9882\n",
      "Epoch 104: val_loss did not improve from 0.12996\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0577 - accuracy: 0.9872 - val_loss: 0.1300 - val_accuracy: 0.9583\n",
      "Epoch 105/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0621 - accuracy: 0.9849\n",
      "Epoch 105: val_loss improved from 0.12996 to 0.12969, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9872 - val_loss: 0.1297 - val_accuracy: 0.9583\n",
      "Epoch 106/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0559 - accuracy: 0.9920\n",
      "Epoch 106: val_loss improved from 0.12969 to 0.12948, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0565 - accuracy: 0.9872 - val_loss: 0.1295 - val_accuracy: 0.9583\n",
      "Epoch 107/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0580 - accuracy: 0.9862\n",
      "Epoch 107: val_loss improved from 0.12948 to 0.12938, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0558 - accuracy: 0.9872 - val_loss: 0.1294 - val_accuracy: 0.9583\n",
      "Epoch 108/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0629 - accuracy: 0.9843\n",
      "Epoch 108: val_loss improved from 0.12938 to 0.12930, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0552 - accuracy: 0.9872 - val_loss: 0.1293 - val_accuracy: 0.9583\n",
      "Epoch 109/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0535 - accuracy: 0.9889\n",
      "Epoch 109: val_loss improved from 0.12930 to 0.12912, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0548 - accuracy: 0.9872 - val_loss: 0.1291 - val_accuracy: 0.9583\n",
      "Epoch 110/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0621 - accuracy: 0.9837\n",
      "Epoch 110: val_loss improved from 0.12912 to 0.12891, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0539 - accuracy: 0.9872 - val_loss: 0.1289 - val_accuracy: 0.9583\n",
      "Epoch 111/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0549 - accuracy: 0.9862\n",
      "Epoch 111: val_loss improved from 0.12891 to 0.12879, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0535 - accuracy: 0.9872 - val_loss: 0.1288 - val_accuracy: 0.9583\n",
      "Epoch 112/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0603 - accuracy: 0.9837\n",
      "Epoch 112: val_loss improved from 0.12879 to 0.12867, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0532 - accuracy: 0.9872 - val_loss: 0.1287 - val_accuracy: 0.9583\n",
      "Epoch 113/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0415 - accuracy: 0.9893\n",
      "Epoch 113: val_loss improved from 0.12867 to 0.12857, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 1s 9ms/step - loss: 0.0530 - accuracy: 0.9872 - val_loss: 0.1286 - val_accuracy: 0.9583\n",
      "Epoch 114/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9872\n",
      "Epoch 114: val_loss improved from 0.12857 to 0.12851, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0521 - accuracy: 0.9872 - val_loss: 0.1285 - val_accuracy: 0.9583\n",
      "Epoch 115/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9869\n",
      "Epoch 115: val_loss improved from 0.12851 to 0.12847, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0515 - accuracy: 0.9872 - val_loss: 0.1285 - val_accuracy: 0.9583\n",
      "Epoch 116/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0494 - accuracy: 0.9887\n",
      "Epoch 116: val_loss improved from 0.12847 to 0.12844, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 0.9872 - val_loss: 0.1284 - val_accuracy: 0.9583\n",
      "Epoch 117/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0534 - accuracy: 0.9860\n",
      "Epoch 117: val_loss improved from 0.12844 to 0.12825, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.9872 - val_loss: 0.1283 - val_accuracy: 0.9583\n",
      "Epoch 118/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0505 - accuracy: 0.9897\n",
      "Epoch 118: val_loss improved from 0.12825 to 0.12820, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0498 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 119/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0536 - accuracy: 0.9885\n",
      "Epoch 119: val_loss improved from 0.12820 to 0.12813, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0493 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 120/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0524 - accuracy: 0.9880\n",
      "Epoch 120: val_loss did not improve from 0.12813\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0489 - accuracy: 0.9904 - val_loss: 0.1283 - val_accuracy: 0.9583\n",
      "Epoch 121/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0512 - accuracy: 0.9893\n",
      "Epoch 121: val_loss did not improve from 0.12813\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 122/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0521 - accuracy: 0.9893\n",
      "Epoch 122: val_loss did not improve from 0.12813\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0478 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 123/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0493 - accuracy: 0.9898\n",
      "Epoch 123: val_loss improved from 0.12813 to 0.12808, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0474 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 124/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0509 - accuracy: 0.9895\n",
      "Epoch 124: val_loss improved from 0.12808 to 0.12803, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0473 - accuracy: 0.9904 - val_loss: 0.1280 - val_accuracy: 0.9583\n",
      "Epoch 125/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0393 - accuracy: 0.9964\n",
      "Epoch 125: val_loss improved from 0.12803 to 0.12784, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0468 - accuracy: 0.9904 - val_loss: 0.1278 - val_accuracy: 0.9583\n",
      "Epoch 126/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0483 - accuracy: 0.9891\n",
      "Epoch 126: val_loss did not improve from 0.12784\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0464 - accuracy: 0.9904 - val_loss: 0.1279 - val_accuracy: 0.9583\n",
      "Epoch 127/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0463 - accuracy: 0.9897\n",
      "Epoch 127: val_loss did not improve from 0.12784\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 0.9904 - val_loss: 0.1279 - val_accuracy: 0.9583\n",
      "Epoch 128/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0458 - accuracy: 0.9903\n",
      "Epoch 128: val_loss did not improve from 0.12784\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0454 - accuracy: 0.9904 - val_loss: 0.1279 - val_accuracy: 0.9583\n",
      "Epoch 129/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9904\n",
      "Epoch 129: val_loss improved from 0.12784 to 0.12783, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0452 - accuracy: 0.9904 - val_loss: 0.1278 - val_accuracy: 0.9583\n",
      "Epoch 130/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0485 - accuracy: 0.9882\n",
      "Epoch 130: val_loss improved from 0.12783 to 0.12782, saving model to model_checkpoint\\GRU_1 layer_RMSprop.h5\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0446 - accuracy: 0.9904 - val_loss: 0.1278 - val_accuracy: 0.9583\n",
      "Epoch 131/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0458 - accuracy: 0.9900\n",
      "Epoch 131: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0448 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 132/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0464 - accuracy: 0.9918\n",
      "Epoch 132: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 133/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0426 - accuracy: 0.9927\n",
      "Epoch 133: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0438 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 134/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0501 - accuracy: 0.9880\n",
      "Epoch 134: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9904 - val_loss: 0.1280 - val_accuracy: 0.9583\n",
      "Epoch 135/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0254 - accuracy: 0.9965\n",
      "Epoch 135: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0430 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 136/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 136: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0427 - accuracy: 0.9904 - val_loss: 0.1280 - val_accuracy: 0.9583\n",
      "Epoch 137/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0451 - accuracy: 0.9895\n",
      "Epoch 137: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 0.9904 - val_loss: 0.1281 - val_accuracy: 0.9583\n",
      "Epoch 138/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0450 - accuracy: 0.9893\n",
      "Epoch 138: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0415 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 139/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0409 - accuracy: 0.9900\n",
      "Epoch 139: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0413 - accuracy: 0.9904 - val_loss: 0.1283 - val_accuracy: 0.9583\n",
      "Epoch 140/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0435 - accuracy: 0.9895\n",
      "Epoch 140: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0408 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 141/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0406 - accuracy: 0.9903\n",
      "Epoch 141: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0403 - accuracy: 0.9904 - val_loss: 0.1282 - val_accuracy: 0.9583\n",
      "Epoch 142/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0373 - accuracy: 0.9923\n",
      "Epoch 142: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0402 - accuracy: 0.9904 - val_loss: 0.1284 - val_accuracy: 0.9583\n",
      "Epoch 143/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0384 - accuracy: 0.9929\n",
      "Epoch 143: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.9904 - val_loss: 0.1286 - val_accuracy: 0.9583\n",
      "Epoch 144/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0434 - accuracy: 0.9889\n",
      "Epoch 144: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.9904 - val_loss: 0.1286 - val_accuracy: 0.9583\n",
      "Epoch 145/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0423 - accuracy: 0.9887\n",
      "Epoch 145: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0393 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9583\n",
      "Epoch 146/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0432 - accuracy: 0.9887\n",
      "Epoch 146: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9583\n",
      "Epoch 147/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0383 - accuracy: 0.9923\n",
      "Epoch 147: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0384 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9583\n",
      "Epoch 148/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0375 - accuracy: 0.9903\n",
      "Epoch 148: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0382 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9583\n",
      "Epoch 149/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0263 - accuracy: 0.9922\n",
      "Epoch 149: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9583\n",
      "Epoch 150/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0409 - accuracy: 0.9885\n",
      "Epoch 150: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9583\n",
      "Epoch 151/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0425 - accuracy: 0.9882\n",
      "Epoch 151: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0370 - accuracy: 0.9904 - val_loss: 0.1289 - val_accuracy: 0.9643\n",
      "Epoch 152/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0426 - accuracy: 0.9878\n",
      "Epoch 152: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0366 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9643\n",
      "Epoch 153/200\n",
      "61/63 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9902\n",
      "Epoch 153: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9904 - val_loss: 0.1288 - val_accuracy: 0.9643\n",
      "Epoch 154/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0377 - accuracy: 0.9897\n",
      "Epoch 154: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0360 - accuracy: 0.9904 - val_loss: 0.1287 - val_accuracy: 0.9643\n",
      "Epoch 155/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0221 - accuracy: 0.9925\n",
      "Epoch 155: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9904 - val_loss: 0.1292 - val_accuracy: 0.9643\n",
      "Epoch 156/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0401 - accuracy: 0.9889\n",
      "Epoch 156: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 157/200\n",
      "57/63 [==========================>...] - ETA: 0s - loss: 0.0319 - accuracy: 0.9930\n",
      "Epoch 157: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 158/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0370 - accuracy: 0.9889\n",
      "Epoch 158: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0345 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 159/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0305 - accuracy: 0.9960\n",
      "Epoch 159: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9904 - val_loss: 0.1290 - val_accuracy: 0.9643\n",
      "Epoch 160/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0375 - accuracy: 0.9882\n",
      "Epoch 160: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0341 - accuracy: 0.9904 - val_loss: 0.1291 - val_accuracy: 0.9643\n",
      "Epoch 161/200\n",
      "48/63 [=====================>........] - ETA: 0s - loss: 0.0216 - accuracy: 0.9917\n",
      "Epoch 161: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0338 - accuracy: 0.9904 - val_loss: 0.1293 - val_accuracy: 0.9643\n",
      "Epoch 162/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0340 - accuracy: 0.9922\n",
      "Epoch 162: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.9904 - val_loss: 0.1293 - val_accuracy: 0.9643\n",
      "Epoch 163/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0333 - accuracy: 0.9903\n",
      "Epoch 163: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0330 - accuracy: 0.9904 - val_loss: 0.1292 - val_accuracy: 0.9643\n",
      "Epoch 164/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0324 - accuracy: 0.9926\n",
      "Epoch 164: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.9904 - val_loss: 0.1293 - val_accuracy: 0.9643\n",
      "Epoch 165/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0204 - accuracy: 0.9929\n",
      "Epoch 165: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0329 - accuracy: 0.9904 - val_loss: 0.1296 - val_accuracy: 0.9643\n",
      "Epoch 166/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0300 - accuracy: 0.9959\n",
      "Epoch 166: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0325 - accuracy: 0.9904 - val_loss: 0.1294 - val_accuracy: 0.9643\n",
      "Epoch 167/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9904\n",
      "Epoch 167: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0320 - accuracy: 0.9904 - val_loss: 0.1295 - val_accuracy: 0.9643\n",
      "Epoch 168/200\n",
      "60/63 [===========================>..] - ETA: 0s - loss: 0.0321 - accuracy: 0.9900\n",
      "Epoch 168: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0317 - accuracy: 0.9904 - val_loss: 0.1296 - val_accuracy: 0.9643\n",
      "Epoch 169/200\n",
      "59/63 [===========================>..] - ETA: 0s - loss: 0.0177 - accuracy: 0.9932\n",
      "Epoch 169: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 0.9904 - val_loss: 0.1299 - val_accuracy: 0.9643\n",
      "Epoch 170/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0341 - accuracy: 0.9891\n",
      "Epoch 170: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.1299 - val_accuracy: 0.9643\n",
      "Epoch 171/200\n",
      "47/63 [=====================>........] - ETA: 0s - loss: 0.0345 - accuracy: 0.9915\n",
      "Epoch 171: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0308 - accuracy: 0.9904 - val_loss: 0.1300 - val_accuracy: 0.9643\n",
      "Epoch 172/200\n",
      "58/63 [==========================>...] - ETA: 0s - loss: 0.0322 - accuracy: 0.9897\n",
      "Epoch 172: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0307 - accuracy: 0.9904 - val_loss: 0.1301 - val_accuracy: 0.9643\n",
      "Epoch 173/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0152 - accuracy: 0.9962\n",
      "Epoch 173: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.1307 - val_accuracy: 0.9643\n",
      "Epoch 174/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0316 - accuracy: 0.9922\n",
      "Epoch 174: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.9904 - val_loss: 0.1306 - val_accuracy: 0.9643\n",
      "Epoch 175/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0146 - accuracy: 0.9963\n",
      "Epoch 175: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9904 - val_loss: 0.1313 - val_accuracy: 0.9643\n",
      "Epoch 176/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0322 - accuracy: 0.9889\n",
      "Epoch 176: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.1311 - val_accuracy: 0.9643\n",
      "Epoch 177/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0316 - accuracy: 0.9889\n",
      "Epoch 177: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.9904 - val_loss: 0.1312 - val_accuracy: 0.9643\n",
      "Epoch 178/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0323 - accuracy: 0.9885\n",
      "Epoch 178: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.9904 - val_loss: 0.1313 - val_accuracy: 0.9643\n",
      "Epoch 179/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0309 - accuracy: 0.9920\n",
      "Epoch 179: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 0.1314 - val_accuracy: 0.9643\n",
      "Epoch 180/200\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0322 - accuracy: 0.9889\n",
      "Epoch 180: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 0.1314 - val_accuracy: 0.9643\n",
      "Epoch 181/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0321 - accuracy: 0.9920\n",
      "Epoch 181: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9936 - val_loss: 0.1314 - val_accuracy: 0.9643\n",
      "Epoch 182/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0311 - accuracy: 0.9918\n",
      "Epoch 182: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9936 - val_loss: 0.1318 - val_accuracy: 0.9643\n",
      "Epoch 183/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0160 - accuracy: 0.9922\n",
      "Epoch 183: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0282 - accuracy: 0.9904 - val_loss: 0.1319 - val_accuracy: 0.9643\n",
      "Epoch 184/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0314 - accuracy: 0.9923\n",
      "Epoch 184: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 0.9936 - val_loss: 0.1319 - val_accuracy: 0.9643\n",
      "Epoch 185/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0310 - accuracy: 0.9925\n",
      "Epoch 185: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.0279 - accuracy: 0.9936 - val_loss: 0.1325 - val_accuracy: 0.9643\n",
      "Epoch 186/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0161 - accuracy: 0.9959\n",
      "Epoch 186: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.9936 - val_loss: 0.1329 - val_accuracy: 0.9643\n",
      "Epoch 187/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0298 - accuracy: 0.9918\n",
      "Epoch 187: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0272 - accuracy: 0.9936 - val_loss: 0.1328 - val_accuracy: 0.9643\n",
      "Epoch 188/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0316 - accuracy: 0.9918\n",
      "Epoch 188: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9936 - val_loss: 0.1328 - val_accuracy: 0.9643\n",
      "Epoch 189/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0298 - accuracy: 0.9922\n",
      "Epoch 189: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.9936 - val_loss: 0.1330 - val_accuracy: 0.9643\n",
      "Epoch 190/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0145 - accuracy: 0.9961\n",
      "Epoch 190: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0266 - accuracy: 0.9936 - val_loss: 0.1333 - val_accuracy: 0.9643\n",
      "Epoch 191/200\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9936\n",
      "Epoch 191: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.9936 - val_loss: 0.1331 - val_accuracy: 0.9643\n",
      "Epoch 192/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0298 - accuracy: 0.9925\n",
      "Epoch 192: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.9936 - val_loss: 0.1331 - val_accuracy: 0.9583\n",
      "Epoch 193/200\n",
      "55/63 [=========================>....] - ETA: 0s - loss: 0.0277 - accuracy: 0.9927\n",
      "Epoch 193: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.9936 - val_loss: 0.1335 - val_accuracy: 0.9583\n",
      "Epoch 194/200\n",
      "51/63 [=======================>......] - ETA: 0s - loss: 0.0293 - accuracy: 0.9922\n",
      "Epoch 194: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 0.1336 - val_accuracy: 0.9583\n",
      "Epoch 195/200\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9968\n",
      "Epoch 195: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 0.1338 - val_accuracy: 0.9583\n",
      "Epoch 196/200\n",
      "50/63 [======================>.......] - ETA: 0s - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 196: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.9936 - val_loss: 0.1342 - val_accuracy: 0.9583\n",
      "Epoch 197/200\n",
      "52/63 [=======================>......] - ETA: 0s - loss: 0.0291 - accuracy: 0.9923\n",
      "Epoch 197: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 0.9936 - val_loss: 0.1341 - val_accuracy: 0.9583\n",
      "Epoch 198/200\n",
      "49/63 [======================>.......] - ETA: 0s - loss: 0.0255 - accuracy: 0.9959\n",
      "Epoch 198: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0251 - accuracy: 0.9936 - val_loss: 0.1343 - val_accuracy: 0.9583\n",
      "Epoch 199/200\n",
      "56/63 [=========================>....] - ETA: 0s - loss: 0.0226 - accuracy: 0.9964\n",
      "Epoch 199: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 0.9936 - val_loss: 0.1344 - val_accuracy: 0.9583\n",
      "Epoch 200/200\n",
      "53/63 [========================>.....] - ETA: 0s - loss: 0.0285 - accuracy: 0.9925\n",
      "Epoch 200: val_loss did not improve from 0.12782\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.9936 - val_loss: 0.1348 - val_accuracy: 0.9583\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_3 (GRU)                 (None, 32)                11520     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.1348 - accuracy: 0.9583\n",
      "Test Loss: 0.13477586209774017\n",
      "Test Accuracy: 0.9583333134651184\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer_RMSprop.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1])))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with RMSprop optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001) \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 layer with dropout Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.7908 - accuracy: 0.4619\n",
      "Epoch 1: val_loss improved from inf to 0.71700, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 3s 10ms/step - loss: 0.7902 - accuracy: 0.4576 - val_loss: 0.7170 - val_accuracy: 0.5833\n",
      "Epoch 2/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.7246 - accuracy: 0.5684\n",
      "Epoch 2: val_loss improved from 0.71700 to 0.65933, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.7189 - accuracy: 0.5670 - val_loss: 0.6593 - val_accuracy: 0.6726\n",
      "Epoch 3/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.6653 - accuracy: 0.6629\n",
      "Epoch 3: val_loss improved from 0.65933 to 0.60728, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.6636 - accuracy: 0.6629 - val_loss: 0.6073 - val_accuracy: 0.7321\n",
      "Epoch 4/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.6276 - accuracy: 0.6634\n",
      "Epoch 4: val_loss improved from 0.60728 to 0.56336, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.6231 - accuracy: 0.6674 - val_loss: 0.5634 - val_accuracy: 0.7440\n",
      "Epoch 5/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.5696 - accuracy: 0.7381\n",
      "Epoch 5: val_loss improved from 0.56336 to 0.52351, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.5775 - accuracy: 0.7254 - val_loss: 0.5235 - val_accuracy: 0.7857\n",
      "Epoch 6/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.5390 - accuracy: 0.7581\n",
      "Epoch 6: val_loss improved from 0.52351 to 0.48782, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.5413 - accuracy: 0.7589 - val_loss: 0.4878 - val_accuracy: 0.8095\n",
      "Epoch 7/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.4992 - accuracy: 0.8000\n",
      "Epoch 7: val_loss improved from 0.48782 to 0.45595, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.5004 - accuracy: 0.7991 - val_loss: 0.4559 - val_accuracy: 0.8571\n",
      "Epoch 8/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.4731 - accuracy: 0.8182\n",
      "Epoch 8: val_loss improved from 0.45595 to 0.42768, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4730 - accuracy: 0.8170 - val_loss: 0.4277 - val_accuracy: 0.8690\n",
      "Epoch 9/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.4522 - accuracy: 0.8322\n",
      "Epoch 9: val_loss improved from 0.42768 to 0.40122, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4536 - accuracy: 0.8304 - val_loss: 0.4012 - val_accuracy: 0.8869\n",
      "Epoch 10/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.4241 - accuracy: 0.8588\n",
      "Epoch 10: val_loss improved from 0.40122 to 0.37960, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4251 - accuracy: 0.8594 - val_loss: 0.3796 - val_accuracy: 0.8929\n",
      "Epoch 11/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.8504\n",
      "Epoch 11: val_loss improved from 0.37960 to 0.35871, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4032 - accuracy: 0.8504 - val_loss: 0.3587 - val_accuracy: 0.8929\n",
      "Epoch 12/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.3839 - accuracy: 0.8750\n",
      "Epoch 12: val_loss improved from 0.35871 to 0.33940, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.3839 - accuracy: 0.8750 - val_loss: 0.3394 - val_accuracy: 0.9048\n",
      "Epoch 13/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.3634 - accuracy: 0.8810\n",
      "Epoch 13: val_loss improved from 0.33940 to 0.32155, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.3625 - accuracy: 0.8817 - val_loss: 0.3216 - val_accuracy: 0.9107\n",
      "Epoch 14/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.3663 - accuracy: 0.8690\n",
      "Epoch 14: val_loss improved from 0.32155 to 0.30524, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3636 - accuracy: 0.8728 - val_loss: 0.3052 - val_accuracy: 0.9167\n",
      "Epoch 15/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.3303 - accuracy: 0.8905\n",
      "Epoch 15: val_loss improved from 0.30524 to 0.29093, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.3293 - accuracy: 0.8884 - val_loss: 0.2909 - val_accuracy: 0.9286\n",
      "Epoch 16/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.3172 - accuracy: 0.8930\n",
      "Epoch 16: val_loss improved from 0.29093 to 0.27715, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.3132 - accuracy: 0.8951 - val_loss: 0.2771 - val_accuracy: 0.9286\n",
      "Epoch 17/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.9149\n",
      "Epoch 17: val_loss improved from 0.27715 to 0.26485, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.3052 - accuracy: 0.9152 - val_loss: 0.2649 - val_accuracy: 0.9345\n",
      "Epoch 18/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.9227\n",
      "Epoch 18: val_loss improved from 0.26485 to 0.25360, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2874 - accuracy: 0.9174 - val_loss: 0.2536 - val_accuracy: 0.9345\n",
      "Epoch 19/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2796 - accuracy: 0.9174\n",
      "Epoch 19: val_loss improved from 0.25360 to 0.24315, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2796 - accuracy: 0.9174 - val_loss: 0.2432 - val_accuracy: 0.9405\n",
      "Epoch 20/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.2818 - accuracy: 0.9224\n",
      "Epoch 20: val_loss improved from 0.24315 to 0.23320, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2783 - accuracy: 0.9241 - val_loss: 0.2332 - val_accuracy: 0.9405\n",
      "Epoch 21/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2673 - accuracy: 0.9301\n",
      "Epoch 21: val_loss improved from 0.23320 to 0.22451, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2738 - accuracy: 0.9219 - val_loss: 0.2245 - val_accuracy: 0.9405\n",
      "Epoch 22/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2591 - accuracy: 0.9205\n",
      "Epoch 22: val_loss improved from 0.22451 to 0.21614, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2586 - accuracy: 0.9219 - val_loss: 0.2161 - val_accuracy: 0.9464\n",
      "Epoch 23/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.2485 - accuracy: 0.9143\n",
      "Epoch 23: val_loss improved from 0.21614 to 0.20836, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2454 - accuracy: 0.9174 - val_loss: 0.2084 - val_accuracy: 0.9524\n",
      "Epoch 24/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.2537 - accuracy: 0.9287\n",
      "Epoch 24: val_loss improved from 0.20836 to 0.20097, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2506 - accuracy: 0.9308 - val_loss: 0.2010 - val_accuracy: 0.9524\n",
      "Epoch 25/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.2444 - accuracy: 0.9317\n",
      "Epoch 25: val_loss improved from 0.20097 to 0.19432, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2442 - accuracy: 0.9330 - val_loss: 0.1943 - val_accuracy: 0.9583\n",
      "Epoch 26/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.2260 - accuracy: 0.9412\n",
      "Epoch 26: val_loss improved from 0.19432 to 0.18787, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2242 - accuracy: 0.9397 - val_loss: 0.1879 - val_accuracy: 0.9583\n",
      "Epoch 27/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.2165 - accuracy: 0.9403\n",
      "Epoch 27: val_loss improved from 0.18787 to 0.18182, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2149 - accuracy: 0.9420 - val_loss: 0.1818 - val_accuracy: 0.9583\n",
      "Epoch 28/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.2256 - accuracy: 0.9377\n",
      "Epoch 28: val_loss improved from 0.18182 to 0.17646, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2193 - accuracy: 0.9420 - val_loss: 0.1765 - val_accuracy: 0.9583\n",
      "Epoch 29/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.2191 - accuracy: 0.9302\n",
      "Epoch 29: val_loss improved from 0.17646 to 0.17098, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2158 - accuracy: 0.9330 - val_loss: 0.1710 - val_accuracy: 0.9583\n",
      "Epoch 30/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.2068 - accuracy: 0.9483\n",
      "Epoch 30: val_loss improved from 0.17098 to 0.16597, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2065 - accuracy: 0.9487 - val_loss: 0.1660 - val_accuracy: 0.9583\n",
      "Epoch 31/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9442\n",
      "Epoch 31: val_loss improved from 0.16597 to 0.16087, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2041 - accuracy: 0.9442 - val_loss: 0.1609 - val_accuracy: 0.9643\n",
      "Epoch 32/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.1976 - accuracy: 0.9506\n",
      "Epoch 32: val_loss improved from 0.16087 to 0.15669, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1931 - accuracy: 0.9509 - val_loss: 0.1567 - val_accuracy: 0.9643\n",
      "Epoch 33/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.1857 - accuracy: 0.9519\n",
      "Epoch 33: val_loss improved from 0.15669 to 0.15269, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1851 - accuracy: 0.9487 - val_loss: 0.1527 - val_accuracy: 0.9643\n",
      "Epoch 34/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.2004 - accuracy: 0.9462\n",
      "Epoch 34: val_loss improved from 0.15269 to 0.14838, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1896 - accuracy: 0.9487 - val_loss: 0.1484 - val_accuracy: 0.9643\n",
      "Epoch 35/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.1809 - accuracy: 0.9481\n",
      "Epoch 35: val_loss improved from 0.14838 to 0.14458, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1837 - accuracy: 0.9487 - val_loss: 0.1446 - val_accuracy: 0.9643\n",
      "Epoch 36/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.1879 - accuracy: 0.9418\n",
      "Epoch 36: val_loss improved from 0.14458 to 0.14077, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1845 - accuracy: 0.9464 - val_loss: 0.1408 - val_accuracy: 0.9643\n",
      "Epoch 37/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.1820 - accuracy: 0.9532\n",
      "Epoch 37: val_loss improved from 0.14077 to 0.13740, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1794 - accuracy: 0.9554 - val_loss: 0.1374 - val_accuracy: 0.9643\n",
      "Epoch 38/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1645 - accuracy: 0.9590\n",
      "Epoch 38: val_loss improved from 0.13740 to 0.13385, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9554 - val_loss: 0.1338 - val_accuracy: 0.9643\n",
      "Epoch 39/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.1716 - accuracy: 0.9544\n",
      "Epoch 39: val_loss improved from 0.13385 to 0.13074, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9554 - val_loss: 0.1307 - val_accuracy: 0.9643\n",
      "Epoch 40/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1603 - accuracy: 0.9513\n",
      "Epoch 40: val_loss improved from 0.13074 to 0.12775, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1609 - accuracy: 0.9531 - val_loss: 0.1278 - val_accuracy: 0.9702\n",
      "Epoch 41/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1487 - accuracy: 0.9615\n",
      "Epoch 41: val_loss improved from 0.12775 to 0.12469, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.9531 - val_loss: 0.1247 - val_accuracy: 0.9702\n",
      "Epoch 42/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1401 - accuracy: 0.9650\n",
      "Epoch 42: val_loss improved from 0.12469 to 0.12202, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9554 - val_loss: 0.1220 - val_accuracy: 0.9702\n",
      "Epoch 43/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1589 - accuracy: 0.9531\n",
      "Epoch 43: val_loss improved from 0.12202 to 0.11939, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1539 - accuracy: 0.9554 - val_loss: 0.1194 - val_accuracy: 0.9702\n",
      "Epoch 44/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.1403 - accuracy: 0.9632\n",
      "Epoch 44: val_loss improved from 0.11939 to 0.11687, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1519 - accuracy: 0.9598 - val_loss: 0.1169 - val_accuracy: 0.9702\n",
      "Epoch 45/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.1585 - accuracy: 0.9474\n",
      "Epoch 45: val_loss improved from 0.11687 to 0.11437, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1526 - accuracy: 0.9487 - val_loss: 0.1144 - val_accuracy: 0.9762\n",
      "Epoch 46/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1497 - accuracy: 0.9538\n",
      "Epoch 46: val_loss improved from 0.11437 to 0.11202, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1522 - accuracy: 0.9531 - val_loss: 0.1120 - val_accuracy: 0.9762\n",
      "Epoch 47/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1487 - accuracy: 0.9564\n",
      "Epoch 47: val_loss improved from 0.11202 to 0.10993, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1436 - accuracy: 0.9576 - val_loss: 0.1099 - val_accuracy: 0.9762\n",
      "Epoch 48/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1450 - accuracy: 0.9551\n",
      "Epoch 48: val_loss improved from 0.10993 to 0.10786, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1444 - accuracy: 0.9554 - val_loss: 0.1079 - val_accuracy: 0.9762\n",
      "Epoch 49/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.9591\n",
      "Epoch 49: val_loss improved from 0.10786 to 0.10554, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1416 - accuracy: 0.9598 - val_loss: 0.1055 - val_accuracy: 0.9762\n",
      "Epoch 50/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1368 - accuracy: 0.9571\n",
      "Epoch 50: val_loss improved from 0.10554 to 0.10356, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9576 - val_loss: 0.1036 - val_accuracy: 0.9762\n",
      "Epoch 51/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1341 - accuracy: 0.9581\n",
      "Epoch 51: val_loss improved from 0.10356 to 0.10167, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1309 - accuracy: 0.9598 - val_loss: 0.1017 - val_accuracy: 0.9762\n",
      "Epoch 52/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1410 - accuracy: 0.9531\n",
      "Epoch 52: val_loss improved from 0.10167 to 0.09989, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1330 - accuracy: 0.9576 - val_loss: 0.0999 - val_accuracy: 0.9762\n",
      "Epoch 53/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1333 - accuracy: 0.9585\n",
      "Epoch 53: val_loss improved from 0.09989 to 0.09793, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1285 - accuracy: 0.9598 - val_loss: 0.0979 - val_accuracy: 0.9762\n",
      "Epoch 54/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9573\n",
      "Epoch 54: val_loss improved from 0.09793 to 0.09623, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1295 - accuracy: 0.9576 - val_loss: 0.0962 - val_accuracy: 0.9762\n",
      "Epoch 55/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1278 - accuracy: 0.9600\n",
      "Epoch 55: val_loss improved from 0.09623 to 0.09454, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1266 - accuracy: 0.9598 - val_loss: 0.0945 - val_accuracy: 0.9762\n",
      "Epoch 56/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9614\n",
      "Epoch 56: val_loss improved from 0.09454 to 0.09300, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1202 - accuracy: 0.9621 - val_loss: 0.0930 - val_accuracy: 0.9762\n",
      "Epoch 57/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1233 - accuracy: 0.9639\n",
      "Epoch 57: val_loss improved from 0.09300 to 0.09147, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1260 - accuracy: 0.9643 - val_loss: 0.0915 - val_accuracy: 0.9762\n",
      "Epoch 58/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1214 - accuracy: 0.9630\n",
      "Epoch 58: val_loss improved from 0.09147 to 0.08998, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9598 - val_loss: 0.0900 - val_accuracy: 0.9762\n",
      "Epoch 59/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9651\n",
      "Epoch 59: val_loss improved from 0.08998 to 0.08854, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1178 - accuracy: 0.9621 - val_loss: 0.0885 - val_accuracy: 0.9762\n",
      "Epoch 60/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1135 - accuracy: 0.9595\n",
      "Epoch 60: val_loss improved from 0.08854 to 0.08708, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1176 - accuracy: 0.9576 - val_loss: 0.0871 - val_accuracy: 0.9762\n",
      "Epoch 61/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0990 - accuracy: 0.9683\n",
      "Epoch 61: val_loss improved from 0.08708 to 0.08572, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1151 - accuracy: 0.9643 - val_loss: 0.0857 - val_accuracy: 0.9762\n",
      "Epoch 62/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1149 - accuracy: 0.9655\n",
      "Epoch 62: val_loss improved from 0.08572 to 0.08454, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1123 - accuracy: 0.9665 - val_loss: 0.0845 - val_accuracy: 0.9762\n",
      "Epoch 63/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.1174 - accuracy: 0.9636\n",
      "Epoch 63: val_loss improved from 0.08454 to 0.08329, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1114 - accuracy: 0.9643 - val_loss: 0.0833 - val_accuracy: 0.9762\n",
      "Epoch 64/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1112 - accuracy: 0.9663\n",
      "Epoch 64: val_loss improved from 0.08329 to 0.08201, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1091 - accuracy: 0.9665 - val_loss: 0.0820 - val_accuracy: 0.9762\n",
      "Epoch 65/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1076 - accuracy: 0.9711\n",
      "Epoch 65: val_loss improved from 0.08201 to 0.08075, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1087 - accuracy: 0.9688 - val_loss: 0.0808 - val_accuracy: 0.9762\n",
      "Epoch 66/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1081 - accuracy: 0.9704\n",
      "Epoch 66: val_loss improved from 0.08075 to 0.07957, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1018 - accuracy: 0.9732 - val_loss: 0.0796 - val_accuracy: 0.9762\n",
      "Epoch 67/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1062 - accuracy: 0.9619\n",
      "Epoch 67: val_loss improved from 0.07957 to 0.07834, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9643 - val_loss: 0.0783 - val_accuracy: 0.9762\n",
      "Epoch 68/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9724\n",
      "Epoch 68: val_loss improved from 0.07834 to 0.07708, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1052 - accuracy: 0.9710 - val_loss: 0.0771 - val_accuracy: 0.9762\n",
      "Epoch 69/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9640\n",
      "Epoch 69: val_loss improved from 0.07708 to 0.07589, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.1035 - accuracy: 0.9643 - val_loss: 0.0759 - val_accuracy: 0.9762\n",
      "Epoch 70/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1030 - accuracy: 0.9632\n",
      "Epoch 70: val_loss improved from 0.07589 to 0.07492, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1007 - accuracy: 0.9643 - val_loss: 0.0749 - val_accuracy: 0.9762\n",
      "Epoch 71/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9724\n",
      "Epoch 71: val_loss improved from 0.07492 to 0.07396, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0970 - accuracy: 0.9732 - val_loss: 0.0740 - val_accuracy: 0.9762\n",
      "Epoch 72/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0952 - accuracy: 0.9701\n",
      "Epoch 72: val_loss improved from 0.07396 to 0.07292, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0953 - accuracy: 0.9688 - val_loss: 0.0729 - val_accuracy: 0.9762\n",
      "Epoch 73/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.1034 - accuracy: 0.9671\n",
      "Epoch 73: val_loss improved from 0.07292 to 0.07209, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0947 - accuracy: 0.9710 - val_loss: 0.0721 - val_accuracy: 0.9762\n",
      "Epoch 74/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1002 - accuracy: 0.9659\n",
      "Epoch 74: val_loss improved from 0.07209 to 0.07117, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0993 - accuracy: 0.9665 - val_loss: 0.0712 - val_accuracy: 0.9762\n",
      "Epoch 75/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0919 - accuracy: 0.9722\n",
      "Epoch 75: val_loss improved from 0.07117 to 0.07018, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0875 - accuracy: 0.9732 - val_loss: 0.0702 - val_accuracy: 0.9762\n",
      "Epoch 76/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1017 - accuracy: 0.9698\n",
      "Epoch 76: val_loss improved from 0.07018 to 0.06948, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0997 - accuracy: 0.9710 - val_loss: 0.0695 - val_accuracy: 0.9821\n",
      "Epoch 77/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0983 - accuracy: 0.9704\n",
      "Epoch 77: val_loss improved from 0.06948 to 0.06874, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0937 - accuracy: 0.9710 - val_loss: 0.0687 - val_accuracy: 0.9821\n",
      "Epoch 78/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0934 - accuracy: 0.9682\n",
      "Epoch 78: val_loss improved from 0.06874 to 0.06796, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0926 - accuracy: 0.9688 - val_loss: 0.0680 - val_accuracy: 0.9821\n",
      "Epoch 79/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0787 - accuracy: 0.9756\n",
      "Epoch 79: val_loss improved from 0.06796 to 0.06697, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0899 - accuracy: 0.9688 - val_loss: 0.0670 - val_accuracy: 0.9821\n",
      "Epoch 80/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0707 - accuracy: 0.9769\n",
      "Epoch 80: val_loss improved from 0.06697 to 0.06619, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0881 - accuracy: 0.9688 - val_loss: 0.0662 - val_accuracy: 0.9821\n",
      "Epoch 81/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0848 - accuracy: 0.9724\n",
      "Epoch 81: val_loss improved from 0.06619 to 0.06541, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0909 - accuracy: 0.9710 - val_loss: 0.0654 - val_accuracy: 0.9821\n",
      "Epoch 82/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0880 - accuracy: 0.9671\n",
      "Epoch 82: val_loss improved from 0.06541 to 0.06464, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0847 - accuracy: 0.9688 - val_loss: 0.0646 - val_accuracy: 0.9821\n",
      "Epoch 83/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9754\n",
      "Epoch 83: val_loss improved from 0.06464 to 0.06394, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0816 - accuracy: 0.9754 - val_loss: 0.0639 - val_accuracy: 0.9821\n",
      "Epoch 84/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9710\n",
      "Epoch 84: val_loss improved from 0.06394 to 0.06337, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0849 - accuracy: 0.9710 - val_loss: 0.0634 - val_accuracy: 0.9821\n",
      "Epoch 85/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0826 - accuracy: 0.9730\n",
      "Epoch 85: val_loss improved from 0.06337 to 0.06282, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0822 - accuracy: 0.9732 - val_loss: 0.0628 - val_accuracy: 0.9821\n",
      "Epoch 86/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9724\n",
      "Epoch 86: val_loss improved from 0.06282 to 0.06224, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0830 - accuracy: 0.9732 - val_loss: 0.0622 - val_accuracy: 0.9821\n",
      "Epoch 87/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0756 - accuracy: 0.9788\n",
      "Epoch 87: val_loss improved from 0.06224 to 0.06156, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0807 - accuracy: 0.9777 - val_loss: 0.0616 - val_accuracy: 0.9881\n",
      "Epoch 88/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0828 - accuracy: 0.9744\n",
      "Epoch 88: val_loss improved from 0.06156 to 0.06095, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0802 - accuracy: 0.9754 - val_loss: 0.0610 - val_accuracy: 0.9881\n",
      "Epoch 89/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0843 - accuracy: 0.9700\n",
      "Epoch 89: val_loss improved from 0.06095 to 0.06037, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9732 - val_loss: 0.0604 - val_accuracy: 0.9881\n",
      "Epoch 90/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0812 - accuracy: 0.9741\n",
      "Epoch 90: val_loss improved from 0.06037 to 0.05982, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0796 - accuracy: 0.9754 - val_loss: 0.0598 - val_accuracy: 0.9881\n",
      "Epoch 91/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0765 - accuracy: 0.9744\n",
      "Epoch 91: val_loss improved from 0.05982 to 0.05917, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0748 - accuracy: 0.9754 - val_loss: 0.0592 - val_accuracy: 0.9881\n",
      "Epoch 92/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0765 - accuracy: 0.9770\n",
      "Epoch 92: val_loss improved from 0.05917 to 0.05866, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0746 - accuracy: 0.9777 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 93/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0746 - accuracy: 0.9753\n",
      "Epoch 93: val_loss improved from 0.05866 to 0.05802, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0744 - accuracy: 0.9754 - val_loss: 0.0580 - val_accuracy: 0.9881\n",
      "Epoch 94/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9753\n",
      "Epoch 94: val_loss improved from 0.05802 to 0.05750, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0750 - accuracy: 0.9754 - val_loss: 0.0575 - val_accuracy: 0.9881\n",
      "Epoch 95/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9793\n",
      "Epoch 95: val_loss improved from 0.05750 to 0.05684, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0690 - accuracy: 0.9799 - val_loss: 0.0568 - val_accuracy: 0.9881\n",
      "Epoch 96/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0693 - accuracy: 0.9730\n",
      "Epoch 96: val_loss improved from 0.05684 to 0.05650, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0689 - accuracy: 0.9732 - val_loss: 0.0565 - val_accuracy: 0.9881\n",
      "Epoch 97/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9727\n",
      "Epoch 97: val_loss improved from 0.05650 to 0.05592, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0750 - accuracy: 0.9732 - val_loss: 0.0559 - val_accuracy: 0.9881\n",
      "Epoch 98/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9818\n",
      "Epoch 98: val_loss improved from 0.05592 to 0.05559, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0727 - accuracy: 0.9821 - val_loss: 0.0556 - val_accuracy: 0.9881\n",
      "Epoch 99/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9799\n",
      "Epoch 99: val_loss improved from 0.05559 to 0.05503, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0631 - accuracy: 0.9799 - val_loss: 0.0550 - val_accuracy: 0.9881\n",
      "Epoch 100/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0721 - accuracy: 0.9772\n",
      "Epoch 100: val_loss improved from 0.05503 to 0.05456, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0699 - accuracy: 0.9777 - val_loss: 0.0546 - val_accuracy: 0.9881\n",
      "Epoch 101/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0645 - accuracy: 0.9792\n",
      "Epoch 101: val_loss improved from 0.05456 to 0.05408, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9799 - val_loss: 0.0541 - val_accuracy: 0.9881\n",
      "Epoch 102/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0614 - accuracy: 0.9797\n",
      "Epoch 102: val_loss improved from 0.05408 to 0.05355, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0656 - accuracy: 0.9754 - val_loss: 0.0536 - val_accuracy: 0.9881\n",
      "Epoch 103/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0689 - accuracy: 0.9792\n",
      "Epoch 103: val_loss improved from 0.05355 to 0.05319, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0656 - accuracy: 0.9799 - val_loss: 0.0532 - val_accuracy: 0.9881\n",
      "Epoch 104/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0510 - accuracy: 0.9850\n",
      "Epoch 104: val_loss improved from 0.05319 to 0.05274, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9821 - val_loss: 0.0527 - val_accuracy: 0.9881\n",
      "Epoch 105/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0704 - accuracy: 0.9765\n",
      "Epoch 105: val_loss improved from 0.05274 to 0.05245, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0685 - accuracy: 0.9777 - val_loss: 0.0525 - val_accuracy: 0.9881\n",
      "Epoch 106/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0634 - accuracy: 0.9802\n",
      "Epoch 106: val_loss improved from 0.05245 to 0.05209, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0591 - accuracy: 0.9821 - val_loss: 0.0521 - val_accuracy: 0.9881\n",
      "Epoch 107/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0627 - accuracy: 0.9805\n",
      "Epoch 107: val_loss improved from 0.05209 to 0.05166, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0663 - accuracy: 0.9799 - val_loss: 0.0517 - val_accuracy: 0.9881\n",
      "Epoch 108/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0593 - accuracy: 0.9800\n",
      "Epoch 108: val_loss improved from 0.05166 to 0.05126, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.9799 - val_loss: 0.0513 - val_accuracy: 0.9881\n",
      "Epoch 109/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0609 - accuracy: 0.9816\n",
      "Epoch 109: val_loss improved from 0.05126 to 0.05090, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0602 - accuracy: 0.9799 - val_loss: 0.0509 - val_accuracy: 0.9881\n",
      "Epoch 110/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0593 - accuracy: 0.9827\n",
      "Epoch 110: val_loss improved from 0.05090 to 0.05049, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9821 - val_loss: 0.0505 - val_accuracy: 0.9881\n",
      "Epoch 111/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0557 - accuracy: 0.9855\n",
      "Epoch 111: val_loss improved from 0.05049 to 0.05015, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9844 - val_loss: 0.0502 - val_accuracy: 0.9881\n",
      "Epoch 112/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0640 - accuracy: 0.9772\n",
      "Epoch 112: val_loss improved from 0.05015 to 0.04998, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.9799 - val_loss: 0.0500 - val_accuracy: 0.9881\n",
      "Epoch 113/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0593 - accuracy: 0.9805\n",
      "Epoch 113: val_loss improved from 0.04998 to 0.04961, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0595 - accuracy: 0.9799 - val_loss: 0.0496 - val_accuracy: 0.9881\n",
      "Epoch 114/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0602 - accuracy: 0.9821\n",
      "Epoch 114: val_loss improved from 0.04961 to 0.04928, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0573 - accuracy: 0.9844 - val_loss: 0.0493 - val_accuracy: 0.9881\n",
      "Epoch 115/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0592 - accuracy: 0.9783\n",
      "Epoch 115: val_loss improved from 0.04928 to 0.04908, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9777 - val_loss: 0.0491 - val_accuracy: 0.9881\n",
      "Epoch 116/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0570 - accuracy: 0.9846\n",
      "Epoch 116: val_loss improved from 0.04908 to 0.04873, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0557 - accuracy: 0.9844 - val_loss: 0.0487 - val_accuracy: 0.9881\n",
      "Epoch 117/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0594 - accuracy: 0.9800\n",
      "Epoch 117: val_loss improved from 0.04873 to 0.04839, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9821 - val_loss: 0.0484 - val_accuracy: 0.9881\n",
      "Epoch 118/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0549 - accuracy: 0.9865\n",
      "Epoch 118: val_loss improved from 0.04839 to 0.04813, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0553 - accuracy: 0.9866 - val_loss: 0.0481 - val_accuracy: 0.9881\n",
      "Epoch 119/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0535 - accuracy: 0.9833\n",
      "Epoch 119: val_loss improved from 0.04813 to 0.04776, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0512 - accuracy: 0.9844 - val_loss: 0.0478 - val_accuracy: 0.9881\n",
      "Epoch 120/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9844\n",
      "Epoch 120: val_loss improved from 0.04776 to 0.04759, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0550 - accuracy: 0.9844 - val_loss: 0.0476 - val_accuracy: 0.9881\n",
      "Epoch 121/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9866\n",
      "Epoch 121: val_loss improved from 0.04759 to 0.04749, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0533 - accuracy: 0.9866 - val_loss: 0.0475 - val_accuracy: 0.9881\n",
      "Epoch 122/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9844\n",
      "Epoch 122: val_loss improved from 0.04749 to 0.04718, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0548 - accuracy: 0.9844 - val_loss: 0.0472 - val_accuracy: 0.9881\n",
      "Epoch 123/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9844\n",
      "Epoch 123: val_loss improved from 0.04718 to 0.04693, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0514 - accuracy: 0.9844 - val_loss: 0.0469 - val_accuracy: 0.9881\n",
      "Epoch 124/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0509 - accuracy: 0.9862\n",
      "Epoch 124: val_loss improved from 0.04693 to 0.04667, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0506 - accuracy: 0.9866 - val_loss: 0.0467 - val_accuracy: 0.9881\n",
      "Epoch 125/200\n",
      "73/90 [=======================>......] - ETA: 0s - loss: 0.0498 - accuracy: 0.9863\n",
      "Epoch 125: val_loss improved from 0.04667 to 0.04650, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.9888 - val_loss: 0.0465 - val_accuracy: 0.9881\n",
      "Epoch 126/200\n",
      "74/90 [=======================>......] - ETA: 0s - loss: 0.0521 - accuracy: 0.9892\n",
      "Epoch 126: val_loss improved from 0.04650 to 0.04603, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9888 - val_loss: 0.0460 - val_accuracy: 0.9881\n",
      "Epoch 127/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0508 - accuracy: 0.9818\n",
      "Epoch 127: val_loss improved from 0.04603 to 0.04584, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0488 - accuracy: 0.9844 - val_loss: 0.0458 - val_accuracy: 0.9881\n",
      "Epoch 128/200\n",
      "73/90 [=======================>......] - ETA: 0s - loss: 0.0510 - accuracy: 0.9863\n",
      "Epoch 128: val_loss improved from 0.04584 to 0.04561, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0535 - accuracy: 0.9844 - val_loss: 0.0456 - val_accuracy: 0.9881\n",
      "Epoch 129/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9888\n",
      "Epoch 129: val_loss improved from 0.04561 to 0.04538, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0445 - accuracy: 0.9888 - val_loss: 0.0454 - val_accuracy: 0.9881\n",
      "Epoch 130/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0500 - accuracy: 0.9859\n",
      "Epoch 130: val_loss improved from 0.04538 to 0.04526, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0495 - accuracy: 0.9866 - val_loss: 0.0453 - val_accuracy: 0.9881\n",
      "Epoch 131/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0505 - accuracy: 0.9859\n",
      "Epoch 131: val_loss improved from 0.04526 to 0.04514, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0484 - accuracy: 0.9866 - val_loss: 0.0451 - val_accuracy: 0.9881\n",
      "Epoch 132/200\n",
      "74/90 [=======================>......] - ETA: 0s - loss: 0.0455 - accuracy: 0.9838\n",
      "Epoch 132: val_loss improved from 0.04514 to 0.04496, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.9866 - val_loss: 0.0450 - val_accuracy: 0.9881\n",
      "Epoch 133/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0498 - accuracy: 0.9818\n",
      "Epoch 133: val_loss improved from 0.04496 to 0.04482, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0491 - accuracy: 0.9821 - val_loss: 0.0448 - val_accuracy: 0.9881\n",
      "Epoch 134/200\n",
      "74/90 [=======================>......] - ETA: 0s - loss: 0.0470 - accuracy: 0.9865\n",
      "Epoch 134: val_loss improved from 0.04482 to 0.04460, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0413 - accuracy: 0.9888 - val_loss: 0.0446 - val_accuracy: 0.9881\n",
      "Epoch 135/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0418 - accuracy: 0.9884\n",
      "Epoch 135: val_loss improved from 0.04460 to 0.04435, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0407 - accuracy: 0.9888 - val_loss: 0.0444 - val_accuracy: 0.9881\n",
      "Epoch 136/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9866\n",
      "Epoch 136: val_loss improved from 0.04435 to 0.04421, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0431 - accuracy: 0.9866 - val_loss: 0.0442 - val_accuracy: 0.9881\n",
      "Epoch 137/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0425 - accuracy: 0.9881\n",
      "Epoch 137: val_loss improved from 0.04421 to 0.04407, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0417 - accuracy: 0.9888 - val_loss: 0.0441 - val_accuracy: 0.9881\n",
      "Epoch 138/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0470 - accuracy: 0.9862\n",
      "Epoch 138: val_loss improved from 0.04407 to 0.04394, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0463 - accuracy: 0.9866 - val_loss: 0.0439 - val_accuracy: 0.9881\n",
      "Epoch 139/200\n",
      "73/90 [=======================>......] - ETA: 0s - loss: 0.0417 - accuracy: 0.9890\n",
      "Epoch 139: val_loss improved from 0.04394 to 0.04374, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.9888 - val_loss: 0.0437 - val_accuracy: 0.9881\n",
      "Epoch 140/200\n",
      "74/90 [=======================>......] - ETA: 0s - loss: 0.0416 - accuracy: 0.9892\n",
      "Epoch 140: val_loss improved from 0.04374 to 0.04345, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0416 - accuracy: 0.9888 - val_loss: 0.0434 - val_accuracy: 0.9881\n",
      "Epoch 141/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0417 - accuracy: 0.9873\n",
      "Epoch 141: val_loss did not improve from 0.04345\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0389 - accuracy: 0.9888 - val_loss: 0.0435 - val_accuracy: 0.9881\n",
      "Epoch 142/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0422 - accuracy: 0.9859\n",
      "Epoch 142: val_loss improved from 0.04345 to 0.04340, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0423 - accuracy: 0.9866 - val_loss: 0.0434 - val_accuracy: 0.9881\n",
      "Epoch 143/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0337 - accuracy: 0.9927\n",
      "Epoch 143: val_loss improved from 0.04340 to 0.04331, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 0.9888 - val_loss: 0.0433 - val_accuracy: 0.9881\n",
      "Epoch 144/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0403 - accuracy: 0.9875\n",
      "Epoch 144: val_loss improved from 0.04331 to 0.04299, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9888 - val_loss: 0.0430 - val_accuracy: 0.9881\n",
      "Epoch 145/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0295 - accuracy: 0.9925\n",
      "Epoch 145: val_loss improved from 0.04299 to 0.04282, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9911 - val_loss: 0.0428 - val_accuracy: 0.9881\n",
      "Epoch 146/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9888\n",
      "Epoch 146: val_loss improved from 0.04282 to 0.04279, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0425 - accuracy: 0.9888 - val_loss: 0.0428 - val_accuracy: 0.9881\n",
      "Epoch 147/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0405 - accuracy: 0.9852\n",
      "Epoch 147: val_loss improved from 0.04279 to 0.04260, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9866 - val_loss: 0.0426 - val_accuracy: 0.9881\n",
      "Epoch 148/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0359 - accuracy: 0.9864\n",
      "Epoch 148: val_loss improved from 0.04260 to 0.04243, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.9866 - val_loss: 0.0424 - val_accuracy: 0.9881\n",
      "Epoch 149/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0346 - accuracy: 0.9888\n",
      "Epoch 149: val_loss improved from 0.04243 to 0.04224, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 0.9888 - val_loss: 0.0422 - val_accuracy: 0.9881\n",
      "Epoch 150/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0371 - accuracy: 0.9884\n",
      "Epoch 150: val_loss did not improve from 0.04224\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0361 - accuracy: 0.9888 - val_loss: 0.0424 - val_accuracy: 0.9881\n",
      "Epoch 151/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0399 - accuracy: 0.9896\n",
      "Epoch 151: val_loss improved from 0.04224 to 0.04214, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0362 - accuracy: 0.9911 - val_loss: 0.0421 - val_accuracy: 0.9881\n",
      "Epoch 152/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0305 - accuracy: 0.9910\n",
      "Epoch 152: val_loss improved from 0.04214 to 0.04209, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.9888 - val_loss: 0.0421 - val_accuracy: 0.9881\n",
      "Epoch 153/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9908\n",
      "Epoch 153: val_loss improved from 0.04209 to 0.04168, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9866 - val_loss: 0.0417 - val_accuracy: 0.9881\n",
      "Epoch 154/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0388 - accuracy: 0.9882\n",
      "Epoch 154: val_loss did not improve from 0.04168\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0374 - accuracy: 0.9888 - val_loss: 0.0417 - val_accuracy: 0.9881\n",
      "Epoch 155/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0264 - accuracy: 0.9929\n",
      "Epoch 155: val_loss improved from 0.04168 to 0.04139, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 0.9888 - val_loss: 0.0414 - val_accuracy: 0.9881\n",
      "Epoch 156/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0318 - accuracy: 0.9875\n",
      "Epoch 156: val_loss improved from 0.04139 to 0.04137, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0369 - accuracy: 0.9866 - val_loss: 0.0414 - val_accuracy: 0.9881\n",
      "Epoch 157/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0326 - accuracy: 0.9902\n",
      "Epoch 157: val_loss improved from 0.04137 to 0.04120, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 0.9911 - val_loss: 0.0412 - val_accuracy: 0.9881\n",
      "Epoch 158/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0311 - accuracy: 0.9907\n",
      "Epoch 158: val_loss did not improve from 0.04120\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.9911 - val_loss: 0.0412 - val_accuracy: 0.9881\n",
      "Epoch 159/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0334 - accuracy: 0.9900\n",
      "Epoch 159: val_loss improved from 0.04120 to 0.04098, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 0.9911 - val_loss: 0.0410 - val_accuracy: 0.9881\n",
      "Epoch 160/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0302 - accuracy: 0.9877\n",
      "Epoch 160: val_loss improved from 0.04098 to 0.04085, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 0.9866 - val_loss: 0.0408 - val_accuracy: 0.9881\n",
      "Epoch 161/200\n",
      "73/90 [=======================>......] - ETA: 0s - loss: 0.0326 - accuracy: 0.9890\n",
      "Epoch 161: val_loss did not improve from 0.04085\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9888 - val_loss: 0.0409 - val_accuracy: 0.9881\n",
      "Epoch 162/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0239 - accuracy: 0.9951\n",
      "Epoch 162: val_loss improved from 0.04085 to 0.04082, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.9911 - val_loss: 0.0408 - val_accuracy: 0.9881\n",
      "Epoch 163/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0319 - accuracy: 0.9896\n",
      "Epoch 163: val_loss improved from 0.04082 to 0.04058, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0292 - accuracy: 0.9911 - val_loss: 0.0406 - val_accuracy: 0.9881\n",
      "Epoch 164/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0303 - accuracy: 0.9904\n",
      "Epoch 164: val_loss improved from 0.04058 to 0.04051, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.9911 - val_loss: 0.0405 - val_accuracy: 0.9881\n",
      "Epoch 165/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0309 - accuracy: 0.9884\n",
      "Epoch 165: val_loss improved from 0.04051 to 0.04036, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0305 - accuracy: 0.9888 - val_loss: 0.0404 - val_accuracy: 0.9881\n",
      "Epoch 166/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0259 - accuracy: 0.9951\n",
      "Epoch 166: val_loss did not improve from 0.04036\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.9933 - val_loss: 0.0405 - val_accuracy: 0.9881\n",
      "Epoch 167/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0309 - accuracy: 0.9909\n",
      "Epoch 167: val_loss improved from 0.04036 to 0.04025, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.9911 - val_loss: 0.0403 - val_accuracy: 0.9881\n",
      "Epoch 168/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0275 - accuracy: 0.9880\n",
      "Epoch 168: val_loss improved from 0.04025 to 0.04017, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.9888 - val_loss: 0.0402 - val_accuracy: 0.9881\n",
      "Epoch 169/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9909\n",
      "Epoch 169: val_loss improved from 0.04017 to 0.04011, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0274 - accuracy: 0.9911 - val_loss: 0.0401 - val_accuracy: 0.9881\n",
      "Epoch 170/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0301 - accuracy: 0.9900\n",
      "Epoch 170: val_loss improved from 0.04011 to 0.03993, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 0.9911 - val_loss: 0.0399 - val_accuracy: 0.9881\n",
      "Epoch 171/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0270 - accuracy: 0.9906\n",
      "Epoch 171: val_loss did not improve from 0.03993\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.9911 - val_loss: 0.0399 - val_accuracy: 0.9881\n",
      "Epoch 172/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0226 - accuracy: 0.9930\n",
      "Epoch 172: val_loss did not improve from 0.03993\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.9888 - val_loss: 0.0401 - val_accuracy: 0.9881\n",
      "Epoch 173/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0255 - accuracy: 0.9925\n",
      "Epoch 173: val_loss did not improve from 0.03993\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.9911 - val_loss: 0.0400 - val_accuracy: 0.9881\n",
      "Epoch 174/200\n",
      "74/90 [=======================>......] - ETA: 0s - loss: 0.0306 - accuracy: 0.9892\n",
      "Epoch 174: val_loss improved from 0.03993 to 0.03992, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 0.9911 - val_loss: 0.0399 - val_accuracy: 0.9881\n",
      "Epoch 175/200\n",
      "75/90 [========================>.....] - ETA: 0s - loss: 0.0263 - accuracy: 0.9920\n",
      "Epoch 175: val_loss improved from 0.03992 to 0.03990, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0252 - accuracy: 0.9933 - val_loss: 0.0399 - val_accuracy: 0.9881\n",
      "Epoch 176/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0231 - accuracy: 0.9929\n",
      "Epoch 176: val_loss improved from 0.03990 to 0.03988, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.9933 - val_loss: 0.0399 - val_accuracy: 0.9881\n",
      "Epoch 177/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0310 - accuracy: 0.9882\n",
      "Epoch 177: val_loss improved from 0.03988 to 0.03978, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.9888 - val_loss: 0.0398 - val_accuracy: 0.9881\n",
      "Epoch 178/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0230 - accuracy: 0.9953\n",
      "Epoch 178: val_loss improved from 0.03978 to 0.03961, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 0.9955 - val_loss: 0.0396 - val_accuracy: 0.9881\n",
      "Epoch 179/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0236 - accuracy: 0.9926\n",
      "Epoch 179: val_loss improved from 0.03961 to 0.03951, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 0.9933 - val_loss: 0.0395 - val_accuracy: 0.9881\n",
      "Epoch 180/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0255 - accuracy: 0.9881\n",
      "Epoch 180: val_loss did not improve from 0.03951\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9888 - val_loss: 0.0396 - val_accuracy: 0.9881\n",
      "Epoch 181/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0280 - accuracy: 0.9906\n",
      "Epoch 181: val_loss improved from 0.03951 to 0.03947, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.9911 - val_loss: 0.0395 - val_accuracy: 0.9881\n",
      "Epoch 182/200\n",
      "73/90 [=======================>......] - ETA: 0s - loss: 0.0255 - accuracy: 0.9863\n",
      "Epoch 182: val_loss did not improve from 0.03947\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.9888 - val_loss: 0.0396 - val_accuracy: 0.9881\n",
      "Epoch 183/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0255 - accuracy: 0.9910\n",
      "Epoch 183: val_loss did not improve from 0.03947\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.0396 - val_accuracy: 0.9881\n",
      "Epoch 184/200\n",
      "75/90 [========================>.....] - ETA: 0s - loss: 0.0254 - accuracy: 0.9920\n",
      "Epoch 184: val_loss did not improve from 0.03947\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.9933 - val_loss: 0.0396 - val_accuracy: 0.9881\n",
      "Epoch 185/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0245 - accuracy: 0.9929\n",
      "Epoch 185: val_loss did not improve from 0.03947\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 0.9933 - val_loss: 0.0395 - val_accuracy: 0.9881\n",
      "Epoch 186/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0186 - accuracy: 0.9950\n",
      "Epoch 186: val_loss did not improve from 0.03947\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0217 - accuracy: 0.9933 - val_loss: 0.0395 - val_accuracy: 0.9881\n",
      "Epoch 187/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0239 - accuracy: 0.9933\n",
      "Epoch 187: val_loss improved from 0.03947 to 0.03923, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0237 - accuracy: 0.9933 - val_loss: 0.0392 - val_accuracy: 0.9881\n",
      "Epoch 188/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.9931\n",
      "Epoch 188: val_loss improved from 0.03923 to 0.03914, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0240 - accuracy: 0.9933 - val_loss: 0.0391 - val_accuracy: 0.9881\n",
      "Epoch 189/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0204 - accuracy: 0.9947\n",
      "Epoch 189: val_loss improved from 0.03914 to 0.03906, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0206 - accuracy: 0.9955 - val_loss: 0.0391 - val_accuracy: 0.9881\n",
      "Epoch 190/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0189 - accuracy: 0.9923\n",
      "Epoch 190: val_loss did not improve from 0.03906\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.9933 - val_loss: 0.0393 - val_accuracy: 0.9881\n",
      "Epoch 191/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0240 - accuracy: 0.9900\n",
      "Epoch 191: val_loss did not improve from 0.03906\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.9911 - val_loss: 0.0394 - val_accuracy: 0.9881\n",
      "Epoch 192/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0203 - accuracy: 0.9952\n",
      "Epoch 192: val_loss did not improve from 0.03906\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0194 - accuracy: 0.9955 - val_loss: 0.0395 - val_accuracy: 0.9881\n",
      "Epoch 193/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0218 - accuracy: 0.9953\n",
      "Epoch 193: val_loss did not improve from 0.03906\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0212 - accuracy: 0.9955 - val_loss: 0.0394 - val_accuracy: 0.9881\n",
      "Epoch 194/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9955\n",
      "Epoch 194: val_loss did not improve from 0.03906\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 0.9955 - val_loss: 0.0392 - val_accuracy: 0.9881\n",
      "Epoch 195/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0207 - accuracy: 0.9952\n",
      "Epoch 195: val_loss did not improve from 0.03906\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.9955 - val_loss: 0.0393 - val_accuracy: 0.9881\n",
      "Epoch 196/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0214 - accuracy: 0.9907\n",
      "Epoch 196: val_loss did not improve from 0.03906\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 0.9911 - val_loss: 0.0392 - val_accuracy: 0.9881\n",
      "Epoch 197/200\n",
      "75/90 [========================>.....] - ETA: 0s - loss: 0.0199 - accuracy: 0.9947\n",
      "Epoch 197: val_loss did not improve from 0.03906\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.9933 - val_loss: 0.0392 - val_accuracy: 0.9881\n",
      "Epoch 198/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0200 - accuracy: 0.9949\n",
      "Epoch 198: val_loss improved from 0.03906 to 0.03904, saving model to model_checkpoint\\GRU_1 layer with dropout_Adam.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0197 - accuracy: 0.9955 - val_loss: 0.0390 - val_accuracy: 0.9881\n",
      "Epoch 199/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9933\n",
      "Epoch 199: val_loss did not improve from 0.03904\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0200 - accuracy: 0.9933 - val_loss: 0.0391 - val_accuracy: 0.9881\n",
      "Epoch 200/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0192 - accuracy: 0.9951\n",
      "Epoch 200: val_loss did not improve from 0.03904\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.9955 - val_loss: 0.0391 - val_accuracy: 0.9881\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_22 (GRU)                (None, 32)                11520     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9821\n",
      "Test Loss: 0.08723723143339157\n",
      "Test Accuracy: 0.9821428656578064\n"
     ]
    }
   ],
   "source": [
    "# This code illustrates the implementation of an improved GRU model with dropout regularization and Adam optimization.\n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer with dropout_Adam.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,input_shape=(None, x_train.shape[-1])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 layer with dropout RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.6972 - accuracy: 0.5812\n",
      "Epoch 1: val_loss improved from inf to 0.64487, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 4s 10ms/step - loss: 0.6968 - accuracy: 0.5804 - val_loss: 0.6449 - val_accuracy: 0.7143\n",
      "Epoch 2/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.6378 - accuracy: 0.6763\n",
      "Epoch 2: val_loss improved from 0.64487 to 0.59213, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.6378 - accuracy: 0.6763 - val_loss: 0.5921 - val_accuracy: 0.7500\n",
      "Epoch 3/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.5972 - accuracy: 0.7277\n",
      "Epoch 3: val_loss improved from 0.59213 to 0.54478, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.5935 - accuracy: 0.7277 - val_loss: 0.5448 - val_accuracy: 0.7976\n",
      "Epoch 4/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.5453 - accuracy: 0.7704\n",
      "Epoch 4: val_loss improved from 0.54478 to 0.50334, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.5397 - accuracy: 0.7790 - val_loss: 0.5033 - val_accuracy: 0.8214\n",
      "Epoch 5/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.5022 - accuracy: 0.8024\n",
      "Epoch 5: val_loss improved from 0.50334 to 0.46665, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4978 - accuracy: 0.8103 - val_loss: 0.4666 - val_accuracy: 0.8333\n",
      "Epoch 6/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.4668 - accuracy: 0.8341\n",
      "Epoch 6: val_loss improved from 0.46665 to 0.43316, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4703 - accuracy: 0.8259 - val_loss: 0.4332 - val_accuracy: 0.8512\n",
      "Epoch 7/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.4470 - accuracy: 0.8188\n",
      "Epoch 7: val_loss improved from 0.43316 to 0.40381, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.4498 - accuracy: 0.8192 - val_loss: 0.4038 - val_accuracy: 0.8690\n",
      "Epoch 8/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.4095 - accuracy: 0.8564\n",
      "Epoch 8: val_loss improved from 0.40381 to 0.37742, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4073 - accuracy: 0.8549 - val_loss: 0.3774 - val_accuracy: 0.8929\n",
      "Epoch 9/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.3752 - accuracy: 0.8930\n",
      "Epoch 9: val_loss improved from 0.37742 to 0.35373, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.3766 - accuracy: 0.8884 - val_loss: 0.3537 - val_accuracy: 0.9107\n",
      "Epoch 10/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.3587 - accuracy: 0.8989\n",
      "Epoch 10: val_loss improved from 0.35373 to 0.33185, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.3608 - accuracy: 0.8929 - val_loss: 0.3319 - val_accuracy: 0.9405\n",
      "Epoch 11/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.3538 - accuracy: 0.9012\n",
      "Epoch 11: val_loss improved from 0.33185 to 0.31283, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.3466 - accuracy: 0.9062 - val_loss: 0.3128 - val_accuracy: 0.9524\n",
      "Epoch 12/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.3322 - accuracy: 0.9070\n",
      "Epoch 12: val_loss improved from 0.31283 to 0.29479, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.9062 - val_loss: 0.2948 - val_accuracy: 0.9583\n",
      "Epoch 13/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.3101 - accuracy: 0.9218\n",
      "Epoch 13: val_loss improved from 0.29479 to 0.27880, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.3100 - accuracy: 0.9219 - val_loss: 0.2788 - val_accuracy: 0.9583\n",
      "Epoch 14/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.9191\n",
      "Epoch 14: val_loss improved from 0.27880 to 0.26458, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2935 - accuracy: 0.9196 - val_loss: 0.2646 - val_accuracy: 0.9583\n",
      "Epoch 15/200\n",
      "73/90 [=======================>......] - ETA: 0s - loss: 0.2755 - accuracy: 0.9342\n",
      "Epoch 15: val_loss improved from 0.26458 to 0.25126, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2791 - accuracy: 0.9308 - val_loss: 0.2513 - val_accuracy: 0.9583\n",
      "Epoch 16/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.2785 - accuracy: 0.9200\n",
      "Epoch 16: val_loss improved from 0.25126 to 0.23919, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2761 - accuracy: 0.9196 - val_loss: 0.2392 - val_accuracy: 0.9583\n",
      "Epoch 17/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.9330\n",
      "Epoch 17: val_loss improved from 0.23919 to 0.22850, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2650 - accuracy: 0.9330 - val_loss: 0.2285 - val_accuracy: 0.9583\n",
      "Epoch 18/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.2533 - accuracy: 0.9342\n",
      "Epoch 18: val_loss improved from 0.22850 to 0.21829, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2530 - accuracy: 0.9353 - val_loss: 0.2183 - val_accuracy: 0.9583\n",
      "Epoch 19/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9310\n",
      "Epoch 19: val_loss improved from 0.21829 to 0.20908, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2459 - accuracy: 0.9330 - val_loss: 0.2091 - val_accuracy: 0.9583\n",
      "Epoch 20/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9393\n",
      "Epoch 20: val_loss improved from 0.20908 to 0.20084, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2399 - accuracy: 0.9375 - val_loss: 0.2008 - val_accuracy: 0.9583\n",
      "Epoch 21/200\n",
      "74/90 [=======================>......] - ETA: 0s - loss: 0.2370 - accuracy: 0.9405\n",
      "Epoch 21: val_loss improved from 0.20084 to 0.19304, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.2283 - accuracy: 0.9442 - val_loss: 0.1930 - val_accuracy: 0.9583\n",
      "Epoch 22/200\n",
      "74/90 [=======================>......] - ETA: 0s - loss: 0.2182 - accuracy: 0.9459\n",
      "Epoch 22: val_loss improved from 0.19304 to 0.18577, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2163 - accuracy: 0.9464 - val_loss: 0.1858 - val_accuracy: 0.9583\n",
      "Epoch 23/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.2187 - accuracy: 0.9381\n",
      "Epoch 23: val_loss improved from 0.18577 to 0.17930, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2161 - accuracy: 0.9397 - val_loss: 0.1793 - val_accuracy: 0.9643\n",
      "Epoch 24/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2074 - accuracy: 0.9432\n",
      "Epoch 24: val_loss improved from 0.17930 to 0.17306, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.2067 - accuracy: 0.9420 - val_loss: 0.1731 - val_accuracy: 0.9643\n",
      "Epoch 25/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 0.9487\n",
      "Epoch 25: val_loss improved from 0.17306 to 0.16751, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1997 - accuracy: 0.9487 - val_loss: 0.1675 - val_accuracy: 0.9643\n",
      "Epoch 26/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9461\n",
      "Epoch 26: val_loss improved from 0.16751 to 0.16230, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1950 - accuracy: 0.9464 - val_loss: 0.1623 - val_accuracy: 0.9762\n",
      "Epoch 27/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1891 - accuracy: 0.9500\n",
      "Epoch 27: val_loss improved from 0.16230 to 0.15736, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1886 - accuracy: 0.9509 - val_loss: 0.1574 - val_accuracy: 0.9762\n",
      "Epoch 28/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1876 - accuracy: 0.9435\n",
      "Epoch 28: val_loss improved from 0.15736 to 0.15295, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1890 - accuracy: 0.9442 - val_loss: 0.1530 - val_accuracy: 0.9762\n",
      "Epoch 29/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1911 - accuracy: 0.9425\n",
      "Epoch 29: val_loss improved from 0.15295 to 0.14884, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1882 - accuracy: 0.9442 - val_loss: 0.1488 - val_accuracy: 0.9762\n",
      "Epoch 30/200\n",
      "74/90 [=======================>......] - ETA: 0s - loss: 0.1852 - accuracy: 0.9432\n",
      "Epoch 30: val_loss improved from 0.14884 to 0.14486, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1818 - accuracy: 0.9487 - val_loss: 0.1449 - val_accuracy: 0.9762\n",
      "Epoch 31/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1824 - accuracy: 0.9448\n",
      "Epoch 31: val_loss improved from 0.14486 to 0.14128, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1788 - accuracy: 0.9464 - val_loss: 0.1413 - val_accuracy: 0.9762\n",
      "Epoch 32/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1685 - accuracy: 0.9461\n",
      "Epoch 32: val_loss improved from 0.14128 to 0.13749, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.9464 - val_loss: 0.1375 - val_accuracy: 0.9762\n",
      "Epoch 33/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9464\n",
      "Epoch 33: val_loss improved from 0.13749 to 0.13422, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1624 - accuracy: 0.9464 - val_loss: 0.1342 - val_accuracy: 0.9762\n",
      "Epoch 34/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1571 - accuracy: 0.9548\n",
      "Epoch 34: val_loss improved from 0.13422 to 0.13115, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1590 - accuracy: 0.9554 - val_loss: 0.1311 - val_accuracy: 0.9762\n",
      "Epoch 35/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9506\n",
      "Epoch 35: val_loss improved from 0.13115 to 0.12822, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1600 - accuracy: 0.9487 - val_loss: 0.1282 - val_accuracy: 0.9762\n",
      "Epoch 36/200\n",
      "75/90 [========================>.....] - ETA: 0s - loss: 0.1651 - accuracy: 0.9467\n",
      "Epoch 36: val_loss improved from 0.12822 to 0.12566, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1610 - accuracy: 0.9509 - val_loss: 0.1257 - val_accuracy: 0.9762\n",
      "Epoch 37/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1658 - accuracy: 0.9494\n",
      "Epoch 37: val_loss improved from 0.12566 to 0.12318, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1568 - accuracy: 0.9531 - val_loss: 0.1232 - val_accuracy: 0.9762\n",
      "Epoch 38/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9506\n",
      "Epoch 38: val_loss improved from 0.12318 to 0.12087, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9509 - val_loss: 0.1209 - val_accuracy: 0.9762\n",
      "Epoch 39/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1487 - accuracy: 0.9517\n",
      "Epoch 39: val_loss improved from 0.12087 to 0.11858, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1482 - accuracy: 0.9509 - val_loss: 0.1186 - val_accuracy: 0.9762\n",
      "Epoch 40/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1385 - accuracy: 0.9600\n",
      "Epoch 40: val_loss improved from 0.11858 to 0.11645, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.9554 - val_loss: 0.1164 - val_accuracy: 0.9762\n",
      "Epoch 41/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1496 - accuracy: 0.9506\n",
      "Epoch 41: val_loss improved from 0.11645 to 0.11433, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1493 - accuracy: 0.9509 - val_loss: 0.1143 - val_accuracy: 0.9762\n",
      "Epoch 42/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1355 - accuracy: 0.9506\n",
      "Epoch 42: val_loss improved from 0.11433 to 0.11227, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9509 - val_loss: 0.1123 - val_accuracy: 0.9762\n",
      "Epoch 43/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.9576\n",
      "Epoch 43: val_loss improved from 0.11227 to 0.11051, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1395 - accuracy: 0.9576 - val_loss: 0.1105 - val_accuracy: 0.9762\n",
      "Epoch 44/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9528\n",
      "Epoch 44: val_loss improved from 0.11051 to 0.10882, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9531 - val_loss: 0.1088 - val_accuracy: 0.9762\n",
      "Epoch 45/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9598\n",
      "Epoch 45: val_loss improved from 0.10882 to 0.10702, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1314 - accuracy: 0.9598 - val_loss: 0.1070 - val_accuracy: 0.9762\n",
      "Epoch 46/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1374 - accuracy: 0.9531\n",
      "Epoch 46: val_loss improved from 0.10702 to 0.10544, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.9531 - val_loss: 0.1054 - val_accuracy: 0.9762\n",
      "Epoch 47/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1378 - accuracy: 0.9500\n",
      "Epoch 47: val_loss improved from 0.10544 to 0.10408, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9554 - val_loss: 0.1041 - val_accuracy: 0.9762\n",
      "Epoch 48/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1344 - accuracy: 0.9548\n",
      "Epoch 48: val_loss improved from 0.10408 to 0.10267, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1282 - accuracy: 0.9576 - val_loss: 0.1027 - val_accuracy: 0.9762\n",
      "Epoch 49/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1386 - accuracy: 0.9564\n",
      "Epoch 49: val_loss improved from 0.10267 to 0.10106, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1305 - accuracy: 0.9598 - val_loss: 0.1011 - val_accuracy: 0.9762\n",
      "Epoch 50/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0991 - accuracy: 0.9662\n",
      "Epoch 50: val_loss improved from 0.10106 to 0.09967, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1199 - accuracy: 0.9576 - val_loss: 0.0997 - val_accuracy: 0.9762\n",
      "Epoch 51/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.1388 - accuracy: 0.9553\n",
      "Epoch 51: val_loss improved from 0.09967 to 0.09839, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1223 - accuracy: 0.9621 - val_loss: 0.0984 - val_accuracy: 0.9762\n",
      "Epoch 52/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.1184 - accuracy: 0.9662\n",
      "Epoch 52: val_loss improved from 0.09839 to 0.09715, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9621 - val_loss: 0.0971 - val_accuracy: 0.9762\n",
      "Epoch 53/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.1212 - accuracy: 0.9553\n",
      "Epoch 53: val_loss improved from 0.09715 to 0.09590, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1160 - accuracy: 0.9598 - val_loss: 0.0959 - val_accuracy: 0.9762\n",
      "Epoch 54/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1196 - accuracy: 0.9605\n",
      "Epoch 54: val_loss improved from 0.09590 to 0.09488, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1207 - accuracy: 0.9598 - val_loss: 0.0949 - val_accuracy: 0.9762\n",
      "Epoch 55/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1151 - accuracy: 0.9624\n",
      "Epoch 55: val_loss improved from 0.09488 to 0.09381, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1146 - accuracy: 0.9621 - val_loss: 0.0938 - val_accuracy: 0.9762\n",
      "Epoch 56/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1091 - accuracy: 0.9651\n",
      "Epoch 56: val_loss improved from 0.09381 to 0.09280, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1151 - accuracy: 0.9643 - val_loss: 0.0928 - val_accuracy: 0.9762\n",
      "Epoch 57/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1102 - accuracy: 0.9614\n",
      "Epoch 57: val_loss improved from 0.09280 to 0.09178, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1057 - accuracy: 0.9621 - val_loss: 0.0918 - val_accuracy: 0.9762\n",
      "Epoch 58/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1112 - accuracy: 0.9641\n",
      "Epoch 58: val_loss improved from 0.09178 to 0.09070, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9665 - val_loss: 0.0907 - val_accuracy: 0.9762\n",
      "Epoch 59/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9640\n",
      "Epoch 59: val_loss improved from 0.09070 to 0.08968, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9643 - val_loss: 0.0897 - val_accuracy: 0.9762\n",
      "Epoch 60/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9636\n",
      "Epoch 60: val_loss improved from 0.08968 to 0.08859, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9643 - val_loss: 0.0886 - val_accuracy: 0.9762\n",
      "Epoch 61/200\n",
      "75/90 [========================>.....] - ETA: 0s - loss: 0.1134 - accuracy: 0.9600\n",
      "Epoch 61: val_loss improved from 0.08859 to 0.08778, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9598 - val_loss: 0.0878 - val_accuracy: 0.9762\n",
      "Epoch 62/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1110 - accuracy: 0.9605\n",
      "Epoch 62: val_loss improved from 0.08778 to 0.08688, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1044 - accuracy: 0.9621 - val_loss: 0.0869 - val_accuracy: 0.9762\n",
      "Epoch 63/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0969 - accuracy: 0.9620\n",
      "Epoch 63: val_loss improved from 0.08688 to 0.08609, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.9598 - val_loss: 0.0861 - val_accuracy: 0.9762\n",
      "Epoch 64/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1038 - accuracy: 0.9636\n",
      "Epoch 64: val_loss improved from 0.08609 to 0.08512, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1026 - accuracy: 0.9643 - val_loss: 0.0851 - val_accuracy: 0.9762\n",
      "Epoch 65/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.1010 - accuracy: 0.9684\n",
      "Epoch 65: val_loss improved from 0.08512 to 0.08419, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1039 - accuracy: 0.9665 - val_loss: 0.0842 - val_accuracy: 0.9762\n",
      "Epoch 66/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1002 - accuracy: 0.9667\n",
      "Epoch 66: val_loss improved from 0.08419 to 0.08359, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1016 - accuracy: 0.9665 - val_loss: 0.0836 - val_accuracy: 0.9762\n",
      "Epoch 67/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1120 - accuracy: 0.9625\n",
      "Epoch 67: val_loss improved from 0.08359 to 0.08290, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1051 - accuracy: 0.9643 - val_loss: 0.0829 - val_accuracy: 0.9762\n",
      "Epoch 68/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0916 - accuracy: 0.9684\n",
      "Epoch 68: val_loss improved from 0.08290 to 0.08222, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9688 - val_loss: 0.0822 - val_accuracy: 0.9762\n",
      "Epoch 69/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1049 - accuracy: 0.9600\n",
      "Epoch 69: val_loss improved from 0.08222 to 0.08159, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.1005 - accuracy: 0.9621 - val_loss: 0.0816 - val_accuracy: 0.9762\n",
      "Epoch 70/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0913 - accuracy: 0.9737\n",
      "Epoch 70: val_loss improved from 0.08159 to 0.08091, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0968 - accuracy: 0.9688 - val_loss: 0.0809 - val_accuracy: 0.9762\n",
      "Epoch 71/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1031 - accuracy: 0.9650\n",
      "Epoch 71: val_loss improved from 0.08091 to 0.08018, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0988 - accuracy: 0.9665 - val_loss: 0.0802 - val_accuracy: 0.9762\n",
      "Epoch 72/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0937 - accuracy: 0.9696\n",
      "Epoch 72: val_loss improved from 0.08018 to 0.07940, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9688 - val_loss: 0.0794 - val_accuracy: 0.9762\n",
      "Epoch 73/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0934 - accuracy: 0.9679\n",
      "Epoch 73: val_loss improved from 0.07940 to 0.07885, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.9688 - val_loss: 0.0789 - val_accuracy: 0.9762\n",
      "Epoch 74/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0794 - accuracy: 0.9763\n",
      "Epoch 74: val_loss improved from 0.07885 to 0.07821, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0910 - accuracy: 0.9688 - val_loss: 0.0782 - val_accuracy: 0.9762\n",
      "Epoch 75/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0921 - accuracy: 0.9675\n",
      "Epoch 75: val_loss improved from 0.07821 to 0.07757, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0881 - accuracy: 0.9688 - val_loss: 0.0776 - val_accuracy: 0.9762\n",
      "Epoch 76/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0898 - accuracy: 0.9690\n",
      "Epoch 76: val_loss improved from 0.07757 to 0.07696, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0911 - accuracy: 0.9688 - val_loss: 0.0770 - val_accuracy: 0.9762\n",
      "Epoch 77/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0832 - accuracy: 0.9725\n",
      "Epoch 77: val_loss improved from 0.07696 to 0.07633, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9688 - val_loss: 0.0763 - val_accuracy: 0.9762\n",
      "Epoch 78/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0894 - accuracy: 0.9704\n",
      "Epoch 78: val_loss improved from 0.07633 to 0.07569, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0870 - accuracy: 0.9710 - val_loss: 0.0757 - val_accuracy: 0.9762\n",
      "Epoch 79/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0894 - accuracy: 0.9679\n",
      "Epoch 79: val_loss improved from 0.07569 to 0.07505, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0874 - accuracy: 0.9688 - val_loss: 0.0751 - val_accuracy: 0.9762\n",
      "Epoch 80/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0926 - accuracy: 0.9662\n",
      "Epoch 80: val_loss improved from 0.07505 to 0.07448, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0855 - accuracy: 0.9710 - val_loss: 0.0745 - val_accuracy: 0.9762\n",
      "Epoch 81/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0822 - accuracy: 0.9692\n",
      "Epoch 81: val_loss improved from 0.07448 to 0.07403, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0918 - accuracy: 0.9688 - val_loss: 0.0740 - val_accuracy: 0.9762\n",
      "Epoch 82/200\n",
      "75/90 [========================>.....] - ETA: 0s - loss: 0.0844 - accuracy: 0.9733\n",
      "Epoch 82: val_loss improved from 0.07403 to 0.07373, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0842 - accuracy: 0.9710 - val_loss: 0.0737 - val_accuracy: 0.9762\n",
      "Epoch 83/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0851 - accuracy: 0.9700\n",
      "Epoch 83: val_loss improved from 0.07373 to 0.07313, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0785 - accuracy: 0.9732 - val_loss: 0.0731 - val_accuracy: 0.9762\n",
      "Epoch 84/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0911 - accuracy: 0.9687\n",
      "Epoch 84: val_loss improved from 0.07313 to 0.07278, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0862 - accuracy: 0.9710 - val_loss: 0.0728 - val_accuracy: 0.9762\n",
      "Epoch 85/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0830 - accuracy: 0.9675\n",
      "Epoch 85: val_loss improved from 0.07278 to 0.07249, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0842 - accuracy: 0.9688 - val_loss: 0.0725 - val_accuracy: 0.9762\n",
      "Epoch 86/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9710\n",
      "Epoch 86: val_loss improved from 0.07249 to 0.07189, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0781 - accuracy: 0.9710 - val_loss: 0.0719 - val_accuracy: 0.9762\n",
      "Epoch 87/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0595 - accuracy: 0.9821\n",
      "Epoch 87: val_loss improved from 0.07189 to 0.07161, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0826 - accuracy: 0.9732 - val_loss: 0.0716 - val_accuracy: 0.9762\n",
      "Epoch 88/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0858 - accuracy: 0.9683\n",
      "Epoch 88: val_loss improved from 0.07161 to 0.07131, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0803 - accuracy: 0.9710 - val_loss: 0.0713 - val_accuracy: 0.9762\n",
      "Epoch 89/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0796 - accuracy: 0.9694\n",
      "Epoch 89: val_loss improved from 0.07131 to 0.07099, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0760 - accuracy: 0.9710 - val_loss: 0.0710 - val_accuracy: 0.9762\n",
      "Epoch 90/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0633 - accuracy: 0.9823\n",
      "Epoch 90: val_loss improved from 0.07099 to 0.07067, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.9754 - val_loss: 0.0707 - val_accuracy: 0.9762\n",
      "Epoch 91/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9747\n",
      "Epoch 91: val_loss improved from 0.07067 to 0.07017, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0770 - accuracy: 0.9732 - val_loss: 0.0702 - val_accuracy: 0.9762\n",
      "Epoch 92/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0812 - accuracy: 0.9711\n",
      "Epoch 92: val_loss improved from 0.07017 to 0.06964, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9710 - val_loss: 0.0696 - val_accuracy: 0.9762\n",
      "Epoch 93/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0579 - accuracy: 0.9846\n",
      "Epoch 93: val_loss improved from 0.06964 to 0.06940, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0764 - accuracy: 0.9754 - val_loss: 0.0694 - val_accuracy: 0.9762\n",
      "Epoch 94/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0697 - accuracy: 0.9722\n",
      "Epoch 94: val_loss improved from 0.06940 to 0.06907, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0714 - accuracy: 0.9732 - val_loss: 0.0691 - val_accuracy: 0.9762\n",
      "Epoch 95/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0721 - accuracy: 0.9725\n",
      "Epoch 95: val_loss improved from 0.06907 to 0.06869, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9732 - val_loss: 0.0687 - val_accuracy: 0.9762\n",
      "Epoch 96/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0705 - accuracy: 0.9753\n",
      "Epoch 96: val_loss improved from 0.06869 to 0.06837, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0728 - accuracy: 0.9732 - val_loss: 0.0684 - val_accuracy: 0.9762\n",
      "Epoch 97/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0771 - accuracy: 0.9725\n",
      "Epoch 97: val_loss improved from 0.06837 to 0.06797, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0724 - accuracy: 0.9732 - val_loss: 0.0680 - val_accuracy: 0.9821\n",
      "Epoch 98/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0765 - accuracy: 0.9688\n",
      "Epoch 98: val_loss improved from 0.06797 to 0.06757, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0732 - accuracy: 0.9710 - val_loss: 0.0676 - val_accuracy: 0.9821\n",
      "Epoch 99/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0751 - accuracy: 0.9759\n",
      "Epoch 99: val_loss improved from 0.06757 to 0.06741, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0722 - accuracy: 0.9777 - val_loss: 0.0674 - val_accuracy: 0.9821\n",
      "Epoch 100/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0656 - accuracy: 0.9741\n",
      "Epoch 100: val_loss improved from 0.06741 to 0.06719, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9732 - val_loss: 0.0672 - val_accuracy: 0.9821\n",
      "Epoch 101/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9750\n",
      "Epoch 101: val_loss improved from 0.06719 to 0.06660, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.9754 - val_loss: 0.0666 - val_accuracy: 0.9821\n",
      "Epoch 102/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0696 - accuracy: 0.9798\n",
      "Epoch 102: val_loss improved from 0.06660 to 0.06623, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9799 - val_loss: 0.0662 - val_accuracy: 0.9821\n",
      "Epoch 103/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0693 - accuracy: 0.9765\n",
      "Epoch 103: val_loss improved from 0.06623 to 0.06605, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9777 - val_loss: 0.0661 - val_accuracy: 0.9821\n",
      "Epoch 104/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0647 - accuracy: 0.9759\n",
      "Epoch 104: val_loss improved from 0.06605 to 0.06577, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.9754 - val_loss: 0.0658 - val_accuracy: 0.9821\n",
      "Epoch 105/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0659 - accuracy: 0.9775\n",
      "Epoch 105: val_loss improved from 0.06577 to 0.06550, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.9754 - val_loss: 0.0655 - val_accuracy: 0.9821\n",
      "Epoch 106/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0568 - accuracy: 0.9786\n",
      "Epoch 106: val_loss improved from 0.06550 to 0.06535, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0582 - accuracy: 0.9777 - val_loss: 0.0654 - val_accuracy: 0.9821\n",
      "Epoch 107/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0741 - accuracy: 0.9732\n",
      "Epoch 107: val_loss improved from 0.06535 to 0.06511, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9754 - val_loss: 0.0651 - val_accuracy: 0.9821\n",
      "Epoch 108/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0642 - accuracy: 0.9793\n",
      "Epoch 108: val_loss improved from 0.06511 to 0.06474, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0630 - accuracy: 0.9799 - val_loss: 0.0647 - val_accuracy: 0.9821\n",
      "Epoch 109/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0645 - accuracy: 0.9769\n",
      "Epoch 109: val_loss improved from 0.06474 to 0.06441, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0656 - accuracy: 0.9777 - val_loss: 0.0644 - val_accuracy: 0.9821\n",
      "Epoch 110/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9777\n",
      "Epoch 110: val_loss improved from 0.06441 to 0.06400, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9777 - val_loss: 0.0640 - val_accuracy: 0.9821\n",
      "Epoch 111/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0605 - accuracy: 0.9792\n",
      "Epoch 111: val_loss improved from 0.06400 to 0.06399, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0609 - accuracy: 0.9777 - val_loss: 0.0640 - val_accuracy: 0.9821\n",
      "Epoch 112/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0598 - accuracy: 0.9800\n",
      "Epoch 112: val_loss improved from 0.06399 to 0.06361, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0662 - accuracy: 0.9777 - val_loss: 0.0636 - val_accuracy: 0.9881\n",
      "Epoch 113/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0624 - accuracy: 0.9795\n",
      "Epoch 113: val_loss improved from 0.06361 to 0.06353, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.9799 - val_loss: 0.0635 - val_accuracy: 0.9881\n",
      "Epoch 114/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0629 - accuracy: 0.9762\n",
      "Epoch 114: val_loss improved from 0.06353 to 0.06328, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.9754 - val_loss: 0.0633 - val_accuracy: 0.9881\n",
      "Epoch 115/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0628 - accuracy: 0.9759\n",
      "Epoch 115: val_loss improved from 0.06328 to 0.06317, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.9777 - val_loss: 0.0632 - val_accuracy: 0.9881\n",
      "Epoch 116/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0681 - accuracy: 0.9778\n",
      "Epoch 116: val_loss improved from 0.06317 to 0.06270, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.9799 - val_loss: 0.0627 - val_accuracy: 0.9881\n",
      "Epoch 117/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0649 - accuracy: 0.9827\n",
      "Epoch 117: val_loss improved from 0.06270 to 0.06249, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0609 - accuracy: 0.9844 - val_loss: 0.0625 - val_accuracy: 0.9881\n",
      "Epoch 118/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0603 - accuracy: 0.9807\n",
      "Epoch 118: val_loss improved from 0.06249 to 0.06237, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.9821 - val_loss: 0.0624 - val_accuracy: 0.9881\n",
      "Epoch 119/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0625 - accuracy: 0.9807\n",
      "Epoch 119: val_loss improved from 0.06237 to 0.06213, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.9821 - val_loss: 0.0621 - val_accuracy: 0.9881\n",
      "Epoch 120/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0605 - accuracy: 0.9766\n",
      "Epoch 120: val_loss improved from 0.06213 to 0.06200, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0561 - accuracy: 0.9777 - val_loss: 0.0620 - val_accuracy: 0.9881\n",
      "Epoch 121/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0591 - accuracy: 0.9765\n",
      "Epoch 121: val_loss did not improve from 0.06200\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9777 - val_loss: 0.0622 - val_accuracy: 0.9881\n",
      "Epoch 122/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9821\n",
      "Epoch 122: val_loss improved from 0.06200 to 0.06187, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9821 - val_loss: 0.0619 - val_accuracy: 0.9881\n",
      "Epoch 123/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0541 - accuracy: 0.9821\n",
      "Epoch 123: val_loss improved from 0.06187 to 0.06170, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 0.9799 - val_loss: 0.0617 - val_accuracy: 0.9881\n",
      "Epoch 124/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0583 - accuracy: 0.9846\n",
      "Epoch 124: val_loss improved from 0.06170 to 0.06155, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0577 - accuracy: 0.9844 - val_loss: 0.0616 - val_accuracy: 0.9881\n",
      "Epoch 125/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0581 - accuracy: 0.9825\n",
      "Epoch 125: val_loss improved from 0.06155 to 0.06150, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0533 - accuracy: 0.9844 - val_loss: 0.0615 - val_accuracy: 0.9881\n",
      "Epoch 126/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0533 - accuracy: 0.9833\n",
      "Epoch 126: val_loss improved from 0.06150 to 0.06141, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0513 - accuracy: 0.9844 - val_loss: 0.0614 - val_accuracy: 0.9881\n",
      "Epoch 127/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9818\n",
      "Epoch 127: val_loss did not improve from 0.06141\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0528 - accuracy: 0.9821 - val_loss: 0.0614 - val_accuracy: 0.9881\n",
      "Epoch 128/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0497 - accuracy: 0.9857\n",
      "Epoch 128: val_loss improved from 0.06141 to 0.06128, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0569 - accuracy: 0.9821 - val_loss: 0.0613 - val_accuracy: 0.9881\n",
      "Epoch 129/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9820\n",
      "Epoch 129: val_loss improved from 0.06128 to 0.06106, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0553 - accuracy: 0.9821 - val_loss: 0.0611 - val_accuracy: 0.9881\n",
      "Epoch 130/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9844\n",
      "Epoch 130: val_loss improved from 0.06106 to 0.06096, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9844 - val_loss: 0.0610 - val_accuracy: 0.9881\n",
      "Epoch 131/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0511 - accuracy: 0.9814\n",
      "Epoch 131: val_loss improved from 0.06096 to 0.06091, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.9821 - val_loss: 0.0609 - val_accuracy: 0.9881\n",
      "Epoch 132/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0553 - accuracy: 0.9823\n",
      "Epoch 132: val_loss improved from 0.06091 to 0.06085, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.9844 - val_loss: 0.0609 - val_accuracy: 0.9881\n",
      "Epoch 133/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9844\n",
      "Epoch 133: val_loss improved from 0.06085 to 0.06072, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0508 - accuracy: 0.9844 - val_loss: 0.0607 - val_accuracy: 0.9881\n",
      "Epoch 134/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0508 - accuracy: 0.9820\n",
      "Epoch 134: val_loss improved from 0.06072 to 0.06065, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0505 - accuracy: 0.9821 - val_loss: 0.0606 - val_accuracy: 0.9881\n",
      "Epoch 135/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0523 - accuracy: 0.9842\n",
      "Epoch 135: val_loss improved from 0.06065 to 0.06055, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.9844 - val_loss: 0.0605 - val_accuracy: 0.9881\n",
      "Epoch 136/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0479 - accuracy: 0.9841\n",
      "Epoch 136: val_loss did not improve from 0.06055\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.9844 - val_loss: 0.0606 - val_accuracy: 0.9881\n",
      "Epoch 137/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0508 - accuracy: 0.9839\n",
      "Epoch 137: val_loss improved from 0.06055 to 0.06046, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0498 - accuracy: 0.9844 - val_loss: 0.0605 - val_accuracy: 0.9881\n",
      "Epoch 138/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0518 - accuracy: 0.9816\n",
      "Epoch 138: val_loss improved from 0.06046 to 0.06038, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9821 - val_loss: 0.0604 - val_accuracy: 0.9881\n",
      "Epoch 139/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0492 - accuracy: 0.9862\n",
      "Epoch 139: val_loss improved from 0.06038 to 0.06029, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0540 - accuracy: 0.9844 - val_loss: 0.0603 - val_accuracy: 0.9881\n",
      "Epoch 140/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0499 - accuracy: 0.9839\n",
      "Epoch 140: val_loss improved from 0.06029 to 0.06015, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0487 - accuracy: 0.9844 - val_loss: 0.0601 - val_accuracy: 0.9881\n",
      "Epoch 141/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0433 - accuracy: 0.9848\n",
      "Epoch 141: val_loss did not improve from 0.06015\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0510 - accuracy: 0.9821 - val_loss: 0.0602 - val_accuracy: 0.9881\n",
      "Epoch 142/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9866\n",
      "Epoch 142: val_loss improved from 0.06015 to 0.05992, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0450 - accuracy: 0.9866 - val_loss: 0.0599 - val_accuracy: 0.9881\n",
      "Epoch 143/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0523 - accuracy: 0.9810\n",
      "Epoch 143: val_loss improved from 0.05992 to 0.05980, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0501 - accuracy: 0.9821 - val_loss: 0.0598 - val_accuracy: 0.9881\n",
      "Epoch 144/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0395 - accuracy: 0.9870\n",
      "Epoch 144: val_loss improved from 0.05980 to 0.05976, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0487 - accuracy: 0.9844 - val_loss: 0.0598 - val_accuracy: 0.9881\n",
      "Epoch 145/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0476 - accuracy: 0.9843\n",
      "Epoch 145: val_loss improved from 0.05976 to 0.05941, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0474 - accuracy: 0.9844 - val_loss: 0.0594 - val_accuracy: 0.9881\n",
      "Epoch 146/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0478 - accuracy: 0.9844\n",
      "Epoch 146: val_loss improved from 0.05941 to 0.05941, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0429 - accuracy: 0.9866 - val_loss: 0.0594 - val_accuracy: 0.9881\n",
      "Epoch 147/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0527 - accuracy: 0.9846\n",
      "Epoch 147: val_loss did not improve from 0.05941\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0475 - accuracy: 0.9866 - val_loss: 0.0595 - val_accuracy: 0.9881\n",
      "Epoch 148/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9866\n",
      "Epoch 148: val_loss improved from 0.05941 to 0.05934, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0448 - accuracy: 0.9866 - val_loss: 0.0593 - val_accuracy: 0.9881\n",
      "Epoch 149/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0476 - accuracy: 0.9839\n",
      "Epoch 149: val_loss improved from 0.05934 to 0.05926, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0466 - accuracy: 0.9844 - val_loss: 0.0593 - val_accuracy: 0.9881\n",
      "Epoch 150/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0429 - accuracy: 0.9882\n",
      "Epoch 150: val_loss improved from 0.05926 to 0.05912, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0448 - accuracy: 0.9866 - val_loss: 0.0591 - val_accuracy: 0.9881\n",
      "Epoch 151/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9844\n",
      "Epoch 151: val_loss improved from 0.05912 to 0.05897, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0454 - accuracy: 0.9844 - val_loss: 0.0590 - val_accuracy: 0.9881\n",
      "Epoch 152/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0504 - accuracy: 0.9827\n",
      "Epoch 152: val_loss improved from 0.05897 to 0.05892, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0470 - accuracy: 0.9844 - val_loss: 0.0589 - val_accuracy: 0.9881\n",
      "Epoch 153/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0435 - accuracy: 0.9864\n",
      "Epoch 153: val_loss did not improve from 0.05892\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0457 - accuracy: 0.9844 - val_loss: 0.0589 - val_accuracy: 0.9881\n",
      "Epoch 154/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0468 - accuracy: 0.9870\n",
      "Epoch 154: val_loss improved from 0.05892 to 0.05871, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.9866 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 155/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0406 - accuracy: 0.9821\n",
      "Epoch 155: val_loss did not improve from 0.05871\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0385 - accuracy: 0.9844 - val_loss: 0.0588 - val_accuracy: 0.9881\n",
      "Epoch 156/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0435 - accuracy: 0.9864\n",
      "Epoch 156: val_loss did not improve from 0.05871\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0427 - accuracy: 0.9866 - val_loss: 0.0588 - val_accuracy: 0.9881\n",
      "Epoch 157/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0468 - accuracy: 0.9860\n",
      "Epoch 157: val_loss improved from 0.05871 to 0.05870, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0450 - accuracy: 0.9866 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 158/200\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0451 - accuracy: 0.9839\n",
      "Epoch 158: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0440 - accuracy: 0.9844 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 159/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0441 - accuracy: 0.9859\n",
      "Epoch 159: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.9866 - val_loss: 0.0588 - val_accuracy: 0.9881\n",
      "Epoch 160/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0411 - accuracy: 0.9857\n",
      "Epoch 160: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9866 - val_loss: 0.0588 - val_accuracy: 0.9881\n",
      "Epoch 161/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0448 - accuracy: 0.9844\n",
      "Epoch 161: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.9866 - val_loss: 0.0588 - val_accuracy: 0.9881\n",
      "Epoch 162/200\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0421 - accuracy: 0.9859\n",
      "Epoch 162: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0404 - accuracy: 0.9866 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 163/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0407 - accuracy: 0.9865\n",
      "Epoch 163: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 0.9866 - val_loss: 0.0589 - val_accuracy: 0.9881\n",
      "Epoch 164/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0391 - accuracy: 0.9860\n",
      "Epoch 164: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9866 - val_loss: 0.0590 - val_accuracy: 0.9881\n",
      "Epoch 165/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0475 - accuracy: 0.9846\n",
      "Epoch 165: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0432 - accuracy: 0.9866 - val_loss: 0.0589 - val_accuracy: 0.9881\n",
      "Epoch 166/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0410 - accuracy: 0.9872\n",
      "Epoch 166: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 0.9866 - val_loss: 0.0591 - val_accuracy: 0.9881\n",
      "Epoch 167/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0414 - accuracy: 0.9852\n",
      "Epoch 167: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 0.9866 - val_loss: 0.0590 - val_accuracy: 0.9881\n",
      "Epoch 168/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9844\n",
      "Epoch 168: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0397 - accuracy: 0.9844 - val_loss: 0.0591 - val_accuracy: 0.9881\n",
      "Epoch 169/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0322 - accuracy: 0.9897\n",
      "Epoch 169: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0395 - accuracy: 0.9866 - val_loss: 0.0591 - val_accuracy: 0.9881\n",
      "Epoch 170/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9844\n",
      "Epoch 170: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 0.9844 - val_loss: 0.0588 - val_accuracy: 0.9881\n",
      "Epoch 171/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0392 - accuracy: 0.9854\n",
      "Epoch 171: val_loss did not improve from 0.05870\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.9866 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 172/200\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0416 - accuracy: 0.9857\n",
      "Epoch 172: val_loss improved from 0.05870 to 0.05860, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0401 - accuracy: 0.9866 - val_loss: 0.0586 - val_accuracy: 0.9881\n",
      "Epoch 173/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0319 - accuracy: 0.9880\n",
      "Epoch 173: val_loss did not improve from 0.05860\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0387 - accuracy: 0.9844 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 174/200\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9865\n",
      "Epoch 174: val_loss did not improve from 0.05860\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0393 - accuracy: 0.9866 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 175/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0373 - accuracy: 0.9846\n",
      "Epoch 175: val_loss did not improve from 0.05860\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0334 - accuracy: 0.9866 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 176/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0412 - accuracy: 0.9844\n",
      "Epoch 176: val_loss did not improve from 0.05860\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0362 - accuracy: 0.9866 - val_loss: 0.0586 - val_accuracy: 0.9881\n",
      "Epoch 177/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0253 - accuracy: 0.9909\n",
      "Epoch 177: val_loss did not improve from 0.05860\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0385 - accuracy: 0.9866 - val_loss: 0.0589 - val_accuracy: 0.9881\n",
      "Epoch 178/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0457 - accuracy: 0.9821\n",
      "Epoch 178: val_loss did not improve from 0.05860\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0410 - accuracy: 0.9844 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 179/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0297 - accuracy: 0.9902\n",
      "Epoch 179: val_loss did not improve from 0.05860\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.9888 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 180/200\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0351 - accuracy: 0.9900\n",
      "Epoch 180: val_loss improved from 0.05860 to 0.05858, saving model to model_checkpoint\\GRU_1 layer with dropout_RMSprop.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0380 - accuracy: 0.9866 - val_loss: 0.0586 - val_accuracy: 0.9881\n",
      "Epoch 181/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0393 - accuracy: 0.9848\n",
      "Epoch 181: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9866 - val_loss: 0.0587 - val_accuracy: 0.9881\n",
      "Epoch 182/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0248 - accuracy: 0.9923\n",
      "Epoch 182: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0382 - accuracy: 0.9866 - val_loss: 0.0588 - val_accuracy: 0.9881\n",
      "Epoch 183/200\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9866\n",
      "Epoch 183: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0353 - accuracy: 0.9866 - val_loss: 0.0589 - val_accuracy: 0.9881\n",
      "Epoch 184/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0351 - accuracy: 0.9868\n",
      "Epoch 184: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 0.9866 - val_loss: 0.0590 - val_accuracy: 0.9881\n",
      "Epoch 185/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0277 - accuracy: 0.9873\n",
      "Epoch 185: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.9866 - val_loss: 0.0594 - val_accuracy: 0.9881\n",
      "Epoch 186/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0365 - accuracy: 0.9872\n",
      "Epoch 186: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.9866 - val_loss: 0.0592 - val_accuracy: 0.9881\n",
      "Epoch 187/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0296 - accuracy: 0.9872\n",
      "Epoch 187: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9866 - val_loss: 0.0592 - val_accuracy: 0.9881\n",
      "Epoch 188/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0273 - accuracy: 0.9896\n",
      "Epoch 188: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.9888 - val_loss: 0.0592 - val_accuracy: 0.9881\n",
      "Epoch 189/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0345 - accuracy: 0.9870\n",
      "Epoch 189: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0335 - accuracy: 0.9866 - val_loss: 0.0593 - val_accuracy: 0.9821\n",
      "Epoch 190/200\n",
      "76/90 [========================>.....] - ETA: 0s - loss: 0.0343 - accuracy: 0.9868\n",
      "Epoch 190: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.9888 - val_loss: 0.0593 - val_accuracy: 0.9821\n",
      "Epoch 191/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0303 - accuracy: 0.9880\n",
      "Epoch 191: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0311 - accuracy: 0.9866 - val_loss: 0.0594 - val_accuracy: 0.9821\n",
      "Epoch 192/200\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9886\n",
      "Epoch 192: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.9866 - val_loss: 0.0596 - val_accuracy: 0.9821\n",
      "Epoch 193/200\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0310 - accuracy: 0.9872\n",
      "Epoch 193: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.9866 - val_loss: 0.0595 - val_accuracy: 0.9821\n",
      "Epoch 194/200\n",
      "75/90 [========================>.....] - ETA: 0s - loss: 0.0205 - accuracy: 0.9947\n",
      "Epoch 194: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.9866 - val_loss: 0.0595 - val_accuracy: 0.9821\n",
      "Epoch 195/200\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0370 - accuracy: 0.9844\n",
      "Epoch 195: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0329 - accuracy: 0.9866 - val_loss: 0.0597 - val_accuracy: 0.9821\n",
      "Epoch 196/200\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0331 - accuracy: 0.9860\n",
      "Epoch 196: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 0.9866 - val_loss: 0.0594 - val_accuracy: 0.9821\n",
      "Epoch 197/200\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0303 - accuracy: 0.9901\n",
      "Epoch 197: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0343 - accuracy: 0.9866 - val_loss: 0.0596 - val_accuracy: 0.9821\n",
      "Epoch 198/200\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0366 - accuracy: 0.9855\n",
      "Epoch 198: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.9866 - val_loss: 0.0597 - val_accuracy: 0.9821\n",
      "Epoch 199/200\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0254 - accuracy: 0.9873\n",
      "Epoch 199: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0267 - accuracy: 0.9866 - val_loss: 0.0599 - val_accuracy: 0.9821\n",
      "Epoch 200/200\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0229 - accuracy: 0.9902\n",
      "Epoch 200: val_loss did not improve from 0.05858\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0283 - accuracy: 0.9888 - val_loss: 0.0596 - val_accuracy: 0.9821\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA6+0lEQVR4nO3dd3hc1Zn48e87MypWtyXZsi03udsYF+QOxnRMx1TjUEIC2IHwC4QEkmyCN1l2N4FNCBtKDKGEdXAogRgwmJhmmsG9yFXuKlaz1duU8/vjXomRrDKyJY00ej/PM49uOffOO3dG75w599xzxRiDUkqp7s8R7ACUUkq1D03oSikVIjShK6VUiNCErpRSIUITulJKhQhN6EopFSI0oasmich7InJre5cNJhE5KCLnd8B+jYiMsKefEZFfBlL2JJ5noYh8cLJxtrDfuSKS1d77VZ3PFewAVPsRkXK/2SigBvDa83cZY5YFui9jzLyOKBvqjDGL2mM/IjIUOACEGWM89r6XAQG/h6rn0YQeQowxMXXTInIQ+L4xZnXjciLiqksSSqnQoU0uPUDdT2oReVBEjgIviEhvEXlHRApE5Lg9neq3zSci8n17+jYR+VxEHrPLHhCReSdZdpiIrBGRMhFZLSJPisj/NRN3IDH+RkS+sPf3gYgk+a2/WUQOiUiRiPyiheMzQ0SOiojTb9nVIrLVnp4mIl+JSLGI5IrIn0QkvJl9vSgi/+E3/xN7mxwRub1R2UtFZJOIlIrIERFZ4rd6jf23WETKRWRm3bH1236WiKwTkRL776xAj01LRGSsvX2xiGSIyBV+6y4RkR32PrNF5AF7eZL9/hSLyDER+UxENL90Mj3gPUcK0AcYAtyJ9d6/YM8PBqqAP7Ww/XRgN5AE/A74i4jISZT9G/ANkAgsAW5u4TkDifEm4LtAXyAcqEsw44Cn7f0PsJ8vlSYYY9YCFcC5jfb7N3vaC9xnv56ZwHnAD1qIGzuGi+14LgBGAo3b7yuAW4AE4FJgsYhcZa+bY/9NMMbEGGO+arTvPsC7wBP2a/s98K6IJDZ6DSccm1ZiDgPeBj6wt/shsExERttF/oLVfBcLnAZ8ZC//MZAFJAP9gJ8DOq5IJ9OE3nP4gIeNMTXGmCpjTJEx5g1jTKUxpgx4BDi7he0PGWOeNcZ4gZeA/lj/uAGXFZHBwFTgV8aYWmPM58CK5p4wwBhfMMbsMcZUAa8Ck+zl1wLvGGPWGGNqgF/ax6A5rwALAEQkFrjEXoYxZoMxZq0xxmOMOQj8uYk4mnK9Hd92Y0wF1heY/+v7xBizzRjjM8ZstZ8vkP2C9QWw1xjzsh3XK8Au4HK/Ms0dm5bMAGKA/7bfo4+Ad7CPDeAGxolInDHmuDFmo9/y/sAQY4zbGPOZ0YGiOp0m9J6jwBhTXTcjIlEi8me7SaIU6yd+gn+zQyNH6yaMMZX2ZEwbyw4AjvktAzjSXMABxnjUb7rSL6YB/vu2E2pRc8+FVRufLyIRwHxgozHmkB3HKLs54agdx39i1dZb0yAG4FCj1zddRD62m5RKgEUB7rdu34caLTsEDPSbb+7YtBqzMcb/y89/v9dgfdkdEpFPRWSmvfxRIBP4QET2i8hDgb0M1Z40ofccjWtLPwZGA9ONMXF8+xO/uWaU9pAL9BGRKL9lg1oofyox5vrv237OxOYKG2N2YCWueTRsbgGr6WYXMNKO4+cnEwNWs5G/v2H9QhlkjIkHnvHbb2u12xyspih/g4HsAOJqbb+DGrV/1+/XGLPOGHMlVnPMW1g1f4wxZcaYHxtj0rB+JdwvIuedYiyqjTSh91yxWG3SxXZ77MMd/YR2jXc9sEREwu3a3eUtbHIqMb4OXCYiZ9onMH9N65/3vwH3Yn1xvNYojlKgXETGAIsDjOFV4DYRGWd/oTSOPxbrF0u1iEzD+iKpU4DVRJTWzL5XAqNE5CYRcYnIDcA4rOaRU/E1Vtv+T0UkTETmYr1Hy+33bKGIxBtj3FjHxAsgIpeJyAj7XEndcm+Tz6A6jCb0nutxoBdQCKwF3u+k512IdWKxCPgP4O9Y/eWb8jgnGaMxJgO4GytJ5wLHsU7ateQVYC7wkTGm0G/5A1jJtgx41o45kBjes1/DR1jNER81KvID4NciUgb8Cru2a29biXXO4Au758iMRvsuAi7D+hVTBPwUuKxR3G1mjKkFrsD6pVIIPAXcYozZZRe5GThoNz0tAr5jLx8JrAbKga+Ap4wxn5xKLKrtRM9bqGASkb8Du4wxHf4LQalQpzV01alEZKqIDBcRh92t70qstlil1CnSK0VVZ0sB/oF1gjILWGyM2RTckJQKDdrkopRSIUKbXJRSKkQErcklKSnJDB06NFhPr5RS3dKGDRsKjTHJTa0LWkIfOnQo69evD9bTK6VUtyQija8QrqdNLkopFSI0oSulVIjQhK6UUiFC+6Er1YO43W6ysrKorq5uvbAKqsjISFJTUwkLCwt4m4ASun1F3x8BJ/CcMea/G63/CdYYHXX7HAskG2OOBRyJUqrDZWVlERsby9ChQ2n+/iQq2IwxFBUVkZWVxbBhwwLertUmF3vs6SexBusZByyw7wbj/+SPGmMmGWMmAT8DPtVkrlTXU11dTWJioibzLk5ESExMbPMvqUDa0KcBmcaY/fZIbMuxxt9ozgLsO70opboeTebdw8m8T4Ek9IE0vOtKFg3viuIfQBRwMfBGM+vvFJH1IrK+oKCgrbECsPtoGY+u2sXxitqT2l4ppUJVIAm9qa+J5gaAuRz4ornmFmPMUmNMujEmPTm5yQudWnWgsIInP95HdnHVSW2vlAqeoqIiJk2axKRJk0hJSWHgwIH187W1LVfS1q9fz7333tvqc8yaNatdYv3kk0+47LLL2mVfnSWQk6JZNLyNVirWbaqaciMd3NySHBsOQGF5c/dEUEp1VYmJiWzevBmAJUuWEBMTwwMPPFC/3uPx4HI1nZbS09NJT09v9Tm+/PLLdom1Owqkhr4OGCkiw+xbed1IE3dqF5F4rDuW/7N9Q2woMToCgMJybXJRKhTcdttt3H///Zxzzjk8+OCDfPPNN8yaNYvJkycza9Ysdu/eDTSsMS9ZsoTbb7+duXPnkpaWxhNPPFG/v5iYmPryc+fO5dprr2XMmDEsXLiQutFlV65cyZgxYzjzzDO59957W62JHzt2jKuuuorTTz+dGTNmsHXrVgA+/fTT+l8YkydPpqysjNzcXObMmcOkSZM47bTT+Oyzz9r9mDWn1Rq6McYjIvcAq7C6LT5vjMkQkUX2+mfsolcDH9h3V+8wSbFWQi/SGrpSp+Tf385gR05pu+5z3IA4Hr58fJu327NnD6tXr8bpdFJaWsqaNWtwuVysXr2an//857zxxomn5Xbt2sXHH39MWVkZo0ePZvHixSf02d60aRMZGRkMGDCA2bNn88UXX5Cens5dd93FmjVrGDZsGAsWLGg1vocffpjJkyfz1ltv8dFHH3HLLbewefNmHnvsMZ588klmz55NeXk5kZGRLF26lIsuuohf/OIXeL1eKisr23w8TlZA/dCNMSuxbkrrv+yZRvMvAi+2V2DNiQ53EuFyaJOLUiHkuuuuw+l0AlBSUsKtt97K3r17ERHcbneT21x66aVEREQQERFB3759ycvLIzU1tUGZadOm1S+bNGkSBw8eJCYmhrS0tPr+3QsWLGDp0qUtxvf555/Xf6mce+65FBUVUVJSwuzZs7n//vtZuHAh8+fPJzU1lalTp3L77bfjdru56qqrmDRp0qkcmjbpdleKighJMREUaZOLUqfkZGrSHSU6Orp++pe//CXnnHMOb775JgcPHmTu3LlNbhMREVE/7XQ68Xg8AZU5mZv6NLWNiPDQQw9x6aWXsnLlSmbMmMHq1auZM2cOa9as4d133+Xmm2/mJz/5Cbfcckubn/NkdMuxXJJiwinQGrpSIamkpISBA62e0S+++GK773/MmDHs37+fgwcPAvD3v/+91W3mzJnDsmXLAKttPikpibi4OPbt28eECRN48MEHSU9PZ9euXRw6dIi+fftyxx138L3vfY+NGze2+2toTreroQMkxUSQW6JjUSgVin76059y66238vvf/55zzz233fffq1cvnnrqKS6++GKSkpKYNm1aq9ssWbKE7373u5x++ulERUXx0ksvAfD444/z8ccf43Q6GTduHPPmzWP58uU8+uijhIWFERMTw1//+td2fw3NCdo9RdPT083J3uDip69v4ZPdBXzzi/PbOSqlQtvOnTsZO3ZssMMIuvLycmJiYjDGcPfddzNy5Ejuu+++YId1gqbeLxHZYIxpsv9mN21yieBYRS0+n97gWinVds8++yyTJk1i/PjxlJSUcNdddwU7pHbRLZtcEmMi8PgMJVVuekeHBzscpVQ3c99993XJGvmp6qY1dCuJF1XoiVGllKrTTRO61RWpoEy7LiqlVJ1undC1hq6UUt/qfgm9poy+FbsJx01hmSZ0pZSq0/0S+p5V9H75PIY68ijSMdGV6lbmzp3LqlWrGix7/PHH+cEPftDiNnVdnC+55BKKi4tPKLNkyRIee+yxFp/7rbfeYseOHfXzv/rVr1i9enUbom9aVxpmt/sl9Jh+AAyPLNfxXJTqZhYsWMDy5csbLFu+fHlAA2SBNUpiQkLCST1344T+61//mvPPD61rWbpfQo/tD8CwyHI9KapUN3PttdfyzjvvUFNjVcYOHjxITk4OZ555JosXLyY9PZ3x48fz8MMPN7n90KFDKSwsBOCRRx5h9OjRnH/++fVD7ILVx3zq1KlMnDiRa665hsrKSr788ktWrFjBT37yEyZNmsS+ffu47bbbeP311wH48MMPmTx5MhMmTOD222+vj2/o0KE8/PDDTJkyhQkTJrBr164WX1+wh9ntfv3QY60a+pDwMtaU6l2LlDpp7z0ER7e17z5TJsC8/252dWJiItOmTeP999/nyiuvZPny5dxwww2ICI888gh9+vTB6/Vy3nnnsXXrVk4//fQm97NhwwaWL1/Opk2b8Hg8TJkyhTPOOAOA+fPnc8cddwDwb//2b/zlL3/hhz/8IVdccQWXXXYZ1157bYN9VVdXc9ttt/Hhhx8yatQobrnlFp5++ml+9KMfAZCUlMTGjRt56qmneOyxx3juueeafX3BHma3+9XQI2IhLJqBrhJyinU8F6W6G/9mF//mlldffZUpU6YwefJkMjIyGjSPNPbZZ59x9dVXExUVRVxcHFdccUX9uu3bt3PWWWcxYcIEli1bRkZGRovx7N69m2HDhjFq1CgAbr31VtasWVO/fv78+QCcccYZ9QN6Nefzzz/n5ptvBpoeZveJJ56guLgYl8vF1KlTeeGFF1iyZAnbtm0jNja2xX0HovvV0AFi+9FPijlWUUtVrZde4c5gR6RU99NCTbojXXXVVdx///1s3LiRqqoqpkyZwoEDB3jsscdYt24dvXv35rbbbqO6uuUKm0hTtzu27oD01ltvMXHiRF588UU++eSTFvfT2nhWdUPwNjdEb2v76sxhdrtfDR0gJoXePus+1Dkl2uyiVHcSExPD3Llzuf322+tr56WlpURHRxMfH09eXh7vvfdei/uYM2cOb775JlVVVZSVlfH222/XrysrK6N///643e76IW8BYmNjKSsrO2FfY8aM4eDBg2RmZgLw8ssvc/bZZ5/Uawv2MLvdtIaeQvRx68XnFFcxPDkmyAEppdpiwYIFzJ8/v77pZeLEiUyePJnx48eTlpbG7NmzW9x+ypQp3HDDDUyaNIkhQ4Zw1lln1a/7zW9+w/Tp0xkyZAgTJkyoT+I33ngjd9xxB0888UT9yVCAyMhIXnjhBa677jo8Hg9Tp05l0aJFJ/W6gj3MbrccPpf3f4Zvw0uklT3Lb6+ZwA1TB7dvcEqFKB0+t3vpEcPnEtMPh7uCGKkiW0+MKqUU0F0TemwKAGNjqsgp1jZ0pZSCABO6iFwsIrtFJFNEHmqmzFwR2SwiGSLyafuG2Yh9tejo6ApN6Eq1UbCaWVXbnMz71GpCFxEn8CQwDxgHLBCRcY3KJABPAVcYY8YD17U5krawrxYdHlmuCV2pNoiMjKSoqEiTehdnjKGoqIjIyMg2bRdIL5dpQKYxZj+AiCwHrgT8e/3fBPzDGHPYDia/TVG0lX216KCwEnJKqjHGNNsnVSn1rdTUVLKysigoKAh2KKoVkZGRpKamtmmbQBL6QOCI33wWML1RmVFAmIh8AsQCfzTGnNAHR0TuBO4EGDz4FHqmRCaAM4IURwm1Hh9FFbX1Y6QrpZoXFhbGsGHDgh2G6iCBtKE3VfVt/HvNBZwBXApcBPxSREadsJExS40x6caY9OTk5DYH+21EArH96GOsi4uyj2uzi1JKBVJDzwIG+c2nAjlNlCk0xlQAFSKyBpgI7GmXKJsSO4A4t/WzMV9vdKGUUgHV0NcBI0VkmIiEAzcCKxqV+Sdwloi4RCQKq0lmZ/uG2kjcACKrrKb6/DLti66UUq0mdGOMB7gHWIWVpF81xmSIyCIRWWSX2Qm8D2wFvgGeM8Zs77iwgbgBOMtzETHkl2oNXSmlAhrLxRizEljZaNkzjeYfBR5tv9BaETcQ8VQxtFctBXrnIqWU6qZXigLEDQBgTHSZ1tCVUopundAHAjA8opQCbUNXSqnunNCtGvqQsGIKtJeLUkp144Qe0w/EwQDHMQrKa/RSZqVUj9d9E7rTBTH9SPYV4fYajle6gx2RUkoFVfdN6ABxA+jttS4u0mYXpVRP1+0TekyNXlyklFLQ7RP6QCIqjwJo10WlVI/XzRP6ABzucmKo1PFclFI9XjdP6FZf9LTwEm1DV0r1eN07ocdbg7+P6VWsbehKqR6veyf0BOsmGSMjirUNXSnV43XvhB6TAo4w0sKKyNZ7iyqlerjundAdDkgYxEAKOFpajcfrC3ZESikVNN07oQMkDCbJm4fXZzhaqu3oSqmeKyQSely1dUe8LL23qFKqBwuJhB5eXUgEtZrQlVI9Wggk9CEApEoB2ZrQlVI9WAgkdKvr4vioErKOVwY5GKWUCp6QSejjoo5r10WlVI8WUEIXkYtFZLeIZIrIQ02snysiJSKy2X78qv1DbYbdF3142DFtQ1dK9Wiu1gqIiBN4ErgAyALWicgKY8yORkU/M8Zc1gExtszui54qBeSWVOH1GZwO6fQwlFIq2AKpoU8DMo0x+40xtcBy4MqODauNEobQ152L22t0TBelVI8VSEIfCBzxm8+ylzU2U0S2iMh7IjK+qR2JyJ0isl5E1hcUFJxEuM3ok0ZcdRYAR45ps4tSqmcKJKE31X7R+I7MG4EhxpiJwP8CbzW1I2PMUmNMujEmPTk5uU2BtqhPGmG1JcRTTmZ+efvtVymlupFAEnoWMMhvPhXI8S9gjCk1xpTb0yuBMBFJarcoW9MnDYBxEYXsyC3ptKdVSqmuJJCEvg4YKSLDRCQcuBFY4V9ARFJEROzpafZ+i9o72GbZCX1G7xIycko77WmVUqorabWXizHGIyL3AKsAJ/C8MSZDRBbZ658BrgUWi4gHqAJuNMY0bpbpOL2HAsLEXoU8k1WmPV2UUj1Sqwkd6ptRVjZa9ozf9J+AP7VvaG0QFgnxqQxz5FHl9nKwqILhyTFBC0cppYKh+18pWqfPMJLdVtO+NrsopXqiEEroafQqP0SYU9ihCV0p1QOFVEKXyiImJQkZOdrTRSnV84RUQgeYk1zO5iPFeH2dd05WKaW6ghBK6MMBmBl/jLJqjza7KKV6nBBK6GkgDsaG5QHw1f7CIAeklFKdK3QSelgkJAwmumw/acnRfLWv865rUkqpriB0EjpA0igozGRmWiLrDh7H4/UFOyKllOo0oZfQizKZmdab8hoP27K1t4tSqucIrYSeOAI8VZyRUAHAztyyIAeklFKdJ7QSetIoAPrWHMbpELKL9abRSqmeIyQTuvNYJv3jI/Ueo0qpHiW0Enp0EkTGQ+FeBib0IlsTulKqBwmthC5i93TZw8Devcgu1oSulOo5QiuhAySNhoJdpPaOIq+0mlqPdl1USvUMoZfQ+42HigKG96rAZ+BoSXWwI1JKqU4Rggl9HADDzWEAsrSni1Kqhwi9hN53PAADavYD6IlRpVSPEXoJPSYZovsSX7YHEfTEqFKqxwi9hA7QbxzO/B30jY3QvuhKqR4jNBN63/FQsItB8eHa5KKU6jECSugicrGI7BaRTBF5qIVyU0XEKyLXtl+IJ6HfePBUMzGmmCPH9aSoUqpnaDWhi4gTeBKYB4wDFojIuGbK/RZY1d5Btpnd02VKRDbZxVVU1XqDHJBSSnW8QGro04BMY8x+Y0wtsBy4solyPwTeAPLbMb6TkzwWHC5GmwMYA5n55cGOSCmlOlwgCX0gcMRvPsteVk9EBgJXA8+0tCMRuVNE1ovI+oKCgrbGGriwSEgeS//K3QDsydNhdJVSoS+QhC5NLDON5h8HHjTGtNi2YYxZaoxJN8akJycnBxjiSRowkaii7YQ7RRO6UqpHcAVQJgsY5DefCuQ0KpMOLBcRgCTgEhHxGGPeao8gT0r/Scim/2NaYrUmdKVUjxBIDX0dMFJEholIOHAjsMK/gDFmmDFmqDFmKPA68IOgJnOA/hMBODsmmz152oaulAp9rSZ0Y4wHuAer98pO4FVjTIaILBKRRR0d4EnrdxqIg4muQ2QXV1FR4wl2REop1aECaXLBGLMSWNloWZMnQI0xt516WO0gPAqSRjPUnQmcw978ciYNSgh2VEop1WFC80rROv0n0qckAzBk5JQEOxqllOpQoZ3QU9NxVeZzRnw5q3fkBTsapZTqUCGf0AG+k5rP55mFlFa7gxyQUkp1nNBO6P1OA1cksyMP4PYaPtyptXSlVOgK7YTuDIMBk0ku3kb/+EhWbjsa7IiUUqrDhHZCB0hNR45u4bJxiXy6u4CSSm12UUqFph6Q0KeCt5YbBxdT6/Xx9tbGF7kqpVRo6BkJHUir2s7ofrG8sTEryAEppVTHCP2EHjcAEgYjR9Zy7RmpbDpczL4CHQpAKRV6Qj+hAwyeBYe+4spJ/XE6hDc2aC1dKRV6ekhCnwGVhfStzebsUcm8uSkbr6/xCMBKKdW99YyEPmSW9ffwl1wzJZXckmq+3FcY3JiUUqqd9YyEnjQKohLh8FrOG9uXuEiXNrsopUJOz0joIjB4Jhz6gsgwJ5dPHMD7GUep8ejNo5VSoaNnJHSAoWfB8YNw7ABnjUyi2u0jI6c02FEppVS76TkJfcT51t99HzJlcG8ANh46HsSAlFKqffWchJ44HBIGQ+ZH9I2LZFCfXmw8rAldKRU6ek5CF7Fq6Qc+BU8tZwzuzYZDxzFGuy8qpUJDz0noAMPPg9pyOPI1U4b0Jq+0huziqmBHpZRS7aJnJfRhc8DhatCOvkHb0ZVSISKghC4iF4vIbhHJFJGHmlh/pYhsFZHNIrJeRM5s/1DbQWQcDJoBmasZkxJLTISLLzOLgh2VUkq1i1YTuog4gSeBecA4YIGIjGtU7ENgojFmEnA78Fw7x9l+RpwLR7fhqixg7uhkPtyVp8MAKKVCQiA19GlApjFmvzGmFlgOXOlfwBhTbr49uxgNdN0MWd998SMuGp9CYXmt9nZRSoWEQBL6QOCI33yWvawBEblaRHYB72LV0k8gInfaTTLrCwoKTibeU9dvAkT3hczVzB2dTLjTwartems6pVT3F0hClyaWnVADN8a8aYwZA1wF/KapHRljlhpj0o0x6cnJyW0KtN04HDD8XNj3EbHhDmaNSGTVjqPafVEp1e0FktCzgEF+86lAs/dxM8asAYaLSNIpxtZxRl8MVcfg0JfMOy2FI8eq2JZdEuyolFLqlASS0NcBI0VkmIiEAzcCK/wLiMgIERF7egoQDnTd7iMjLgBXJOxcwcWn9Sfc6eDNTdnBjkoppU5JqwndGOMB7gFWATuBV40xGSKySEQW2cWuAbaLyGasHjE3mK7chhERY50c3fk28RFOzhvbl7e35ODx+oIdmVJKnTRXIIWMMSuBlY2WPeM3/Vvgt+0bWgcbewXsegey13PV5MG8t/0on2UWcs7ovsGOTCmlTkrPulLU3+iLwREGGW8yd3QySTHh3PvKJl768qCeIFVKdUs9N6FHxltJfeurRIiP5XfOZNKgBB5ekcGX+7pu879SSjWn5yZ0gEnfgcpC2PsBI/rG8Owt6STHRvDMp/uCHZlSSrVZz07oI863LjLatAyAyDAn3ztzGJ/tLWRblnZjVEp1Lz07oTtdMPEG2LsKyq0rVxdOH0xspIs/r9FaulKqe+nZCR1g0kLweWDbawDERoZx49RBvLf9KLklOla6Uqr70ITedywMmAKbl4Hdu+WWmUMxxrBs7eEgB6eUUoHThA4w6SbI2w5HtwIwqE8U54/tx9++OUxVrTfIwSmlVGA0oQNMuBacEbDxr/WL7pyTxrGKWn79zo4gBqaUUoHThA7QqzecNh+2LIdqq3dL+tA+LDp7OK98c5i3tzQ7FplSSnUZmtDrTL/LuoH05r/VL/rxhaM4Y0hvfvL6Fu3GqJTq8jSh1xkwGQZNh6//DD5rkK4wp4NnvnMGidER3P7SOorKa4IcpFJKNU8Tur/pi+D4Adj5z/pFybERPHtLOoXlNbz01aEgBqeUUi3ThO5v3JWQNAo+/V19LR1g3IA4zh3dl2VrD1Ht1l4vSqmuSRO6P4cT5vwU8nfArrcbrPremcMoqqjln5v1RhhKqa5JE3pjp82HxJEn1NJnDk9kTEosz3+uw+sqpbomTeiNOZww5yfWhUa7361fLCJ878xh7M4r44tMHV5XKdX1aEJvymnXQJ/h8Olv64cDALh84gCSYsL5y+f7gxicUko1TRN6U5wuOPuncHQbZPyjfnFkmJPvzBjCx7sLWLB0LS9+cSCIQSqlVEOa0Jsz4TroNwFWLwF3df3iW2cO5cJx/Sgor+Hf39lBRo5ecKSU6ho0oTfH4YSL/gOKD8Pap+oX944OZ+kt6byxeBbxvcL4r5W79CSpUqpLCCihi8jFIrJbRDJF5KEm1i8Uka3240sRmdj+oQZB2lwYc5nV4+VYw+aV+F5h/PDckXyeWciqjLzgxKeUUn5aTegi4gSeBOYB44AFIjKuUbEDwNnGmNOB3wBL2zvQoJn3O3C44O17G5wgBbh5xhDGD4jjoX9sJadYb4ahlAquQGro04BMY8x+Y0wtsBy40r+AMeZLY8xxe3YtkNq+YQZR/EC48NdwYA1sernBqnCXgz/dNAW3x8fi/9tASaU7SEEqpVRgCX0gcMRvPste1pzvAe81tUJE7hSR9SKyvqCgIPAog23KbTDkTFj1b1Ca22DVsKRo/nDDJHbmlnH9n78iv7S66X0opVQHCyShSxPLmjwLKCLnYCX0B5tab4xZaoxJN8akJycnBx5lsDkccMUT4K2Bd+8/oenlwvEpvPjdqRw5XsmdL2+gxqPjvSilOl8gCT0LGOQ3nwqccMcHETkdeA640hgTepdSJg6Hc38Ju1fChhdPWD1rRBK/v34im48Us/DZr7njr+t58PWtvLEhq/NjVUr1SIEk9HXASBEZJiLhwI3ACv8CIjIY+AdwszFmT/uH2UXM+AGknQPv/wzyd52w+uLT+vPzS8aQW1LN4aJKVu/M48evbWHZ1zrsrlKq40kgfahF5BLgccAJPG+MeUREFgEYY54RkeeAa4C6zOUxxqS3tM/09HSzfv36U4k9OEpz4c9zICIG7vjIun1dM7w+w+0vruPLfYX86aYpXDQ+pRMDVUqFIhHZ0Fx+DSihd4Rum9ABDn8NL10GQ2bDwtetoQKaUVLp5oalX7HraBkXjOvH0wun4HLq9VxKqZPTUkLXzHIyBk+HS38P+z+Gf/2yxaLxUWGsuOdMfnT+SP61I4/3th/tpCCVUj2NJvSTNeVm65Z1a5+Cb55tsWi4y8G9544kLSmaP6/Zp0MFKKU6hCb0U3HhIzD6Elj5AGx8ucWiDodw55w0tmeX6njqSqkOoQn9VDhdcN2LMPw8WPFD2Ppai8WvmjyQlLhIfvzaZvYVlHdOjEqpHkMT+qlyRcAN/wdDz4Q374LtbzRbNDLMyUu3T8PjNdzw57V8vDufareXLUeKWf7NYbYcKe68uJVSIUd7ubSXmnJYdh0cWQuXP2G1sTcjM7+Mxf+3kb355TgdgtdnvQdOh/Bvl47ltllDEWnqAl2lVE/XUi+X5vvbqbaJiIHvvAF/Xwgr7oFj+60rSx0n/gga0TeWd+49k79+eYiSKjenDYxjRN8Yfvv+bv797R1Eh7u4fuqgJp5EKaWapzX09uaphfd+ChtegFHzYP5SiIwLaFOfz7Dwua/ZklXMu/eexbCk6Pp1xhittSultB96p3KFw2V/gEseg70fwHPnQ96OgDZ1OITf3zCRMKeDS/74Gbc8/w1788r4en8Rs/77Iz7d041GqFRKdTqtoXek/Z/CG9+HmlK46D8h/XYIoJadkVPCa+uzeHtLDm6vDwOUVXsY1z+Od+89U2vqSvVgWkMPlrSzYfEX1hAB794Pr94Mlcda3Wz8gHiWXDGet+6eTVJsBLERLh64cBQ7ckv5YEceZdV6Iw2l1Im0ht4ZfD746k/w4b9DTApc/TQMmxPQprUeH26vjwiXg3P/51MOH6sE4P+dN5L7LhgFQHZxFRnZJVyog38pFfK0hh5sDgfMvhe+94HVxv7S5fCPO6E8v9VNw10OoiNcuJwO/nDDRBadPZyLxvfjjx/u5dV1Rygoq+GGP3/FnS9vYO1+vQJVqZ5Ma+idrbYSPvsf+OKPEB4Fc39mta27IgLfhcfHLc9/zdr9x4hwORCB2MgwkmMiePuHZ+L2+vhwZz6np8YzqE9UB74YpVRn0+Fzu6LCvdYYMPs/gfjBcM7P4PQbwOEMaPNqt5cVW3JYtf0o35kxhLIaD/e+somRfWMoqqjlWEUt6UN689qimYgImfnl/H3dYe67YBRR4Xr5gVLdlV5Y1BUljYSb37KG4F397/DWYljzGMy8GybdBGG9Wtw8MszJ9emDuD7dugDJGMPevDIyckoZ2z+O3lFhvPTVIb7aV8SgPlEsfG4teaU1DEjoxXdnD+uEF6iU6mxaQ+8KjIGdb8Pnf4CcjRCVCNPuhKnfh+ikk9pltdvLnN99TK9wJ6VVbnwGkmMjcHt9vHbXTF7bkMXNM4cQFxnWzi9GKdWRtMmluzAGDn0JXz4Be94HVyScdi2kfxcGnhFQH3Z/L391kF/+M4Nzx/TlpxePZm9eOT98ZRPxvcIoqXJz1sgknr9tKmF6ByWlug1N6N1RwW7r5hlbXwN3BaRMsE6eTrgOImID2oUxhuOVbvpEhwPg9vqY++gnVNR6uGHqIP786X4GxEcC8PR3zmDioISOejVKqXaiCb07qy6Fba/B+uchbzu4esHI82HcVTDqooCTe53s4irCnELf2Ehe/uogn2cWsu7gcYYlRfO6fQK1oKyGgrIaxvaPJet4lT2AWHzHvD6lVJucckIXkYuBPwJO4DljzH83Wj8GeAGYAvzCGPNYa/vUhN5GxkDWOtj6KuxcAeV54IyAEefDuCth9MUQeXJJ95VvDvOzf2xjyeXjEBEeW7WbshoPAxN6kVNShQB/umkKl0zo376vSSnVZqeU0EXECewBLgCygHXAAmPMDr8yfYEhwFXAcU3oHcznhSPfwI5/Wo+yHBAnDJhsXYE67CwYNMPq5x4Aj9fHvD9+xt586y5KM9L6cOmE/ny4K5/TB8bzxb4ithwp5vtnpTF3dDIA27NLKCiv4fLTB/Dmpmw+3pXP8rtm0Dc2ssNetlLq1BP6TGCJMeYie/5nAMaY/2qi7BKgXBN6J/L5IHs97FkFBz+D7A3g84AjDFLTYehZVpJPnQphzSfb7OIqduSUkhIXyfgBcTgc356ALat288BrW1i9M7/+ZhxAg5tziMDis4fznRlDeHntIW6dOZSU+EhqPF4iXIH1rVdKte5UE/q1wMXGmO/b8zcD040x9zRRdgktJHQRuRO4E2Dw4MFnHDp0qC2vQwWiphwOr4WDa+DAZ5C7GYzPap4ZNM2uwc+BAVOsYQjaoLC8hoycUhwCI/rGEOFy8vaWHEb1i+XltQf5bG8hQxKj2J5dSkJUGAMTepGRU8riucN54MLROB06SqRSp+pUE/p1wEWNEvo0Y8wPmyi7BK2hdy3VJVZXyAOfWUn+6HbAWCdXUyZA/4nWY8AkSB4DzpPrl77lSDFXPvkFAA9fPo5VGUepqPGS2rsX720/yrj+cVyfnsrlEweQGBPYMAd6Uw+lTnSqV4pmAf73Q0sFctojMNUJIuNh9DzrAdbwvQc/t5J87hbY8gqse9Za5wyHfuO/TfL9J0Lf8S021dSZOCiB22cPY1Af60pU/6tR39yUxbNrDrDk7R38x7s7uei0FO44K43X1h8h63gVC6cP5ryx/RrU4LOOV3Lj0rXMn5LK/faokkqplgVSQ3dhnRQ9D8jGOil6kzEmo4myS9Aaevfi81n3P83dbCX4ukd1sbVenJA8GpJGWTX4ZPtv4og2DSgGsOtoKW9syGLZ14eprPXicgiJMeHkldaQHBvBlMEJVNZ6mZiawOqdeew6WoYIvHrXTKYO7UNlrYfP9xYyc3gisXqFq+qh2qPb4iXA41jdFp83xjwiIosAjDHPiEgKsB6IA3xAOTDOGFPa3D41oXdhxkDx4W+Te95260Kn4wcB+/MiDug9zEr2fdIgYQgkDIbe9t/w6GZ3n19azYotOZw9KplhSdH8a0ce/9ycw578MnqFOdmZa31snrxpCv/53k4E4Z93z+bX7+zgzU3ZRLgc3HfBKBadPZy9eVbSH9G3bf3xlequ9MIi1T7cVVCUaSX3gt1QuBsK9liJ3lPVsGxUUsMEnzDk26SfMLjFZpz80mqOVdYyJiWODYeOseDZr0mJi+TwsUpumj6Y/NIaVu/MY+H0wby2IYtaj4+5o5N55OoJDExoeVAzpbo7TeiqYxkDFQVw/BAU1z0O2/OHoeQIeGsbbhPT78RaffwgiBsIcf0hIq5+7Jr3tx/lB8s2MCYljrfung3AbS98w5f7ipg6tDdzRiazdM1+IsIcPHDhaCLDnGw6fJzSag+D+0ThcgijU2L1jk4qJGhCV8Hl80H50W8TfF3Sr0/4WWC8DbcJi4a4AdZok736UGSi6RWfTFR8MvTqQ2VYAl/lOThz4lgi4pLJLHHw/b+u52CRdYu+XmFOekeFkVNSXb/LH18witMHJbC/oByvzxAb6UJEOFhYQZ/ocCYMjOeMIb2p9frIL61haFLzzUZKBYsmdNW1eT3W1a7FR6AsF0pzrEdZDlQUQdUxq3dO1bETa/p1xImJjMcdHo83IoGI2D44ovrgjeyNL7I3K3ZX8XmWm1KiKDNRlBJNqYmilCiqHb3w+KxfA3GRLqrdPmq9PmamJfKj80cyPS2Ruv+Tum6Uxhg2HSlmwsB4Ha1SdSpN6Co0GAO1FXaCL4KKQuu+rFXHv31UFzecrzwONSUt7xbBRMRR7Yyh2NcLtysWb0QcO44JBe4IXNG9ya0Op9oZQ9qgAcyfOY4NeT5+8f4Rzp04gl9cM4P1h8sYnRJbP7JlZn45W44Uc/nEAYS7NOGr9qMJXfVsXjdUFUNNqZXwq0utC66qS+xlJU0uM1XF1FYWE+Epb/UpKkwEZURDRAxR0bHsO+6h3BuGhEWSnBBLn7hYkhLiKKcXWVXhDOzfn7jYWGuIBmcYOFz23zBwur5d7gxvft0J8642j5mvuh+9BZ3q2ZxhEJNsPdpAgAiwBkOrKYOaUp5+fyOfbMkkViq5Y2ofcvPzqSgpYmqKk4rSY+QVFOCqqiHe5WBckoOSsuN4C3OpLHRzzOUlwlvJWKmC3R3xQrGSemtfEg2W182H218g9nKHy7q/bd36uvn66br5lta7rOeVul8oYn/h2F86ddP1X0JiNanVPcRprastt96DuvIOl3UNRN2XnTiaeGD9ojPGGvoC+2+D+cbT9ry31voF6HNbMTjsNGl8fg/TaN7XcF8n/LXVLRt2tjVCajvThK5UaxxO6JUAvRL4zlX9eWHfp7gcwuTLz2F6o+aU/LJqln9zhCsmDiAxKZpEoKrWy2sbjvCHf+1h4tAEfjBnKF9s38era/fixEu4eHEaL1MHxZIQCYfySygsqcAlHsLwEuHwctMZAzh7RII18JrXzfHyCj7KyCE2zDAiKZJB8WGE4bWSkNddX86a9/gtbzzvsZqxvMftbexk6vNa8/WPRvPGF4x3oosS+0vE/wuqib/w7XREbIckdG1yUaqNsour8PkMg/oENjxxHZ/PNBjFcs2eAt7YmMXd54xgzZ4C/vRxJvG9whjVL5ZpQ/swdVgf3F4fT3+yj4925XPNlFSuT08ls6Ccx1btptrtw+sz1Hp9RLgc3DxjCPddMIroiIb1tKMl1eSVVnN6anz9SV2fz7Dsm8P87evD/PrK8Uwd2qdtB8Hna5Twm0j6dV8qTdZW/ZbBt9POcGsgOVe4tT/jg/AY60u1rozPA56ab794mqs5+9fWxcGJideebrzO4bJ6VznDv30d9esbP/x/YXQObUNXqhvz+gy/e38XL3x5kFqPVTMekxLLUwunkBIfydcHjvHOllze2JhFuMtBrzAnYU7B6RAcIuTaXTcXTBvMzOGJvLs1hx25pRw5VkVkmAOnCBeM68fHuwv4z6snMGVIAo++v5vk2Ahmj0hi6tA+9Apv2xDI+aXVfLK7gPlTBuLSXkDtShO6UiGgpNLN55mFjOwXw8i+MSeMRLnh0HHe25aL2+vD7TN4vD48PsPw5BiKK2t59rMDAAxM6MXpqfFcMK4fs4YnccPSr8grraZfXCS5JdUkx0RQWF6DzxjcXkO4y8GPLxjFzOGJ/GDZRsYPiOPCcSl8kVlI/4RIZg1PorC8hq/2FbEnr4wx/eNYuS2X4ko3v7v2dK5Pt8b2yy+rprTKzdDEaE3yp0ATulKKj3fl43AIZ41IatD0U1HjwWcMHq9h/tNfUlRew1+/N51R/WL45sAxln19mH/tyMPlEJJiIqio8VBW4yEhKozSKjd19zyJjXAxsl8MO3JLGds/jooaD26v4W93TOe/Vu5i5bZcPD5DrzAnZ49K5pozUjl/bF/A+hXSOMl/vDuff2zM5swRicyb0J+4RgOy7Ssop3dUeH1X0dYYY5p8nu5GE7pSKiBl1W6q3N4GtxL0+QyPr97DpiPF/M91Ewl3OThQWMGEgfEUV7nZmVtK39hIhiVFE+5y1J8reG9bLouXbSQq3IkxcNP0wYztH8eWI8W8n3GUgrIahidHU1Llpqiilj5R4bicQr+4SC4an8IfV+8FgVqPj4SoMK6ZkkpxpRu310deaTVfHzhGSlwk/3HVaazKOEp5jYczhvSmtMrN5CG9OWe09WVR7fbyj43ZPPvZfrw+wxuLZxHfK4yKGg+9o8OpqvVSUuUmJb573D5RE7pSqtP5fIZL//dzyqrdPHtLOmP7x9Wv83h9/HNzDq98c5hBfaIY1CfKaubxGdYdPMa+ggrGpMSy/M4ZHCis4A+r9/LZ3gL6xUYSEeYgzOng0gn9eW39EXJKqokMc9A7Krz+fIFD4I83Tia3pIqlaw5QWF7DaQPjyMwvJy0phrIaN9nHqzh7VDJbs0ooqqjlstP7M6pfLLUeH2eNTKJ/fC8MhkG9oxr8omnrMSgsr8FrDP3j22fgOE3oSqmgqKz14HRIm+4r6/UZ1uwtYPKgBBKiwhssb3wbw7zSat7clM3VkwfSNzaCwvJawl0Obnn+G7YcKQbgrJFJLJ47nJlpiazKOMriZRsZnhzDnJHJrNiSw7gBcYxNieWlrw5S7fY1uFcuWE1JQ5KiiI0Io7jKTa8wB6NTYjlzRDJnj04mxu5VtOtoKa+vz6KwvIZwl4MhidG8uSmbTPvm64vnDufBi8ec7KGspwldKdWjFJbX8MfVe5k3IYVZw5MarDtcVElKfOQJQzJU1XoRAY/P8EVmIeXVHjw+H9uyS8g6XkV5tXXeoLzGw46cUkqrPcRGupg/eSAZOaWsP3ScCJeDfnGRlNd4OFZRy5iUWK5PH8T6Q8d4b/tRlt8xA7fX0DcuglH9Tm4Mf03oSinVjjxeHxsOHef5Lw6wKiOPMSmxXDV5IDdOHURCVDjGGI5V1NInOhwRobzGw0V/WEN2sXXfgJtnDOE3V512Us+tl/4rpVQ7cjkdTE9LZHpaItVuL5FhDZuURKTBzdBjIlz86abJvL4hi7NHJXPWyLYNQxFwXB2yV6WU6iEaJ/PmTB7cm8mDe3doLN27Q6ZSSql6mtCVUipEaEJXSqkQEVBCF5GLRWS3iGSKyENNrBcRecJev1VEprR/qEoppVrSakIXESfwJDAPGAcsEJFxjYrNA0bajzuBp9s5TqWUUq0IpIY+Dcg0xuw3xtQCy4ErG5W5EvirsawFEkSkfzvHqpRSqgWBJPSBwBG/+Sx7WVvLICJ3ish6EVlfUFDQ1liVUkq1IJCE3tSoNI0vLw2kDMaYpcaYdGNMenJyx3SsV0qpniqQC4uygEF+86lAzkmUaWDDhg2FInIokCCbkAQUnuS2Ha2rxqZxtU1XjQu6bmwaV9ucbFxDmlsRSEJfB4wUkWFANnAjcFOjMiuAe0RkOTAdKDHG5La0U2PMSVfRRWR9c2MZBFtXjU3japuuGhd03dg0rrbpiLhaTejGGI+I3AOsApzA88aYDBFZZK9/BlgJXAJkApXAd9szSKWUUq0LaCwXY8xKrKTtv+wZv2kD3N2+oSmllGqL7nql6NJgB9CCrhqbxtU2XTUu6LqxaVxt0+5xBW08dKWUUu2ru9bQlVJKNaIJXSmlQkS3S+itDRTWiXEMEpGPRWSniGSIyP+zly8RkWwR2Ww/LglCbAdFZJv9/OvtZX1E5F8istf+27Ej7Tcd12i/47JZREpF5EfBOGYi8ryI5IvIdr9lzR4jEfmZ/ZnbLSIXdXJcj4rILnvguzdFJMFePlREqvyO2zPN7rhj4mr2feus49VCbH/3i+ugiGy2l3fKMWshP3TsZ8wY020eWN0m9wFpQDiwBRgXpFj6A1Ps6VhgD9bgZUuAB4J8nA4CSY2W/Q54yJ5+CPhtF3gvj2JdJNHpxwyYA0wBtrd2jOz3dQsQAQyzP4POTozrQsBlT//WL66h/uWCcLyafN8683g1F1uj9f8D/Kozj1kL+aFDP2PdrYYeyEBhncIYk2uM2WhPlwE7aWL8mi7kSuAle/ol4KrghQLAecA+Y8zJXi18Sowxa4BjjRY3d4yuBJYbY2qMMQewrreY1llxGWM+MMZ47Nm1WFdid6pmjldzOu14tRabiAhwPfBKRz1/MzE1lx869DPW3RJ6QIOAdTYRGQpMBr62F91j/zx+PhhNG1jj6HwgIhtE5E57WT9jX71r/+0bhLj83UjDf7JgHzNo/hh1pc/d7cB7fvPDRGSTiHwqImcFIZ6m3reudLzOAvKMMXv9lnXqMWuUHzr0M9bdEnpAg4B1JhGJAd4AfmSMKcUaC344MAnIxfq519lmG2OmYI1Tf7eIzAlCDM0SkXDgCuA1e1FXOGYt6RKfOxH5BeABltmLcoHBxpjJwP3A30QkrhNDau596xLHy7aAhhWHTj1mTeSHZos2sazNx6y7JfQ2DwLWkUQkDOvNWmaM+QeAMSbPGOM1xviAZ+nAn5rNMcbk2H/zgTftGPLEHqPe/pvf2XH5mQdsNMbkQdc4ZrbmjlHQP3cicitwGbDQ2I2u9s/zInt6A1a766jOiqmF9y3oxwtARFzAfODvdcs685g1lR/o4M9Yd0vo9QOF2bW8G7EGBut0dtvcX4Cdxpjf+y33v7HH1cD2xtt2cFzRIhJbN411Qm071nG61S52K/DPzoyrkQa1pmAfMz/NHaMVwI0iEiHWIHUjgW86KygRuRh4ELjCGFPptzxZrDuKISJpdlz7OzGu5t63oB4vP+cDu4wxWXULOuuYNZcf6OjPWEef7e2As8eXYJ0x3gf8IohxnIn1k2grsNl+XAK8DGyzl68A+ndyXGlYZ8u3ABl1xwhIBD4E9tp/+wTpuEUBRUC837JOP2ZYXyi5gBurdvS9lo4R8Av7M7cbmNfJcWVita/Wfc6escteY7/HW4CNwOWdHFez71tnHa/mYrOXvwgsalS2U45ZC/mhQz9jeum/UkqFiO7W5KKUUqoZmtCVUipEaEJXSqkQoQldKaVChCZ0pZQKEZrQlVIqRGhCV0qpEPH/AWhrPXU3Fsx/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_26 (GRU)                (None, 32)                11520     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,553\n",
      "Trainable params: 11,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1105 - accuracy: 0.9643\n",
      "Test Loss: 0.11047140508890152\n",
      "Test Accuracy: 0.9642857313156128\n"
     ]
    }
   ],
   "source": [
    "# This code illustrates the implementation of an improved GRU model with dropout regularization and Root Mean Square Propagation optimization.\n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_1 layer with dropout_RMSprop.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,input_shape=(None, x_train.shape[-1])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with RMSprop optimizer\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=200, validation_data=(x_val, y_val), callbacks=[callbacks_list])\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 2 layers SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.6829 - accuracy: 0.5571\n",
      "Epoch 1: val_loss improved from inf to 0.67686, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 9s 24ms/step - loss: 0.6817 - accuracy: 0.5625 - val_loss: 0.6769 - val_accuracy: 0.5893 - lr: 0.0010\n",
      "Epoch 2/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.6680 - accuracy: 0.6429\n",
      "Epoch 2: val_loss improved from 0.67686 to 0.66378, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.6683 - accuracy: 0.6429 - val_loss: 0.6638 - val_accuracy: 0.6488 - lr: 0.0010\n",
      "Epoch 3/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.6556 - accuracy: 0.7091\n",
      "Epoch 3: val_loss improved from 0.66378 to 0.65128, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.6555 - accuracy: 0.7098 - val_loss: 0.6513 - val_accuracy: 0.7440 - lr: 0.0010\n",
      "Epoch 4/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.6449 - accuracy: 0.7704\n",
      "Epoch 4: val_loss improved from 0.65128 to 0.63917, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.6433 - accuracy: 0.7768 - val_loss: 0.6392 - val_accuracy: 0.7560 - lr: 0.0010\n",
      "Epoch 5/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.6315 - accuracy: 0.7857\n",
      "Epoch 5: val_loss improved from 0.63917 to 0.62747, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.6315 - accuracy: 0.7857 - val_loss: 0.6275 - val_accuracy: 0.7738 - lr: 0.0010\n",
      "Epoch 6/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.6181 - accuracy: 0.8200\n",
      "Epoch 6: val_loss improved from 0.62747 to 0.61608, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.6201 - accuracy: 0.8058 - val_loss: 0.6161 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Epoch 7/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.6080 - accuracy: 0.8250\n",
      "Epoch 7: val_loss improved from 0.61608 to 0.60500, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.6090 - accuracy: 0.8192 - val_loss: 0.6050 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 8/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.5978 - accuracy: 0.8270\n",
      "Epoch 8: val_loss improved from 0.60500 to 0.59417, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5982 - accuracy: 0.8259 - val_loss: 0.5942 - val_accuracy: 0.8095 - lr: 0.0010\n",
      "Epoch 9/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.5864 - accuracy: 0.8279\n",
      "Epoch 9: val_loss improved from 0.59417 to 0.58345, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5877 - accuracy: 0.8281 - val_loss: 0.5834 - val_accuracy: 0.8155 - lr: 0.0010\n",
      "Epoch 10/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.5763 - accuracy: 0.8395\n",
      "Epoch 10: val_loss improved from 0.58345 to 0.57291, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.5773 - accuracy: 0.8393 - val_loss: 0.5729 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 11/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.5672 - accuracy: 0.8460\n",
      "Epoch 11: val_loss improved from 0.57291 to 0.56248, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5672 - accuracy: 0.8460 - val_loss: 0.5625 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 12/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.5578 - accuracy: 0.8494\n",
      "Epoch 12: val_loss improved from 0.56248 to 0.55215, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.5572 - accuracy: 0.8482 - val_loss: 0.5521 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 13/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.5474 - accuracy: 0.8482\n",
      "Epoch 13: val_loss improved from 0.55215 to 0.54184, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.5473 - accuracy: 0.8527 - val_loss: 0.5418 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 14/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.5392 - accuracy: 0.8525\n",
      "Epoch 14: val_loss improved from 0.54184 to 0.53157, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5375 - accuracy: 0.8527 - val_loss: 0.5316 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Epoch 15/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.5294 - accuracy: 0.8595\n",
      "Epoch 15: val_loss improved from 0.53157 to 0.52137, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.5279 - accuracy: 0.8594 - val_loss: 0.5214 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 16/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.5194 - accuracy: 0.8595\n",
      "Epoch 16: val_loss improved from 0.52137 to 0.51125, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.5184 - accuracy: 0.8594 - val_loss: 0.5112 - val_accuracy: 0.8571 - lr: 0.0010\n",
      "Epoch 17/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.8644\n",
      "Epoch 17: val_loss improved from 0.51125 to 0.50120, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.5090 - accuracy: 0.8683 - val_loss: 0.5012 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 18/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.5010 - accuracy: 0.8651\n",
      "Epoch 18: val_loss improved from 0.50120 to 0.49126, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4997 - accuracy: 0.8683 - val_loss: 0.4913 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 19/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.4949 - accuracy: 0.8600\n",
      "Epoch 19: val_loss improved from 0.49126 to 0.48143, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4906 - accuracy: 0.8683 - val_loss: 0.4814 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Epoch 20/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.4804 - accuracy: 0.8682\n",
      "Epoch 20: val_loss improved from 0.48143 to 0.47175, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4817 - accuracy: 0.8683 - val_loss: 0.4717 - val_accuracy: 0.8810 - lr: 0.0010\n",
      "Epoch 21/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.8705\n",
      "Epoch 21: val_loss improved from 0.47175 to 0.46231, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.4729 - accuracy: 0.8705 - val_loss: 0.4623 - val_accuracy: 0.8810 - lr: 0.0010\n",
      "Epoch 22/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.8759\n",
      "Epoch 22: val_loss improved from 0.46231 to 0.45307, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4643 - accuracy: 0.8772 - val_loss: 0.4531 - val_accuracy: 0.8810 - lr: 0.0010\n",
      "Epoch 23/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.4559 - accuracy: 0.8772\n",
      "Epoch 23: val_loss improved from 0.45307 to 0.44402, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.4559 - accuracy: 0.8772 - val_loss: 0.4440 - val_accuracy: 0.8810 - lr: 0.0010\n",
      "Epoch 24/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.4477 - accuracy: 0.8839\n",
      "Epoch 24: val_loss improved from 0.44402 to 0.43514, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4477 - accuracy: 0.8839 - val_loss: 0.4351 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 25/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.4428 - accuracy: 0.8894\n",
      "Epoch 25: val_loss improved from 0.43514 to 0.42646, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4396 - accuracy: 0.8929 - val_loss: 0.4265 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 26/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.4321 - accuracy: 0.8943\n",
      "Epoch 26: val_loss improved from 0.42646 to 0.41794, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4317 - accuracy: 0.8929 - val_loss: 0.4179 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 27/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8943\n",
      "Epoch 27: val_loss improved from 0.41794 to 0.40959, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4240 - accuracy: 0.8973 - val_loss: 0.4096 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Epoch 28/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.4165 - accuracy: 0.8996\n",
      "Epoch 28: val_loss improved from 0.40959 to 0.40144, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4165 - accuracy: 0.8996 - val_loss: 0.4014 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 29/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.4094 - accuracy: 0.9000\n",
      "Epoch 29: val_loss improved from 0.40144 to 0.39347, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4091 - accuracy: 0.9018 - val_loss: 0.3935 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 30/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.4016 - accuracy: 0.9023\n",
      "Epoch 30: val_loss improved from 0.39347 to 0.38570, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.4019 - accuracy: 0.9018 - val_loss: 0.3857 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 31/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.3960 - accuracy: 0.9000\n",
      "Epoch 31: val_loss improved from 0.38570 to 0.37814, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3949 - accuracy: 0.9018 - val_loss: 0.3781 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 32/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.3881 - accuracy: 0.9018\n",
      "Epoch 32: val_loss improved from 0.37814 to 0.37083, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3881 - accuracy: 0.9018 - val_loss: 0.3708 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 33/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.3823 - accuracy: 0.9045\n",
      "Epoch 33: val_loss improved from 0.37083 to 0.36369, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.3816 - accuracy: 0.9040 - val_loss: 0.3637 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 34/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.3762 - accuracy: 0.9045\n",
      "Epoch 34: val_loss improved from 0.36369 to 0.35679, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3751 - accuracy: 0.9040 - val_loss: 0.3568 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 35/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.3746 - accuracy: 0.8988\n",
      "Epoch 35: val_loss improved from 0.35679 to 0.35008, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3690 - accuracy: 0.9040 - val_loss: 0.3501 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 36/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.3624 - accuracy: 0.9070\n",
      "Epoch 36: val_loss improved from 0.35008 to 0.34358, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3629 - accuracy: 0.9062 - val_loss: 0.3436 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 37/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.9045\n",
      "Epoch 37: val_loss improved from 0.34358 to 0.33730, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3571 - accuracy: 0.9062 - val_loss: 0.3373 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 38/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.3513 - accuracy: 0.9057\n",
      "Epoch 38: val_loss improved from 0.33730 to 0.33122, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3515 - accuracy: 0.9062 - val_loss: 0.3312 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 39/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.3467 - accuracy: 0.9070\n",
      "Epoch 39: val_loss improved from 0.33122 to 0.32535, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3461 - accuracy: 0.9062 - val_loss: 0.3254 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Epoch 40/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.3326 - accuracy: 0.9150\n",
      "Epoch 40: val_loss improved from 0.32535 to 0.31967, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3409 - accuracy: 0.9062 - val_loss: 0.3197 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 41/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.9101\n",
      "Epoch 41: val_loss improved from 0.31967 to 0.31418, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3358 - accuracy: 0.9107 - val_loss: 0.3142 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 42/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.3310 - accuracy: 0.9103\n",
      "Epoch 42: val_loss improved from 0.31418 to 0.30888, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3309 - accuracy: 0.9107 - val_loss: 0.3089 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Epoch 43/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.3290 - accuracy: 0.9093\n",
      "Epoch 43: val_loss improved from 0.30888 to 0.30376, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3262 - accuracy: 0.9107 - val_loss: 0.3038 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 44/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.9172\n",
      "Epoch 44: val_loss improved from 0.30376 to 0.29881, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3217 - accuracy: 0.9129 - val_loss: 0.2988 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 45/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.3200 - accuracy: 0.9114\n",
      "Epoch 45: val_loss improved from 0.29881 to 0.29403, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.3173 - accuracy: 0.9129 - val_loss: 0.2940 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 46/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.3183 - accuracy: 0.9082\n",
      "Epoch 46: val_loss improved from 0.29403 to 0.28941, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3131 - accuracy: 0.9129 - val_loss: 0.2894 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 47/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.9149\n",
      "Epoch 47: val_loss improved from 0.28941 to 0.28495, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3090 - accuracy: 0.9129 - val_loss: 0.2849 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 48/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.2991 - accuracy: 0.9186\n",
      "Epoch 48: val_loss improved from 0.28495 to 0.28062, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3051 - accuracy: 0.9152 - val_loss: 0.2806 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 49/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.2906 - accuracy: 0.9241\n",
      "Epoch 49: val_loss improved from 0.28062 to 0.27647, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.3012 - accuracy: 0.9152 - val_loss: 0.2765 - val_accuracy: 0.9167 - lr: 0.0010\n",
      "Epoch 50/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2977 - accuracy: 0.9160\n",
      "Epoch 50: val_loss improved from 0.27647 to 0.27243, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2976 - accuracy: 0.9152 - val_loss: 0.2724 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Epoch 51/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.2961 - accuracy: 0.9150\n",
      "Epoch 51: val_loss improved from 0.27243 to 0.26852, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2940 - accuracy: 0.9196 - val_loss: 0.2685 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 52/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.2884 - accuracy: 0.9209\n",
      "Epoch 52: val_loss improved from 0.26852 to 0.26475, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2906 - accuracy: 0.9196 - val_loss: 0.2648 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 53/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2914 - accuracy: 0.9181\n",
      "Epoch 53: val_loss improved from 0.26475 to 0.26108, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2872 - accuracy: 0.9219 - val_loss: 0.2611 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Epoch 54/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.2788 - accuracy: 0.9256\n",
      "Epoch 54: val_loss improved from 0.26108 to 0.25752, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2840 - accuracy: 0.9219 - val_loss: 0.2575 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 55/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9241\n",
      "Epoch 55: val_loss improved from 0.25752 to 0.25407, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.2808 - accuracy: 0.9241 - val_loss: 0.2541 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 56/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.2821 - accuracy: 0.9220\n",
      "Epoch 56: val_loss improved from 0.25407 to 0.25072, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2778 - accuracy: 0.9241 - val_loss: 0.2507 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 57/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.2757 - accuracy: 0.9236\n",
      "Epoch 57: val_loss improved from 0.25072 to 0.24748, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2748 - accuracy: 0.9241 - val_loss: 0.2475 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 58/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2765 - accuracy: 0.9210\n",
      "Epoch 58: val_loss improved from 0.24748 to 0.24432, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2720 - accuracy: 0.9241 - val_loss: 0.2443 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 59/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2692 - accuracy: 0.9241\n",
      "Epoch 59: val_loss improved from 0.24432 to 0.24127, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2692 - accuracy: 0.9241 - val_loss: 0.2413 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 60/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.9227\n",
      "Epoch 60: val_loss improved from 0.24127 to 0.23830, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2664 - accuracy: 0.9241 - val_loss: 0.2383 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 61/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2647 - accuracy: 0.9250\n",
      "Epoch 61: val_loss improved from 0.23830 to 0.23540, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2638 - accuracy: 0.9263 - val_loss: 0.2354 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 62/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.9273\n",
      "Epoch 62: val_loss improved from 0.23540 to 0.23259, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2612 - accuracy: 0.9263 - val_loss: 0.2326 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 63/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2635 - accuracy: 0.9229\n",
      "Epoch 63: val_loss improved from 0.23259 to 0.22984, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2587 - accuracy: 0.9263 - val_loss: 0.2298 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 64/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2481 - accuracy: 0.9301\n",
      "Epoch 64: val_loss improved from 0.22984 to 0.22722, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2562 - accuracy: 0.9263 - val_loss: 0.2272 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 65/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2595 - accuracy: 0.9210\n",
      "Epoch 65: val_loss improved from 0.22722 to 0.22462, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2539 - accuracy: 0.9263 - val_loss: 0.2246 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 66/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.2493 - accuracy: 0.9271\n",
      "Epoch 66: val_loss improved from 0.22462 to 0.22210, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2516 - accuracy: 0.9263 - val_loss: 0.2221 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 67/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.2514 - accuracy: 0.9244\n",
      "Epoch 67: val_loss improved from 0.22210 to 0.21965, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2493 - accuracy: 0.9263 - val_loss: 0.2196 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 68/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.2557 - accuracy: 0.9205\n",
      "Epoch 68: val_loss improved from 0.21965 to 0.21724, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2471 - accuracy: 0.9263 - val_loss: 0.2172 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 69/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2448 - accuracy: 0.9229\n",
      "Epoch 69: val_loss improved from 0.21724 to 0.21490, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2449 - accuracy: 0.9263 - val_loss: 0.2149 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 70/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.2430 - accuracy: 0.9247\n",
      "Epoch 70: val_loss improved from 0.21490 to 0.21260, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2428 - accuracy: 0.9263 - val_loss: 0.2126 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 71/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2401 - accuracy: 0.9284\n",
      "Epoch 71: val_loss improved from 0.21260 to 0.21038, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2407 - accuracy: 0.9263 - val_loss: 0.2104 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 72/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.2346 - accuracy: 0.9294\n",
      "Epoch 72: val_loss improved from 0.21038 to 0.20820, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2386 - accuracy: 0.9263 - val_loss: 0.2082 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 73/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.2385 - accuracy: 0.9250\n",
      "Epoch 73: val_loss improved from 0.20820 to 0.20607, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2367 - accuracy: 0.9263 - val_loss: 0.2061 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 74/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.2298 - accuracy: 0.9310\n",
      "Epoch 74: val_loss improved from 0.20607 to 0.20399, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2347 - accuracy: 0.9263 - val_loss: 0.2040 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 75/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2352 - accuracy: 0.9253\n",
      "Epoch 75: val_loss improved from 0.20399 to 0.20196, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2328 - accuracy: 0.9263 - val_loss: 0.2020 - val_accuracy: 0.9345 - lr: 0.0010\n",
      "Epoch 76/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.2267 - accuracy: 0.9293\n",
      "Epoch 76: val_loss improved from 0.20196 to 0.19996, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2309 - accuracy: 0.9263 - val_loss: 0.2000 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 77/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2261 - accuracy: 0.9277\n",
      "Epoch 77: val_loss improved from 0.19996 to 0.19802, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2291 - accuracy: 0.9286 - val_loss: 0.1980 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 78/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2260 - accuracy: 0.9309\n",
      "Epoch 78: val_loss improved from 0.19802 to 0.19611, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2273 - accuracy: 0.9286 - val_loss: 0.1961 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 79/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.2241 - accuracy: 0.9310\n",
      "Epoch 79: val_loss improved from 0.19611 to 0.19425, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2255 - accuracy: 0.9286 - val_loss: 0.1942 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 80/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.2170 - accuracy: 0.9333\n",
      "Epoch 80: val_loss improved from 0.19425 to 0.19245, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2238 - accuracy: 0.9286 - val_loss: 0.1925 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 81/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2164 - accuracy: 0.9309\n",
      "Epoch 81: val_loss improved from 0.19245 to 0.19069, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2221 - accuracy: 0.9286 - val_loss: 0.1907 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 82/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2224 - accuracy: 0.9277\n",
      "Epoch 82: val_loss improved from 0.19069 to 0.18894, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2204 - accuracy: 0.9308 - val_loss: 0.1889 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 83/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.2224 - accuracy: 0.9293\n",
      "Epoch 83: val_loss improved from 0.18894 to 0.18724, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2188 - accuracy: 0.9330 - val_loss: 0.1872 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 84/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.2228 - accuracy: 0.9294\n",
      "Epoch 84: val_loss improved from 0.18724 to 0.18556, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2172 - accuracy: 0.9330 - val_loss: 0.1856 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 85/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.2151 - accuracy: 0.9349\n",
      "Epoch 85: val_loss improved from 0.18556 to 0.18395, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2157 - accuracy: 0.9330 - val_loss: 0.1840 - val_accuracy: 0.9405 - lr: 0.0010\n",
      "Epoch 86/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2054 - accuracy: 0.9383\n",
      "Epoch 86: val_loss improved from 0.18395 to 0.18237, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2141 - accuracy: 0.9330 - val_loss: 0.1824 - val_accuracy: 0.9464 - lr: 0.0010\n",
      "Epoch 87/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.2111 - accuracy: 0.9395\n",
      "Epoch 87: val_loss improved from 0.18237 to 0.18078, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2127 - accuracy: 0.9375 - val_loss: 0.1808 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 88/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2091 - accuracy: 0.9407\n",
      "Epoch 88: val_loss improved from 0.18078 to 0.17923, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.2112 - accuracy: 0.9375 - val_loss: 0.1792 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 89/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.2105 - accuracy: 0.9405\n",
      "Epoch 89: val_loss improved from 0.17923 to 0.17770, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2097 - accuracy: 0.9397 - val_loss: 0.1777 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 90/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.2096 - accuracy: 0.9407\n",
      "Epoch 90: val_loss improved from 0.17770 to 0.17623, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2083 - accuracy: 0.9397 - val_loss: 0.1762 - val_accuracy: 0.9524 - lr: 0.0010\n",
      "Epoch 91/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1972 - accuracy: 0.9494\n",
      "Epoch 91: val_loss improved from 0.17623 to 0.17476, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.2069 - accuracy: 0.9420 - val_loss: 0.1748 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 92/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2055 - accuracy: 0.9420\n",
      "Epoch 92: val_loss improved from 0.17476 to 0.17331, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.2055 - accuracy: 0.9420 - val_loss: 0.1733 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 93/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.2047 - accuracy: 0.9416\n",
      "Epoch 93: val_loss improved from 0.17331 to 0.17190, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.2042 - accuracy: 0.9420 - val_loss: 0.1719 - val_accuracy: 0.9583 - lr: 0.0010\n",
      "Epoch 94/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1827 - accuracy: 0.9506\n",
      "Epoch 94: val_loss improved from 0.17190 to 0.17052, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.2029 - accuracy: 0.9420 - val_loss: 0.1705 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 95/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.2052 - accuracy: 0.9419\n",
      "Epoch 95: val_loss improved from 0.17052 to 0.16916, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.2016 - accuracy: 0.9442 - val_loss: 0.1692 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 96/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9442\n",
      "Epoch 96: val_loss improved from 0.16916 to 0.16784, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.2003 - accuracy: 0.9442 - val_loss: 0.1678 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 97/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1991 - accuracy: 0.9442\n",
      "Epoch 97: val_loss improved from 0.16784 to 0.16654, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1991 - accuracy: 0.9442 - val_loss: 0.1665 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 98/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1968 - accuracy: 0.9452\n",
      "Epoch 98: val_loss improved from 0.16654 to 0.16526, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.1979 - accuracy: 0.9442 - val_loss: 0.1653 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 99/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1934 - accuracy: 0.9452\n",
      "Epoch 99: val_loss improved from 0.16526 to 0.16401, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1967 - accuracy: 0.9442 - val_loss: 0.1640 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 100/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9438\n",
      "Epoch 100: val_loss improved from 0.16401 to 0.16278, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1955 - accuracy: 0.9442 - val_loss: 0.1628 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 101/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1953 - accuracy: 0.9432\n",
      "Epoch 101: val_loss improved from 0.16278 to 0.16156, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.1944 - accuracy: 0.9442 - val_loss: 0.1616 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 102/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1939 - accuracy: 0.9438\n",
      "Epoch 102: val_loss improved from 0.16156 to 0.16037, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1932 - accuracy: 0.9442 - val_loss: 0.1604 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 103/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1831 - accuracy: 0.9500\n",
      "Epoch 103: val_loss improved from 0.16037 to 0.15919, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1921 - accuracy: 0.9442 - val_loss: 0.1592 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 104/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1918 - accuracy: 0.9438\n",
      "Epoch 104: val_loss improved from 0.15919 to 0.15805, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1910 - accuracy: 0.9442 - val_loss: 0.1580 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 105/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1924 - accuracy: 0.9422\n",
      "Epoch 105: val_loss improved from 0.15805 to 0.15695, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1899 - accuracy: 0.9442 - val_loss: 0.1570 - val_accuracy: 0.9643 - lr: 0.0010\n",
      "Epoch 106/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1888 - accuracy: 0.9442\n",
      "Epoch 106: val_loss improved from 0.15695 to 0.15585, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1888 - accuracy: 0.9442 - val_loss: 0.1558 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 107/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1859 - accuracy: 0.9455\n",
      "Epoch 107: val_loss improved from 0.15585 to 0.15476, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1878 - accuracy: 0.9442 - val_loss: 0.1548 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 108/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9438\n",
      "Epoch 108: val_loss improved from 0.15476 to 0.15369, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1868 - accuracy: 0.9442 - val_loss: 0.1537 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 109/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1875 - accuracy: 0.9432\n",
      "Epoch 109: val_loss improved from 0.15369 to 0.15264, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1858 - accuracy: 0.9442 - val_loss: 0.1526 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 110/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1878 - accuracy: 0.9419\n",
      "Epoch 110: val_loss improved from 0.15264 to 0.15160, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1848 - accuracy: 0.9442 - val_loss: 0.1516 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 111/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1902 - accuracy: 0.9422\n",
      "Epoch 111: val_loss improved from 0.15160 to 0.15059, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1838 - accuracy: 0.9464 - val_loss: 0.1506 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 112/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9455\n",
      "Epoch 112: val_loss improved from 0.15059 to 0.14959, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1828 - accuracy: 0.9464 - val_loss: 0.1496 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 113/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1778 - accuracy: 0.9488\n",
      "Epoch 113: val_loss improved from 0.14959 to 0.14861, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1819 - accuracy: 0.9464 - val_loss: 0.1486 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 114/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1827 - accuracy: 0.9455\n",
      "Epoch 114: val_loss improved from 0.14861 to 0.14765, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1809 - accuracy: 0.9464 - val_loss: 0.1476 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 115/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1825 - accuracy: 0.9470\n",
      "Epoch 115: val_loss improved from 0.14765 to 0.14670, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1800 - accuracy: 0.9464 - val_loss: 0.1467 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 116/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1800 - accuracy: 0.9471\n",
      "Epoch 116: val_loss improved from 0.14670 to 0.14576, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1791 - accuracy: 0.9464 - val_loss: 0.1458 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 117/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9464\n",
      "Epoch 117: val_loss improved from 0.14576 to 0.14485, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1782 - accuracy: 0.9464 - val_loss: 0.1448 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 118/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.9461\n",
      "Epoch 118: val_loss improved from 0.14485 to 0.14394, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1773 - accuracy: 0.9464 - val_loss: 0.1439 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 119/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1783 - accuracy: 0.9465\n",
      "Epoch 119: val_loss improved from 0.14394 to 0.14306, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1764 - accuracy: 0.9464 - val_loss: 0.1431 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 120/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1657 - accuracy: 0.9524\n",
      "Epoch 120: val_loss improved from 0.14306 to 0.14219, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1755 - accuracy: 0.9464 - val_loss: 0.1422 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 121/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9464\n",
      "Epoch 121: val_loss improved from 0.14219 to 0.14134, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1747 - accuracy: 0.9464 - val_loss: 0.1413 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 122/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1738 - accuracy: 0.9464\n",
      "Epoch 122: val_loss improved from 0.14134 to 0.14050, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1738 - accuracy: 0.9464 - val_loss: 0.1405 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 123/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9461\n",
      "Epoch 123: val_loss improved from 0.14050 to 0.13965, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1730 - accuracy: 0.9464 - val_loss: 0.1397 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 124/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1755 - accuracy: 0.9442\n",
      "Epoch 124: val_loss improved from 0.13965 to 0.13882, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1721 - accuracy: 0.9464 - val_loss: 0.1388 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 125/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1693 - accuracy: 0.9500\n",
      "Epoch 125: val_loss improved from 0.13882 to 0.13803, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1713 - accuracy: 0.9464 - val_loss: 0.1380 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 126/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1725 - accuracy: 0.9459\n",
      "Epoch 126: val_loss improved from 0.13803 to 0.13726, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1705 - accuracy: 0.9464 - val_loss: 0.1373 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 127/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1698 - accuracy: 0.9477\n",
      "Epoch 127: val_loss improved from 0.13726 to 0.13647, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1697 - accuracy: 0.9464 - val_loss: 0.1365 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 128/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1681 - accuracy: 0.9488\n",
      "Epoch 128: val_loss improved from 0.13647 to 0.13570, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1689 - accuracy: 0.9464 - val_loss: 0.1357 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 129/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9483\n",
      "Epoch 129: val_loss improved from 0.13570 to 0.13494, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1681 - accuracy: 0.9487 - val_loss: 0.1349 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 130/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1646 - accuracy: 0.9500\n",
      "Epoch 130: val_loss improved from 0.13494 to 0.13419, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1674 - accuracy: 0.9487 - val_loss: 0.1342 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 131/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9477\n",
      "Epoch 131: val_loss improved from 0.13419 to 0.13345, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1666 - accuracy: 0.9487 - val_loss: 0.1335 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 132/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1671 - accuracy: 0.9475\n",
      "Epoch 132: val_loss improved from 0.13345 to 0.13272, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1659 - accuracy: 0.9487 - val_loss: 0.1327 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 133/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9506\n",
      "Epoch 133: val_loss improved from 0.13272 to 0.13201, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1651 - accuracy: 0.9509 - val_loss: 0.1320 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 134/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1433 - accuracy: 0.9634\n",
      "Epoch 134: val_loss improved from 0.13201 to 0.13131, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1644 - accuracy: 0.9509 - val_loss: 0.1313 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 135/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1580 - accuracy: 0.9548\n",
      "Epoch 135: val_loss improved from 0.13131 to 0.13062, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1636 - accuracy: 0.9509 - val_loss: 0.1306 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 136/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1599 - accuracy: 0.9524\n",
      "Epoch 136: val_loss improved from 0.13062 to 0.12996, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1629 - accuracy: 0.9509 - val_loss: 0.1300 - val_accuracy: 0.9702 - lr: 0.0010\n",
      "Epoch 137/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1624 - accuracy: 0.9523\n",
      "Epoch 137: val_loss improved from 0.12996 to 0.12929, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1622 - accuracy: 0.9509 - val_loss: 0.1293 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 138/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1625 - accuracy: 0.9500\n",
      "Epoch 138: val_loss improved from 0.12929 to 0.12862, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1615 - accuracy: 0.9509 - val_loss: 0.1286 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 139/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1624 - accuracy: 0.9500\n",
      "Epoch 139: val_loss improved from 0.12862 to 0.12797, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1608 - accuracy: 0.9509 - val_loss: 0.1280 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 140/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9517\n",
      "Epoch 140: val_loss improved from 0.12797 to 0.12732, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1601 - accuracy: 0.9509 - val_loss: 0.1273 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 141/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1613 - accuracy: 0.9500\n",
      "Epoch 141: val_loss improved from 0.12732 to 0.12668, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1594 - accuracy: 0.9509 - val_loss: 0.1267 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 142/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1591 - accuracy: 0.9506\n",
      "Epoch 142: val_loss improved from 0.12668 to 0.12605, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1588 - accuracy: 0.9509 - val_loss: 0.1261 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 143/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1496 - accuracy: 0.9558\n",
      "Epoch 143: val_loss improved from 0.12605 to 0.12544, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1581 - accuracy: 0.9509 - val_loss: 0.1254 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 144/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1563 - accuracy: 0.9548\n",
      "Epoch 144: val_loss improved from 0.12544 to 0.12482, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1574 - accuracy: 0.9509 - val_loss: 0.1248 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 145/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1572 - accuracy: 0.9488\n",
      "Epoch 145: val_loss improved from 0.12482 to 0.12421, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1568 - accuracy: 0.9509 - val_loss: 0.1242 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 146/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1517 - accuracy: 0.9512\n",
      "Epoch 146: val_loss improved from 0.12421 to 0.12362, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1561 - accuracy: 0.9509 - val_loss: 0.1236 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 147/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1585 - accuracy: 0.9494\n",
      "Epoch 147: val_loss improved from 0.12362 to 0.12303, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1555 - accuracy: 0.9509 - val_loss: 0.1230 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 148/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1585 - accuracy: 0.9518\n",
      "Epoch 148: val_loss improved from 0.12303 to 0.12245, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1548 - accuracy: 0.9509 - val_loss: 0.1225 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 149/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1553 - accuracy: 0.9500\n",
      "Epoch 149: val_loss improved from 0.12245 to 0.12188, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1542 - accuracy: 0.9509 - val_loss: 0.1219 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 150/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1590 - accuracy: 0.9500\n",
      "Epoch 150: val_loss improved from 0.12188 to 0.12130, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1536 - accuracy: 0.9531 - val_loss: 0.1213 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 151/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1487 - accuracy: 0.9558\n",
      "Epoch 151: val_loss improved from 0.12130 to 0.12075, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1530 - accuracy: 0.9531 - val_loss: 0.1208 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 152/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1581 - accuracy: 0.9524\n",
      "Epoch 152: val_loss improved from 0.12075 to 0.12020, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1524 - accuracy: 0.9554 - val_loss: 0.1202 - val_accuracy: 0.9762 - lr: 0.0010\n",
      "Epoch 153/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1567 - accuracy: 0.9550\n",
      "Epoch 153: val_loss improved from 0.12020 to 0.11966, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1517 - accuracy: 0.9576 - val_loss: 0.1197 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 154/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1528 - accuracy: 0.9568\n",
      "Epoch 154: val_loss improved from 0.11966 to 0.11912, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1511 - accuracy: 0.9576 - val_loss: 0.1191 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 155/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9568\n",
      "Epoch 155: val_loss improved from 0.11912 to 0.11860, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1505 - accuracy: 0.9576 - val_loss: 0.1186 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 156/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.9573\n",
      "Epoch 156: val_loss improved from 0.11860 to 0.11807, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1500 - accuracy: 0.9576 - val_loss: 0.1181 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 157/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9573\n",
      "Epoch 157: val_loss improved from 0.11807 to 0.11755, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1494 - accuracy: 0.9576 - val_loss: 0.1176 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 158/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1528 - accuracy: 0.9558\n",
      "Epoch 158: val_loss improved from 0.11755 to 0.11704, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1488 - accuracy: 0.9576 - val_loss: 0.1170 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 159/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1561 - accuracy: 0.9566\n",
      "Epoch 159: val_loss improved from 0.11704 to 0.11654, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1482 - accuracy: 0.9598 - val_loss: 0.1165 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 160/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1495 - accuracy: 0.9591\n",
      "Epoch 160: val_loss improved from 0.11654 to 0.11604, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1476 - accuracy: 0.9598 - val_loss: 0.1160 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 161/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1457 - accuracy: 0.9614\n",
      "Epoch 161: val_loss improved from 0.11604 to 0.11555, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1471 - accuracy: 0.9598 - val_loss: 0.1156 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 162/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1378 - accuracy: 0.9628\n",
      "Epoch 162: val_loss improved from 0.11555 to 0.11509, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1465 - accuracy: 0.9598 - val_loss: 0.1151 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 163/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1303 - accuracy: 0.9679\n",
      "Epoch 163: val_loss improved from 0.11509 to 0.11461, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1460 - accuracy: 0.9598 - val_loss: 0.1146 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 164/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1511 - accuracy: 0.9576\n",
      "Epoch 164: val_loss improved from 0.11461 to 0.11414, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1455 - accuracy: 0.9598 - val_loss: 0.1141 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 165/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1444 - accuracy: 0.9585\n",
      "Epoch 165: val_loss improved from 0.11414 to 0.11367, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1449 - accuracy: 0.9598 - val_loss: 0.1137 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 166/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1355 - accuracy: 0.9630\n",
      "Epoch 166: val_loss improved from 0.11367 to 0.11322, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1443 - accuracy: 0.9598 - val_loss: 0.1132 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 167/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1516 - accuracy: 0.9566\n",
      "Epoch 167: val_loss improved from 0.11322 to 0.11276, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1438 - accuracy: 0.9598 - val_loss: 0.1128 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 168/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1480 - accuracy: 0.9576\n",
      "Epoch 168: val_loss improved from 0.11276 to 0.11231, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1433 - accuracy: 0.9598 - val_loss: 0.1123 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 169/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1319 - accuracy: 0.9643\n",
      "Epoch 169: val_loss improved from 0.11231 to 0.11189, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1427 - accuracy: 0.9598 - val_loss: 0.1119 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 170/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1460 - accuracy: 0.9585\n",
      "Epoch 170: val_loss improved from 0.11189 to 0.11144, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1422 - accuracy: 0.9598 - val_loss: 0.1114 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 171/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1431 - accuracy: 0.9591\n",
      "Epoch 171: val_loss improved from 0.11144 to 0.11100, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1417 - accuracy: 0.9598 - val_loss: 0.1110 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 172/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1414 - accuracy: 0.9596\n",
      "Epoch 172: val_loss improved from 0.11100 to 0.11056, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1412 - accuracy: 0.9598 - val_loss: 0.1106 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 173/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1385 - accuracy: 0.9590\n",
      "Epoch 173: val_loss improved from 0.11056 to 0.11013, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1407 - accuracy: 0.9598 - val_loss: 0.1101 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 174/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9586\n",
      "Epoch 174: val_loss improved from 0.11013 to 0.10970, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1402 - accuracy: 0.9598 - val_loss: 0.1097 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 175/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9636\n",
      "Epoch 175: val_loss improved from 0.10970 to 0.10931, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1397 - accuracy: 0.9598 - val_loss: 0.1093 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 176/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1471 - accuracy: 0.9556\n",
      "Epoch 176: val_loss improved from 0.10931 to 0.10889, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1392 - accuracy: 0.9598 - val_loss: 0.1089 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 177/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9596\n",
      "Epoch 177: val_loss improved from 0.10889 to 0.10848, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1387 - accuracy: 0.9598 - val_loss: 0.1085 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 178/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1340 - accuracy: 0.9654\n",
      "Epoch 178: val_loss improved from 0.10848 to 0.10807, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1382 - accuracy: 0.9598 - val_loss: 0.1081 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 179/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9598\n",
      "Epoch 179: val_loss improved from 0.10807 to 0.10767, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1377 - accuracy: 0.9598 - val_loss: 0.1077 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 180/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1443 - accuracy: 0.9556\n",
      "Epoch 180: val_loss improved from 0.10767 to 0.10727, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1372 - accuracy: 0.9598 - val_loss: 0.1073 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 181/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9598\n",
      "Epoch 181: val_loss improved from 0.10727 to 0.10688, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1367 - accuracy: 0.9598 - val_loss: 0.1069 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 182/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1387 - accuracy: 0.9600\n",
      "Epoch 182: val_loss improved from 0.10688 to 0.10652, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1363 - accuracy: 0.9598 - val_loss: 0.1065 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 183/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1278 - accuracy: 0.9639\n",
      "Epoch 183: val_loss improved from 0.10652 to 0.10613, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1358 - accuracy: 0.9598 - val_loss: 0.1061 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 184/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1337 - accuracy: 0.9595\n",
      "Epoch 184: val_loss improved from 0.10613 to 0.10575, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1353 - accuracy: 0.9598 - val_loss: 0.1057 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 185/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1298 - accuracy: 0.9647\n",
      "Epoch 185: val_loss improved from 0.10575 to 0.10537, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1349 - accuracy: 0.9598 - val_loss: 0.1054 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 186/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1372 - accuracy: 0.9586\n",
      "Epoch 186: val_loss improved from 0.10537 to 0.10499, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1344 - accuracy: 0.9598 - val_loss: 0.1050 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 187/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1271 - accuracy: 0.9628\n",
      "Epoch 187: val_loss improved from 0.10499 to 0.10461, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1339 - accuracy: 0.9598 - val_loss: 0.1046 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 188/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9609\n",
      "Epoch 188: val_loss improved from 0.10461 to 0.10426, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1335 - accuracy: 0.9598 - val_loss: 0.1043 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 189/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1357 - accuracy: 0.9575\n",
      "Epoch 189: val_loss improved from 0.10426 to 0.10389, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1330 - accuracy: 0.9598 - val_loss: 0.1039 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 190/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1400 - accuracy: 0.9561\n",
      "Epoch 190: val_loss improved from 0.10389 to 0.10352, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1326 - accuracy: 0.9598 - val_loss: 0.1035 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 191/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9586\n",
      "Epoch 191: val_loss improved from 0.10352 to 0.10317, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1321 - accuracy: 0.9598 - val_loss: 0.1032 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 192/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1352 - accuracy: 0.9576\n",
      "Epoch 192: val_loss improved from 0.10317 to 0.10281, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1317 - accuracy: 0.9598 - val_loss: 0.1028 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 193/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1174 - accuracy: 0.9683\n",
      "Epoch 193: val_loss improved from 0.10281 to 0.10246, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1313 - accuracy: 0.9598 - val_loss: 0.1025 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 194/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1285 - accuracy: 0.9600\n",
      "Epoch 194: val_loss improved from 0.10246 to 0.10212, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1309 - accuracy: 0.9598 - val_loss: 0.1021 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 195/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1311 - accuracy: 0.9596\n",
      "Epoch 195: val_loss improved from 0.10212 to 0.10177, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1304 - accuracy: 0.9598 - val_loss: 0.1018 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 196/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1340 - accuracy: 0.9590\n",
      "Epoch 196: val_loss improved from 0.10177 to 0.10143, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1300 - accuracy: 0.9598 - val_loss: 0.1014 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 197/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1304 - accuracy: 0.9590\n",
      "Epoch 197: val_loss improved from 0.10143 to 0.10109, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1296 - accuracy: 0.9598 - val_loss: 0.1011 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 198/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1278 - accuracy: 0.9595\n",
      "Epoch 198: val_loss improved from 0.10109 to 0.10075, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1291 - accuracy: 0.9598 - val_loss: 0.1007 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 199/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1278 - accuracy: 0.9605\n",
      "Epoch 199: val_loss improved from 0.10075 to 0.10042, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1287 - accuracy: 0.9598 - val_loss: 0.1004 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 200/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1074 - accuracy: 0.9728\n",
      "Epoch 200: val_loss improved from 0.10042 to 0.10013, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1283 - accuracy: 0.9598 - val_loss: 0.1001 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 201/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9596\n",
      "Epoch 201: val_loss improved from 0.10013 to 0.09980, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1279 - accuracy: 0.9598 - val_loss: 0.0998 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 202/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.1255 - accuracy: 0.9620\n",
      "Epoch 202: val_loss improved from 0.09980 to 0.09948, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1275 - accuracy: 0.9598 - val_loss: 0.0995 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 203/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9596\n",
      "Epoch 203: val_loss improved from 0.09948 to 0.09915, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1271 - accuracy: 0.9598 - val_loss: 0.0992 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 204/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1320 - accuracy: 0.9566\n",
      "Epoch 204: val_loss improved from 0.09915 to 0.09883, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1267 - accuracy: 0.9598 - val_loss: 0.0988 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 205/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1278 - accuracy: 0.9595\n",
      "Epoch 205: val_loss improved from 0.09883 to 0.09851, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1263 - accuracy: 0.9598 - val_loss: 0.0985 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 206/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1290 - accuracy: 0.9581\n",
      "Epoch 206: val_loss improved from 0.09851 to 0.09819, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1259 - accuracy: 0.9598 - val_loss: 0.0982 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 207/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1249 - accuracy: 0.9619\n",
      "Epoch 207: val_loss improved from 0.09819 to 0.09788, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1255 - accuracy: 0.9598 - val_loss: 0.0979 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 208/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1260 - accuracy: 0.9590\n",
      "Epoch 208: val_loss improved from 0.09788 to 0.09758, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1251 - accuracy: 0.9598 - val_loss: 0.0976 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 209/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1237 - accuracy: 0.9614\n",
      "Epoch 209: val_loss improved from 0.09758 to 0.09728, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1247 - accuracy: 0.9598 - val_loss: 0.0973 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 210/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1296 - accuracy: 0.9566\n",
      "Epoch 210: val_loss improved from 0.09728 to 0.09698, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1243 - accuracy: 0.9598 - val_loss: 0.0970 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 211/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1130 - accuracy: 0.9634\n",
      "Epoch 211: val_loss improved from 0.09698 to 0.09668, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1239 - accuracy: 0.9598 - val_loss: 0.0967 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 212/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1275 - accuracy: 0.9571\n",
      "Epoch 212: val_loss improved from 0.09668 to 0.09639, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1235 - accuracy: 0.9598 - val_loss: 0.0964 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 213/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1198 - accuracy: 0.9628\n",
      "Epoch 213: val_loss improved from 0.09639 to 0.09608, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1231 - accuracy: 0.9598 - val_loss: 0.0961 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 214/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1231 - accuracy: 0.9590\n",
      "Epoch 214: val_loss improved from 0.09608 to 0.09578, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1227 - accuracy: 0.9598 - val_loss: 0.0958 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 215/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9596\n",
      "Epoch 215: val_loss improved from 0.09578 to 0.09549, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1224 - accuracy: 0.9598 - val_loss: 0.0955 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 216/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1220 - accuracy: 0.9598\n",
      "Epoch 216: val_loss improved from 0.09549 to 0.09520, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1220 - accuracy: 0.9598 - val_loss: 0.0952 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 217/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9591\n",
      "Epoch 217: val_loss improved from 0.09520 to 0.09492, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1216 - accuracy: 0.9598 - val_loss: 0.0949 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 218/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.1197 - accuracy: 0.9620\n",
      "Epoch 218: val_loss improved from 0.09492 to 0.09464, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1212 - accuracy: 0.9598 - val_loss: 0.0946 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 219/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1210 - accuracy: 0.9634\n",
      "Epoch 219: val_loss improved from 0.09464 to 0.09436, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1209 - accuracy: 0.9621 - val_loss: 0.0944 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 220/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9636\n",
      "Epoch 220: val_loss improved from 0.09436 to 0.09410, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1205 - accuracy: 0.9621 - val_loss: 0.0941 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 221/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9618\n",
      "Epoch 221: val_loss improved from 0.09410 to 0.09382, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1201 - accuracy: 0.9621 - val_loss: 0.0938 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 222/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1236 - accuracy: 0.9605\n",
      "Epoch 222: val_loss improved from 0.09382 to 0.09355, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1198 - accuracy: 0.9621 - val_loss: 0.0935 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 223/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1210 - accuracy: 0.9605\n",
      "Epoch 223: val_loss improved from 0.09355 to 0.09328, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1194 - accuracy: 0.9621 - val_loss: 0.0933 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 224/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9618\n",
      "Epoch 224: val_loss improved from 0.09328 to 0.09301, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1191 - accuracy: 0.9621 - val_loss: 0.0930 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 225/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1209 - accuracy: 0.9630\n",
      "Epoch 225: val_loss improved from 0.09301 to 0.09274, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1187 - accuracy: 0.9643 - val_loss: 0.0927 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 226/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9640\n",
      "Epoch 226: val_loss improved from 0.09274 to 0.09247, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1183 - accuracy: 0.9643 - val_loss: 0.0925 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 227/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9655\n",
      "Epoch 227: val_loss improved from 0.09247 to 0.09221, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1180 - accuracy: 0.9643 - val_loss: 0.0922 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 228/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9636\n",
      "Epoch 228: val_loss improved from 0.09221 to 0.09195, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1176 - accuracy: 0.9643 - val_loss: 0.0919 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 229/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1218 - accuracy: 0.9619\n",
      "Epoch 229: val_loss improved from 0.09195 to 0.09169, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1173 - accuracy: 0.9643 - val_loss: 0.0917 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 230/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1145 - accuracy: 0.9667\n",
      "Epoch 230: val_loss improved from 0.09169 to 0.09143, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1169 - accuracy: 0.9643 - val_loss: 0.0914 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 231/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.1208 - accuracy: 0.9667\n",
      "Epoch 231: val_loss improved from 0.09143 to 0.09117, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1166 - accuracy: 0.9665 - val_loss: 0.0912 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 232/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9659\n",
      "Epoch 232: val_loss improved from 0.09117 to 0.09091, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1162 - accuracy: 0.9665 - val_loss: 0.0909 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 233/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9663\n",
      "Epoch 233: val_loss improved from 0.09091 to 0.09066, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1159 - accuracy: 0.9665 - val_loss: 0.0907 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 234/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.1106 - accuracy: 0.9696\n",
      "Epoch 234: val_loss improved from 0.09066 to 0.09041, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1155 - accuracy: 0.9665 - val_loss: 0.0904 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 235/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9659\n",
      "Epoch 235: val_loss improved from 0.09041 to 0.09016, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1152 - accuracy: 0.9665 - val_loss: 0.0902 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 236/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1105 - accuracy: 0.9694\n",
      "Epoch 236: val_loss improved from 0.09016 to 0.08990, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1149 - accuracy: 0.9665 - val_loss: 0.0899 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 237/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1150 - accuracy: 0.9663\n",
      "Epoch 237: val_loss improved from 0.08990 to 0.08965, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1145 - accuracy: 0.9665 - val_loss: 0.0896 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 238/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1137 - accuracy: 0.9674\n",
      "Epoch 238: val_loss improved from 0.08965 to 0.08940, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1142 - accuracy: 0.9665 - val_loss: 0.0894 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 239/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1139 - accuracy: 0.9665\n",
      "Epoch 239: val_loss improved from 0.08940 to 0.08916, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1139 - accuracy: 0.9665 - val_loss: 0.0892 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 240/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1142 - accuracy: 0.9671\n",
      "Epoch 240: val_loss improved from 0.08916 to 0.08892, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1135 - accuracy: 0.9665 - val_loss: 0.0889 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 241/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1129 - accuracy: 0.9663\n",
      "Epoch 241: val_loss improved from 0.08892 to 0.08865, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1132 - accuracy: 0.9665 - val_loss: 0.0887 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 242/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1151 - accuracy: 0.9651\n",
      "Epoch 242: val_loss improved from 0.08865 to 0.08841, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1129 - accuracy: 0.9665 - val_loss: 0.0884 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 243/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1104 - accuracy: 0.9678\n",
      "Epoch 243: val_loss improved from 0.08841 to 0.08819, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1126 - accuracy: 0.9665 - val_loss: 0.0882 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 244/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.1154 - accuracy: 0.9647\n",
      "Epoch 244: val_loss improved from 0.08819 to 0.08795, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1122 - accuracy: 0.9665 - val_loss: 0.0880 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 245/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1065 - accuracy: 0.9700\n",
      "Epoch 245: val_loss improved from 0.08795 to 0.08772, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.1119 - accuracy: 0.9665 - val_loss: 0.0877 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 246/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1078 - accuracy: 0.9704\n",
      "Epoch 246: val_loss improved from 0.08772 to 0.08749, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1116 - accuracy: 0.9665 - val_loss: 0.0875 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 247/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9665\n",
      "Epoch 247: val_loss improved from 0.08749 to 0.08725, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1113 - accuracy: 0.9665 - val_loss: 0.0873 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 248/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9665\n",
      "Epoch 248: val_loss improved from 0.08725 to 0.08705, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1110 - accuracy: 0.9665 - val_loss: 0.0870 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 249/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1120 - accuracy: 0.9650\n",
      "Epoch 249: val_loss improved from 0.08705 to 0.08682, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1106 - accuracy: 0.9665 - val_loss: 0.0868 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 250/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1080 - accuracy: 0.9678\n",
      "Epoch 250: val_loss improved from 0.08682 to 0.08658, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1103 - accuracy: 0.9665 - val_loss: 0.0866 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 251/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9659\n",
      "Epoch 251: val_loss improved from 0.08658 to 0.08635, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1100 - accuracy: 0.9665 - val_loss: 0.0864 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 252/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.1164 - accuracy: 0.9620\n",
      "Epoch 252: val_loss improved from 0.08635 to 0.08612, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1097 - accuracy: 0.9665 - val_loss: 0.0861 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 253/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9701\n",
      "Epoch 253: val_loss improved from 0.08612 to 0.08588, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1094 - accuracy: 0.9665 - val_loss: 0.0859 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 254/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9678\n",
      "Epoch 254: val_loss improved from 0.08588 to 0.08566, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1091 - accuracy: 0.9665 - val_loss: 0.0857 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 255/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9678\n",
      "Epoch 255: val_loss improved from 0.08566 to 0.08543, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1087 - accuracy: 0.9688 - val_loss: 0.0854 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 256/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1089 - accuracy: 0.9685\n",
      "Epoch 256: val_loss improved from 0.08543 to 0.08522, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1084 - accuracy: 0.9688 - val_loss: 0.0852 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 257/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.1139 - accuracy: 0.9659\n",
      "Epoch 257: val_loss improved from 0.08522 to 0.08499, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1081 - accuracy: 0.9688 - val_loss: 0.0850 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 258/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1051 - accuracy: 0.9690\n",
      "Epoch 258: val_loss improved from 0.08499 to 0.08477, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1078 - accuracy: 0.9688 - val_loss: 0.0848 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 259/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1084 - accuracy: 0.9678\n",
      "Epoch 259: val_loss improved from 0.08477 to 0.08455, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1075 - accuracy: 0.9688 - val_loss: 0.0846 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 260/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1130 - accuracy: 0.9663\n",
      "Epoch 260: val_loss improved from 0.08455 to 0.08433, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1072 - accuracy: 0.9688 - val_loss: 0.0843 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 261/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1090 - accuracy: 0.9687\n",
      "Epoch 261: val_loss improved from 0.08433 to 0.08412, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1069 - accuracy: 0.9688 - val_loss: 0.0841 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 262/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0995 - accuracy: 0.9711\n",
      "Epoch 262: val_loss improved from 0.08412 to 0.08390, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1066 - accuracy: 0.9688 - val_loss: 0.0839 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 263/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9705\n",
      "Epoch 263: val_loss improved from 0.08390 to 0.08369, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1063 - accuracy: 0.9688 - val_loss: 0.0837 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 264/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9682\n",
      "Epoch 264: val_loss improved from 0.08369 to 0.08348, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1061 - accuracy: 0.9688 - val_loss: 0.0835 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 265/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1060 - accuracy: 0.9690\n",
      "Epoch 265: val_loss improved from 0.08348 to 0.08328, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1058 - accuracy: 0.9688 - val_loss: 0.0833 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 266/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1015 - accuracy: 0.9711\n",
      "Epoch 266: val_loss improved from 0.08328 to 0.08308, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1055 - accuracy: 0.9688 - val_loss: 0.0831 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 267/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1022 - accuracy: 0.9679\n",
      "Epoch 267: val_loss improved from 0.08308 to 0.08287, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1052 - accuracy: 0.9688 - val_loss: 0.0829 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 268/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0940 - accuracy: 0.9747\n",
      "Epoch 268: val_loss improved from 0.08287 to 0.08266, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1049 - accuracy: 0.9688 - val_loss: 0.0827 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 269/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1064 - accuracy: 0.9690\n",
      "Epoch 269: val_loss improved from 0.08266 to 0.08246, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1046 - accuracy: 0.9688 - val_loss: 0.0825 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 270/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1122 - accuracy: 0.9650\n",
      "Epoch 270: val_loss improved from 0.08246 to 0.08225, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1043 - accuracy: 0.9688 - val_loss: 0.0823 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 271/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9705\n",
      "Epoch 271: val_loss improved from 0.08225 to 0.08205, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1040 - accuracy: 0.9688 - val_loss: 0.0820 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 272/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9688\n",
      "Epoch 272: val_loss improved from 0.08205 to 0.08185, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.1037 - accuracy: 0.9688 - val_loss: 0.0818 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 273/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.9678\n",
      "Epoch 273: val_loss improved from 0.08185 to 0.08163, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.1035 - accuracy: 0.9688 - val_loss: 0.0816 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 274/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1088 - accuracy: 0.9650\n",
      "Epoch 274: val_loss improved from 0.08163 to 0.08143, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1032 - accuracy: 0.9688 - val_loss: 0.0814 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 275/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1015 - accuracy: 0.9708\n",
      "Epoch 275: val_loss improved from 0.08143 to 0.08126, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1029 - accuracy: 0.9688 - val_loss: 0.0813 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 276/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0987 - accuracy: 0.9711\n",
      "Epoch 276: val_loss improved from 0.08126 to 0.08106, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1026 - accuracy: 0.9688 - val_loss: 0.0811 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 277/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1009 - accuracy: 0.9704\n",
      "Epoch 277: val_loss improved from 0.08106 to 0.08087, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1023 - accuracy: 0.9688 - val_loss: 0.0809 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 278/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0969 - accuracy: 0.9718\n",
      "Epoch 278: val_loss improved from 0.08087 to 0.08068, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1021 - accuracy: 0.9688 - val_loss: 0.0807 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 279/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1027 - accuracy: 0.9687\n",
      "Epoch 279: val_loss improved from 0.08068 to 0.08048, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1018 - accuracy: 0.9688 - val_loss: 0.0805 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 280/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1047 - accuracy: 0.9667\n",
      "Epoch 280: val_loss improved from 0.08048 to 0.08028, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1015 - accuracy: 0.9688 - val_loss: 0.0803 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 281/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9688\n",
      "Epoch 281: val_loss improved from 0.08028 to 0.08009, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1012 - accuracy: 0.9688 - val_loss: 0.0801 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 282/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1022 - accuracy: 0.9682\n",
      "Epoch 282: val_loss improved from 0.08009 to 0.07989, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1010 - accuracy: 0.9688 - val_loss: 0.0799 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 283/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.1000 - accuracy: 0.9698\n",
      "Epoch 283: val_loss improved from 0.07989 to 0.07971, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1007 - accuracy: 0.9688 - val_loss: 0.0797 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 284/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0970 - accuracy: 0.9718\n",
      "Epoch 284: val_loss improved from 0.07971 to 0.07957, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1004 - accuracy: 0.9688 - val_loss: 0.0796 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 285/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1029 - accuracy: 0.9663\n",
      "Epoch 285: val_loss improved from 0.07957 to 0.07937, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.1002 - accuracy: 0.9688 - val_loss: 0.0794 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 286/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.1004 - accuracy: 0.9685\n",
      "Epoch 286: val_loss improved from 0.07937 to 0.07917, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0999 - accuracy: 0.9688 - val_loss: 0.0792 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 287/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0952 - accuracy: 0.9707\n",
      "Epoch 287: val_loss improved from 0.07917 to 0.07899, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0996 - accuracy: 0.9688 - val_loss: 0.0790 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 288/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.1002 - accuracy: 0.9682\n",
      "Epoch 288: val_loss improved from 0.07899 to 0.07879, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0994 - accuracy: 0.9688 - val_loss: 0.0788 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 289/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0972 - accuracy: 0.9704\n",
      "Epoch 289: val_loss improved from 0.07879 to 0.07860, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0991 - accuracy: 0.9688 - val_loss: 0.0786 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 290/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.1009 - accuracy: 0.9667\n",
      "Epoch 290: val_loss improved from 0.07860 to 0.07841, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0988 - accuracy: 0.9688 - val_loss: 0.0784 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 291/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0990 - accuracy: 0.9685\n",
      "Epoch 291: val_loss improved from 0.07841 to 0.07822, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0986 - accuracy: 0.9688 - val_loss: 0.0782 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 292/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.1031 - accuracy: 0.9663\n",
      "Epoch 292: val_loss improved from 0.07822 to 0.07804, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0983 - accuracy: 0.9688 - val_loss: 0.0780 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 293/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0991 - accuracy: 0.9687\n",
      "Epoch 293: val_loss improved from 0.07804 to 0.07784, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0980 - accuracy: 0.9688 - val_loss: 0.0778 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 294/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.1044 - accuracy: 0.9650\n",
      "Epoch 294: val_loss improved from 0.07784 to 0.07766, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0978 - accuracy: 0.9688 - val_loss: 0.0777 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 295/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9685\n",
      "Epoch 295: val_loss improved from 0.07766 to 0.07748, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0975 - accuracy: 0.9688 - val_loss: 0.0775 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 296/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.1034 - accuracy: 0.9654\n",
      "Epoch 296: val_loss improved from 0.07748 to 0.07730, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0972 - accuracy: 0.9688 - val_loss: 0.0773 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 297/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0994 - accuracy: 0.9700\n",
      "Epoch 297: val_loss improved from 0.07730 to 0.07712, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0970 - accuracy: 0.9688 - val_loss: 0.0771 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 298/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0932 - accuracy: 0.9714\n",
      "Epoch 298: val_loss improved from 0.07712 to 0.07693, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0968 - accuracy: 0.9688 - val_loss: 0.0769 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 299/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9688\n",
      "Epoch 299: val_loss improved from 0.07693 to 0.07674, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0965 - accuracy: 0.9688 - val_loss: 0.0767 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 300/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0826 - accuracy: 0.9735\n",
      "Epoch 300: val_loss improved from 0.07674 to 0.07657, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0963 - accuracy: 0.9688 - val_loss: 0.0766 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 301/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9708\n",
      "Epoch 301: val_loss improved from 0.07657 to 0.07643, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0960 - accuracy: 0.9688 - val_loss: 0.0764 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 302/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0984 - accuracy: 0.9671\n",
      "Epoch 302: val_loss improved from 0.07643 to 0.07625, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0957 - accuracy: 0.9688 - val_loss: 0.0763 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 303/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0977 - accuracy: 0.9683\n",
      "Epoch 303: val_loss improved from 0.07625 to 0.07607, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0955 - accuracy: 0.9688 - val_loss: 0.0761 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 304/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0939 - accuracy: 0.9683\n",
      "Epoch 304: val_loss improved from 0.07607 to 0.07589, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0952 - accuracy: 0.9688 - val_loss: 0.0759 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 305/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0953 - accuracy: 0.9698\n",
      "Epoch 305: val_loss improved from 0.07589 to 0.07571, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0950 - accuracy: 0.9688 - val_loss: 0.0757 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 306/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9688\n",
      "Epoch 306: val_loss improved from 0.07571 to 0.07553, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0948 - accuracy: 0.9688 - val_loss: 0.0755 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 307/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0939 - accuracy: 0.9711\n",
      "Epoch 307: val_loss improved from 0.07553 to 0.07536, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0945 - accuracy: 0.9688 - val_loss: 0.0754 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 308/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.9685\n",
      "Epoch 308: val_loss improved from 0.07536 to 0.07519, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0942 - accuracy: 0.9688 - val_loss: 0.0752 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 309/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9685\n",
      "Epoch 309: val_loss improved from 0.07519 to 0.07502, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0940 - accuracy: 0.9688 - val_loss: 0.0750 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 310/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0908 - accuracy: 0.9687\n",
      "Epoch 310: val_loss improved from 0.07502 to 0.07485, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0938 - accuracy: 0.9688 - val_loss: 0.0748 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 311/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0964 - accuracy: 0.9674\n",
      "Epoch 311: val_loss improved from 0.07485 to 0.07468, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0935 - accuracy: 0.9688 - val_loss: 0.0747 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 312/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0948 - accuracy: 0.9690\n",
      "Epoch 312: val_loss improved from 0.07468 to 0.07451, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0933 - accuracy: 0.9688 - val_loss: 0.0745 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 313/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9688\n",
      "Epoch 313: val_loss improved from 0.07451 to 0.07435, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0930 - accuracy: 0.9688 - val_loss: 0.0743 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 314/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0946 - accuracy: 0.9687\n",
      "Epoch 314: val_loss improved from 0.07435 to 0.07418, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0928 - accuracy: 0.9688 - val_loss: 0.0742 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 315/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0926 - accuracy: 0.9688\n",
      "Epoch 315: val_loss improved from 0.07418 to 0.07401, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0926 - accuracy: 0.9688 - val_loss: 0.0740 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 316/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0912 - accuracy: 0.9679\n",
      "Epoch 316: val_loss improved from 0.07401 to 0.07384, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0924 - accuracy: 0.9688 - val_loss: 0.0738 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 317/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0976 - accuracy: 0.9654\n",
      "Epoch 317: val_loss improved from 0.07384 to 0.07367, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0921 - accuracy: 0.9688 - val_loss: 0.0737 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 318/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9688\n",
      "Epoch 318: val_loss improved from 0.07367 to 0.07351, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0919 - accuracy: 0.9688 - val_loss: 0.0735 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 319/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0957 - accuracy: 0.9675\n",
      "Epoch 319: val_loss improved from 0.07351 to 0.07335, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0916 - accuracy: 0.9688 - val_loss: 0.0733 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 320/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0877 - accuracy: 0.9687\n",
      "Epoch 320: val_loss improved from 0.07335 to 0.07318, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0914 - accuracy: 0.9688 - val_loss: 0.0732 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 321/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0941 - accuracy: 0.9683\n",
      "Epoch 321: val_loss improved from 0.07318 to 0.07301, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0912 - accuracy: 0.9688 - val_loss: 0.0730 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 322/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0908 - accuracy: 0.9694\n",
      "Epoch 322: val_loss improved from 0.07301 to 0.07285, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0909 - accuracy: 0.9688 - val_loss: 0.0729 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 323/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0932 - accuracy: 0.9674\n",
      "Epoch 323: val_loss improved from 0.07285 to 0.07269, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0907 - accuracy: 0.9688 - val_loss: 0.0727 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 324/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0867 - accuracy: 0.9698\n",
      "Epoch 324: val_loss improved from 0.07269 to 0.07253, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0905 - accuracy: 0.9688 - val_loss: 0.0725 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 325/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0948 - accuracy: 0.9663\n",
      "Epoch 325: val_loss improved from 0.07253 to 0.07237, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0902 - accuracy: 0.9688 - val_loss: 0.0724 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 326/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0911 - accuracy: 0.9694\n",
      "Epoch 326: val_loss improved from 0.07237 to 0.07221, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0900 - accuracy: 0.9688 - val_loss: 0.0722 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 327/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0909 - accuracy: 0.9694\n",
      "Epoch 327: val_loss improved from 0.07221 to 0.07205, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0898 - accuracy: 0.9688 - val_loss: 0.0721 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 328/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9682\n",
      "Epoch 328: val_loss improved from 0.07205 to 0.07188, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0896 - accuracy: 0.9688 - val_loss: 0.0719 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 329/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0848 - accuracy: 0.9687\n",
      "Epoch 329: val_loss improved from 0.07188 to 0.07171, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0893 - accuracy: 0.9688 - val_loss: 0.0717 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 330/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0855 - accuracy: 0.9690\n",
      "Epoch 330: val_loss improved from 0.07171 to 0.07156, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0891 - accuracy: 0.9688 - val_loss: 0.0716 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 331/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0844 - accuracy: 0.9738\n",
      "Epoch 331: val_loss improved from 0.07156 to 0.07139, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0889 - accuracy: 0.9688 - val_loss: 0.0714 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 332/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0949 - accuracy: 0.9654\n",
      "Epoch 332: val_loss improved from 0.07139 to 0.07122, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0886 - accuracy: 0.9688 - val_loss: 0.0712 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 333/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9682\n",
      "Epoch 333: val_loss improved from 0.07122 to 0.07106, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0884 - accuracy: 0.9688 - val_loss: 0.0711 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 334/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0886 - accuracy: 0.9685\n",
      "Epoch 334: val_loss improved from 0.07106 to 0.07090, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0882 - accuracy: 0.9688 - val_loss: 0.0709 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 335/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0831 - accuracy: 0.9711\n",
      "Epoch 335: val_loss improved from 0.07090 to 0.07074, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0880 - accuracy: 0.9688 - val_loss: 0.0707 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 336/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0826 - accuracy: 0.9718\n",
      "Epoch 336: val_loss improved from 0.07074 to 0.07058, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0878 - accuracy: 0.9688 - val_loss: 0.0706 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 337/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0895 - accuracy: 0.9671\n",
      "Epoch 337: val_loss improved from 0.07058 to 0.07042, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0875 - accuracy: 0.9688 - val_loss: 0.0704 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 338/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9685\n",
      "Epoch 338: val_loss improved from 0.07042 to 0.07026, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0873 - accuracy: 0.9688 - val_loss: 0.0703 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 339/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0894 - accuracy: 0.9674\n",
      "Epoch 339: val_loss improved from 0.07026 to 0.07011, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0871 - accuracy: 0.9688 - val_loss: 0.0701 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 340/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0789 - accuracy: 0.9747\n",
      "Epoch 340: val_loss improved from 0.07011 to 0.06996, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0869 - accuracy: 0.9688 - val_loss: 0.0700 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 341/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9701\n",
      "Epoch 341: val_loss improved from 0.06996 to 0.06980, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0866 - accuracy: 0.9710 - val_loss: 0.0698 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 342/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0850 - accuracy: 0.9675\n",
      "Epoch 342: val_loss improved from 0.06980 to 0.06966, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0864 - accuracy: 0.9688 - val_loss: 0.0697 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 343/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0837 - accuracy: 0.9696\n",
      "Epoch 343: val_loss improved from 0.06966 to 0.06950, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0862 - accuracy: 0.9710 - val_loss: 0.0695 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 344/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0872 - accuracy: 0.9705\n",
      "Epoch 344: val_loss improved from 0.06950 to 0.06935, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0860 - accuracy: 0.9710 - val_loss: 0.0694 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 345/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0868 - accuracy: 0.9671\n",
      "Epoch 345: val_loss improved from 0.06935 to 0.06920, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0858 - accuracy: 0.9688 - val_loss: 0.0692 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 346/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0878 - accuracy: 0.9698\n",
      "Epoch 346: val_loss improved from 0.06920 to 0.06905, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0856 - accuracy: 0.9710 - val_loss: 0.0690 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 347/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9732\n",
      "Epoch 347: val_loss improved from 0.06905 to 0.06891, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0854 - accuracy: 0.9732 - val_loss: 0.0689 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 348/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0892 - accuracy: 0.9714\n",
      "Epoch 348: val_loss improved from 0.06891 to 0.06876, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0851 - accuracy: 0.9732 - val_loss: 0.0688 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 349/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0872 - accuracy: 0.9718\n",
      "Epoch 349: val_loss improved from 0.06876 to 0.06861, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0849 - accuracy: 0.9732 - val_loss: 0.0686 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 350/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0888 - accuracy: 0.9696\n",
      "Epoch 350: val_loss improved from 0.06861 to 0.06848, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0847 - accuracy: 0.9732 - val_loss: 0.0685 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 351/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0847 - accuracy: 0.9722\n",
      "Epoch 351: val_loss improved from 0.06848 to 0.06833, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0845 - accuracy: 0.9732 - val_loss: 0.0683 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 352/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0806 - accuracy: 0.9753\n",
      "Epoch 352: val_loss improved from 0.06833 to 0.06817, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0843 - accuracy: 0.9732 - val_loss: 0.0682 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 353/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0819 - accuracy: 0.9753\n",
      "Epoch 353: val_loss improved from 0.06817 to 0.06802, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0841 - accuracy: 0.9732 - val_loss: 0.0680 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 354/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0858 - accuracy: 0.9718\n",
      "Epoch 354: val_loss improved from 0.06802 to 0.06787, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0839 - accuracy: 0.9732 - val_loss: 0.0679 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 355/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0897 - accuracy: 0.9700\n",
      "Epoch 355: val_loss improved from 0.06787 to 0.06772, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0837 - accuracy: 0.9732 - val_loss: 0.0677 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 356/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0861 - accuracy: 0.9711\n",
      "Epoch 356: val_loss improved from 0.06772 to 0.06756, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0835 - accuracy: 0.9732 - val_loss: 0.0676 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 357/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0855 - accuracy: 0.9722\n",
      "Epoch 357: val_loss improved from 0.06756 to 0.06742, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0833 - accuracy: 0.9732 - val_loss: 0.0674 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 358/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0783 - accuracy: 0.9753\n",
      "Epoch 358: val_loss improved from 0.06742 to 0.06729, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0831 - accuracy: 0.9732 - val_loss: 0.0673 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 359/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0781 - accuracy: 0.9750\n",
      "Epoch 359: val_loss improved from 0.06729 to 0.06715, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0829 - accuracy: 0.9732 - val_loss: 0.0672 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 360/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0831 - accuracy: 0.9727\n",
      "Epoch 360: val_loss improved from 0.06715 to 0.06701, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0827 - accuracy: 0.9732 - val_loss: 0.0670 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 361/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0812 - accuracy: 0.9753\n",
      "Epoch 361: val_loss improved from 0.06701 to 0.06686, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0825 - accuracy: 0.9732 - val_loss: 0.0669 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 362/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0835 - accuracy: 0.9727\n",
      "Epoch 362: val_loss improved from 0.06686 to 0.06672, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0822 - accuracy: 0.9732 - val_loss: 0.0667 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 363/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0794 - accuracy: 0.9728\n",
      "Epoch 363: val_loss improved from 0.06672 to 0.06657, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0820 - accuracy: 0.9732 - val_loss: 0.0666 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 364/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0745 - accuracy: 0.9767\n",
      "Epoch 364: val_loss improved from 0.06657 to 0.06644, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0819 - accuracy: 0.9732 - val_loss: 0.0664 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 365/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9732\n",
      "Epoch 365: val_loss improved from 0.06644 to 0.06629, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0816 - accuracy: 0.9732 - val_loss: 0.0663 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 366/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0869 - accuracy: 0.9704\n",
      "Epoch 366: val_loss improved from 0.06629 to 0.06615, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0815 - accuracy: 0.9732 - val_loss: 0.0661 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 367/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0847 - accuracy: 0.9714\n",
      "Epoch 367: val_loss improved from 0.06615 to 0.06599, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0812 - accuracy: 0.9732 - val_loss: 0.0660 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 368/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0847 - accuracy: 0.9725\n",
      "Epoch 368: val_loss improved from 0.06599 to 0.06585, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0810 - accuracy: 0.9732 - val_loss: 0.0659 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 369/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0824 - accuracy: 0.9728\n",
      "Epoch 369: val_loss improved from 0.06585 to 0.06571, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0808 - accuracy: 0.9732 - val_loss: 0.0657 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 370/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0819 - accuracy: 0.9724\n",
      "Epoch 370: val_loss improved from 0.06571 to 0.06557, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0806 - accuracy: 0.9732 - val_loss: 0.0656 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 371/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0831 - accuracy: 0.9718\n",
      "Epoch 371: val_loss improved from 0.06557 to 0.06544, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0805 - accuracy: 0.9732 - val_loss: 0.0654 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 372/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0811 - accuracy: 0.9727\n",
      "Epoch 372: val_loss improved from 0.06544 to 0.06530, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0802 - accuracy: 0.9732 - val_loss: 0.0653 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 373/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9727\n",
      "Epoch 373: val_loss improved from 0.06530 to 0.06515, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0800 - accuracy: 0.9732 - val_loss: 0.0651 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 374/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9727\n",
      "Epoch 374: val_loss improved from 0.06515 to 0.06499, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0799 - accuracy: 0.9732 - val_loss: 0.0650 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 375/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0760 - accuracy: 0.9753\n",
      "Epoch 375: val_loss improved from 0.06499 to 0.06486, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0797 - accuracy: 0.9732 - val_loss: 0.0649 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 376/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0826 - accuracy: 0.9718\n",
      "Epoch 376: val_loss improved from 0.06486 to 0.06474, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0795 - accuracy: 0.9732 - val_loss: 0.0647 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 377/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0771 - accuracy: 0.9756\n",
      "Epoch 377: val_loss improved from 0.06474 to 0.06459, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0793 - accuracy: 0.9732 - val_loss: 0.0646 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 378/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0803 - accuracy: 0.9724\n",
      "Epoch 378: val_loss improved from 0.06459 to 0.06446, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0791 - accuracy: 0.9732 - val_loss: 0.0645 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 379/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0798 - accuracy: 0.9727\n",
      "Epoch 379: val_loss improved from 0.06446 to 0.06432, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0789 - accuracy: 0.9732 - val_loss: 0.0643 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 380/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0823 - accuracy: 0.9707\n",
      "Epoch 380: val_loss improved from 0.06432 to 0.06419, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0787 - accuracy: 0.9732 - val_loss: 0.0642 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 381/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0823 - accuracy: 0.9714\n",
      "Epoch 381: val_loss improved from 0.06419 to 0.06406, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0785 - accuracy: 0.9732 - val_loss: 0.0641 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 382/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9732\n",
      "Epoch 382: val_loss improved from 0.06406 to 0.06397, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0783 - accuracy: 0.9732 - val_loss: 0.0640 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 383/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0771 - accuracy: 0.9741\n",
      "Epoch 383: val_loss improved from 0.06397 to 0.06383, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0781 - accuracy: 0.9732 - val_loss: 0.0638 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 384/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0817 - accuracy: 0.9707\n",
      "Epoch 384: val_loss improved from 0.06383 to 0.06366, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0779 - accuracy: 0.9732 - val_loss: 0.0637 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 385/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0791 - accuracy: 0.9750\n",
      "Epoch 385: val_loss improved from 0.06366 to 0.06353, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0777 - accuracy: 0.9754 - val_loss: 0.0635 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 386/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0785 - accuracy: 0.9744\n",
      "Epoch 386: val_loss improved from 0.06353 to 0.06339, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0776 - accuracy: 0.9754 - val_loss: 0.0634 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 387/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0793 - accuracy: 0.9718\n",
      "Epoch 387: val_loss improved from 0.06339 to 0.06327, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0774 - accuracy: 0.9732 - val_loss: 0.0633 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 388/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0675 - accuracy: 0.9825\n",
      "Epoch 388: val_loss improved from 0.06327 to 0.06314, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0772 - accuracy: 0.9754 - val_loss: 0.0631 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 389/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9793\n",
      "Epoch 389: val_loss improved from 0.06314 to 0.06302, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0770 - accuracy: 0.9754 - val_loss: 0.0630 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 390/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0757 - accuracy: 0.9778\n",
      "Epoch 390: val_loss improved from 0.06302 to 0.06288, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0768 - accuracy: 0.9777 - val_loss: 0.0629 - val_accuracy: 0.9821 - lr: 0.0010\n",
      "Epoch 391/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0803 - accuracy: 0.9732\n",
      "Epoch 391: val_loss improved from 0.06288 to 0.06274, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0766 - accuracy: 0.9754 - val_loss: 0.0627 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 392/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0797 - accuracy: 0.9762\n",
      "Epoch 392: val_loss improved from 0.06274 to 0.06260, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0765 - accuracy: 0.9777 - val_loss: 0.0626 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 393/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0784 - accuracy: 0.9788\n",
      "Epoch 393: val_loss improved from 0.06260 to 0.06246, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0763 - accuracy: 0.9799 - val_loss: 0.0625 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 394/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0763 - accuracy: 0.9798\n",
      "Epoch 394: val_loss improved from 0.06246 to 0.06232, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0761 - accuracy: 0.9799 - val_loss: 0.0623 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 395/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0756 - accuracy: 0.9791\n",
      "Epoch 395: val_loss improved from 0.06232 to 0.06218, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0759 - accuracy: 0.9799 - val_loss: 0.0622 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 396/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0793 - accuracy: 0.9786\n",
      "Epoch 396: val_loss improved from 0.06218 to 0.06205, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0757 - accuracy: 0.9799 - val_loss: 0.0620 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 397/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9795\n",
      "Epoch 397: val_loss improved from 0.06205 to 0.06191, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0756 - accuracy: 0.9799 - val_loss: 0.0619 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 398/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0761 - accuracy: 0.9827\n",
      "Epoch 398: val_loss improved from 0.06191 to 0.06181, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0754 - accuracy: 0.9799 - val_loss: 0.0618 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 399/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0763 - accuracy: 0.9810\n",
      "Epoch 399: val_loss improved from 0.06181 to 0.06169, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0752 - accuracy: 0.9799 - val_loss: 0.0617 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 400/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9798\n",
      "Epoch 400: val_loss improved from 0.06169 to 0.06156, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0750 - accuracy: 0.9799 - val_loss: 0.0616 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 401/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0752 - accuracy: 0.9798\n",
      "Epoch 401: val_loss improved from 0.06156 to 0.06142, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0748 - accuracy: 0.9799 - val_loss: 0.0614 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 402/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0739 - accuracy: 0.9807\n",
      "Epoch 402: val_loss improved from 0.06142 to 0.06129, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0747 - accuracy: 0.9799 - val_loss: 0.0613 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 403/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0763 - accuracy: 0.9791\n",
      "Epoch 403: val_loss improved from 0.06129 to 0.06117, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0745 - accuracy: 0.9799 - val_loss: 0.0612 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 404/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9799\n",
      "Epoch 404: val_loss improved from 0.06117 to 0.06103, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0743 - accuracy: 0.9799 - val_loss: 0.0610 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 405/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0788 - accuracy: 0.9780\n",
      "Epoch 405: val_loss improved from 0.06103 to 0.06091, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0741 - accuracy: 0.9799 - val_loss: 0.0609 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 406/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0751 - accuracy: 0.9807\n",
      "Epoch 406: val_loss improved from 0.06091 to 0.06078, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0739 - accuracy: 0.9799 - val_loss: 0.0608 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 407/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9799\n",
      "Epoch 407: val_loss improved from 0.06078 to 0.06066, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0737 - accuracy: 0.9799 - val_loss: 0.0607 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 408/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0756 - accuracy: 0.9786\n",
      "Epoch 408: val_loss improved from 0.06066 to 0.06054, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0736 - accuracy: 0.9799 - val_loss: 0.0605 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 409/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0776 - accuracy: 0.9778\n",
      "Epoch 409: val_loss improved from 0.06054 to 0.06042, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0734 - accuracy: 0.9799 - val_loss: 0.0604 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 410/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9839\n",
      "Epoch 410: val_loss improved from 0.06042 to 0.06031, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0733 - accuracy: 0.9799 - val_loss: 0.0603 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 411/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0728 - accuracy: 0.9793\n",
      "Epoch 411: val_loss improved from 0.06031 to 0.06017, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0731 - accuracy: 0.9799 - val_loss: 0.0602 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 412/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0733 - accuracy: 0.9798\n",
      "Epoch 412: val_loss improved from 0.06017 to 0.06004, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0729 - accuracy: 0.9799 - val_loss: 0.0600 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 413/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0732 - accuracy: 0.9795\n",
      "Epoch 413: val_loss improved from 0.06004 to 0.05990, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0727 - accuracy: 0.9799 - val_loss: 0.0599 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 414/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0730 - accuracy: 0.9786\n",
      "Epoch 414: val_loss improved from 0.05990 to 0.05977, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0726 - accuracy: 0.9799 - val_loss: 0.0598 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 415/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0651 - accuracy: 0.9850\n",
      "Epoch 415: val_loss improved from 0.05977 to 0.05972, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0724 - accuracy: 0.9799 - val_loss: 0.0597 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 416/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0722 - accuracy: 0.9795\n",
      "Epoch 416: val_loss improved from 0.05972 to 0.05959, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0722 - accuracy: 0.9799 - val_loss: 0.0596 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 417/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0714 - accuracy: 0.9814\n",
      "Epoch 417: val_loss improved from 0.05959 to 0.05945, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0720 - accuracy: 0.9799 - val_loss: 0.0595 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 418/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0721 - accuracy: 0.9798\n",
      "Epoch 418: val_loss improved from 0.05945 to 0.05931, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0719 - accuracy: 0.9799 - val_loss: 0.0593 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 419/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9799\n",
      "Epoch 419: val_loss improved from 0.05931 to 0.05920, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0717 - accuracy: 0.9799 - val_loss: 0.0592 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 420/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0719 - accuracy: 0.9798\n",
      "Epoch 420: val_loss improved from 0.05920 to 0.05908, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0715 - accuracy: 0.9799 - val_loss: 0.0591 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 421/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9820\n",
      "Epoch 421: val_loss improved from 0.05908 to 0.05895, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0714 - accuracy: 0.9821 - val_loss: 0.0589 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 422/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0706 - accuracy: 0.9795\n",
      "Epoch 422: val_loss improved from 0.05895 to 0.05887, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0712 - accuracy: 0.9799 - val_loss: 0.0589 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 423/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0718 - accuracy: 0.9818\n",
      "Epoch 423: val_loss improved from 0.05887 to 0.05874, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0710 - accuracy: 0.9821 - val_loss: 0.0587 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 424/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0700 - accuracy: 0.9820\n",
      "Epoch 424: val_loss improved from 0.05874 to 0.05864, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9821 - val_loss: 0.0586 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 425/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0659 - accuracy: 0.9846\n",
      "Epoch 425: val_loss improved from 0.05864 to 0.05851, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0707 - accuracy: 0.9821 - val_loss: 0.0585 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 426/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0718 - accuracy: 0.9825\n",
      "Epoch 426: val_loss improved from 0.05851 to 0.05841, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0705 - accuracy: 0.9821 - val_loss: 0.0584 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 427/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9818\n",
      "Epoch 427: val_loss improved from 0.05841 to 0.05829, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0704 - accuracy: 0.9821 - val_loss: 0.0583 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 428/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0683 - accuracy: 0.9823\n",
      "Epoch 428: val_loss improved from 0.05829 to 0.05817, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0702 - accuracy: 0.9821 - val_loss: 0.0582 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 429/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0690 - accuracy: 0.9846\n",
      "Epoch 429: val_loss improved from 0.05817 to 0.05801, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0700 - accuracy: 0.9821 - val_loss: 0.0580 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 430/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9821\n",
      "Epoch 430: val_loss improved from 0.05801 to 0.05789, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0699 - accuracy: 0.9821 - val_loss: 0.0579 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 431/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0591 - accuracy: 0.9872\n",
      "Epoch 431: val_loss improved from 0.05789 to 0.05779, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0697 - accuracy: 0.9821 - val_loss: 0.0578 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 432/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0654 - accuracy: 0.9848\n",
      "Epoch 432: val_loss improved from 0.05779 to 0.05767, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 0.9821 - val_loss: 0.0577 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 433/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0698 - accuracy: 0.9820\n",
      "Epoch 433: val_loss improved from 0.05767 to 0.05755, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0694 - accuracy: 0.9821 - val_loss: 0.0576 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 434/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0668 - accuracy: 0.9821\n",
      "Epoch 434: val_loss improved from 0.05755 to 0.05743, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0692 - accuracy: 0.9821 - val_loss: 0.0574 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 435/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0756 - accuracy: 0.9795\n",
      "Epoch 435: val_loss improved from 0.05743 to 0.05731, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9821 - val_loss: 0.0573 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 436/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0731 - accuracy: 0.9800\n",
      "Epoch 436: val_loss improved from 0.05731 to 0.05720, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9821 - val_loss: 0.0572 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 437/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0690 - accuracy: 0.9820\n",
      "Epoch 437: val_loss improved from 0.05720 to 0.05708, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0688 - accuracy: 0.9821 - val_loss: 0.0571 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 438/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9821\n",
      "Epoch 438: val_loss improved from 0.05708 to 0.05695, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0686 - accuracy: 0.9821 - val_loss: 0.0570 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 439/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9821\n",
      "Epoch 439: val_loss improved from 0.05695 to 0.05682, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0685 - accuracy: 0.9821 - val_loss: 0.0568 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 440/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0732 - accuracy: 0.9797\n",
      "Epoch 440: val_loss improved from 0.05682 to 0.05671, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0683 - accuracy: 0.9821 - val_loss: 0.0567 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 441/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0738 - accuracy: 0.9795\n",
      "Epoch 441: val_loss improved from 0.05671 to 0.05660, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0681 - accuracy: 0.9821 - val_loss: 0.0566 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 442/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0693 - accuracy: 0.9814\n",
      "Epoch 442: val_loss improved from 0.05660 to 0.05648, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0680 - accuracy: 0.9821 - val_loss: 0.0565 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 443/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0684 - accuracy: 0.9816\n",
      "Epoch 443: val_loss improved from 0.05648 to 0.05634, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0678 - accuracy: 0.9821 - val_loss: 0.0563 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 444/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0680 - accuracy: 0.9818\n",
      "Epoch 444: val_loss improved from 0.05634 to 0.05623, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0677 - accuracy: 0.9821 - val_loss: 0.0562 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 445/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9820\n",
      "Epoch 445: val_loss improved from 0.05623 to 0.05613, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0675 - accuracy: 0.9821 - val_loss: 0.0561 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 446/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0688 - accuracy: 0.9812\n",
      "Epoch 446: val_loss improved from 0.05613 to 0.05602, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0673 - accuracy: 0.9821 - val_loss: 0.0560 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 447/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9818\n",
      "Epoch 447: val_loss improved from 0.05602 to 0.05591, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0672 - accuracy: 0.9821 - val_loss: 0.0559 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 448/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0635 - accuracy: 0.9835\n",
      "Epoch 448: val_loss improved from 0.05591 to 0.05581, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0670 - accuracy: 0.9821 - val_loss: 0.0558 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 449/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9821\n",
      "Epoch 449: val_loss improved from 0.05581 to 0.05569, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0669 - accuracy: 0.9821 - val_loss: 0.0557 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 450/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0629 - accuracy: 0.9833\n",
      "Epoch 450: val_loss improved from 0.05569 to 0.05559, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0667 - accuracy: 0.9821 - val_loss: 0.0556 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 451/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0649 - accuracy: 0.9823\n",
      "Epoch 451: val_loss improved from 0.05559 to 0.05551, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0666 - accuracy: 0.9821 - val_loss: 0.0555 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 452/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9820\n",
      "Epoch 452: val_loss improved from 0.05551 to 0.05540, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.9821 - val_loss: 0.0554 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 453/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0651 - accuracy: 0.9837\n",
      "Epoch 453: val_loss improved from 0.05540 to 0.05530, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0663 - accuracy: 0.9821 - val_loss: 0.0553 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 454/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9821\n",
      "Epoch 454: val_loss improved from 0.05530 to 0.05518, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0661 - accuracy: 0.9821 - val_loss: 0.0552 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 455/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0662 - accuracy: 0.9820\n",
      "Epoch 455: val_loss improved from 0.05518 to 0.05506, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0660 - accuracy: 0.9821 - val_loss: 0.0551 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 456/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0611 - accuracy: 0.9848\n",
      "Epoch 456: val_loss improved from 0.05506 to 0.05496, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0658 - accuracy: 0.9821 - val_loss: 0.0550 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 457/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9818\n",
      "Epoch 457: val_loss improved from 0.05496 to 0.05485, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0657 - accuracy: 0.9821 - val_loss: 0.0548 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 458/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0659 - accuracy: 0.9818\n",
      "Epoch 458: val_loss improved from 0.05485 to 0.05475, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0655 - accuracy: 0.9821 - val_loss: 0.0547 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 459/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0672 - accuracy: 0.9814\n",
      "Epoch 459: val_loss improved from 0.05475 to 0.05463, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0654 - accuracy: 0.9821 - val_loss: 0.0546 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 460/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0667 - accuracy: 0.9814\n",
      "Epoch 460: val_loss improved from 0.05463 to 0.05451, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0652 - accuracy: 0.9821 - val_loss: 0.0545 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 461/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0660 - accuracy: 0.9818\n",
      "Epoch 461: val_loss improved from 0.05451 to 0.05441, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0651 - accuracy: 0.9821 - val_loss: 0.0544 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 462/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0649 - accuracy: 0.9833\n",
      "Epoch 462: val_loss improved from 0.05441 to 0.05429, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0649 - accuracy: 0.9821 - val_loss: 0.0543 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 463/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9818\n",
      "Epoch 463: val_loss improved from 0.05429 to 0.05420, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0648 - accuracy: 0.9821 - val_loss: 0.0542 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 464/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0677 - accuracy: 0.9805\n",
      "Epoch 464: val_loss improved from 0.05420 to 0.05409, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0647 - accuracy: 0.9821 - val_loss: 0.0541 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 465/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0660 - accuracy: 0.9812\n",
      "Epoch 465: val_loss improved from 0.05409 to 0.05398, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0645 - accuracy: 0.9821 - val_loss: 0.0540 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 466/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0625 - accuracy: 0.9843\n",
      "Epoch 466: val_loss improved from 0.05398 to 0.05394, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0644 - accuracy: 0.9821 - val_loss: 0.0539 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 467/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0587 - accuracy: 0.9852\n",
      "Epoch 467: val_loss improved from 0.05394 to 0.05384, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0642 - accuracy: 0.9821 - val_loss: 0.0538 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 468/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0681 - accuracy: 0.9802\n",
      "Epoch 468: val_loss improved from 0.05384 to 0.05374, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0641 - accuracy: 0.9821 - val_loss: 0.0537 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 469/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9821\n",
      "Epoch 469: val_loss improved from 0.05374 to 0.05363, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0639 - accuracy: 0.9821 - val_loss: 0.0536 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 470/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0608 - accuracy: 0.9833\n",
      "Epoch 470: val_loss improved from 0.05363 to 0.05354, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0638 - accuracy: 0.9821 - val_loss: 0.0535 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 471/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0676 - accuracy: 0.9805\n",
      "Epoch 471: val_loss improved from 0.05354 to 0.05343, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0636 - accuracy: 0.9821 - val_loss: 0.0534 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 472/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0612 - accuracy: 0.9833\n",
      "Epoch 472: val_loss improved from 0.05343 to 0.05333, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0635 - accuracy: 0.9821 - val_loss: 0.0533 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 473/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0664 - accuracy: 0.9800\n",
      "Epoch 473: val_loss improved from 0.05333 to 0.05322, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0634 - accuracy: 0.9821 - val_loss: 0.0532 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 474/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0591 - accuracy: 0.9827\n",
      "Epoch 474: val_loss improved from 0.05322 to 0.05311, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0632 - accuracy: 0.9821 - val_loss: 0.0531 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 475/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0650 - accuracy: 0.9814\n",
      "Epoch 475: val_loss improved from 0.05311 to 0.05300, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0631 - accuracy: 0.9821 - val_loss: 0.0530 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 476/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0650 - accuracy: 0.9812\n",
      "Epoch 476: val_loss improved from 0.05300 to 0.05289, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0630 - accuracy: 0.9821 - val_loss: 0.0529 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 477/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0626 - accuracy: 0.9818\n",
      "Epoch 477: val_loss improved from 0.05289 to 0.05278, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0628 - accuracy: 0.9821 - val_loss: 0.0528 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 478/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0666 - accuracy: 0.9802\n",
      "Epoch 478: val_loss improved from 0.05278 to 0.05267, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0627 - accuracy: 0.9821 - val_loss: 0.0527 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 479/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0639 - accuracy: 0.9816\n",
      "Epoch 479: val_loss improved from 0.05267 to 0.05255, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0625 - accuracy: 0.9821 - val_loss: 0.0525 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 480/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0631 - accuracy: 0.9816\n",
      "Epoch 480: val_loss improved from 0.05255 to 0.05244, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0624 - accuracy: 0.9821 - val_loss: 0.0524 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 481/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0646 - accuracy: 0.9827\n",
      "Epoch 481: val_loss improved from 0.05244 to 0.05234, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0622 - accuracy: 0.9821 - val_loss: 0.0523 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 482/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0619 - accuracy: 0.9820\n",
      "Epoch 482: val_loss improved from 0.05234 to 0.05223, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0621 - accuracy: 0.9821 - val_loss: 0.0522 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 483/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0609 - accuracy: 0.9823\n",
      "Epoch 483: val_loss improved from 0.05223 to 0.05212, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0620 - accuracy: 0.9821 - val_loss: 0.0521 - val_accuracy: 0.9881 - lr: 0.0010\n",
      "Epoch 484/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0623 - accuracy: 0.9814\n",
      "Epoch 484: val_loss improved from 0.05212 to 0.05203, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0619 - accuracy: 0.9821 - val_loss: 0.0520 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 485/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0634 - accuracy: 0.9837\n",
      "Epoch 485: val_loss improved from 0.05203 to 0.05193, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0617 - accuracy: 0.9844 - val_loss: 0.0519 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 486/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0553 - accuracy: 0.9865\n",
      "Epoch 486: val_loss improved from 0.05193 to 0.05192, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0616 - accuracy: 0.9844 - val_loss: 0.0519 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 487/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0549 - accuracy: 0.9881\n",
      "Epoch 487: val_loss improved from 0.05192 to 0.05188, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0614 - accuracy: 0.9844 - val_loss: 0.0519 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 488/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0644 - accuracy: 0.9833\n",
      "Epoch 488: val_loss improved from 0.05188 to 0.05177, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0613 - accuracy: 0.9844 - val_loss: 0.0518 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 489/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0637 - accuracy: 0.9831\n",
      "Epoch 489: val_loss improved from 0.05177 to 0.05165, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0612 - accuracy: 0.9844 - val_loss: 0.0516 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 490/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0544 - accuracy: 0.9882\n",
      "Epoch 490: val_loss improved from 0.05165 to 0.05156, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0610 - accuracy: 0.9844 - val_loss: 0.0516 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 491/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0615 - accuracy: 0.9841\n",
      "Epoch 491: val_loss improved from 0.05156 to 0.05146, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0609 - accuracy: 0.9844 - val_loss: 0.0515 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 492/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0615 - accuracy: 0.9855\n",
      "Epoch 492: val_loss improved from 0.05146 to 0.05135, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0608 - accuracy: 0.9844 - val_loss: 0.0514 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 493/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9844\n",
      "Epoch 493: val_loss improved from 0.05135 to 0.05126, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0606 - accuracy: 0.9844 - val_loss: 0.0513 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 494/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0607 - accuracy: 0.9843\n",
      "Epoch 494: val_loss improved from 0.05126 to 0.05115, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0605 - accuracy: 0.9844 - val_loss: 0.0512 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 495/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0528 - accuracy: 0.9902\n",
      "Epoch 495: val_loss did not improve from 0.05115\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0604 - accuracy: 0.9844 - val_loss: 0.0512 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 496/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0615 - accuracy: 0.9833\n",
      "Epoch 496: val_loss improved from 0.05115 to 0.05108, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0602 - accuracy: 0.9844 - val_loss: 0.0511 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 497/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0615 - accuracy: 0.9837\n",
      "Epoch 497: val_loss improved from 0.05108 to 0.05098, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0601 - accuracy: 0.9844 - val_loss: 0.0510 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 498/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0602 - accuracy: 0.9843\n",
      "Epoch 498: val_loss improved from 0.05098 to 0.05086, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0600 - accuracy: 0.9844 - val_loss: 0.0509 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 499/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0621 - accuracy: 0.9833\n",
      "Epoch 499: val_loss improved from 0.05086 to 0.05074, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0598 - accuracy: 0.9844 - val_loss: 0.0507 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 500/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0493 - accuracy: 0.9882\n",
      "Epoch 500: val_loss improved from 0.05074 to 0.05066, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0597 - accuracy: 0.9844 - val_loss: 0.0507 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 501/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0559 - accuracy: 0.9854\n",
      "Epoch 501: val_loss improved from 0.05066 to 0.05055, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0596 - accuracy: 0.9844 - val_loss: 0.0506 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 502/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0609 - accuracy: 0.9837\n",
      "Epoch 502: val_loss improved from 0.05055 to 0.05044, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0595 - accuracy: 0.9844 - val_loss: 0.0504 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 503/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0605 - accuracy: 0.9835\n",
      "Epoch 503: val_loss improved from 0.05044 to 0.05036, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0594 - accuracy: 0.9844 - val_loss: 0.0504 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 504/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0595 - accuracy: 0.9843\n",
      "Epoch 504: val_loss improved from 0.05036 to 0.05025, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0592 - accuracy: 0.9844 - val_loss: 0.0502 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 505/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0621 - accuracy: 0.9831\n",
      "Epoch 505: val_loss improved from 0.05025 to 0.05017, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0591 - accuracy: 0.9844 - val_loss: 0.0502 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 506/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0604 - accuracy: 0.9833\n",
      "Epoch 506: val_loss improved from 0.05017 to 0.05005, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0590 - accuracy: 0.9844 - val_loss: 0.0500 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 507/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0577 - accuracy: 0.9859\n",
      "Epoch 507: val_loss improved from 0.05005 to 0.04995, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0588 - accuracy: 0.9844 - val_loss: 0.0500 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 508/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0606 - accuracy: 0.9835\n",
      "Epoch 508: val_loss improved from 0.04995 to 0.04985, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0587 - accuracy: 0.9844 - val_loss: 0.0499 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 509/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0546 - accuracy: 0.9880\n",
      "Epoch 509: val_loss improved from 0.04985 to 0.04977, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0586 - accuracy: 0.9844 - val_loss: 0.0498 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 510/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0588 - accuracy: 0.9843\n",
      "Epoch 510: val_loss improved from 0.04977 to 0.04966, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0584 - accuracy: 0.9844 - val_loss: 0.0497 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 511/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0599 - accuracy: 0.9837\n",
      "Epoch 511: val_loss improved from 0.04966 to 0.04957, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0583 - accuracy: 0.9844 - val_loss: 0.0496 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 512/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0598 - accuracy: 0.9837\n",
      "Epoch 512: val_loss improved from 0.04957 to 0.04947, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0582 - accuracy: 0.9844 - val_loss: 0.0495 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 513/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0608 - accuracy: 0.9829\n",
      "Epoch 513: val_loss improved from 0.04947 to 0.04938, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0581 - accuracy: 0.9844 - val_loss: 0.0494 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 514/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9841\n",
      "Epoch 514: val_loss improved from 0.04938 to 0.04931, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0580 - accuracy: 0.9844 - val_loss: 0.0493 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 515/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9844\n",
      "Epoch 515: val_loss improved from 0.04931 to 0.04922, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0578 - accuracy: 0.9844 - val_loss: 0.0492 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 516/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9844\n",
      "Epoch 516: val_loss improved from 0.04922 to 0.04913, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0577 - accuracy: 0.9844 - val_loss: 0.0491 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 517/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0583 - accuracy: 0.9841\n",
      "Epoch 517: val_loss improved from 0.04913 to 0.04905, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0576 - accuracy: 0.9844 - val_loss: 0.0490 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 518/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0578 - accuracy: 0.9841\n",
      "Epoch 518: val_loss improved from 0.04905 to 0.04895, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0575 - accuracy: 0.9844 - val_loss: 0.0490 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 519/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0547 - accuracy: 0.9854\n",
      "Epoch 519: val_loss improved from 0.04895 to 0.04889, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0573 - accuracy: 0.9844 - val_loss: 0.0489 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 520/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0581 - accuracy: 0.9854\n",
      "Epoch 520: val_loss improved from 0.04889 to 0.04879, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0572 - accuracy: 0.9844 - val_loss: 0.0488 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 521/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0591 - accuracy: 0.9837\n",
      "Epoch 521: val_loss improved from 0.04879 to 0.04869, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0571 - accuracy: 0.9844 - val_loss: 0.0487 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 522/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0566 - accuracy: 0.9878\n",
      "Epoch 522: val_loss improved from 0.04869 to 0.04867, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0570 - accuracy: 0.9844 - val_loss: 0.0487 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 523/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0595 - accuracy: 0.9829\n",
      "Epoch 523: val_loss improved from 0.04867 to 0.04859, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0568 - accuracy: 0.9844 - val_loss: 0.0486 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 524/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0588 - accuracy: 0.9835\n",
      "Epoch 524: val_loss improved from 0.04859 to 0.04852, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0567 - accuracy: 0.9844 - val_loss: 0.0485 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 525/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0568 - accuracy: 0.9843\n",
      "Epoch 525: val_loss improved from 0.04852 to 0.04843, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0566 - accuracy: 0.9844 - val_loss: 0.0484 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 526/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0506 - accuracy: 0.9880\n",
      "Epoch 526: val_loss improved from 0.04843 to 0.04835, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0565 - accuracy: 0.9844 - val_loss: 0.0483 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 527/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9839\n",
      "Epoch 527: val_loss improved from 0.04835 to 0.04824, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0564 - accuracy: 0.9844 - val_loss: 0.0482 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 528/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9844\n",
      "Epoch 528: val_loss improved from 0.04824 to 0.04813, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0563 - accuracy: 0.9844 - val_loss: 0.0481 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 529/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9843\n",
      "Epoch 529: val_loss improved from 0.04813 to 0.04800, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0561 - accuracy: 0.9844 - val_loss: 0.0480 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 530/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0484 - accuracy: 0.9884\n",
      "Epoch 530: val_loss improved from 0.04800 to 0.04792, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0560 - accuracy: 0.9844 - val_loss: 0.0479 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 531/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0576 - accuracy: 0.9831\n",
      "Epoch 531: val_loss improved from 0.04792 to 0.04781, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0559 - accuracy: 0.9844 - val_loss: 0.0478 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 532/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0561 - accuracy: 0.9843\n",
      "Epoch 532: val_loss improved from 0.04781 to 0.04774, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0558 - accuracy: 0.9844 - val_loss: 0.0477 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 533/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0560 - accuracy: 0.9859\n",
      "Epoch 533: val_loss improved from 0.04774 to 0.04766, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0557 - accuracy: 0.9844 - val_loss: 0.0477 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 534/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0444 - accuracy: 0.9900\n",
      "Epoch 534: val_loss improved from 0.04766 to 0.04761, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0555 - accuracy: 0.9844 - val_loss: 0.0476 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 535/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0503 - accuracy: 0.9859\n",
      "Epoch 535: val_loss improved from 0.04761 to 0.04749, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0554 - accuracy: 0.9844 - val_loss: 0.0475 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 536/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0530 - accuracy: 0.9850\n",
      "Epoch 536: val_loss improved from 0.04749 to 0.04739, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0553 - accuracy: 0.9844 - val_loss: 0.0474 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 537/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9843\n",
      "Epoch 537: val_loss improved from 0.04739 to 0.04733, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0552 - accuracy: 0.9844 - val_loss: 0.0473 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 538/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0574 - accuracy: 0.9854\n",
      "Epoch 538: val_loss improved from 0.04733 to 0.04726, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0551 - accuracy: 0.9844 - val_loss: 0.0473 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 539/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0564 - accuracy: 0.9835\n",
      "Epoch 539: val_loss improved from 0.04726 to 0.04716, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0550 - accuracy: 0.9844 - val_loss: 0.0472 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 540/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9844\n",
      "Epoch 540: val_loss improved from 0.04716 to 0.04713, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0549 - accuracy: 0.9844 - val_loss: 0.0471 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 541/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9888\n",
      "Epoch 541: val_loss improved from 0.04713 to 0.04704, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0547 - accuracy: 0.9888 - val_loss: 0.0470 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 542/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0552 - accuracy: 0.9841\n",
      "Epoch 542: val_loss improved from 0.04704 to 0.04698, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0546 - accuracy: 0.9844 - val_loss: 0.0470 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 543/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0558 - accuracy: 0.9859\n",
      "Epoch 543: val_loss improved from 0.04698 to 0.04691, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0545 - accuracy: 0.9866 - val_loss: 0.0469 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 544/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0517 - accuracy: 0.9900\n",
      "Epoch 544: val_loss improved from 0.04691 to 0.04681, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0544 - accuracy: 0.9888 - val_loss: 0.0468 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 545/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0502 - accuracy: 0.9902\n",
      "Epoch 545: val_loss improved from 0.04681 to 0.04674, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0543 - accuracy: 0.9888 - val_loss: 0.0467 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 546/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0520 - accuracy: 0.9902\n",
      "Epoch 546: val_loss improved from 0.04674 to 0.04665, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0542 - accuracy: 0.9888 - val_loss: 0.0467 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 547/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0501 - accuracy: 0.9902\n",
      "Epoch 547: val_loss improved from 0.04665 to 0.04657, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0541 - accuracy: 0.9888 - val_loss: 0.0466 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 548/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0559 - accuracy: 0.9880\n",
      "Epoch 548: val_loss improved from 0.04657 to 0.04648, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0539 - accuracy: 0.9888 - val_loss: 0.0465 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 549/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9888\n",
      "Epoch 549: val_loss improved from 0.04648 to 0.04641, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0538 - accuracy: 0.9888 - val_loss: 0.0464 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 550/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0546 - accuracy: 0.9885\n",
      "Epoch 550: val_loss improved from 0.04641 to 0.04633, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0537 - accuracy: 0.9888 - val_loss: 0.0463 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 551/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0568 - accuracy: 0.9877\n",
      "Epoch 551: val_loss improved from 0.04633 to 0.04625, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0536 - accuracy: 0.9888 - val_loss: 0.0463 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 552/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0549 - accuracy: 0.9873\n",
      "Epoch 552: val_loss improved from 0.04625 to 0.04617, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0535 - accuracy: 0.9888 - val_loss: 0.0462 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 553/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9888\n",
      "Epoch 553: val_loss improved from 0.04617 to 0.04612, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0534 - accuracy: 0.9888 - val_loss: 0.0461 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 554/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0508 - accuracy: 0.9902\n",
      "Epoch 554: val_loss improved from 0.04612 to 0.04605, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0533 - accuracy: 0.9888 - val_loss: 0.0461 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 555/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0561 - accuracy: 0.9877\n",
      "Epoch 555: val_loss improved from 0.04605 to 0.04595, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0532 - accuracy: 0.9888 - val_loss: 0.0460 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 556/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0532 - accuracy: 0.9888\n",
      "Epoch 556: val_loss improved from 0.04595 to 0.04588, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0531 - accuracy: 0.9888 - val_loss: 0.0459 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 557/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0541 - accuracy: 0.9885\n",
      "Epoch 557: val_loss improved from 0.04588 to 0.04580, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0530 - accuracy: 0.9888 - val_loss: 0.0458 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 558/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0577 - accuracy: 0.9875\n",
      "Epoch 558: val_loss improved from 0.04580 to 0.04572, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0528 - accuracy: 0.9888 - val_loss: 0.0457 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 559/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0511 - accuracy: 0.9897\n",
      "Epoch 559: val_loss improved from 0.04572 to 0.04564, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0528 - accuracy: 0.9888 - val_loss: 0.0456 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 560/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9888\n",
      "Epoch 560: val_loss improved from 0.04564 to 0.04553, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0526 - accuracy: 0.9888 - val_loss: 0.0455 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 561/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0532 - accuracy: 0.9886\n",
      "Epoch 561: val_loss improved from 0.04553 to 0.04543, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0525 - accuracy: 0.9888 - val_loss: 0.0454 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 562/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0542 - accuracy: 0.9884\n",
      "Epoch 562: val_loss improved from 0.04543 to 0.04534, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0524 - accuracy: 0.9888 - val_loss: 0.0453 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 563/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0490 - accuracy: 0.9901\n",
      "Epoch 563: val_loss improved from 0.04534 to 0.04527, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0523 - accuracy: 0.9888 - val_loss: 0.0453 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 564/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0483 - accuracy: 0.9904\n",
      "Epoch 564: val_loss improved from 0.04527 to 0.04522, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0522 - accuracy: 0.9888 - val_loss: 0.0452 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 565/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0552 - accuracy: 0.9878\n",
      "Epoch 565: val_loss improved from 0.04522 to 0.04515, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0521 - accuracy: 0.9888 - val_loss: 0.0452 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 566/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0548 - accuracy: 0.9878\n",
      "Epoch 566: val_loss improved from 0.04515 to 0.04507, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0520 - accuracy: 0.9888 - val_loss: 0.0451 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 567/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0536 - accuracy: 0.9881\n",
      "Epoch 567: val_loss improved from 0.04507 to 0.04500, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0519 - accuracy: 0.9888 - val_loss: 0.0450 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 568/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0534 - accuracy: 0.9884\n",
      "Epoch 568: val_loss improved from 0.04500 to 0.04491, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0518 - accuracy: 0.9888 - val_loss: 0.0449 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 569/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0480 - accuracy: 0.9905\n",
      "Epoch 569: val_loss improved from 0.04491 to 0.04484, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0517 - accuracy: 0.9888 - val_loss: 0.0448 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 570/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0521 - accuracy: 0.9886\n",
      "Epoch 570: val_loss improved from 0.04484 to 0.04475, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0516 - accuracy: 0.9888 - val_loss: 0.0447 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 571/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0533 - accuracy: 0.9882\n",
      "Epoch 571: val_loss improved from 0.04475 to 0.04467, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0515 - accuracy: 0.9888 - val_loss: 0.0447 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 572/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9888\n",
      "Epoch 572: val_loss improved from 0.04467 to 0.04457, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0514 - accuracy: 0.9888 - val_loss: 0.0446 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 573/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0551 - accuracy: 0.9877\n",
      "Epoch 573: val_loss improved from 0.04457 to 0.04452, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0513 - accuracy: 0.9888 - val_loss: 0.0445 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 574/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0533 - accuracy: 0.9877\n",
      "Epoch 574: val_loss improved from 0.04452 to 0.04445, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0512 - accuracy: 0.9888 - val_loss: 0.0444 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 575/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0531 - accuracy: 0.9899\n",
      "Epoch 575: val_loss improved from 0.04445 to 0.04438, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0510 - accuracy: 0.9888 - val_loss: 0.0444 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 576/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0478 - accuracy: 0.9897\n",
      "Epoch 576: val_loss improved from 0.04438 to 0.04433, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0510 - accuracy: 0.9888 - val_loss: 0.0443 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 577/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0514 - accuracy: 0.9885\n",
      "Epoch 577: val_loss improved from 0.04433 to 0.04428, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0509 - accuracy: 0.9888 - val_loss: 0.0443 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 578/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0529 - accuracy: 0.9882\n",
      "Epoch 578: val_loss improved from 0.04428 to 0.04419, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0507 - accuracy: 0.9888 - val_loss: 0.0442 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 579/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0526 - accuracy: 0.9882\n",
      "Epoch 579: val_loss improved from 0.04419 to 0.04411, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0506 - accuracy: 0.9888 - val_loss: 0.0441 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 580/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0524 - accuracy: 0.9882\n",
      "Epoch 580: val_loss improved from 0.04411 to 0.04405, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0505 - accuracy: 0.9888 - val_loss: 0.0440 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 581/700\n",
      "77/90 [========================>.....] - ETA: 0s - loss: 0.0543 - accuracy: 0.9870\n",
      "Epoch 581: val_loss improved from 0.04405 to 0.04395, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0505 - accuracy: 0.9888 - val_loss: 0.0440 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 582/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0498 - accuracy: 0.9877\n",
      "Epoch 582: val_loss improved from 0.04395 to 0.04389, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0503 - accuracy: 0.9888 - val_loss: 0.0439 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 583/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0509 - accuracy: 0.9886\n",
      "Epoch 583: val_loss improved from 0.04389 to 0.04384, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0502 - accuracy: 0.9888 - val_loss: 0.0438 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 584/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0474 - accuracy: 0.9900\n",
      "Epoch 584: val_loss improved from 0.04384 to 0.04377, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0501 - accuracy: 0.9888 - val_loss: 0.0438 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 585/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0524 - accuracy: 0.9877\n",
      "Epoch 585: val_loss improved from 0.04377 to 0.04368, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.9888 - val_loss: 0.0437 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 586/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0460 - accuracy: 0.9899\n",
      "Epoch 586: val_loss improved from 0.04368 to 0.04364, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0499 - accuracy: 0.9888 - val_loss: 0.0436 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 587/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0471 - accuracy: 0.9899\n",
      "Epoch 587: val_loss improved from 0.04364 to 0.04358, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0498 - accuracy: 0.9888 - val_loss: 0.0436 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 588/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0521 - accuracy: 0.9878\n",
      "Epoch 588: val_loss improved from 0.04358 to 0.04350, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.9888 - val_loss: 0.0435 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 589/700\n",
      "78/90 [=========================>....] - ETA: 0s - loss: 0.0508 - accuracy: 0.9897\n",
      "Epoch 589: val_loss improved from 0.04350 to 0.04344, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0496 - accuracy: 0.9888 - val_loss: 0.0434 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 590/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0523 - accuracy: 0.9875\n",
      "Epoch 590: val_loss improved from 0.04344 to 0.04339, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0495 - accuracy: 0.9888 - val_loss: 0.0434 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 591/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9885\n",
      "Epoch 591: val_loss improved from 0.04339 to 0.04331, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0494 - accuracy: 0.9888 - val_loss: 0.0433 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 592/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0503 - accuracy: 0.9882\n",
      "Epoch 592: val_loss improved from 0.04331 to 0.04324, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.9888 - val_loss: 0.0432 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 593/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0472 - accuracy: 0.9899\n",
      "Epoch 593: val_loss improved from 0.04324 to 0.04316, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 0.9888 - val_loss: 0.0432 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 594/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0463 - accuracy: 0.9905\n",
      "Epoch 594: val_loss improved from 0.04316 to 0.04308, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0491 - accuracy: 0.9888 - val_loss: 0.0431 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 595/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0430 - accuracy: 0.9905\n",
      "Epoch 595: val_loss improved from 0.04308 to 0.04308, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0490 - accuracy: 0.9888 - val_loss: 0.0431 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 596/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0516 - accuracy: 0.9875\n",
      "Epoch 596: val_loss improved from 0.04308 to 0.04301, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0489 - accuracy: 0.9888 - val_loss: 0.0430 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 597/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0503 - accuracy: 0.9878\n",
      "Epoch 597: val_loss improved from 0.04301 to 0.04291, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0488 - accuracy: 0.9888 - val_loss: 0.0429 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 598/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0431 - accuracy: 0.9904\n",
      "Epoch 598: val_loss improved from 0.04291 to 0.04285, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0487 - accuracy: 0.9888 - val_loss: 0.0428 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 599/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9888\n",
      "Epoch 599: val_loss improved from 0.04285 to 0.04274, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0486 - accuracy: 0.9888 - val_loss: 0.0427 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 600/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0433 - accuracy: 0.9908\n",
      "Epoch 600: val_loss improved from 0.04274 to 0.04268, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0486 - accuracy: 0.9888 - val_loss: 0.0427 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 601/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0478 - accuracy: 0.9884\n",
      "Epoch 601: val_loss improved from 0.04268 to 0.04261, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0485 - accuracy: 0.9888 - val_loss: 0.0426 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 602/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0494 - accuracy: 0.9884\n",
      "Epoch 602: val_loss improved from 0.04261 to 0.04255, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 0.9888 - val_loss: 0.0426 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 603/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0485 - accuracy: 0.9888\n",
      "Epoch 603: val_loss improved from 0.04255 to 0.04249, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0483 - accuracy: 0.9888 - val_loss: 0.0425 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 604/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0530 - accuracy: 0.9873\n",
      "Epoch 604: val_loss improved from 0.04249 to 0.04243, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0482 - accuracy: 0.9888 - val_loss: 0.0424 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 605/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0497 - accuracy: 0.9881\n",
      "Epoch 605: val_loss improved from 0.04243 to 0.04235, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.0481 - accuracy: 0.9888 - val_loss: 0.0423 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 606/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0495 - accuracy: 0.9878\n",
      "Epoch 606: val_loss improved from 0.04235 to 0.04227, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0480 - accuracy: 0.9888 - val_loss: 0.0423 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 607/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0447 - accuracy: 0.9902\n",
      "Epoch 607: val_loss improved from 0.04227 to 0.04222, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0479 - accuracy: 0.9888 - val_loss: 0.0422 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 608/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9888\n",
      "Epoch 608: val_loss improved from 0.04222 to 0.04216, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0478 - accuracy: 0.9888 - val_loss: 0.0422 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 609/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0483 - accuracy: 0.9885\n",
      "Epoch 609: val_loss improved from 0.04216 to 0.04207, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0477 - accuracy: 0.9888 - val_loss: 0.0421 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 610/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0490 - accuracy: 0.9884\n",
      "Epoch 610: val_loss improved from 0.04207 to 0.04201, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0476 - accuracy: 0.9888 - val_loss: 0.0420 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 611/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9888\n",
      "Epoch 611: val_loss improved from 0.04201 to 0.04191, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0475 - accuracy: 0.9888 - val_loss: 0.0419 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 612/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0430 - accuracy: 0.9901\n",
      "Epoch 612: val_loss improved from 0.04191 to 0.04186, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0474 - accuracy: 0.9888 - val_loss: 0.0419 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 613/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0463 - accuracy: 0.9907\n",
      "Epoch 613: val_loss improved from 0.04186 to 0.04183, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0473 - accuracy: 0.9888 - val_loss: 0.0418 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 614/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0432 - accuracy: 0.9901\n",
      "Epoch 614: val_loss improved from 0.04183 to 0.04178, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0472 - accuracy: 0.9888 - val_loss: 0.0418 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 615/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0476 - accuracy: 0.9882\n",
      "Epoch 615: val_loss improved from 0.04178 to 0.04173, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0471 - accuracy: 0.9888 - val_loss: 0.0417 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 616/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9888\n",
      "Epoch 616: val_loss improved from 0.04173 to 0.04163, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0470 - accuracy: 0.9888 - val_loss: 0.0416 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 617/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0438 - accuracy: 0.9905\n",
      "Epoch 617: val_loss did not improve from 0.04163\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0469 - accuracy: 0.9888 - val_loss: 0.0417 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 618/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9910\n",
      "Epoch 618: val_loss did not improve from 0.04163\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 0.0468 - accuracy: 0.9888 - val_loss: 0.0417 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 619/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0450 - accuracy: 0.9900\n",
      "Epoch 619: val_loss improved from 0.04163 to 0.04156, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0467 - accuracy: 0.9888 - val_loss: 0.0416 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 620/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0490 - accuracy: 0.9877\n",
      "Epoch 620: val_loss improved from 0.04156 to 0.04147, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0467 - accuracy: 0.9888 - val_loss: 0.0415 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 621/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0482 - accuracy: 0.9880\n",
      "Epoch 621: val_loss improved from 0.04147 to 0.04139, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0466 - accuracy: 0.9888 - val_loss: 0.0414 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 622/700\n",
      "79/90 [=========================>....] - ETA: 0s - loss: 0.0419 - accuracy: 0.9899\n",
      "Epoch 622: val_loss improved from 0.04139 to 0.04131, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0465 - accuracy: 0.9888 - val_loss: 0.0413 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 623/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0475 - accuracy: 0.9881\n",
      "Epoch 623: val_loss improved from 0.04131 to 0.04124, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0464 - accuracy: 0.9888 - val_loss: 0.0412 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 624/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0476 - accuracy: 0.9884\n",
      "Epoch 624: val_loss improved from 0.04124 to 0.04118, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0463 - accuracy: 0.9888 - val_loss: 0.0412 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 625/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0471 - accuracy: 0.9885\n",
      "Epoch 625: val_loss improved from 0.04118 to 0.04112, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0462 - accuracy: 0.9888 - val_loss: 0.0411 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 626/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0477 - accuracy: 0.9882\n",
      "Epoch 626: val_loss improved from 0.04112 to 0.04102, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0461 - accuracy: 0.9888 - val_loss: 0.0410 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 627/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0474 - accuracy: 0.9882\n",
      "Epoch 627: val_loss improved from 0.04102 to 0.04093, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0460 - accuracy: 0.9888 - val_loss: 0.0409 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 628/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0472 - accuracy: 0.9881\n",
      "Epoch 628: val_loss improved from 0.04093 to 0.04088, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0459 - accuracy: 0.9888 - val_loss: 0.0409 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 629/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9888\n",
      "Epoch 629: val_loss improved from 0.04088 to 0.04082, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0458 - accuracy: 0.9888 - val_loss: 0.0408 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 630/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0458 - accuracy: 0.9888\n",
      "Epoch 630: val_loss improved from 0.04082 to 0.04077, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0458 - accuracy: 0.9888 - val_loss: 0.0408 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 631/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0475 - accuracy: 0.9880\n",
      "Epoch 631: val_loss improved from 0.04077 to 0.04069, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0457 - accuracy: 0.9888 - val_loss: 0.0407 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 632/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9888\n",
      "Epoch 632: val_loss improved from 0.04069 to 0.04061, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0456 - accuracy: 0.9888 - val_loss: 0.0406 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 633/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0453 - accuracy: 0.9885\n",
      "Epoch 633: val_loss improved from 0.04061 to 0.04055, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0455 - accuracy: 0.9888 - val_loss: 0.0406 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 634/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0454 - accuracy: 0.9888\n",
      "Epoch 634: val_loss improved from 0.04055 to 0.04048, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0454 - accuracy: 0.9888 - val_loss: 0.0405 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 635/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9885\n",
      "Epoch 635: val_loss improved from 0.04048 to 0.04042, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0454 - accuracy: 0.9888 - val_loss: 0.0404 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 636/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0454 - accuracy: 0.9886\n",
      "Epoch 636: val_loss improved from 0.04042 to 0.04033, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0452 - accuracy: 0.9888 - val_loss: 0.0403 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 637/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0475 - accuracy: 0.9881\n",
      "Epoch 637: val_loss improved from 0.04033 to 0.04028, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0451 - accuracy: 0.9888 - val_loss: 0.0403 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 638/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0475 - accuracy: 0.9878\n",
      "Epoch 638: val_loss improved from 0.04028 to 0.04025, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0450 - accuracy: 0.9888 - val_loss: 0.0402 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 639/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0457 - accuracy: 0.9884\n",
      "Epoch 639: val_loss improved from 0.04025 to 0.04018, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0449 - accuracy: 0.9888 - val_loss: 0.0402 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 640/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0473 - accuracy: 0.9877\n",
      "Epoch 640: val_loss improved from 0.04018 to 0.04011, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0449 - accuracy: 0.9888 - val_loss: 0.0401 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 641/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0450 - accuracy: 0.9881\n",
      "Epoch 641: val_loss improved from 0.04011 to 0.04010, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0448 - accuracy: 0.9888 - val_loss: 0.0401 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 642/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0454 - accuracy: 0.9886\n",
      "Epoch 642: val_loss improved from 0.04010 to 0.04002, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0447 - accuracy: 0.9888 - val_loss: 0.0400 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 643/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0456 - accuracy: 0.9882\n",
      "Epoch 643: val_loss improved from 0.04002 to 0.03993, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0446 - accuracy: 0.9888 - val_loss: 0.0399 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 644/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0457 - accuracy: 0.9884\n",
      "Epoch 644: val_loss improved from 0.03993 to 0.03989, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0445 - accuracy: 0.9888 - val_loss: 0.0399 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 645/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9909\n",
      "Epoch 645: val_loss improved from 0.03989 to 0.03985, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0444 - accuracy: 0.9888 - val_loss: 0.0399 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 646/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9888\n",
      "Epoch 646: val_loss improved from 0.03985 to 0.03985, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0444 - accuracy: 0.9888 - val_loss: 0.0398 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 647/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9886\n",
      "Epoch 647: val_loss improved from 0.03985 to 0.03980, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0443 - accuracy: 0.9888 - val_loss: 0.0398 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 648/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0463 - accuracy: 0.9878\n",
      "Epoch 648: val_loss improved from 0.03980 to 0.03968, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0442 - accuracy: 0.9888 - val_loss: 0.0397 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 649/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0391 - accuracy: 0.9907\n",
      "Epoch 649: val_loss improved from 0.03968 to 0.03962, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0441 - accuracy: 0.9888 - val_loss: 0.0396 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 650/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 0.9888\n",
      "Epoch 650: val_loss improved from 0.03962 to 0.03956, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0440 - accuracy: 0.9888 - val_loss: 0.0396 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 651/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9886\n",
      "Epoch 651: val_loss improved from 0.03956 to 0.03947, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0439 - accuracy: 0.9888 - val_loss: 0.0395 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 652/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0452 - accuracy: 0.9881\n",
      "Epoch 652: val_loss improved from 0.03947 to 0.03943, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0438 - accuracy: 0.9888 - val_loss: 0.0394 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 653/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0443 - accuracy: 0.9886\n",
      "Epoch 653: val_loss improved from 0.03943 to 0.03939, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0437 - accuracy: 0.9888 - val_loss: 0.0394 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 654/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0402 - accuracy: 0.9902\n",
      "Epoch 654: val_loss improved from 0.03939 to 0.03934, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0437 - accuracy: 0.9888 - val_loss: 0.0393 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 655/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0438 - accuracy: 0.9888\n",
      "Epoch 655: val_loss improved from 0.03934 to 0.03931, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0436 - accuracy: 0.9888 - val_loss: 0.0393 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 656/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0451 - accuracy: 0.9877\n",
      "Epoch 656: val_loss improved from 0.03931 to 0.03920, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0435 - accuracy: 0.9888 - val_loss: 0.0392 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 657/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0449 - accuracy: 0.9881\n",
      "Epoch 657: val_loss improved from 0.03920 to 0.03918, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0434 - accuracy: 0.9888 - val_loss: 0.0392 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 658/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0389 - accuracy: 0.9907\n",
      "Epoch 658: val_loss improved from 0.03918 to 0.03910, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0433 - accuracy: 0.9888 - val_loss: 0.0391 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 659/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0395 - accuracy: 0.9909\n",
      "Epoch 659: val_loss did not improve from 0.03910\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0432 - accuracy: 0.9888 - val_loss: 0.0391 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 660/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0459 - accuracy: 0.9875\n",
      "Epoch 660: val_loss improved from 0.03910 to 0.03905, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0431 - accuracy: 0.9888 - val_loss: 0.0390 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 661/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0444 - accuracy: 0.9877\n",
      "Epoch 661: val_loss improved from 0.03905 to 0.03896, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0431 - accuracy: 0.9888 - val_loss: 0.0390 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 662/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0390 - accuracy: 0.9906\n",
      "Epoch 662: val_loss improved from 0.03896 to 0.03892, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0430 - accuracy: 0.9888 - val_loss: 0.0389 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 663/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0431 - accuracy: 0.9888\n",
      "Epoch 663: val_loss improved from 0.03892 to 0.03883, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0429 - accuracy: 0.9888 - val_loss: 0.0388 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 664/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0434 - accuracy: 0.9884\n",
      "Epoch 664: val_loss improved from 0.03883 to 0.03872, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0428 - accuracy: 0.9888 - val_loss: 0.0387 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 665/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0443 - accuracy: 0.9882\n",
      "Epoch 665: val_loss improved from 0.03872 to 0.03867, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0427 - accuracy: 0.9888 - val_loss: 0.0387 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 666/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0436 - accuracy: 0.9881\n",
      "Epoch 666: val_loss improved from 0.03867 to 0.03859, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0426 - accuracy: 0.9888 - val_loss: 0.0386 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 667/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0435 - accuracy: 0.9885\n",
      "Epoch 667: val_loss improved from 0.03859 to 0.03855, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0425 - accuracy: 0.9888 - val_loss: 0.0386 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 668/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0410 - accuracy: 0.9907\n",
      "Epoch 668: val_loss improved from 0.03855 to 0.03852, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0425 - accuracy: 0.9888 - val_loss: 0.0385 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 669/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0426 - accuracy: 0.9888\n",
      "Epoch 669: val_loss improved from 0.03852 to 0.03846, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0424 - accuracy: 0.9888 - val_loss: 0.0385 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 670/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0423 - accuracy: 0.9888\n",
      "Epoch 670: val_loss improved from 0.03846 to 0.03841, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 0.0423 - accuracy: 0.9888 - val_loss: 0.0384 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 671/700\n",
      "87/90 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9908\n",
      "Epoch 671: val_loss improved from 0.03841 to 0.03834, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 0.0422 - accuracy: 0.9888 - val_loss: 0.0383 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 672/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0433 - accuracy: 0.9882\n",
      "Epoch 672: val_loss improved from 0.03834 to 0.03826, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0421 - accuracy: 0.9888 - val_loss: 0.0383 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 673/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0427 - accuracy: 0.9881\n",
      "Epoch 673: val_loss improved from 0.03826 to 0.03823, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0421 - accuracy: 0.9888 - val_loss: 0.0382 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 674/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0424 - accuracy: 0.9886\n",
      "Epoch 674: val_loss improved from 0.03823 to 0.03818, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0420 - accuracy: 0.9888 - val_loss: 0.0382 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 675/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0366 - accuracy: 0.9907\n",
      "Epoch 675: val_loss improved from 0.03818 to 0.03813, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0419 - accuracy: 0.9888 - val_loss: 0.0381 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 676/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0405 - accuracy: 0.9906\n",
      "Epoch 676: val_loss improved from 0.03813 to 0.03811, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0418 - accuracy: 0.9888 - val_loss: 0.0381 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 677/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0428 - accuracy: 0.9884\n",
      "Epoch 677: val_loss improved from 0.03811 to 0.03803, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0417 - accuracy: 0.9888 - val_loss: 0.0380 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 678/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0445 - accuracy: 0.9875\n",
      "Epoch 678: val_loss improved from 0.03803 to 0.03796, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0416 - accuracy: 0.9888 - val_loss: 0.0380 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 679/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9888\n",
      "Epoch 679: val_loss improved from 0.03796 to 0.03790, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0416 - accuracy: 0.9888 - val_loss: 0.0379 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 680/700\n",
      "88/90 [============================>.] - ETA: 0s - loss: 0.0379 - accuracy: 0.9909\n",
      "Epoch 680: val_loss improved from 0.03790 to 0.03788, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0415 - accuracy: 0.9888 - val_loss: 0.0379 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 681/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9888\n",
      "Epoch 681: val_loss improved from 0.03788 to 0.03782, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0414 - accuracy: 0.9888 - val_loss: 0.0378 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 682/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0353 - accuracy: 0.9907\n",
      "Epoch 682: val_loss improved from 0.03782 to 0.03777, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0413 - accuracy: 0.9888 - val_loss: 0.0378 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 683/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0424 - accuracy: 0.9882\n",
      "Epoch 683: val_loss improved from 0.03777 to 0.03770, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0413 - accuracy: 0.9888 - val_loss: 0.0377 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 684/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9888\n",
      "Epoch 684: val_loss improved from 0.03770 to 0.03766, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0412 - accuracy: 0.9888 - val_loss: 0.0377 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 685/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9888\n",
      "Epoch 685: val_loss improved from 0.03766 to 0.03756, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0411 - accuracy: 0.9888 - val_loss: 0.0376 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 686/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0428 - accuracy: 0.9877\n",
      "Epoch 686: val_loss improved from 0.03756 to 0.03745, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 12ms/step - loss: 0.0410 - accuracy: 0.9888 - val_loss: 0.0374 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 687/700\n",
      "84/90 [===========================>..] - ETA: 0s - loss: 0.0427 - accuracy: 0.9881\n",
      "Epoch 687: val_loss improved from 0.03745 to 0.03740, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0410 - accuracy: 0.9888 - val_loss: 0.0374 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 688/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0433 - accuracy: 0.9875\n",
      "Epoch 688: val_loss improved from 0.03740 to 0.03735, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0408 - accuracy: 0.9888 - val_loss: 0.0373 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 689/700\n",
      "80/90 [=========================>....] - ETA: 0s - loss: 0.0421 - accuracy: 0.9875\n",
      "Epoch 689: val_loss improved from 0.03735 to 0.03732, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0408 - accuracy: 0.9888 - val_loss: 0.0373 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 690/700\n",
      "81/90 [==========================>...] - ETA: 0s - loss: 0.0436 - accuracy: 0.9877\n",
      "Epoch 690: val_loss improved from 0.03732 to 0.03731, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0407 - accuracy: 0.9888 - val_loss: 0.0373 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 691/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0368 - accuracy: 0.9910\n",
      "Epoch 691: val_loss did not improve from 0.03731\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 0.0406 - accuracy: 0.9888 - val_loss: 0.0374 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 692/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0407 - accuracy: 0.9888\n",
      "Epoch 692: val_loss improved from 0.03731 to 0.03729, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0405 - accuracy: 0.9888 - val_loss: 0.0373 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 693/700\n",
      "82/90 [==========================>...] - ETA: 0s - loss: 0.0419 - accuracy: 0.9878\n",
      "Epoch 693: val_loss improved from 0.03729 to 0.03726, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0404 - accuracy: 0.9888 - val_loss: 0.0373 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 694/700\n",
      "83/90 [==========================>...] - ETA: 0s - loss: 0.0426 - accuracy: 0.9880\n",
      "Epoch 694: val_loss improved from 0.03726 to 0.03720, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0404 - accuracy: 0.9888 - val_loss: 0.0372 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 695/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0405 - accuracy: 0.9888\n",
      "Epoch 695: val_loss improved from 0.03720 to 0.03715, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0403 - accuracy: 0.9888 - val_loss: 0.0372 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 696/700\n",
      "85/90 [===========================>..] - ETA: 0s - loss: 0.0418 - accuracy: 0.9882\n",
      "Epoch 696: val_loss improved from 0.03715 to 0.03707, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0402 - accuracy: 0.9888 - val_loss: 0.0371 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 697/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9888\n",
      "Epoch 697: val_loss improved from 0.03707 to 0.03701, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0401 - accuracy: 0.9888 - val_loss: 0.0370 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 698/700\n",
      "89/90 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9888\n",
      "Epoch 698: val_loss improved from 0.03701 to 0.03694, saving model to model_checkpoint\\GRU_2 layer_SGD.h5\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0401 - accuracy: 0.9888 - val_loss: 0.0369 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 699/700\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9888\n",
      "Epoch 699: val_loss did not improve from 0.03694\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 0.0400 - accuracy: 0.9888 - val_loss: 0.0370 - val_accuracy: 0.9940 - lr: 0.0010\n",
      "Epoch 700/700\n",
      "86/90 [===========================>..] - ETA: 0s - loss: 0.0319 - accuracy: 0.9930\n",
      "Epoch 700: val_loss did not improve from 0.03694\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 0.0399 - accuracy: 0.9888 - val_loss: 0.0371 - val_accuracy: 0.9940 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_12 (GRU)                (None, None, 32)          11520     \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 32)                6336      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,889\n",
      "Trainable params: 17,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.9821\n",
      "Test Loss: 0.07802385091781616\n",
      "Test Accuracy: 0.9821428656578064\n"
     ]
    }
   ],
   "source": [
    "# This code implements a deep Gated Recurrent Unit (GRU) architecture with two layers, using the SGD optimization.\n",
    "\n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_2 layers_SGD.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "# Definition of the model\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1]),return_sequences=True))\n",
    "model.add(layers.GRU(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training of the model\n",
    "history = model.fit(x_train, y_train, batch_size=5, epochs=700, validation_data=(x_val, y_val), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "\n",
    "plot_2(history)\n",
    "\n",
    "# Evaluation of the model on the testing set\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 layers k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing GRU with K-fold\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.73069, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 2: val_loss improved from 0.73069 to 0.71418, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 3: val_loss improved from 0.71418 to 0.69845, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 4: val_loss improved from 0.69845 to 0.68319, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.68319 to 0.66849, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 6: val_loss improved from 0.66849 to 0.65412, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 7: val_loss improved from 0.65412 to 0.64010, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 8: val_loss improved from 0.64010 to 0.62648, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 9: val_loss improved from 0.62648 to 0.61321, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 10: val_loss improved from 0.61321 to 0.60021, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 11: val_loss improved from 0.60021 to 0.58739, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 12: val_loss improved from 0.58739 to 0.57485, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 13: val_loss improved from 0.57485 to 0.56255, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 14: val_loss improved from 0.56255 to 0.55045, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 15: val_loss improved from 0.55045 to 0.53868, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 16: val_loss improved from 0.53868 to 0.52708, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 17: val_loss improved from 0.52708 to 0.51575, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 18: val_loss improved from 0.51575 to 0.50468, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 19: val_loss improved from 0.50468 to 0.49394, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 20: val_loss improved from 0.49394 to 0.48348, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 21: val_loss improved from 0.48348 to 0.47324, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 22: val_loss improved from 0.47324 to 0.46333, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 23: val_loss improved from 0.46333 to 0.45373, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 24: val_loss improved from 0.45373 to 0.44446, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 25: val_loss improved from 0.44446 to 0.43554, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 26: val_loss improved from 0.43554 to 0.42689, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 27: val_loss improved from 0.42689 to 0.41855, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 28: val_loss improved from 0.41855 to 0.41049, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 29: val_loss improved from 0.41049 to 0.40279, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 30: val_loss improved from 0.40279 to 0.39535, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 31: val_loss improved from 0.39535 to 0.38818, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 32: val_loss improved from 0.38818 to 0.38125, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 33: val_loss improved from 0.38125 to 0.37462, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 34: val_loss improved from 0.37462 to 0.36825, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 35: val_loss improved from 0.36825 to 0.36215, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 36: val_loss improved from 0.36215 to 0.35631, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 37: val_loss improved from 0.35631 to 0.35076, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 38: val_loss improved from 0.35076 to 0.34539, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 39: val_loss improved from 0.34539 to 0.34025, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 40: val_loss improved from 0.34025 to 0.33533, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 41: val_loss improved from 0.33533 to 0.33063, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 42: val_loss improved from 0.33063 to 0.32612, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 43: val_loss improved from 0.32612 to 0.32181, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 44: val_loss improved from 0.32181 to 0.31768, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 45: val_loss improved from 0.31768 to 0.31371, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 46: val_loss improved from 0.31371 to 0.30990, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 47: val_loss improved from 0.30990 to 0.30628, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 48: val_loss improved from 0.30628 to 0.30278, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 49: val_loss improved from 0.30278 to 0.29944, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 50: val_loss improved from 0.29944 to 0.29624, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 51: val_loss improved from 0.29624 to 0.29315, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 52: val_loss improved from 0.29315 to 0.29016, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 53: val_loss improved from 0.29016 to 0.28731, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 54: val_loss improved from 0.28731 to 0.28458, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 55: val_loss improved from 0.28458 to 0.28195, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 56: val_loss improved from 0.28195 to 0.27942, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 57: val_loss improved from 0.27942 to 0.27699, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 58: val_loss improved from 0.27699 to 0.27464, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 59: val_loss improved from 0.27464 to 0.27239, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 60: val_loss improved from 0.27239 to 0.27022, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 61: val_loss improved from 0.27022 to 0.26814, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 62: val_loss improved from 0.26814 to 0.26610, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 63: val_loss improved from 0.26610 to 0.26415, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 64: val_loss improved from 0.26415 to 0.26226, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 65: val_loss improved from 0.26226 to 0.26044, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 66: val_loss improved from 0.26044 to 0.25868, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 67: val_loss improved from 0.25868 to 0.25697, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 68: val_loss improved from 0.25697 to 0.25533, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 69: val_loss improved from 0.25533 to 0.25373, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 70: val_loss improved from 0.25373 to 0.25218, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 71: val_loss improved from 0.25218 to 0.25068, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 72: val_loss improved from 0.25068 to 0.24922, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 73: val_loss improved from 0.24922 to 0.24781, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 74: val_loss improved from 0.24781 to 0.24644, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 75: val_loss improved from 0.24644 to 0.24511, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 76: val_loss improved from 0.24511 to 0.24381, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 77: val_loss improved from 0.24381 to 0.24255, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 78: val_loss improved from 0.24255 to 0.24134, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 79: val_loss improved from 0.24134 to 0.24014, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 80: val_loss improved from 0.24014 to 0.23898, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 81: val_loss improved from 0.23898 to 0.23787, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 82: val_loss improved from 0.23787 to 0.23676, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 83: val_loss improved from 0.23676 to 0.23568, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 84: val_loss improved from 0.23568 to 0.23463, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 85: val_loss improved from 0.23463 to 0.23359, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 86: val_loss improved from 0.23359 to 0.23259, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 87: val_loss improved from 0.23259 to 0.23161, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 88: val_loss improved from 0.23161 to 0.23065, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 89: val_loss improved from 0.23065 to 0.22972, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 90: val_loss improved from 0.22972 to 0.22880, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 91: val_loss improved from 0.22880 to 0.22791, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 92: val_loss improved from 0.22791 to 0.22704, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 93: val_loss improved from 0.22704 to 0.22618, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 94: val_loss improved from 0.22618 to 0.22534, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 95: val_loss improved from 0.22534 to 0.22452, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 96: val_loss improved from 0.22452 to 0.22372, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 97: val_loss improved from 0.22372 to 0.22293, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 98: val_loss improved from 0.22293 to 0.22216, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 99: val_loss improved from 0.22216 to 0.22141, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 100: val_loss improved from 0.22141 to 0.22066, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 101: val_loss improved from 0.22066 to 0.21993, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 102: val_loss improved from 0.21993 to 0.21922, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 103: val_loss improved from 0.21922 to 0.21852, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 104: val_loss improved from 0.21852 to 0.21783, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 105: val_loss improved from 0.21783 to 0.21715, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 106: val_loss improved from 0.21715 to 0.21649, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 107: val_loss improved from 0.21649 to 0.21584, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 108: val_loss improved from 0.21584 to 0.21519, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 109: val_loss improved from 0.21519 to 0.21456, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 110: val_loss improved from 0.21456 to 0.21394, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 111: val_loss improved from 0.21394 to 0.21333, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 112: val_loss improved from 0.21333 to 0.21273, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 113: val_loss improved from 0.21273 to 0.21214, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 114: val_loss improved from 0.21214 to 0.21157, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 115: val_loss improved from 0.21157 to 0.21099, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 116: val_loss improved from 0.21099 to 0.21043, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 117: val_loss improved from 0.21043 to 0.20988, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 118: val_loss improved from 0.20988 to 0.20934, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 119: val_loss improved from 0.20934 to 0.20880, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 120: val_loss improved from 0.20880 to 0.20827, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 121: val_loss improved from 0.20827 to 0.20776, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 122: val_loss improved from 0.20776 to 0.20725, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 123: val_loss improved from 0.20725 to 0.20674, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 124: val_loss improved from 0.20674 to 0.20624, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 125: val_loss improved from 0.20624 to 0.20575, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 126: val_loss improved from 0.20575 to 0.20527, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 127: val_loss improved from 0.20527 to 0.20479, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 128: val_loss improved from 0.20479 to 0.20431, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 129: val_loss improved from 0.20431 to 0.20385, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 130: val_loss improved from 0.20385 to 0.20339, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 131: val_loss improved from 0.20339 to 0.20293, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 132: val_loss improved from 0.20293 to 0.20248, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 133: val_loss improved from 0.20248 to 0.20204, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 134: val_loss improved from 0.20204 to 0.20160, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 135: val_loss improved from 0.20160 to 0.20117, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 136: val_loss improved from 0.20117 to 0.20075, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 137: val_loss improved from 0.20075 to 0.20033, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 138: val_loss improved from 0.20033 to 0.19991, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 139: val_loss improved from 0.19991 to 0.19950, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 140: val_loss improved from 0.19950 to 0.19909, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 141: val_loss improved from 0.19909 to 0.19869, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 142: val_loss improved from 0.19869 to 0.19829, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 143: val_loss improved from 0.19829 to 0.19789, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 144: val_loss improved from 0.19789 to 0.19751, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 145: val_loss improved from 0.19751 to 0.19712, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 146: val_loss improved from 0.19712 to 0.19674, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 147: val_loss improved from 0.19674 to 0.19636, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 148: val_loss improved from 0.19636 to 0.19599, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 149: val_loss improved from 0.19599 to 0.19562, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 150: val_loss improved from 0.19562 to 0.19525, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 151: val_loss improved from 0.19525 to 0.19489, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 152: val_loss improved from 0.19489 to 0.19453, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 153: val_loss improved from 0.19453 to 0.19418, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 154: val_loss improved from 0.19418 to 0.19383, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 155: val_loss improved from 0.19383 to 0.19348, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 156: val_loss improved from 0.19348 to 0.19313, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 157: val_loss improved from 0.19313 to 0.19279, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 158: val_loss improved from 0.19279 to 0.19245, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 159: val_loss improved from 0.19245 to 0.19211, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 160: val_loss improved from 0.19211 to 0.19178, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 161: val_loss improved from 0.19178 to 0.19145, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 162: val_loss improved from 0.19145 to 0.19112, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 163: val_loss improved from 0.19112 to 0.19079, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 164: val_loss improved from 0.19079 to 0.19047, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 165: val_loss improved from 0.19047 to 0.19014, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 166: val_loss improved from 0.19014 to 0.18982, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 167: val_loss improved from 0.18982 to 0.18951, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 168: val_loss improved from 0.18951 to 0.18920, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 169: val_loss improved from 0.18920 to 0.18890, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 170: val_loss improved from 0.18890 to 0.18860, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 171: val_loss improved from 0.18860 to 0.18830, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 172: val_loss improved from 0.18830 to 0.18798, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 173: val_loss improved from 0.18798 to 0.18768, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 174: val_loss improved from 0.18768 to 0.18739, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 175: val_loss improved from 0.18739 to 0.18709, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 176: val_loss improved from 0.18709 to 0.18680, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 177: val_loss improved from 0.18680 to 0.18651, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 178: val_loss improved from 0.18651 to 0.18622, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 179: val_loss improved from 0.18622 to 0.18590, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 180: val_loss improved from 0.18590 to 0.18562, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 181: val_loss improved from 0.18562 to 0.18534, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 182: val_loss improved from 0.18534 to 0.18506, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 183: val_loss improved from 0.18506 to 0.18478, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 184: val_loss improved from 0.18478 to 0.18451, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 185: val_loss improved from 0.18451 to 0.18423, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 186: val_loss improved from 0.18423 to 0.18396, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 187: val_loss improved from 0.18396 to 0.18369, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 188: val_loss improved from 0.18369 to 0.18342, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 189: val_loss improved from 0.18342 to 0.18313, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 190: val_loss improved from 0.18313 to 0.18287, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 191: val_loss improved from 0.18287 to 0.18261, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 192: val_loss improved from 0.18261 to 0.18234, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 193: val_loss improved from 0.18234 to 0.18208, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 194: val_loss improved from 0.18208 to 0.18181, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 195: val_loss improved from 0.18181 to 0.18155, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 196: val_loss improved from 0.18155 to 0.18129, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 197: val_loss improved from 0.18129 to 0.18104, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 198: val_loss improved from 0.18104 to 0.18078, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 199: val_loss improved from 0.18078 to 0.18053, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 200: val_loss improved from 0.18053 to 0.18028, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 201: val_loss improved from 0.18028 to 0.18002, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 202: val_loss improved from 0.18002 to 0.17977, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 203: val_loss improved from 0.17977 to 0.17953, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 204: val_loss improved from 0.17953 to 0.17928, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 205: val_loss improved from 0.17928 to 0.17904, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 206: val_loss improved from 0.17904 to 0.17879, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 207: val_loss improved from 0.17879 to 0.17854, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 208: val_loss improved from 0.17854 to 0.17830, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 209: val_loss improved from 0.17830 to 0.17803, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 210: val_loss improved from 0.17803 to 0.17778, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 211: val_loss improved from 0.17778 to 0.17754, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 212: val_loss improved from 0.17754 to 0.17730, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 213: val_loss improved from 0.17730 to 0.17706, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 214: val_loss improved from 0.17706 to 0.17682, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.17682 to 0.17659, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.17659 to 0.17636, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.17636 to 0.17609, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.17609 to 0.17586, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.17586 to 0.17563, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.17563 to 0.17540, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.17540 to 0.17518, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.17518 to 0.17496, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.17496 to 0.17473, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.17473 to 0.17451, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.17451 to 0.17429, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.17429 to 0.17406, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.17406 to 0.17384, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.17384 to 0.17362, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.17362 to 0.17340, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.17340 to 0.17318, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.17318 to 0.17296, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.17296 to 0.17275, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.17275 to 0.17253, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.17253 to 0.17231, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.17231 to 0.17209, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.17209 to 0.17185, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.17185 to 0.17164, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.17164 to 0.17143, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.17143 to 0.17121, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.17121 to 0.17100, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.17100 to 0.17079, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.17079 to 0.17058, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.17058 to 0.17037, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.17037 to 0.17016, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.17016 to 0.16995, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.16995 to 0.16974, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.16974 to 0.16953, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.16953 to 0.16932, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.16932 to 0.16912, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.16912 to 0.16892, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.16892 to 0.16872, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.16872 to 0.16852, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.16852 to 0.16831, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.16831 to 0.16811, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.16811 to 0.16791, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.16791 to 0.16768, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.16768 to 0.16748, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.16748 to 0.16729, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.16729 to 0.16709, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.16709 to 0.16689, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.16689 to 0.16669, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.16669 to 0.16649, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.16649 to 0.16630, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.16630 to 0.16610, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.16610 to 0.16590, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.16590 to 0.16571, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.16571 to 0.16552, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.16552 to 0.16531, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.16531 to 0.16512, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.16512 to 0.16494, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.16494 to 0.16475, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.16475 to 0.16455, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.16455 to 0.16436, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.16436 to 0.16417, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.16417 to 0.16398, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.16398 to 0.16379, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.16379 to 0.16361, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.16361 to 0.16342, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.16342 to 0.16323, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.16323 to 0.16306, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.16306 to 0.16287, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.16287 to 0.16270, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.16270 to 0.16252, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.16252 to 0.16233, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.16233 to 0.16214, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.16214 to 0.16196, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.16196 to 0.16179, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.16179 to 0.16159, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.16159 to 0.16141, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.16141 to 0.16124, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.16124 to 0.16106, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.16106 to 0.16088, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.16088 to 0.16068, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.16068 to 0.16046, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.16046 to 0.16028, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.16028 to 0.16010, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.16010 to 0.15992, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.15992 to 0.15976, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.15976 to 0.15958, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.15958 to 0.15942, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 301: val_loss improved from 0.15942 to 0.15925, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 302: val_loss improved from 0.15925 to 0.15908, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 303: val_loss improved from 0.15908 to 0.15890, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 304: val_loss improved from 0.15890 to 0.15872, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 305: val_loss improved from 0.15872 to 0.15855, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 306: val_loss improved from 0.15855 to 0.15838, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 307: val_loss improved from 0.15838 to 0.15821, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 308: val_loss improved from 0.15821 to 0.15805, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 309: val_loss improved from 0.15805 to 0.15787, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 310: val_loss improved from 0.15787 to 0.15770, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 311: val_loss improved from 0.15770 to 0.15752, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 312: val_loss improved from 0.15752 to 0.15735, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 313: val_loss improved from 0.15735 to 0.15718, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 314: val_loss improved from 0.15718 to 0.15702, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 315: val_loss improved from 0.15702 to 0.15683, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 316: val_loss improved from 0.15683 to 0.15667, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 317: val_loss improved from 0.15667 to 0.15652, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 318: val_loss improved from 0.15652 to 0.15636, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 319: val_loss improved from 0.15636 to 0.15620, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 320: val_loss improved from 0.15620 to 0.15600, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 321: val_loss improved from 0.15600 to 0.15584, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 322: val_loss improved from 0.15584 to 0.15567, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 323: val_loss improved from 0.15567 to 0.15552, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 324: val_loss improved from 0.15552 to 0.15533, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 325: val_loss improved from 0.15533 to 0.15518, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 326: val_loss improved from 0.15518 to 0.15501, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 327: val_loss improved from 0.15501 to 0.15486, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 328: val_loss improved from 0.15486 to 0.15470, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 329: val_loss improved from 0.15470 to 0.15454, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 330: val_loss improved from 0.15454 to 0.15437, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 331: val_loss improved from 0.15437 to 0.15421, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 332: val_loss improved from 0.15421 to 0.15405, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 333: val_loss improved from 0.15405 to 0.15389, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 334: val_loss improved from 0.15389 to 0.15374, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 335: val_loss improved from 0.15374 to 0.15357, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 336: val_loss improved from 0.15357 to 0.15343, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 337: val_loss improved from 0.15343 to 0.15329, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 338: val_loss improved from 0.15329 to 0.15313, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 339: val_loss improved from 0.15313 to 0.15297, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 340: val_loss improved from 0.15297 to 0.15283, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 341: val_loss improved from 0.15283 to 0.15267, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 342: val_loss improved from 0.15267 to 0.15251, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 343: val_loss improved from 0.15251 to 0.15236, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 344: val_loss improved from 0.15236 to 0.15221, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 345: val_loss improved from 0.15221 to 0.15207, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 346: val_loss improved from 0.15207 to 0.15192, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 347: val_loss improved from 0.15192 to 0.15171, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 348: val_loss improved from 0.15171 to 0.15158, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 349: val_loss improved from 0.15158 to 0.15142, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 350: val_loss improved from 0.15142 to 0.15127, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 351: val_loss improved from 0.15127 to 0.15114, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 352: val_loss improved from 0.15114 to 0.15101, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 353: val_loss improved from 0.15101 to 0.15086, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 354: val_loss improved from 0.15086 to 0.15072, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 355: val_loss improved from 0.15072 to 0.15057, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 356: val_loss improved from 0.15057 to 0.15042, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 357: val_loss improved from 0.15042 to 0.15028, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 358: val_loss improved from 0.15028 to 0.15013, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 359: val_loss improved from 0.15013 to 0.14998, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 360: val_loss improved from 0.14998 to 0.14980, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 361: val_loss improved from 0.14980 to 0.14966, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 362: val_loss improved from 0.14966 to 0.14951, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 363: val_loss improved from 0.14951 to 0.14936, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 364: val_loss improved from 0.14936 to 0.14922, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 365: val_loss improved from 0.14922 to 0.14907, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 366: val_loss improved from 0.14907 to 0.14893, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 367: val_loss improved from 0.14893 to 0.14879, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 368: val_loss improved from 0.14879 to 0.14864, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 369: val_loss improved from 0.14864 to 0.14851, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 370: val_loss improved from 0.14851 to 0.14836, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 371: val_loss improved from 0.14836 to 0.14822, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 372: val_loss improved from 0.14822 to 0.14806, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 373: val_loss improved from 0.14806 to 0.14792, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 374: val_loss improved from 0.14792 to 0.14777, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 375: val_loss improved from 0.14777 to 0.14765, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 376: val_loss improved from 0.14765 to 0.14750, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 377: val_loss improved from 0.14750 to 0.14736, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 378: val_loss improved from 0.14736 to 0.14724, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 379: val_loss improved from 0.14724 to 0.14711, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 380: val_loss improved from 0.14711 to 0.14697, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 381: val_loss improved from 0.14697 to 0.14682, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 382: val_loss improved from 0.14682 to 0.14668, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 383: val_loss improved from 0.14668 to 0.14654, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 384: val_loss improved from 0.14654 to 0.14640, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 385: val_loss improved from 0.14640 to 0.14626, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 386: val_loss improved from 0.14626 to 0.14613, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 387: val_loss improved from 0.14613 to 0.14600, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 388: val_loss improved from 0.14600 to 0.14586, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 389: val_loss improved from 0.14586 to 0.14573, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 390: val_loss improved from 0.14573 to 0.14558, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 391: val_loss improved from 0.14558 to 0.14546, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 392: val_loss improved from 0.14546 to 0.14534, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 393: val_loss improved from 0.14534 to 0.14521, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 394: val_loss improved from 0.14521 to 0.14509, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 395: val_loss improved from 0.14509 to 0.14490, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 396: val_loss improved from 0.14490 to 0.14476, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 397: val_loss improved from 0.14476 to 0.14464, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 398: val_loss improved from 0.14464 to 0.14450, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 399: val_loss improved from 0.14450 to 0.14441, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 400: val_loss improved from 0.14441 to 0.14429, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 401: val_loss improved from 0.14429 to 0.14415, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 402: val_loss improved from 0.14415 to 0.14401, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 403: val_loss improved from 0.14401 to 0.14389, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 404: val_loss improved from 0.14389 to 0.14376, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 405: val_loss improved from 0.14376 to 0.14364, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 406: val_loss improved from 0.14364 to 0.14352, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 407: val_loss improved from 0.14352 to 0.14340, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 408: val_loss improved from 0.14340 to 0.14327, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 409: val_loss improved from 0.14327 to 0.14313, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 410: val_loss improved from 0.14313 to 0.14300, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 411: val_loss improved from 0.14300 to 0.14287, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 412: val_loss improved from 0.14287 to 0.14276, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 413: val_loss improved from 0.14276 to 0.14264, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 414: val_loss improved from 0.14264 to 0.14250, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 415: val_loss improved from 0.14250 to 0.14238, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 416: val_loss improved from 0.14238 to 0.14226, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 417: val_loss improved from 0.14226 to 0.14214, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 418: val_loss improved from 0.14214 to 0.14200, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 419: val_loss improved from 0.14200 to 0.14189, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 420: val_loss improved from 0.14189 to 0.14178, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 421: val_loss improved from 0.14178 to 0.14166, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 422: val_loss improved from 0.14166 to 0.14154, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 423: val_loss improved from 0.14154 to 0.14144, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 424: val_loss improved from 0.14144 to 0.14132, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 425: val_loss improved from 0.14132 to 0.14121, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 426: val_loss improved from 0.14121 to 0.14108, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 427: val_loss improved from 0.14108 to 0.14094, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 428: val_loss improved from 0.14094 to 0.14083, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 429: val_loss improved from 0.14083 to 0.14072, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 430: val_loss improved from 0.14072 to 0.14058, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 431: val_loss improved from 0.14058 to 0.14046, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 432: val_loss improved from 0.14046 to 0.14033, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 433: val_loss improved from 0.14033 to 0.14022, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 434: val_loss improved from 0.14022 to 0.14009, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 435: val_loss improved from 0.14009 to 0.13998, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 436: val_loss improved from 0.13998 to 0.13986, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 437: val_loss improved from 0.13986 to 0.13975, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 438: val_loss improved from 0.13975 to 0.13962, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 439: val_loss improved from 0.13962 to 0.13947, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 440: val_loss improved from 0.13947 to 0.13936, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 441: val_loss improved from 0.13936 to 0.13925, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 442: val_loss improved from 0.13925 to 0.13913, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 443: val_loss improved from 0.13913 to 0.13900, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 444: val_loss improved from 0.13900 to 0.13889, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 445: val_loss improved from 0.13889 to 0.13877, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 446: val_loss improved from 0.13877 to 0.13860, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 447: val_loss improved from 0.13860 to 0.13850, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 448: val_loss improved from 0.13850 to 0.13840, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 449: val_loss improved from 0.13840 to 0.13829, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 450: val_loss improved from 0.13829 to 0.13820, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 451: val_loss improved from 0.13820 to 0.13809, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 452: val_loss improved from 0.13809 to 0.13798, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 453: val_loss improved from 0.13798 to 0.13788, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 454: val_loss improved from 0.13788 to 0.13776, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 455: val_loss improved from 0.13776 to 0.13755, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 456: val_loss improved from 0.13755 to 0.13745, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 457: val_loss improved from 0.13745 to 0.13736, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 458: val_loss improved from 0.13736 to 0.13725, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 459: val_loss improved from 0.13725 to 0.13716, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 460: val_loss improved from 0.13716 to 0.13706, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 461: val_loss improved from 0.13706 to 0.13696, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 462: val_loss improved from 0.13696 to 0.13686, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 463: val_loss improved from 0.13686 to 0.13676, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 464: val_loss improved from 0.13676 to 0.13667, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 465: val_loss improved from 0.13667 to 0.13658, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 466: val_loss improved from 0.13658 to 0.13647, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 467: val_loss improved from 0.13647 to 0.13637, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 468: val_loss improved from 0.13637 to 0.13626, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 469: val_loss improved from 0.13626 to 0.13618, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 470: val_loss improved from 0.13618 to 0.13606, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 471: val_loss improved from 0.13606 to 0.13596, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 472: val_loss improved from 0.13596 to 0.13586, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 473: val_loss improved from 0.13586 to 0.13575, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 474: val_loss improved from 0.13575 to 0.13567, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 475: val_loss improved from 0.13567 to 0.13558, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 476: val_loss improved from 0.13558 to 0.13549, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 477: val_loss improved from 0.13549 to 0.13537, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 478: val_loss improved from 0.13537 to 0.13530, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 479: val_loss improved from 0.13530 to 0.13519, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 480: val_loss improved from 0.13519 to 0.13510, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 481: val_loss improved from 0.13510 to 0.13499, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 482: val_loss improved from 0.13499 to 0.13488, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 483: val_loss improved from 0.13488 to 0.13479, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 484: val_loss improved from 0.13479 to 0.13468, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 485: val_loss improved from 0.13468 to 0.13459, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 486: val_loss improved from 0.13459 to 0.13447, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 487: val_loss improved from 0.13447 to 0.13437, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 488: val_loss improved from 0.13437 to 0.13430, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 489: val_loss improved from 0.13430 to 0.13419, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 490: val_loss improved from 0.13419 to 0.13407, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 491: val_loss improved from 0.13407 to 0.13399, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 492: val_loss improved from 0.13399 to 0.13388, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 493: val_loss improved from 0.13388 to 0.13380, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 494: val_loss improved from 0.13380 to 0.13373, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 495: val_loss improved from 0.13373 to 0.13365, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 496: val_loss improved from 0.13365 to 0.13355, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 497: val_loss improved from 0.13355 to 0.13329, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 498: val_loss improved from 0.13329 to 0.13320, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 499: val_loss improved from 0.13320 to 0.13311, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 500: val_loss improved from 0.13311 to 0.13302, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 501: val_loss improved from 0.13302 to 0.13292, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 502: val_loss improved from 0.13292 to 0.13285, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 503: val_loss improved from 0.13285 to 0.13278, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 504: val_loss improved from 0.13278 to 0.13267, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 505: val_loss improved from 0.13267 to 0.13257, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 506: val_loss improved from 0.13257 to 0.13248, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 507: val_loss improved from 0.13248 to 0.13238, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 508: val_loss improved from 0.13238 to 0.13227, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 509: val_loss improved from 0.13227 to 0.13218, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 510: val_loss improved from 0.13218 to 0.13210, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 511: val_loss improved from 0.13210 to 0.13200, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 512: val_loss improved from 0.13200 to 0.13188, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 513: val_loss improved from 0.13188 to 0.13179, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 514: val_loss improved from 0.13179 to 0.13171, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 515: val_loss improved from 0.13171 to 0.13162, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 516: val_loss improved from 0.13162 to 0.13153, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 517: val_loss improved from 0.13153 to 0.13147, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 518: val_loss improved from 0.13147 to 0.13138, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 519: val_loss improved from 0.13138 to 0.13126, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 520: val_loss improved from 0.13126 to 0.13116, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 521: val_loss improved from 0.13116 to 0.13110, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 522: val_loss improved from 0.13110 to 0.13104, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 523: val_loss improved from 0.13104 to 0.13095, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 524: val_loss improved from 0.13095 to 0.13083, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 525: val_loss improved from 0.13083 to 0.13077, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 526: val_loss improved from 0.13077 to 0.13067, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 527: val_loss improved from 0.13067 to 0.13060, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 528: val_loss improved from 0.13060 to 0.13053, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 529: val_loss improved from 0.13053 to 0.13043, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 530: val_loss improved from 0.13043 to 0.13036, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 531: val_loss improved from 0.13036 to 0.13026, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 532: val_loss improved from 0.13026 to 0.13016, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 533: val_loss improved from 0.13016 to 0.13007, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 534: val_loss improved from 0.13007 to 0.12997, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 535: val_loss improved from 0.12997 to 0.12985, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 536: val_loss improved from 0.12985 to 0.12981, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 537: val_loss improved from 0.12981 to 0.12971, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 538: val_loss improved from 0.12971 to 0.12961, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 539: val_loss improved from 0.12961 to 0.12951, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 540: val_loss improved from 0.12951 to 0.12943, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 541: val_loss improved from 0.12943 to 0.12932, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 542: val_loss improved from 0.12932 to 0.12923, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 543: val_loss improved from 0.12923 to 0.12912, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 544: val_loss improved from 0.12912 to 0.12903, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 545: val_loss improved from 0.12903 to 0.12894, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 546: val_loss improved from 0.12894 to 0.12887, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 547: val_loss improved from 0.12887 to 0.12880, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 548: val_loss improved from 0.12880 to 0.12871, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 549: val_loss improved from 0.12871 to 0.12861, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 550: val_loss improved from 0.12861 to 0.12850, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 551: val_loss improved from 0.12850 to 0.12846, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 552: val_loss improved from 0.12846 to 0.12831, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 553: val_loss improved from 0.12831 to 0.12823, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 554: val_loss improved from 0.12823 to 0.12815, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 555: val_loss improved from 0.12815 to 0.12806, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 556: val_loss improved from 0.12806 to 0.12797, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 557: val_loss improved from 0.12797 to 0.12791, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 558: val_loss improved from 0.12791 to 0.12785, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 559: val_loss improved from 0.12785 to 0.12777, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 560: val_loss improved from 0.12777 to 0.12770, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 561: val_loss improved from 0.12770 to 0.12759, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 562: val_loss improved from 0.12759 to 0.12749, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 563: val_loss improved from 0.12749 to 0.12739, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 564: val_loss improved from 0.12739 to 0.12729, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 565: val_loss improved from 0.12729 to 0.12720, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 566: val_loss improved from 0.12720 to 0.12713, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 567: val_loss improved from 0.12713 to 0.12705, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 568: val_loss improved from 0.12705 to 0.12696, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 569: val_loss improved from 0.12696 to 0.12689, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 570: val_loss improved from 0.12689 to 0.12677, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 571: val_loss improved from 0.12677 to 0.12663, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 572: val_loss improved from 0.12663 to 0.12654, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 573: val_loss improved from 0.12654 to 0.12649, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 574: val_loss improved from 0.12649 to 0.12639, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 575: val_loss improved from 0.12639 to 0.12632, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 576: val_loss improved from 0.12632 to 0.12601, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 577: val_loss improved from 0.12601 to 0.12594, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 578: val_loss improved from 0.12594 to 0.12585, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 579: val_loss improved from 0.12585 to 0.12584, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 580: val_loss improved from 0.12584 to 0.12576, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 581: val_loss improved from 0.12576 to 0.12548, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 582: val_loss improved from 0.12548 to 0.12543, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 583: val_loss improved from 0.12543 to 0.12538, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 584: val_loss improved from 0.12538 to 0.12530, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 585: val_loss improved from 0.12530 to 0.12525, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 586: val_loss improved from 0.12525 to 0.12523, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 587: val_loss improved from 0.12523 to 0.12516, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 588: val_loss improved from 0.12516 to 0.12511, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 589: val_loss improved from 0.12511 to 0.12506, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 590: val_loss improved from 0.12506 to 0.12500, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 591: val_loss improved from 0.12500 to 0.12491, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 592: val_loss improved from 0.12491 to 0.12486, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 593: val_loss improved from 0.12486 to 0.12478, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 594: val_loss improved from 0.12478 to 0.12470, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 595: val_loss improved from 0.12470 to 0.12451, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 596: val_loss improved from 0.12451 to 0.12441, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 597: val_loss improved from 0.12441 to 0.12433, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 598: val_loss improved from 0.12433 to 0.12431, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 599: val_loss improved from 0.12431 to 0.12425, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 600: val_loss improved from 0.12425 to 0.12420, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 601: val_loss improved from 0.12420 to 0.12413, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 602: val_loss improved from 0.12413 to 0.12410, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 603: val_loss improved from 0.12410 to 0.12403, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 604: val_loss improved from 0.12403 to 0.12393, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 605: val_loss improved from 0.12393 to 0.12386, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 606: val_loss improved from 0.12386 to 0.12385, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 607: val_loss improved from 0.12385 to 0.12376, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 608: val_loss improved from 0.12376 to 0.12371, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 609: val_loss improved from 0.12371 to 0.12363, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 610: val_loss improved from 0.12363 to 0.12354, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 611: val_loss improved from 0.12354 to 0.12347, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 612: val_loss improved from 0.12347 to 0.12337, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 613: val_loss improved from 0.12337 to 0.12327, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 614: val_loss improved from 0.12327 to 0.12318, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 615: val_loss improved from 0.12318 to 0.12308, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 616: val_loss improved from 0.12308 to 0.12301, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 617: val_loss improved from 0.12301 to 0.12294, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 618: val_loss improved from 0.12294 to 0.12285, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 619: val_loss improved from 0.12285 to 0.12278, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 620: val_loss improved from 0.12278 to 0.12270, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 621: val_loss improved from 0.12270 to 0.12264, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 622: val_loss improved from 0.12264 to 0.12257, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 623: val_loss improved from 0.12257 to 0.12247, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 624: val_loss improved from 0.12247 to 0.12238, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 625: val_loss improved from 0.12238 to 0.12229, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 626: val_loss improved from 0.12229 to 0.12220, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 627: val_loss improved from 0.12220 to 0.12208, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 628: val_loss improved from 0.12208 to 0.12204, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 629: val_loss improved from 0.12204 to 0.12197, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 630: val_loss improved from 0.12197 to 0.12193, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 631: val_loss improved from 0.12193 to 0.12185, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 632: val_loss improved from 0.12185 to 0.12174, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 633: val_loss improved from 0.12174 to 0.12168, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 634: val_loss improved from 0.12168 to 0.12158, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 635: val_loss improved from 0.12158 to 0.12150, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 636: val_loss improved from 0.12150 to 0.12141, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 637: val_loss improved from 0.12141 to 0.12133, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 638: val_loss improved from 0.12133 to 0.12125, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 639: val_loss improved from 0.12125 to 0.12114, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 640: val_loss improved from 0.12114 to 0.12105, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 641: val_loss improved from 0.12105 to 0.12095, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 642: val_loss improved from 0.12095 to 0.12092, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 643: val_loss improved from 0.12092 to 0.12082, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 644: val_loss improved from 0.12082 to 0.12078, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 645: val_loss improved from 0.12078 to 0.12041, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 646: val_loss improved from 0.12041 to 0.12038, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 647: val_loss improved from 0.12038 to 0.12030, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 648: val_loss improved from 0.12030 to 0.12023, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 649: val_loss improved from 0.12023 to 0.12018, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 650: val_loss improved from 0.12018 to 0.12014, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 651: val_loss improved from 0.12014 to 0.12013, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 652: val_loss improved from 0.12013 to 0.12006, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 653: val_loss improved from 0.12006 to 0.11997, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 654: val_loss improved from 0.11997 to 0.11993, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 655: val_loss improved from 0.11993 to 0.11987, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 656: val_loss improved from 0.11987 to 0.11979, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 657: val_loss improved from 0.11979 to 0.11972, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 658: val_loss improved from 0.11972 to 0.11964, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 659: val_loss improved from 0.11964 to 0.11951, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 660: val_loss improved from 0.11951 to 0.11942, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.11942\n",
      "\n",
      "Epoch 662: val_loss improved from 0.11942 to 0.11938, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 663: val_loss improved from 0.11938 to 0.11928, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 664: val_loss improved from 0.11928 to 0.11917, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.11917\n",
      "\n",
      "Epoch 666: val_loss improved from 0.11917 to 0.11908, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 667: val_loss improved from 0.11908 to 0.11899, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 668: val_loss improved from 0.11899 to 0.11889, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 669: val_loss improved from 0.11889 to 0.11883, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 670: val_loss improved from 0.11883 to 0.11878, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 671: val_loss improved from 0.11878 to 0.11872, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 672: val_loss improved from 0.11872 to 0.11862, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 673: val_loss improved from 0.11862 to 0.11856, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 674: val_loss improved from 0.11856 to 0.11844, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 675: val_loss improved from 0.11844 to 0.11840, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 676: val_loss improved from 0.11840 to 0.11832, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 677: val_loss improved from 0.11832 to 0.11826, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 678: val_loss improved from 0.11826 to 0.11816, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 679: val_loss improved from 0.11816 to 0.11814, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 680: val_loss improved from 0.11814 to 0.11807, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 681: val_loss improved from 0.11807 to 0.11796, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 682: val_loss improved from 0.11796 to 0.11790, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 683: val_loss improved from 0.11790 to 0.11785, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 684: val_loss improved from 0.11785 to 0.11778, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 685: val_loss improved from 0.11778 to 0.11772, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 686: val_loss improved from 0.11772 to 0.11764, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 687: val_loss improved from 0.11764 to 0.11758, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 688: val_loss improved from 0.11758 to 0.11751, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 689: val_loss improved from 0.11751 to 0.11743, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 690: val_loss improved from 0.11743 to 0.11733, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 691: val_loss improved from 0.11733 to 0.11729, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 692: val_loss improved from 0.11729 to 0.11720, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 693: val_loss improved from 0.11720 to 0.11716, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 694: val_loss improved from 0.11716 to 0.11714, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 695: val_loss improved from 0.11714 to 0.11703, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 696: val_loss improved from 0.11703 to 0.11694, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 697: val_loss improved from 0.11694 to 0.11686, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 698: val_loss improved from 0.11686 to 0.11681, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 699: val_loss improved from 0.11681 to 0.11675, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 700: val_loss improved from 0.11675 to 0.11670, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "Loss: 0.1167, Accuracy: 96.43%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.11670\n",
      "Loss: 0.1598, Accuracy: 92.86%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.11670\n",
      "\n",
      "Epoch 214: val_loss improved from 0.11670 to 0.11664, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 215: val_loss improved from 0.11664 to 0.11646, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 216: val_loss improved from 0.11646 to 0.11625, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 217: val_loss improved from 0.11625 to 0.11607, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 218: val_loss improved from 0.11607 to 0.11587, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 219: val_loss improved from 0.11587 to 0.11570, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 220: val_loss improved from 0.11570 to 0.11551, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 221: val_loss improved from 0.11551 to 0.11532, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 222: val_loss improved from 0.11532 to 0.11516, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 223: val_loss improved from 0.11516 to 0.11499, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 224: val_loss improved from 0.11499 to 0.11482, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 225: val_loss improved from 0.11482 to 0.11465, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 226: val_loss improved from 0.11465 to 0.11448, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 227: val_loss improved from 0.11448 to 0.11430, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 228: val_loss improved from 0.11430 to 0.11418, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 229: val_loss improved from 0.11418 to 0.11402, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 230: val_loss improved from 0.11402 to 0.11393, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 231: val_loss improved from 0.11393 to 0.11377, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 232: val_loss improved from 0.11377 to 0.11360, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 233: val_loss improved from 0.11360 to 0.11344, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 234: val_loss improved from 0.11344 to 0.11328, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 235: val_loss improved from 0.11328 to 0.11314, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 236: val_loss improved from 0.11314 to 0.11299, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 237: val_loss improved from 0.11299 to 0.11282, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 238: val_loss improved from 0.11282 to 0.11267, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 239: val_loss improved from 0.11267 to 0.11252, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 240: val_loss improved from 0.11252 to 0.11236, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 241: val_loss improved from 0.11236 to 0.11223, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 242: val_loss improved from 0.11223 to 0.11208, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 243: val_loss improved from 0.11208 to 0.11194, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 244: val_loss improved from 0.11194 to 0.11180, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 245: val_loss improved from 0.11180 to 0.11165, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 246: val_loss improved from 0.11165 to 0.11152, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 247: val_loss improved from 0.11152 to 0.11138, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 248: val_loss improved from 0.11138 to 0.11124, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 249: val_loss improved from 0.11124 to 0.11111, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 250: val_loss improved from 0.11111 to 0.11097, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 251: val_loss improved from 0.11097 to 0.11089, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 252: val_loss improved from 0.11089 to 0.11075, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 253: val_loss improved from 0.11075 to 0.11062, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 254: val_loss improved from 0.11062 to 0.11047, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 255: val_loss improved from 0.11047 to 0.11035, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 256: val_loss improved from 0.11035 to 0.11022, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 257: val_loss improved from 0.11022 to 0.11008, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 258: val_loss improved from 0.11008 to 0.10995, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 259: val_loss improved from 0.10995 to 0.10982, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 260: val_loss improved from 0.10982 to 0.10969, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 261: val_loss improved from 0.10969 to 0.10957, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 262: val_loss improved from 0.10957 to 0.10944, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 263: val_loss improved from 0.10944 to 0.10938, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 264: val_loss improved from 0.10938 to 0.10926, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 265: val_loss improved from 0.10926 to 0.10913, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 266: val_loss improved from 0.10913 to 0.10902, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 267: val_loss improved from 0.10902 to 0.10889, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 268: val_loss improved from 0.10889 to 0.10878, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 269: val_loss improved from 0.10878 to 0.10874, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 270: val_loss improved from 0.10874 to 0.10862, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 271: val_loss improved from 0.10862 to 0.10850, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 272: val_loss improved from 0.10850 to 0.10837, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 273: val_loss improved from 0.10837 to 0.10826, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 274: val_loss improved from 0.10826 to 0.10814, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 275: val_loss improved from 0.10814 to 0.10803, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 276: val_loss improved from 0.10803 to 0.10792, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 277: val_loss improved from 0.10792 to 0.10780, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 278: val_loss improved from 0.10780 to 0.10770, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 279: val_loss improved from 0.10770 to 0.10758, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 280: val_loss improved from 0.10758 to 0.10748, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 281: val_loss improved from 0.10748 to 0.10738, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 282: val_loss improved from 0.10738 to 0.10728, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 283: val_loss improved from 0.10728 to 0.10715, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 284: val_loss improved from 0.10715 to 0.10704, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 285: val_loss improved from 0.10704 to 0.10693, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 286: val_loss improved from 0.10693 to 0.10682, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 287: val_loss improved from 0.10682 to 0.10671, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 288: val_loss improved from 0.10671 to 0.10660, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 289: val_loss improved from 0.10660 to 0.10651, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 290: val_loss improved from 0.10651 to 0.10640, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 291: val_loss improved from 0.10640 to 0.10631, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 292: val_loss improved from 0.10631 to 0.10621, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 293: val_loss improved from 0.10621 to 0.10611, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 294: val_loss improved from 0.10611 to 0.10602, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 295: val_loss improved from 0.10602 to 0.10594, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 296: val_loss improved from 0.10594 to 0.10586, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 297: val_loss improved from 0.10586 to 0.10577, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 298: val_loss improved from 0.10577 to 0.10565, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 299: val_loss improved from 0.10565 to 0.10555, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 300: val_loss improved from 0.10555 to 0.10547, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 301: val_loss improved from 0.10547 to 0.10540, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 302: val_loss improved from 0.10540 to 0.10534, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 303: val_loss improved from 0.10534 to 0.10526, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 304: val_loss improved from 0.10526 to 0.10518, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 305: val_loss improved from 0.10518 to 0.10508, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 306: val_loss improved from 0.10508 to 0.10500, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 307: val_loss improved from 0.10500 to 0.10491, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 308: val_loss improved from 0.10491 to 0.10483, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 309: val_loss improved from 0.10483 to 0.10474, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 310: val_loss improved from 0.10474 to 0.10466, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 311: val_loss improved from 0.10466 to 0.10463, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 312: val_loss improved from 0.10463 to 0.10455, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 313: val_loss improved from 0.10455 to 0.10447, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 314: val_loss improved from 0.10447 to 0.10443, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 315: val_loss improved from 0.10443 to 0.10433, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 316: val_loss improved from 0.10433 to 0.10424, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 317: val_loss improved from 0.10424 to 0.10413, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 318: val_loss improved from 0.10413 to 0.10405, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 319: val_loss improved from 0.10405 to 0.10398, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 320: val_loss improved from 0.10398 to 0.10388, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 321: val_loss improved from 0.10388 to 0.10381, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 322: val_loss improved from 0.10381 to 0.10372, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 323: val_loss improved from 0.10372 to 0.10365, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 324: val_loss improved from 0.10365 to 0.10359, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 325: val_loss improved from 0.10359 to 0.10350, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 326: val_loss improved from 0.10350 to 0.10342, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 327: val_loss improved from 0.10342 to 0.10335, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 328: val_loss improved from 0.10335 to 0.10333, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 329: val_loss improved from 0.10333 to 0.10326, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 330: val_loss improved from 0.10326 to 0.10318, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 331: val_loss improved from 0.10318 to 0.10312, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 332: val_loss improved from 0.10312 to 0.10305, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 333: val_loss improved from 0.10305 to 0.10298, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 334: val_loss improved from 0.10298 to 0.10291, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 335: val_loss improved from 0.10291 to 0.10284, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 336: val_loss improved from 0.10284 to 0.10279, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 337: val_loss improved from 0.10279 to 0.10270, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 338: val_loss improved from 0.10270 to 0.10263, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 339: val_loss improved from 0.10263 to 0.10257, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 340: val_loss improved from 0.10257 to 0.10252, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 341: val_loss improved from 0.10252 to 0.10246, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 342: val_loss improved from 0.10246 to 0.10239, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 343: val_loss improved from 0.10239 to 0.10233, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 344: val_loss improved from 0.10233 to 0.10227, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 345: val_loss improved from 0.10227 to 0.10219, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 346: val_loss improved from 0.10219 to 0.10212, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 347: val_loss improved from 0.10212 to 0.10206, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 348: val_loss improved from 0.10206 to 0.10199, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 349: val_loss improved from 0.10199 to 0.10194, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 350: val_loss improved from 0.10194 to 0.10188, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.10188\n",
      "\n",
      "Epoch 352: val_loss improved from 0.10188 to 0.10185, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 353: val_loss improved from 0.10185 to 0.10179, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 354: val_loss improved from 0.10179 to 0.10173, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 355: val_loss improved from 0.10173 to 0.10162, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 356: val_loss improved from 0.10162 to 0.10155, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 357: val_loss improved from 0.10155 to 0.10147, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 358: val_loss improved from 0.10147 to 0.10143, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 359: val_loss improved from 0.10143 to 0.10136, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 360: val_loss improved from 0.10136 to 0.10130, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 361: val_loss improved from 0.10130 to 0.10124, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 362: val_loss improved from 0.10124 to 0.10119, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 363: val_loss improved from 0.10119 to 0.10115, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 364: val_loss improved from 0.10115 to 0.10111, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 365: val_loss improved from 0.10111 to 0.10106, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 366: val_loss improved from 0.10106 to 0.10099, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 367: val_loss improved from 0.10099 to 0.10095, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 368: val_loss improved from 0.10095 to 0.10090, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 369: val_loss improved from 0.10090 to 0.10087, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 370: val_loss improved from 0.10087 to 0.10081, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 371: val_loss improved from 0.10081 to 0.10077, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 372: val_loss improved from 0.10077 to 0.10072, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 373: val_loss improved from 0.10072 to 0.10068, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 374: val_loss improved from 0.10068 to 0.10064, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 375: val_loss improved from 0.10064 to 0.10061, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 376: val_loss improved from 0.10061 to 0.10057, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 377: val_loss improved from 0.10057 to 0.10050, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 378: val_loss improved from 0.10050 to 0.10036, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 379: val_loss improved from 0.10036 to 0.10032, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 380: val_loss improved from 0.10032 to 0.10028, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 381: val_loss improved from 0.10028 to 0.10025, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 382: val_loss improved from 0.10025 to 0.10022, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 383: val_loss improved from 0.10022 to 0.10019, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 384: val_loss improved from 0.10019 to 0.10014, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 385: val_loss improved from 0.10014 to 0.10010, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 386: val_loss improved from 0.10010 to 0.10006, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 387: val_loss improved from 0.10006 to 0.10002, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 388: val_loss improved from 0.10002 to 0.09998, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 389: val_loss improved from 0.09998 to 0.09995, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 390: val_loss improved from 0.09995 to 0.09988, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 391: val_loss improved from 0.09988 to 0.09985, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 392: val_loss improved from 0.09985 to 0.09980, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 393: val_loss improved from 0.09980 to 0.09978, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 394: val_loss improved from 0.09978 to 0.09976, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 395: val_loss improved from 0.09976 to 0.09972, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 396: val_loss improved from 0.09972 to 0.09968, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.09968\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.09968\n",
      "\n",
      "Epoch 399: val_loss improved from 0.09968 to 0.09967, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 400: val_loss improved from 0.09967 to 0.09961, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 401: val_loss improved from 0.09961 to 0.09959, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 402: val_loss improved from 0.09959 to 0.09955, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 403: val_loss improved from 0.09955 to 0.09953, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 404: val_loss improved from 0.09953 to 0.09951, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.09951\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.09951\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.09951\n",
      "\n",
      "Epoch 408: val_loss improved from 0.09951 to 0.09949, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 409: val_loss improved from 0.09949 to 0.09945, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 410: val_loss improved from 0.09945 to 0.09940, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 411: val_loss improved from 0.09940 to 0.09935, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 412: val_loss improved from 0.09935 to 0.09934, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 413: val_loss improved from 0.09934 to 0.09928, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 414: val_loss improved from 0.09928 to 0.09924, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 415: val_loss improved from 0.09924 to 0.09920, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 416: val_loss improved from 0.09920 to 0.09917, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 417: val_loss improved from 0.09917 to 0.09913, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 418: val_loss improved from 0.09913 to 0.09908, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 419: val_loss improved from 0.09908 to 0.09905, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 420: val_loss improved from 0.09905 to 0.09903, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 421: val_loss improved from 0.09903 to 0.09901, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 422: val_loss improved from 0.09901 to 0.09896, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 423: val_loss improved from 0.09896 to 0.09892, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 424: val_loss improved from 0.09892 to 0.09891, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 425: val_loss improved from 0.09891 to 0.09886, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 426: val_loss improved from 0.09886 to 0.09879, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 427: val_loss improved from 0.09879 to 0.09878, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 428: val_loss improved from 0.09878 to 0.09877, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 429: val_loss improved from 0.09877 to 0.09873, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.09873\n",
      "\n",
      "Epoch 431: val_loss improved from 0.09873 to 0.09870, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 432: val_loss improved from 0.09870 to 0.09869, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 433: val_loss improved from 0.09869 to 0.09866, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 434: val_loss improved from 0.09866 to 0.09865, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 435: val_loss improved from 0.09865 to 0.09861, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.09861\n",
      "\n",
      "Epoch 437: val_loss improved from 0.09861 to 0.09855, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 438: val_loss improved from 0.09855 to 0.09851, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 439: val_loss improved from 0.09851 to 0.09850, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 440: val_loss improved from 0.09850 to 0.09849, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.09849\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.09849\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.09849\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.09849\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.09849\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.09849\n",
      "\n",
      "Epoch 447: val_loss improved from 0.09849 to 0.09849, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 448: val_loss improved from 0.09849 to 0.09846, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 449: val_loss improved from 0.09846 to 0.09839, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 450: val_loss improved from 0.09839 to 0.09835, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 451: val_loss improved from 0.09835 to 0.09832, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 452: val_loss improved from 0.09832 to 0.09830, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 453: val_loss improved from 0.09830 to 0.09824, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 454: val_loss improved from 0.09824 to 0.09819, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 455: val_loss improved from 0.09819 to 0.09815, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 456: val_loss improved from 0.09815 to 0.09814, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 457: val_loss improved from 0.09814 to 0.09809, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 458: val_loss improved from 0.09809 to 0.09807, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 459: val_loss improved from 0.09807 to 0.09806, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 460: val_loss improved from 0.09806 to 0.09806, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 461: val_loss improved from 0.09806 to 0.09800, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 462: val_loss improved from 0.09800 to 0.09797, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.09797\n",
      "\n",
      "Epoch 464: val_loss improved from 0.09797 to 0.09793, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 465: val_loss improved from 0.09793 to 0.09788, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 466: val_loss improved from 0.09788 to 0.09786, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 467: val_loss improved from 0.09786 to 0.09780, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 468: val_loss improved from 0.09780 to 0.09776, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 469: val_loss improved from 0.09776 to 0.09775, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 470: val_loss improved from 0.09775 to 0.09775, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 471: val_loss improved from 0.09775 to 0.09766, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 472: val_loss improved from 0.09766 to 0.09762, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 473: val_loss improved from 0.09762 to 0.09760, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.09760\n",
      "\n",
      "Epoch 475: val_loss improved from 0.09760 to 0.09759, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 476: val_loss improved from 0.09759 to 0.09759, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 477: val_loss improved from 0.09759 to 0.09753, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.09753\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.09753\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.09753\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.09753\n",
      "\n",
      "Epoch 482: val_loss improved from 0.09753 to 0.09748, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 493: val_loss improved from 0.09748 to 0.09748, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.09748\n",
      "\n",
      "Epoch 496: val_loss improved from 0.09748 to 0.09744, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 497: val_loss improved from 0.09744 to 0.09742, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 498: val_loss improved from 0.09742 to 0.09740, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 499: val_loss improved from 0.09740 to 0.09740, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.09740\n",
      "\n",
      "Epoch 501: val_loss improved from 0.09740 to 0.09737, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.09737\n",
      "\n",
      "Epoch 503: val_loss improved from 0.09737 to 0.09735, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.09735\n",
      "\n",
      "Epoch 513: val_loss improved from 0.09735 to 0.09733, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 514: val_loss improved from 0.09733 to 0.09733, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 515: val_loss improved from 0.09733 to 0.09733, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 516: val_loss improved from 0.09733 to 0.09729, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 517: val_loss improved from 0.09729 to 0.09727, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 518: val_loss improved from 0.09727 to 0.09726, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 519: val_loss improved from 0.09726 to 0.09726, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.09726\n",
      "\n",
      "Epoch 562: val_loss improved from 0.09726 to 0.09722, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.09722\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.09722\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.09722\n",
      "\n",
      "Epoch 566: val_loss improved from 0.09722 to 0.09721, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.09721\n",
      "Loss: 0.0991, Accuracy: 95.54%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 650: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 651: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 652: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 653: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 654: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 655: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 656: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 657: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 658: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 659: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 660: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 661: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 662: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 663: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 664: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 665: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 666: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 667: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 668: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 669: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 670: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 671: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 672: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 673: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 674: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 675: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 676: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 677: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 678: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 679: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 680: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 681: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 682: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 683: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 684: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 685: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 686: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 687: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 688: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 689: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 690: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 691: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 692: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 693: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 694: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 695: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 696: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 697: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 698: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 699: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 700: val_loss did not improve from 0.09721\n",
      "Loss: 0.1069, Accuracy: 97.32%\n",
      "\n",
      "Epoch 1: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 101: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 102: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 103: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 104: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 105: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 106: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 107: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 108: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 109: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 110: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 111: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 112: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 113: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 114: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 115: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 116: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 117: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 118: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 119: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 120: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 121: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 122: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 123: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 124: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 125: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 126: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 127: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 128: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 129: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 130: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 131: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 132: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 133: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 134: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 135: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 136: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 137: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 138: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 139: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 140: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 141: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 142: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 143: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 144: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 145: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 146: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 147: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 148: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 149: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 150: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 151: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 152: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 153: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 154: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 155: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 156: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 157: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 158: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 159: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 160: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 161: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 162: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 163: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 164: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 165: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 166: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 167: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 168: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 169: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 170: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 171: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 172: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 173: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 174: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 175: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 176: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 177: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 178: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 179: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 180: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 181: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 182: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 183: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 184: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 185: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 186: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 187: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 188: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 189: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 190: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 191: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 192: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 193: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 194: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 195: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 196: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 197: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 198: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 199: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 200: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 201: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 202: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 203: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 204: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 205: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 206: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 207: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 208: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 209: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 210: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 211: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 212: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 213: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 214: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 215: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 216: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 217: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 218: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 219: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 220: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 221: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 222: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 223: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 224: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 225: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 226: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 227: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 228: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 229: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 230: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 231: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 232: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 233: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 234: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 235: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 236: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 237: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 238: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 239: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 240: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 241: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 242: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 243: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 244: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 245: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 246: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 247: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 248: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 249: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 250: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 251: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 252: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 253: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 254: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 255: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 256: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 257: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 258: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 259: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 260: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 261: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 262: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 263: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 264: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 265: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 266: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 267: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 268: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 269: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 270: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 271: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 272: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 273: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 274: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 275: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 276: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 277: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 278: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 279: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 280: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 281: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 282: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 283: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 284: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 285: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 286: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 287: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 288: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 289: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 290: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 291: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 292: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 293: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 294: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 295: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 296: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 297: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 298: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 299: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 300: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 301: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 302: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 303: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 304: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 305: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 306: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 307: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 308: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 309: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 310: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 311: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 312: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 313: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 314: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 315: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 316: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 317: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 318: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 319: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 320: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 321: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 322: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 323: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 324: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 325: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 326: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 327: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 328: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 329: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 330: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 331: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 332: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 333: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 334: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 335: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 336: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 337: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 338: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 339: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 340: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 341: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 342: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 343: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 344: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 345: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 346: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 347: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 348: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 349: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 350: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 351: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 352: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 353: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 354: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 355: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 356: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 357: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 358: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 359: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 360: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 361: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 362: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 363: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 364: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 365: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 366: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 367: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 368: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 369: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 370: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 371: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 372: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 373: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 374: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 375: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 376: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 377: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 378: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 379: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 380: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 381: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 382: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 383: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 384: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 385: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 386: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 387: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 388: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 389: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 390: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 391: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 392: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 393: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 394: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 395: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 396: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 397: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 398: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 399: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 400: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 401: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 402: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 403: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 404: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 405: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 406: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 407: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 408: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 409: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 410: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 411: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 412: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 413: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 414: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 415: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 416: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 417: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 418: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 419: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 420: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 421: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 422: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 423: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 424: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 425: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 426: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 427: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 428: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 429: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 430: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 431: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 432: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 433: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 434: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 435: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 436: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 437: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 438: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 439: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 440: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 441: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 442: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 443: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 444: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 445: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 446: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 447: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 448: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 449: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 450: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 451: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 452: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 453: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 454: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 455: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 456: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 457: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 458: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 459: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 460: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 461: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 462: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 463: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 464: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 465: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 466: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 467: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 468: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 469: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 470: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 471: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 472: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 473: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 474: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 475: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 476: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 477: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 478: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 479: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 480: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 481: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 482: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 483: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 484: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 485: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 486: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 487: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 488: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 489: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 490: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 491: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 492: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 493: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 494: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 495: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 496: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 497: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 498: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 499: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 500: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 501: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 502: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 503: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 504: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 505: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 506: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 507: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 508: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 509: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 510: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 511: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 512: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 513: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 514: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 515: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 516: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 517: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 518: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 519: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 520: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 521: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 522: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 523: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 524: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 525: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 526: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 527: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 528: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 529: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 530: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 531: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 532: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 533: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 534: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 535: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 536: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 537: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 538: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 539: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 540: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 541: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 542: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 543: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 544: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 545: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 546: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 547: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 548: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 549: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 550: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 551: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 552: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 553: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 554: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 555: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 556: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 557: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 558: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 559: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 560: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 561: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 562: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 563: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 564: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 565: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 566: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 567: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 568: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 569: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 570: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 571: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 572: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 573: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 574: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 575: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 576: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 577: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 578: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 579: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 580: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 581: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 582: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 583: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 584: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 585: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 586: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 587: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 588: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 589: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 590: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 591: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 592: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 593: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 594: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 595: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 596: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 597: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 598: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 599: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 600: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 601: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 602: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 603: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 604: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 605: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 606: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 607: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 608: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 609: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 610: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 611: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 612: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 613: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 614: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 615: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 616: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 617: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 618: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 619: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 620: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 621: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 622: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 623: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 624: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 625: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 626: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 627: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 628: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 629: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 630: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 631: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 632: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 633: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 634: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 635: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 636: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 637: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 638: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 639: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 640: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 641: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 642: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 643: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 644: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 645: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 646: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 647: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 648: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 649: val_loss did not improve from 0.09721\n",
      "\n",
      "Epoch 650: val_loss improved from 0.09721 to 0.09715, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 651: val_loss improved from 0.09715 to 0.09707, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 652: val_loss improved from 0.09707 to 0.09699, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 653: val_loss improved from 0.09699 to 0.09691, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 654: val_loss improved from 0.09691 to 0.09680, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 655: val_loss improved from 0.09680 to 0.09671, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 656: val_loss improved from 0.09671 to 0.09661, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 657: val_loss improved from 0.09661 to 0.09653, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 658: val_loss improved from 0.09653 to 0.09643, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 659: val_loss improved from 0.09643 to 0.09631, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 660: val_loss improved from 0.09631 to 0.09623, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 661: val_loss improved from 0.09623 to 0.09615, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 662: val_loss improved from 0.09615 to 0.09603, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 663: val_loss improved from 0.09603 to 0.09583, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 664: val_loss improved from 0.09583 to 0.09562, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 665: val_loss improved from 0.09562 to 0.09557, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 666: val_loss improved from 0.09557 to 0.09552, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 667: val_loss improved from 0.09552 to 0.09543, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 668: val_loss improved from 0.09543 to 0.09534, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 669: val_loss improved from 0.09534 to 0.09523, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 670: val_loss improved from 0.09523 to 0.09515, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 671: val_loss improved from 0.09515 to 0.09507, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 672: val_loss improved from 0.09507 to 0.09499, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 673: val_loss improved from 0.09499 to 0.09488, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 674: val_loss improved from 0.09488 to 0.09480, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 675: val_loss improved from 0.09480 to 0.09471, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 676: val_loss improved from 0.09471 to 0.09462, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 677: val_loss improved from 0.09462 to 0.09454, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 678: val_loss improved from 0.09454 to 0.09446, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 679: val_loss improved from 0.09446 to 0.09438, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 680: val_loss improved from 0.09438 to 0.09426, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 681: val_loss improved from 0.09426 to 0.09419, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 682: val_loss improved from 0.09419 to 0.09409, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 683: val_loss improved from 0.09409 to 0.09401, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 684: val_loss improved from 0.09401 to 0.09391, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 685: val_loss improved from 0.09391 to 0.09384, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 686: val_loss improved from 0.09384 to 0.09374, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 687: val_loss improved from 0.09374 to 0.09366, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 688: val_loss improved from 0.09366 to 0.09360, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 689: val_loss improved from 0.09360 to 0.09351, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 690: val_loss improved from 0.09351 to 0.09343, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 691: val_loss improved from 0.09343 to 0.09333, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 692: val_loss improved from 0.09333 to 0.09325, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 693: val_loss improved from 0.09325 to 0.09303, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 694: val_loss improved from 0.09303 to 0.09293, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 695: val_loss improved from 0.09293 to 0.09285, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 696: val_loss improved from 0.09285 to 0.09276, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 697: val_loss improved from 0.09276 to 0.09268, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 698: val_loss improved from 0.09268 to 0.09256, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 699: val_loss improved from 0.09256 to 0.09245, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "\n",
      "Epoch 700: val_loss improved from 0.09245 to 0.09237, saving model to model_checkpoint\\GRU_2 layers k fold.h5\n",
      "Loss: 0.0924, Accuracy: 95.54%\n",
      "GRU finished in 1718.02 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_35 (GRU)                (None, None, 32)          11520     \n",
      "                                                                 \n",
      " gru_36 (GRU)                (None, 32)                6336      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,889\n",
      "Trainable params: 17,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Average accuracy: 0.9554\n",
      "Average loss: 0.1150\n"
     ]
    }
   ],
   "source": [
    "# This code implements a K-Fold Cross Validation technique combined with a deep Gated Recurrent Unit (GRU) architecture. \n",
    "\n",
    "dir_name = 'model_checkpoint'\n",
    "if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "save_path = os.path.join(dir_name, 'GRU_2 layers k fold.h5')\n",
    "\n",
    "callbacks_list = tf.keras.callbacks.ModelCheckpoint(filepath=save_path, monitor=\"val_loss\", verbose=1, save_best_only=True)\n",
    "\n",
    "k_fold = 5 # number of folds for the K-fold cross validation\n",
    "x_train, x_test, y_train, y_test, kf = trainTestData_1 (ft, test_ratio, k_fold)\n",
    "\n",
    "# Arrays to store the learning curves at each k-th iteration\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "\n",
    "print('Implementing GRU with K-fold')\n",
    "start = time.time()\n",
    "for train, test in kf.split(ft):\n",
    "    x_train = ft.iloc[train,:ft.shape[1]-1]\n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], 1, x_train.shape[1]))\n",
    "    y_train = ft.loc[train,'seizure'].values.astype(int)\n",
    "    x_test = ft.iloc[test,:ft.shape[1]-1]\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "    y_test = ft.loc[test,'seizure'].values.astype(int)\n",
    "\n",
    "    # Definition of the model\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(32, input_shape=(None, x_train.shape[-1]),return_sequences=True))\n",
    "    model.add(layers.GRU(32))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with a SGD optimizer with an exponential decaying learning rate\n",
    "    optimizer, lr_schedule = optimizer_SGD(0.001, 1000, 0.1)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Training of the model\n",
    "    history = model.fit(x_train, y_train, batch_size = 5, epochs = 700, verbose = 0, validation_data=(x_test,y_test), callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_schedule), callbacks_list])\n",
    "\n",
    "    # Store the metrics values for each epoch and for each fold\n",
    "    train_loss.append(history.history['loss'])\n",
    "    train_acc.append(history.history['accuracy'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    val_acc.append(history.history['val_accuracy'])\n",
    "\n",
    "    # Evaluation of the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_acc.append(accuracy)\n",
    "    test_loss.append(loss)\n",
    "\n",
    "    # Print of the loss and accuracy scores at the end of each fold\n",
    "    print(\"Loss: {:.4f}, Accuracy: {:.2f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "end = time.time()\n",
    "t = round(end - start,2)\n",
    "print('GRU finished in', t,'sec\\n')\n",
    "\n",
    "# Plot of the average learning curves\n",
    "plot_1(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "# Calculate average performance\n",
    "avg_accuracy = np.mean(test_acc)\n",
    "avg_loss = np.mean(test_loss)\n",
    "print(f'Average accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average loss: {avg_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
